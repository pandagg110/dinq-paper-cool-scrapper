<html><head>
    <title>ICLR.2025 - Spotlight | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for ICLR.2025 - Spotlight, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_Typewriter; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf?V=2.7.9') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_SansSerif; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf?V=2.7.9') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.9') format('opentype')}
</style></head>
<body id="venue"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">ICLR.2025 - Spotlight</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 373
    </p>
    <div class="papers">
        <div id="TeVAZXr3yv@OpenReview" class="panel paper" keywords="mmau,audio,reasoning,sounds,tasks,speech,advanced,understanding,music,multimodal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TeVAZXr3yv" target="_blank" title="1/373"><span class="index notranslate">#1</span></a>
                <a id="title-TeVAZXr3yv@OpenReview" class="title-link" href="/venue/TeVAZXr3yv@OpenReview" target="_blank">MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</a>
                <a id="pdf-TeVAZXr3yv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TeVAZXr3yv@OpenReview', this)" data="https://openreview.net/pdf?id=TeVAZXr3yv">[PDF<sup id="pdf-stars-TeVAZXr3yv@OpenReview">67</sup>]</a>
                <a id="copy-TeVAZXr3yv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TeVAZXr3yv@OpenReview')">[Copy]</a>
                <a id="kimi-TeVAZXr3yv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TeVAZXr3yv@OpenReview', this)">[Kimi<sup id="kimi-stars-TeVAZXr3yv@OpenReview">54</sup>]</a>
                <a id="rel-TeVAZXr3yv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TeVAZXr3yv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TeVAZXr3yv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sakshi" target="_blank">Sakshi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Utkarsh Tyagi" target="_blank">Utkarsh Tyagi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sonal Kumar" target="_blank">Sonal Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ashish Seth" target="_blank">Ashish Seth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ramaneswaran S" target="_blank">Ramaneswaran S</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oriol Nieto" target="_blank">Oriol Nieto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ramani Duraiswami" target="_blank">Ramani Duraiswami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sreyan Ghosh" target="_blank">Sreyan Ghosh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dinesh Manocha" target="_blank">Dinesh Manocha</a>
            </p>
            <p id="summary-TeVAZXr3yv@OpenReview" class="summary">The ability to comprehend audio—which includes speech, non-speech sounds, and music—is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challengesposed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.</p>
            <p id="subjects-TeVAZXr3yv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TeVAZXr3yv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TeVAZXr3yv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TeVAZXr3yv@OpenReview" onclick="foldPdfKimi('TeVAZXr3yv@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Im2neAMlre@OpenReview" class="panel paper" keywords="t2i,evaluation,metrics,human,annotations,suite,slice,conclusions,tifa160,templates">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Im2neAMlre" target="_blank" title="2/373"><span class="index notranslate">#2</span></a>
                <a id="title-Im2neAMlre@OpenReview" class="title-link" href="/venue/Im2neAMlre@OpenReview" target="_blank">One slice is not enough: In search of stable conclusions in text-to-image evaluation</a>
                <a id="pdf-Im2neAMlre@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Im2neAMlre@OpenReview', this)" data="https://openreview.net/pdf?id=Im2neAMlre">[PDF<sup id="pdf-stars-Im2neAMlre@OpenReview">33</sup>]</a>
                <a id="copy-Im2neAMlre@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Im2neAMlre@OpenReview')">[Copy]</a>
                <a id="kimi-Im2neAMlre@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Im2neAMlre@OpenReview', this)">[Kimi<sup id="kimi-stars-Im2neAMlre@OpenReview">23</sup>]</a>
                <a id="rel-Im2neAMlre@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Im2neAMlre@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Im2neAMlre@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Olivia Wiles" target="_blank">Olivia Wiles</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuhan Zhang" target="_blank">Chuhan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Isabela Albuquerque" target="_blank">Isabela Albuquerque</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivana Kajić" target="_blank">Ivana Kajić</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Su Wang" target="_blank">Su Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emanuele Bugliarello" target="_blank">Emanuele Bugliarello</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yasumasa Onoe" target="_blank">Yasumasa Onoe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pinelopi Papalampidi" target="_blank">Pinelopi Papalampidi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ira Ktena" target="_blank">Ira Ktena</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Knutsen" target="_blank">Christopher Knutsen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cyrus Rashtchian" target="_blank">Cyrus Rashtchian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anant Nawalgaria" target="_blank">Anant Nawalgaria</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jordi Pont-Tuset" target="_blank">Jordi Pont-Tuset</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aida Nematzadeh" target="_blank">Aida Nematzadeh</a>
            </p>
            <p id="summary-Im2neAMlre@OpenReview" class="summary">While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While many metrics and benchmarks have been proposed to evaluate T2I models and alignment metrics, the impact of the evaluation components (prompt sets, human annotations, evaluation task) has not been systematically measured.We find that looking at only *one slice of data*, i.e. one set of capabilities or human annotations, is not enough to obtain stable conclusions that generalise to new conditions or slices when evaluating T2I models or alignment metrics. We address this by introducing an evaluation suite of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1000.68em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mo" id="MathJax-Span-3" style="font-family: MathJax_Main;">&gt;</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">></script>100K annotations across four human annotation templates that comprehensively evaluates models' capabilities across a range of methods for gathering human annotations and comparing models.In particular, we propose (1) a carefully curated set of prompts -- *Gecko2K*; (2) a statistically grounded method of comparing T2I models; and (3) how to systematically evaluate metrics under three *evaluation tasks* -- *model ordering, pair-wise instance scoring, point-wise instance scoring*.Using this evaluation suite, we evaluate a wide range of metrics and find that a metric may do better in one setting but worse in another.As a result, we introduce a new, interpretable auto-eval metric that is consistently better correlated with human ratings than such existing metrics on our evaluation suite--across different human templates and evaluation settings--and on TIFA160.</p>
            <p id="subjects-Im2neAMlre@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Im2neAMlre@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Im2neAMlre@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Im2neAMlre@OpenReview" onclick="foldPdfKimi('Im2neAMlre@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="FPfCUJTsCn@OpenReview" class="panel paper" keywords="ilps,diffilo,training,differentiable,generating,solutions,erentiable,nteger,inear,integer">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FPfCUJTsCn" target="_blank" title="3/373"><span class="index notranslate">#3</span></a>
                <a id="title-FPfCUJTsCn@OpenReview" class="title-link" href="/venue/FPfCUJTsCn@OpenReview" target="_blank">Differentiable Integer Linear Programming</a>
                <a id="pdf-FPfCUJTsCn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FPfCUJTsCn@OpenReview', this)" data="https://openreview.net/pdf?id=FPfCUJTsCn">[PDF<sup id="pdf-stars-FPfCUJTsCn@OpenReview">33</sup>]</a>
                <a id="copy-FPfCUJTsCn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FPfCUJTsCn@OpenReview')">[Copy]</a>
                <a id="kimi-FPfCUJTsCn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FPfCUJTsCn@OpenReview', this)">[Kimi<sup id="kimi-stars-FPfCUJTsCn@OpenReview">22</sup>]</a>
                <a id="rel-FPfCUJTsCn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FPfCUJTsCn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FPfCUJTsCn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zijie Geng" target="_blank">Zijie Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Wang" target="_blank">Jie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xijun Li" target="_blank">Xijun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangzhou Zhu" target="_blank">Fangzhou Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianye HAO" target="_blank">Jianye HAO</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Li" target="_blank">Bin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Wu" target="_blank">Feng Wu</a>
            </p>
            <p id="summary-FPfCUJTsCn@OpenReview" class="summary">Machine learning (ML) techniques have shown great potential in generating high-quality solutions for integer linear programs (ILPs).However, existing methods typically rely on a *supervised learning* paradigm, leading to (1) *expensive training cost* due to repeated invocations of traditional solvers to generate training labels, and (2) *plausible yet infeasible solutions* due to the misalignment between the training objective (minimizing prediction loss) and the inference objective (generating high-quality solutions).To tackle this challenge, we propose **DiffILO** (**Diff**erentiable **I**nteger **L**inear Programming **O**ptimization), an *unsupervised learning paradigm for learning to solve ILPs*.Specifically, through a novel probabilistic modeling, DiffILO reformulates ILPs---discrete and constrained optimization problems---into continuous, differentiable (almost everywhere), and unconstrained optimization problems.This reformulation enables DiffILO to simultaneously solve ILPs and train the model via straightforward gradient descent, providing two major advantages.First, it significantly reduces the training cost, as the training process does not need the aid of traditional solvers at all.Second, it facilitates the generation of feasible and high-quality solutions, as the model *learns to solve ILPs* in an end-to-end manner, thus aligning the training and inference objectives.Experiments on commonly used ILP datasets demonstrate that DiffILO not only achieves an average training speedup of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;13.2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.72em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-5"><span class="mn" id="MathJax-Span-6" style="font-family: MathJax_Main;">13.2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>13.2</mn></math></span></span><script type="math/tex" id="MathJax-Element-2">13.2</script> times compared to supervised methods, but also outperforms them by generating heuristic solutions with significantly higher feasibility ratios and much better solution qualities.</p>
            <p id="subjects-FPfCUJTsCn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FPfCUJTsCn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FPfCUJTsCn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FPfCUJTsCn@OpenReview" onclick="foldPdfKimi('FPfCUJTsCn@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="FDnZFpHmU4@OpenReview" class="panel paper" keywords="ensembling,textsc,vocabulary,unite,union,nsembling,ensemble,top,across,llm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FDnZFpHmU4" target="_blank" title="4/373"><span class="index notranslate">#4</span></a>
                <a id="title-FDnZFpHmU4@OpenReview" class="title-link" href="/venue/FDnZFpHmU4@OpenReview" target="_blank">Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling</a>
                <a id="pdf-FDnZFpHmU4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FDnZFpHmU4@OpenReview', this)" data="https://openreview.net/pdf?id=FDnZFpHmU4">[PDF<sup id="pdf-stars-FDnZFpHmU4@OpenReview">26</sup>]</a>
                <a id="copy-FDnZFpHmU4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FDnZFpHmU4@OpenReview')">[Copy]</a>
                <a id="kimi-FDnZFpHmU4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FDnZFpHmU4@OpenReview', this)">[Kimi<sup id="kimi-stars-FDnZFpHmU4@OpenReview">32</sup>]</a>
                <a id="rel-FDnZFpHmU4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FDnZFpHmU4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FDnZFpHmU4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan YAO" target="_blank">Yuxuan YAO</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Wu" target="_blank">Han Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyang LIU" target="_blank">Mingyang LIU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sichun Luo" target="_blank">Sichun Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiongwei Han" target="_blank">Xiongwei Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Liu" target="_blank">Jie Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijiang Guo" target="_blank">Zhijiang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linqi Song" target="_blank">Linqi Song</a>
            </p>
            <p id="summary-FDnZFpHmU4@OpenReview" class="summary">Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \textsc{Uni}on \textsc{T}op-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-8"><span class="mi" id="MathJax-Span-9" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">k</script> \textsc{E}nsembling (\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling.</p>
            <p id="subjects-FDnZFpHmU4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FDnZFpHmU4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FDnZFpHmU4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FDnZFpHmU4@OpenReview" onclick="foldPdfKimi('FDnZFpHmU4@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="3b9SKkRAKw@OpenReview" class="panel paper" keywords="lesion,lefusion,diffusion,synthesis,control,lesions,swinunetr,focused,pathology,backgrounds">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3b9SKkRAKw" target="_blank" title="5/373"><span class="index notranslate">#5</span></a>
                <a id="title-3b9SKkRAKw@OpenReview" class="title-link" href="/venue/3b9SKkRAKw@OpenReview" target="_blank">LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models</a>
                <a id="pdf-3b9SKkRAKw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3b9SKkRAKw@OpenReview', this)" data="https://openreview.net/pdf?id=3b9SKkRAKw">[PDF<sup id="pdf-stars-3b9SKkRAKw@OpenReview">34</sup>]</a>
                <a id="copy-3b9SKkRAKw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3b9SKkRAKw@OpenReview')">[Copy]</a>
                <a id="kimi-3b9SKkRAKw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3b9SKkRAKw@OpenReview', this)">[Kimi<sup id="kimi-stars-3b9SKkRAKw@OpenReview">23</sup>]</a>
                <a id="rel-3b9SKkRAKw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3b9SKkRAKw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3b9SKkRAKw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hantao Zhang" target="_blank">Hantao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhe Liu" target="_blank">Yuhe Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiancheng Yang" target="_blank">Jiancheng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shouhong Wan" target="_blank">Shouhong Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyuan Wang" target="_blank">Xinyuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Peng" target="_blank">Wei Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Fua" target="_blank">Pascal Fua</a>
            </p>
            <p id="summary-3b9SKkRAKw@OpenReview" class="summary">Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR.</p>
            <p id="subjects-3b9SKkRAKw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3b9SKkRAKw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3b9SKkRAKw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3b9SKkRAKw@OpenReview" onclick="foldPdfKimi('3b9SKkRAKw@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="OwpLQrpdwE@OpenReview" class="panel paper" keywords="ode,solver,manifolds,geometrically,kernel,vector,fields,odes,constrained,valued">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OwpLQrpdwE" target="_blank" title="6/373"><span class="index notranslate">#6</span></a>
                <a id="title-OwpLQrpdwE@OpenReview" class="title-link" href="/venue/OwpLQrpdwE@OpenReview" target="_blank">Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels</a>
                <a id="pdf-OwpLQrpdwE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OwpLQrpdwE@OpenReview', this)" data="https://openreview.net/pdf?id=OwpLQrpdwE">[PDF<sup id="pdf-stars-OwpLQrpdwE@OpenReview">20</sup>]</a>
                <a id="copy-OwpLQrpdwE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OwpLQrpdwE@OpenReview')">[Copy]</a>
                <a id="kimi-OwpLQrpdwE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OwpLQrpdwE@OpenReview', this)">[Kimi<sup id="kimi-stars-OwpLQrpdwE@OpenReview">15</sup>]</a>
                <a id="rel-OwpLQrpdwE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OwpLQrpdwE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OwpLQrpdwE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daning Huang" target="_blank">Daning Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanyang He" target="_blank">Hanyang He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Harlim" target="_blank">John Harlim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Li" target="_blank">Yan Li</a>
            </p>
            <p id="summary-OwpLQrpdwE@OpenReview" class="summary">We address the problem of learning ordinary differential equations (ODEs) on manifolds. Existing machine learning methods, particularly those using neural networks, often struggle with high computational demands. To overcome this issue, we introduce a geometrically constrained operator-valued kernel that allows us to represent vector fields on tangent bundles of smooth manifolds. The construction of the kernel imposes the geometric constraints that are estimated from the data and ensures the computational feasibility for learning high dimensional systems of ODEs. Once the vector fields are estimated, e.g., by the kernel ridge regression, we need an ODE solver that guarantees the solution to stay on (or close to) the manifold. To overcome this issue, we propose a geometry-preserving ODE solver that approximates the exponential maps corresponding to the ODE solutions. We deduce a theoretical error bound for the proposed solver that guarantees the approximate solutions to lie on the manifold in the limit of large data. We verify the effectiveness of the proposed approach on high-dimensional dynamical systems, including the cavity flow problem, the beating and travelling waves in Kuramoto-Sivashinsky equations, and the reaction-diffusion dynamics.</p>
            <p id="subjects-OwpLQrpdwE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-OwpLQrpdwE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OwpLQrpdwE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OwpLQrpdwE@OpenReview" onclick="foldPdfKimi('OwpLQrpdwE@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="oQ4igHyh3N@OpenReview" class="panel paper" keywords="tokenformer,tokens,scratch,parameters,transformers,tokenized,model,scaling,architectural,retraining">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oQ4igHyh3N" target="_blank" title="7/373"><span class="index notranslate">#7</span></a>
                <a id="title-oQ4igHyh3N@OpenReview" class="title-link" href="/venue/oQ4igHyh3N@OpenReview" target="_blank">TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters</a>
                <a id="pdf-oQ4igHyh3N@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oQ4igHyh3N@OpenReview', this)" data="https://openreview.net/pdf?id=oQ4igHyh3N">[PDF<sup id="pdf-stars-oQ4igHyh3N@OpenReview">44</sup>]</a>
                <a id="copy-oQ4igHyh3N@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oQ4igHyh3N@OpenReview')">[Copy]</a>
                <a id="kimi-oQ4igHyh3N@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oQ4igHyh3N@OpenReview', this)">[Kimi<sup id="kimi-stars-oQ4igHyh3N@OpenReview">31</sup>]</a>
                <a id="rel-oQ4igHyh3N@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oQ4igHyh3N@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oQ4igHyh3N@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyang Wang" target="_blank">Haiyang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Fan" target="_blank">Yue Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Ferjad Naeem" target="_blank">Muhammad Ferjad Naeem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Wang" target="_blank">Liwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongqin Xian" target="_blank">Yongqin Xian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan E Lenssen" target="_blank">Jan E Lenssen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Tombari" target="_blank">Federico Tombari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernt Schiele" target="_blank">Bernt Schiele</a>
            </p>
            <p id="summary-oQ4igHyh3N@OpenReview" class="summary">Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code will be available.</p>
            <p id="subjects-oQ4igHyh3N@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-oQ4igHyh3N@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oQ4igHyh3N@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oQ4igHyh3N@OpenReview" onclick="foldPdfKimi('oQ4igHyh3N@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="xaYlO03tIk@OpenReview" class="panel paper" keywords="stem,inversion,visual,imitation,generalizable,convergent,observation,diffusion,settings,appearance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xaYlO03tIk" target="_blank" title="8/373"><span class="index notranslate">#8</span></a>
                <a id="title-xaYlO03tIk@OpenReview" class="title-link" href="/venue/xaYlO03tIk@OpenReview" target="_blank">Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion</a>
                <a id="pdf-xaYlO03tIk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xaYlO03tIk@OpenReview', this)" data="https://openreview.net/pdf?id=xaYlO03tIk">[PDF<sup id="pdf-stars-xaYlO03tIk@OpenReview">11</sup>]</a>
                <a id="copy-xaYlO03tIk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xaYlO03tIk@OpenReview')">[Copy]</a>
                <a id="kimi-xaYlO03tIk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xaYlO03tIk@OpenReview', this)">[Kimi<sup id="kimi-stars-xaYlO03tIk@OpenReview">9</sup>]</a>
                <a id="rel-xaYlO03tIk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xaYlO03tIk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xaYlO03tIk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaizhe Hu" target="_blank">Kaizhe Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihang Rui" target="_blank">Zihang Rui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao He" target="_blank">Yao He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuyao Liu" target="_blank">Yuyao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pu Hua" target="_blank">Pu Hua</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huazhe Xu" target="_blank">Huazhe Xu</a>
            </p>
            <p id="summary-xaYlO03tIk@OpenReview" class="summary">Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose ***Stem-OB*** that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. *Stem-OB* offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. *Stem-OB* shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of **22.2%** in success rates compared to the best baseline. Please refer to [this link](https://stem-ob.github.io/) for more videos and details.</p>
            <p id="subjects-xaYlO03tIk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xaYlO03tIk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xaYlO03tIk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xaYlO03tIk@OpenReview" onclick="foldPdfKimi('xaYlO03tIk@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="nYpPAT4L3D@OpenReview" class="panel paper" keywords="fvlm,anatomy,medical,interpretation,image,language,diagnosis,reports,contrastive,grained">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=nYpPAT4L3D" target="_blank" title="9/373"><span class="index notranslate">#9</span></a>
                <a id="title-nYpPAT4L3D@OpenReview" class="title-link" href="/venue/nYpPAT4L3D@OpenReview" target="_blank">Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding</a>
                <a id="pdf-nYpPAT4L3D@OpenReview" class="title-pdf notranslate" onclick="togglePdf('nYpPAT4L3D@OpenReview', this)" data="https://openreview.net/pdf?id=nYpPAT4L3D">[PDF<sup id="pdf-stars-nYpPAT4L3D@OpenReview">28</sup>]</a>
                <a id="copy-nYpPAT4L3D@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('nYpPAT4L3D@OpenReview')">[Copy]</a>
                <a id="kimi-nYpPAT4L3D@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('nYpPAT4L3D@OpenReview', this)">[Kimi<sup id="kimi-stars-nYpPAT4L3D@OpenReview">23</sup>]</a>
                <a id="rel-nYpPAT4L3D@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('nYpPAT4L3D@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-nYpPAT4L3D@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongyi Shui" target="_blank">Zhongyi Shui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianpeng Zhang" target="_blank">Jianpeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiwei Cao" target="_blank">Weiwei Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sinuo Wang" target="_blank">Sinuo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruizhe Guo" target="_blank">Ruizhe Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Lu" target="_blank">Le Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling Zhang" target="_blank">Ling Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tingbo Liang" target="_blank">Tingbo Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Yang" target="_blank">Lin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianghua Ye" target="_blank">Xianghua Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Zhang" target="_blank">Qi Zhang</a>
            </p>
            <p id="summary-nYpPAT4L3D@OpenReview" class="summary">Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively.</p>
            <p id="subjects-nYpPAT4L3D@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-nYpPAT4L3D@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-nYpPAT4L3D@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-nYpPAT4L3D@OpenReview" onclick="foldPdfKimi('nYpPAT4L3D@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="UvfI4grcM7@OpenReview" class="panel paper" keywords="barrel,whisker,cortex,biological,neural,neuronal,biologically,constrained,model,brain">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UvfI4grcM7" target="_blank" title="10/373"><span class="index notranslate">#10</span></a>
                <a id="title-UvfI4grcM7@OpenReview" class="title-link" href="/venue/UvfI4grcM7@OpenReview" target="_blank">Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics</a>
                <a id="pdf-UvfI4grcM7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UvfI4grcM7@OpenReview', this)" data="https://openreview.net/pdf?id=UvfI4grcM7">[PDF<sup id="pdf-stars-UvfI4grcM7@OpenReview">9</sup>]</a>
                <a id="copy-UvfI4grcM7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UvfI4grcM7@OpenReview')">[Copy]</a>
                <a id="kimi-UvfI4grcM7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UvfI4grcM7@OpenReview', this)">[Kimi<sup id="kimi-stars-UvfI4grcM7@OpenReview">10</sup>]</a>
                <a id="rel-UvfI4grcM7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UvfI4grcM7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UvfI4grcM7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianfang Zhu" target="_blank">Tianfang Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongli Hu" target="_blank">Dongli Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiandong Zhou" target="_blank">Jiandong Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Du" target="_blank">Kai Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anan LI" target="_blank">Anan LI</a>
            </p>
            <p id="summary-UvfI4grcM7@OpenReview" class="summary">The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap between biological realism and behavioral functionality presents a formidable challenge. In this study, we focus on the columnar structure of the superficial layers of mouse barrel cortex as a model system. We constructed a model comprising 4,218 neurons across 13 neuronal subtypes, with neural distribution and connection strengths constrained by anatomical experimental findings. A key innovation of our work is the development of an effective construction and training pipeline tailored for this biologically constrained model. Additionally, we converted an existing simulated whisker sweep dataset into a spiking-based format, enabling our network to be trained and tested on neural signals that more closely mimic those observed in biological systems. The results of object discrimination utilizing whisker signals demonstrate that our barrel cortex model, grounded in biological constraints, achieves a classification accuracy exceeds classical convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory networks (LSTMs), by an average of 8.6%, and is on par with recent spiking neural networks (SNNs) in performance. Interestingly, a whisker deprivation experiment, designed in accordance with neuroscience practices, further validates the perceptual capabilities of our model in behavioral tasks.Critically, it offers significant biological interpretability: post-training analysis reveals that neurons within our model exhibit firing characteristics and distribution patterns similar to those observed in the actual neuronal systems of the barrel cortex. This study advances our understanding of neural processing in the barrel cortex and exemplifies how integrating detailed biological structures into neural network models can enhance both scientific inquiry and artificial intelligence applications. The code is available at https://github.com/fun0515/RSNN_bfd.</p>
            <p id="subjects-UvfI4grcM7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-UvfI4grcM7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UvfI4grcM7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UvfI4grcM7@OpenReview" onclick="foldPdfKimi('UvfI4grcM7@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Mn2qgIcIPS@OpenReview" class="panel paper" keywords="unsupervised,enhancement,adjustment,curve,light,image,exposure,continuous,low,approaches">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Mn2qgIcIPS" target="_blank" title="11/373"><span class="index notranslate">#11</span></a>
                <a id="title-Mn2qgIcIPS@OpenReview" class="title-link" href="/venue/Mn2qgIcIPS@OpenReview" target="_blank">Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs</a>
                <a id="pdf-Mn2qgIcIPS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Mn2qgIcIPS@OpenReview', this)" data="https://openreview.net/pdf?id=Mn2qgIcIPS">[PDF<sup id="pdf-stars-Mn2qgIcIPS@OpenReview">19</sup>]</a>
                <a id="copy-Mn2qgIcIPS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Mn2qgIcIPS@OpenReview')">[Copy]</a>
                <a id="kimi-Mn2qgIcIPS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Mn2qgIcIPS@OpenReview', this)">[Kimi<sup id="kimi-stars-Mn2qgIcIPS@OpenReview">7</sup>]</a>
                <a id="rel-Mn2qgIcIPS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Mn2qgIcIPS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Mn2qgIcIPS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Donggoo Jung" target="_blank">Donggoo Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daehyun Kim" target="_blank">Daehyun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tae Hyun Kim" target="_blank">Tae Hyun Kim</a>
            </p>
            <p id="summary-Mn2qgIcIPS@OpenReview" class="summary">Low-light image enhancement poses a significant challenge due to the limited information captured by image sensors in low-light environments. Despite recent improvements in deep learning models, the lack of paired training datasets remains a significant obstacle. Therefore, unsupervised methods have emerged as a promising solution. In this work, we focus on the strength of curve-adjustment-based approaches to tackle unsupervised methods. The majority of existing unsupervised curve-adjustment approaches iteratively estimate higher order curve parameters to enhance the exposure of images while efficiently preserving the details of the images. However, the convergence of the enhancement procedure cannot be guaranteed, leading to sensitivity to the number of iterations and limited performance. To address this problem, we consider the iterative curve-adjustment update process as a dynamic system and formulate it as a Neural Ordinary Differential Equations (NODE) for the first time, and this allows us to learn a continuous dynamics of the latent image. The strategy of utilizing NODE to leverage continuous dynamics in iterative methods enhances unsupervised learning and aids in achieving better convergence compared to discrete-space approaches. Consequently, we achieve state-of-the-art performance in unsupervised low-light image enhancement across various benchmark datasets.</p>
            <p id="subjects-Mn2qgIcIPS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Mn2qgIcIPS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mn2qgIcIPS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mn2qgIcIPS@OpenReview" onclick="foldPdfKimi('Mn2qgIcIPS@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="J9VogDTa1W@OpenReview" class="panel paper" keywords="causal,meta,emerge,qualitative,behavior,states,relationships,consolidates,switching,tipping">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=J9VogDTa1W" target="_blank" title="12/373"><span class="index notranslate">#12</span></a>
                <a id="title-J9VogDTa1W@OpenReview" class="title-link" href="/venue/J9VogDTa1W@OpenReview" target="_blank">Systems with Switching Causal Relations: A Meta-Causal Perspective</a>
                <a id="pdf-J9VogDTa1W@OpenReview" class="title-pdf notranslate" onclick="togglePdf('J9VogDTa1W@OpenReview', this)" data="https://openreview.net/pdf?id=J9VogDTa1W">[PDF<sup id="pdf-stars-J9VogDTa1W@OpenReview">9</sup>]</a>
                <a id="copy-J9VogDTa1W@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('J9VogDTa1W@OpenReview')">[Copy]</a>
                <a id="kimi-J9VogDTa1W@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('J9VogDTa1W@OpenReview', this)">[Kimi<sup id="kimi-stars-J9VogDTa1W@OpenReview">15</sup>]</a>
                <a id="rel-J9VogDTa1W@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('J9VogDTa1W@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-J9VogDTa1W@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Moritz Willig" target="_blank">Moritz Willig</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Tobiasch" target="_blank">Tim Tobiasch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Busch" target="_blank">Florian Busch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Seng" target="_blank">Jonas Seng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Devendra Singh Dhami" target="_blank">Devendra Singh Dhami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kristian Kersting" target="_blank">Kristian Kersting</a>
            </p>
            <p id="summary-J9VogDTa1W@OpenReview" class="summary">Most works on causality in machine learning assume that causal relationships are governed by a constant underlying process. However, the flexibility of agents' actions or tipping point behavior in the environmental process can change the qualitative dynamics of the system. As a result, new causal relationships may emerge, while existing ones change or disappear, resulting in an altered causal graph. To analyze these qualitative changes on the causal graph, we propose the concept of *meta-causal states*, which groups classical causal models into clusters based on equivalent qualitative behavior and consolidates specific mechanism parameterizations. We demonstrate how meta-causal states can be inferred from observed agent behavior, and discuss potential methods for disentangling these states from unlabeled data. Finally, we direct our analysis toward the application of a dynamical system, demonstrating that meta-causal states can also emerge from inherent system dynamics, and thus constitute more than a context-dependent framework in which mechanisms emerge merely as a result of external factors.</p>
            <p id="subjects-J9VogDTa1W@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-J9VogDTa1W@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-J9VogDTa1W@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-J9VogDTa1W@OpenReview" onclick="foldPdfKimi('J9VogDTa1W@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="QogcGNXJVw@OpenReview" class="panel paper" keywords="queries,circuit,complexity,interpretability,hard,affordances,discovery,tractability,intractable,inner">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QogcGNXJVw" target="_blank" title="13/373"><span class="index notranslate">#13</span></a>
                <a id="title-QogcGNXJVw@OpenReview" class="title-link" href="/venue/QogcGNXJVw@OpenReview" target="_blank">The Computational Complexity of Circuit Discovery for Inner Interpretability</a>
                <a id="pdf-QogcGNXJVw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QogcGNXJVw@OpenReview', this)" data="https://openreview.net/pdf?id=QogcGNXJVw">[PDF<sup id="pdf-stars-QogcGNXJVw@OpenReview">11</sup>]</a>
                <a id="copy-QogcGNXJVw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QogcGNXJVw@OpenReview')">[Copy]</a>
                <a id="kimi-QogcGNXJVw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QogcGNXJVw@OpenReview', this)">[Kimi<sup id="kimi-stars-QogcGNXJVw@OpenReview">10</sup>]</a>
                <a id="rel-QogcGNXJVw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QogcGNXJVw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QogcGNXJVw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Adolfi" target="_blank">Federico Adolfi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martina G. Vilas" target="_blank">Martina G. Vilas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Todd Wareham" target="_blank">Todd Wareham</a>
            </p>
            <p id="summary-QogcGNXJVw@OpenReview" class="summary">Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and testing of heuristics, there are concerns about their scalability and faithfulness at a time when we lack understanding of the complexity properties of the problems they are deployed to solve. To address this, we study circuit discovery with classical and parameterized computational complexity theory: (1) we describe a conceptual scaffolding to reason about circuit finding queries in terms of affordances for description, explanation, prediction and control; (2) we formalize a comprehensive set of queries that capture mechanistic explanation, and propose a formal framework for their analysis; (3) we use it to settle the complexity of many query variants and relaxations of practical interest on multi-layer perceptrons (part of, e.g., transformers). Our findings reveal a challenging complexity landscape. Many queries are intractable (NP-hard, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A3;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-10" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1001.15em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="msubsup" id="MathJax-Span-12"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-13" style="font-family: MathJax_Main;">Σ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.451em, -999.997em); top: -2.602em; left: 0.732em;"><span class="mi" id="MathJax-Span-14" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -1.82em; left: 0.732em;"><span class="mn" id="MathJax-Span-15" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi mathvariant="normal">Σ</mi><mn>2</mn><mi>p</mi></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-4">\Sigma^p_2</script>-hard), remain fixed-parameter intractable (W[1]-hard) when constraining model/circuit features (e.g., depth), and are inapproximable under additive, multiplicative, and probabilistic approximation schemes. To navigate this landscape, we prove there exist transformations to tackle some of these hard problems (NP- vs. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A3;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1001.15em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="msubsup" id="MathJax-Span-18"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-19" style="font-family: MathJax_Main;">Σ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.451em, -999.997em); top: -2.602em; left: 0.732em;"><span class="mi" id="MathJax-Span-20" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -1.82em; left: 0.732em;"><span class="mn" id="MathJax-Span-21" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi mathvariant="normal">Σ</mi><mn>2</mn><mi>p</mi></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-5">\Sigma^p_2</script>-complete) with better-understood heuristics, and prove the tractability (PTIME) or fixed-parameter tractability (FPT) of more modest queries which retain useful affordances. This framework allows us to understand the scope and limits of interpretability queries, explore viable options, and compare their resource demands among existing and future architectures.</p>
            <p id="subjects-QogcGNXJVw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-QogcGNXJVw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QogcGNXJVw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QogcGNXJVw@OpenReview" onclick="foldPdfKimi('QogcGNXJVw@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="03OkC0LKDD@OpenReview" class="panel paper" keywords="clipping,dgd,arc,robust,byzantine,resilient,robustness,gradient,distributed,sota">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=03OkC0LKDD" target="_blank" title="14/373"><span class="index notranslate">#14</span></a>
                <a id="title-03OkC0LKDD@OpenReview" class="title-link" href="/venue/03OkC0LKDD@OpenReview" target="_blank">The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed Learning</a>
                <a id="pdf-03OkC0LKDD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('03OkC0LKDD@OpenReview', this)" data="https://openreview.net/pdf?id=03OkC0LKDD">[PDF<sup id="pdf-stars-03OkC0LKDD@OpenReview">10</sup>]</a>
                <a id="copy-03OkC0LKDD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('03OkC0LKDD@OpenReview')">[Copy]</a>
                <a id="kimi-03OkC0LKDD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('03OkC0LKDD@OpenReview', this)">[Kimi<sup id="kimi-stars-03OkC0LKDD@OpenReview">5</sup>]</a>
                <a id="rel-03OkC0LKDD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('03OkC0LKDD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-03OkC0LKDD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Youssef Allouah" target="_blank">Youssef Allouah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rachid Guerraoui" target="_blank">Rachid Guerraoui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nirupam Gupta" target="_blank">Nirupam Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ahmed Jellouli" target="_blank">Ahmed Jellouli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Geovani Rizk" target="_blank">Geovani Rizk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Stephan" target="_blank">John Stephan</a>
            </p>
            <p id="summary-03OkC0LKDD@OpenReview" class="summary">Byzantine-resilient distributed machine learning seeks to achieve robust learning performance in the presence of misbehaving or adversarial workers.While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods wereproven theoretically optimal, their empirical success has often relied on pre-aggregation gradient clipping.However, the currently considered staticclipping strategy exhibits mixed results: improving robustness against some attacks while being ineffective or detrimental against others.We address this gap by proposing a principled adaptive clipping strategy, termed Adaptive Robust Clipping (ARC).We show that ARC consistently enhances the empirical robustness of SOTA Robust-DGD methods, while preserving the theoretical robustness guarantees. Our analysis shows that ARC provably improves the asymptotic convergence guarantee of Robust-DGD in the case when the model is well-initialized.We validate this theoretical insight through an exhaustive set of experiments on benchmark image classification tasks.We observe that the improvement induced by ARC is more pronounced in highly heterogeneous and adversarial settings.</p>
            <p id="subjects-03OkC0LKDD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-03OkC0LKDD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-03OkC0LKDD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-03OkC0LKDD@OpenReview" onclick="foldPdfKimi('03OkC0LKDD@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="M7KyLjuN0A@OpenReview" class="panel paper" keywords="hexplane,dynamiccity,lidar,generation,scenes,dit,feature,miou,dynamic,06x">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=M7KyLjuN0A" target="_blank" title="15/373"><span class="index notranslate">#15</span></a>
                <a id="title-M7KyLjuN0A@OpenReview" class="title-link" href="/venue/M7KyLjuN0A@OpenReview" target="_blank">DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</a>
                <a id="pdf-M7KyLjuN0A@OpenReview" class="title-pdf notranslate" onclick="togglePdf('M7KyLjuN0A@OpenReview', this)" data="https://openreview.net/pdf?id=M7KyLjuN0A">[PDF<sup id="pdf-stars-M7KyLjuN0A@OpenReview">10</sup>]</a>
                <a id="copy-M7KyLjuN0A@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('M7KyLjuN0A@OpenReview')">[Copy]</a>
                <a id="kimi-M7KyLjuN0A@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('M7KyLjuN0A@OpenReview', this)">[Kimi<sup id="kimi-stars-M7KyLjuN0A@OpenReview">9</sup>]</a>
                <a id="rel-M7KyLjuN0A@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('M7KyLjuN0A@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-M7KyLjuN0A@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hengwei Bian" target="_blank">Hengwei Bian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingdong Kong" target="_blank">Lingdong Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haozhe Xie" target="_blank">Haozhe Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Pan" target="_blank">Liang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Qiao" target="_blank">Yu Qiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Liu" target="_blank">Ziwei Liu</a>
            </p>
            <p id="summary-M7KyLjuN0A@OpenReview" class="summary">LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic LiDAR scenes with semantics. DynamicCity mainly consists of two key models. **1)** A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel **Projection Module** to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to **12.56** mIoU gain). Furthermore, we utilize an Expansion &amp; Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to **7.05** mIoU gain, **2.06x** training speedup, and **70.84\%** memory reduction). **2)** A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a **Padded Rollout Operation** is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting **versatile 4D generation applications**, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research.</p>
            <p id="subjects-M7KyLjuN0A@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-M7KyLjuN0A@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-M7KyLjuN0A@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-M7KyLjuN0A@OpenReview" onclick="foldPdfKimi('M7KyLjuN0A@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="cTR17xl89h@OpenReview" class="panel paper" keywords="bodygen,embodiment,morphology,design,control,reward,advancing,efficient,towards,signals">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cTR17xl89h" target="_blank" title="16/373"><span class="index notranslate">#16</span></a>
                <a id="title-cTR17xl89h@OpenReview" class="title-link" href="/venue/cTR17xl89h@OpenReview" target="_blank">BodyGen: Advancing Towards Efficient Embodiment Co-Design</a>
                <a id="pdf-cTR17xl89h@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cTR17xl89h@OpenReview', this)" data="https://openreview.net/pdf?id=cTR17xl89h">[PDF<sup id="pdf-stars-cTR17xl89h@OpenReview">9</sup>]</a>
                <a id="copy-cTR17xl89h@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cTR17xl89h@OpenReview')">[Copy]</a>
                <a id="kimi-cTR17xl89h@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cTR17xl89h@OpenReview', this)">[Kimi<sup id="kimi-stars-cTR17xl89h@OpenReview">3</sup>]</a>
                <a id="rel-cTR17xl89h@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cTR17xl89h@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cTR17xl89h@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haofei Lu" target="_blank">Haofei Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Wu" target="_blank">Zhe Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junliang Xing" target="_blank">Junliang Xing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianshu Li" target="_blank">Jianshu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoyu Li" target="_blank">Ruoyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Li" target="_blank">Zhe Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanchun Shi" target="_blank">Yuanchun Shi</a>
            </p>
            <p id="summary-cTR17xl89h@OpenReview" class="summary">Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of morphological search spaces and (ii) intricate dependencies between morphology and control.We prove that the ineffective morphology representation and unbalanced reward signals between the design and control stages are key obstacles to efficiency.To advance towards efficient embodiment co-design, we propose **BodyGen**, which utilizes (1) topology-aware self-attention for both design and control, enabling efficient morphology representation with lightweight model sizes; (2) a temporal credit assignment mechanism that ensures balanced reward signals for optimization. With our findings, BodyGen achieves an average **60.03%** performance improvement against state-of-the-art baselines. We provide codes and more results on the website: https://genesisorigin.github.io.</p>
            <p id="subjects-cTR17xl89h@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cTR17xl89h@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cTR17xl89h@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cTR17xl89h@OpenReview" onclick="foldPdfKimi('cTR17xl89h@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="7BQkXXM8Fy@OpenReview" class="panel paper" keywords="diffusion,planning,planner,decision,components,choices,offline,sampling,good,making">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7BQkXXM8Fy" target="_blank" title="17/373"><span class="index notranslate">#17</span></a>
                <a id="title-7BQkXXM8Fy@OpenReview" class="title-link" href="/venue/7BQkXXM8Fy@OpenReview" target="_blank">What Makes a Good Diffusion Planner for Decision Making?</a>
                <a id="pdf-7BQkXXM8Fy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7BQkXXM8Fy@OpenReview', this)" data="https://openreview.net/pdf?id=7BQkXXM8Fy">[PDF<sup id="pdf-stars-7BQkXXM8Fy@OpenReview">25</sup>]</a>
                <a id="copy-7BQkXXM8Fy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7BQkXXM8Fy@OpenReview')">[Copy]</a>
                <a id="kimi-7BQkXXM8Fy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7BQkXXM8Fy@OpenReview', this)">[Kimi<sup id="kimi-stars-7BQkXXM8Fy@OpenReview">11</sup>]</a>
                <a id="rel-7BQkXXM8Fy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7BQkXXM8Fy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7BQkXXM8Fy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haofei Lu" target="_blank">Haofei Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongqi Han" target="_blank">Dongqi Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Shen" target="_blank">Yifei Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongsheng Li" target="_blank">Dongsheng Li</a>
            </p>
            <p id="summary-7BQkXXM8Fy@OpenReview" class="summary">Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.</p>
            <p id="subjects-7BQkXXM8Fy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-7BQkXXM8Fy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7BQkXXM8Fy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7BQkXXM8Fy@OpenReview" onclick="foldPdfKimi('7BQkXXM8Fy@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="mOpNrrV2zH@OpenReview" class="panel paper" keywords="cbgbench,blank,sbdd,fill,protein,tasks,graph,novo,molecule,binding">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mOpNrrV2zH" target="_blank" title="18/373"><span class="index notranslate">#18</span></a>
                <a id="title-mOpNrrV2zH@OpenReview" class="title-link" href="/venue/mOpNrrV2zH@OpenReview" target="_blank">CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph</a>
                <a id="pdf-mOpNrrV2zH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mOpNrrV2zH@OpenReview', this)" data="https://openreview.net/pdf?id=mOpNrrV2zH">[PDF<sup id="pdf-stars-mOpNrrV2zH@OpenReview">13</sup>]</a>
                <a id="copy-mOpNrrV2zH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mOpNrrV2zH@OpenReview')">[Copy]</a>
                <a id="kimi-mOpNrrV2zH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mOpNrrV2zH@OpenReview', this)">[Kimi<sup id="kimi-stars-mOpNrrV2zH@OpenReview">6</sup>]</a>
                <a id="rel-mOpNrrV2zH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mOpNrrV2zH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mOpNrrV2zH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haitao Lin" target="_blank">Haitao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guojiang Zhao" target="_blank">Guojiang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Odin Zhang" target="_blank">Odin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufei Huang" target="_blank">Yufei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lirong Wu" target="_blank">Lirong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Tan" target="_blank">Cheng Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zicheng Liu" target="_blank">Zicheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhifeng Gao" target="_blank">Zhifeng Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stan Z Li" target="_blank">Stan Z Li</a>
            </p>
            <p id="summary-mOpNrrV2zH@OpenReview" class="summary">Structure-based drug design (SBDD) aims to generate potential drugs that can bind to a target protein and is greatly expedited by the aid of AI techniques in generative models. However, a lack of systematic understanding persists due to the diverse settings, complex implementation, difficult reproducibility, and task singularity. Firstly, the absence of standardization can lead to unfair comparisons and inconclusive insights. To address this dilemma, we propose CBGBench, a comprehensive benchmark for SBDD, that unifies the task as a generative heterogeneous graph completion, analogous to fill-in-the-blank of the 3D complex binding graph. By categorizing existing methods based on their attributes, CBGBench facilitates a modular and extensible framework that implements various cutting-edge methods. Secondly, a single de novo molecule generation task can hardly reflect their capabilities. To broaden the scope, we adapt these models to a range of tasks essential in drug design, considered sub-tasks within the graph fill-in-the-blank tasks. These tasks include the generative designation of de novo molecules, linkers, fragments, scaffolds, and sidechains, all conditioned on the structures of protein pockets. Our evaluations are conducted with fairness, encompassing comprehensive perspectives on interaction, chemical properties, geometry authenticity, and substructure validity. We further provide deep insights with analysis from empirical studies. Our results indicate that there is potential for further improvements on many tasks, optimization in network architectures, and incorporation of chemical prior knowledge. To lower the barrier to entry and facilitate further developments in the field, we also provide a unified codebase (supplementary) that includes the discussed state-of-the-art models, data pre-processing, training, sampling, and evaluation.</p>
            <p id="subjects-mOpNrrV2zH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mOpNrrV2zH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mOpNrrV2zH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mOpNrrV2zH@OpenReview" onclick="foldPdfKimi('mOpNrrV2zH@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="wJv4AIt4sK@OpenReview" class="panel paper" keywords="sparsity,quantization,footprints,compression,dnns,reduce,methods,accuracy,tacitly,125m">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wJv4AIt4sK" target="_blank" title="19/373"><span class="index notranslate">#19</span></a>
                <a id="title-wJv4AIt4sK@OpenReview" class="title-link" href="/venue/wJv4AIt4sK@OpenReview" target="_blank">Effective Interplay between Sparsity and Quantization: From Theory to Practice</a>
                <a id="pdf-wJv4AIt4sK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wJv4AIt4sK@OpenReview', this)" data="https://openreview.net/pdf?id=wJv4AIt4sK">[PDF<sup id="pdf-stars-wJv4AIt4sK@OpenReview">24</sup>]</a>
                <a id="copy-wJv4AIt4sK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wJv4AIt4sK@OpenReview')">[Copy]</a>
                <a id="kimi-wJv4AIt4sK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wJv4AIt4sK@OpenReview', this)">[Kimi<sup id="kimi-stars-wJv4AIt4sK@OpenReview">12</sup>]</a>
                <a id="rel-wJv4AIt4sK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wJv4AIt4sK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wJv4AIt4sK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Simla Harma" target="_blank">Simla Harma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayan Chakraborty" target="_blank">Ayan Chakraborty</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elizaveta Kostenok" target="_blank">Elizaveta Kostenok</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danila Mishin" target="_blank">Danila Mishin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongho Ha" target="_blank">Dongho Ha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Babak Falsafi" target="_blank">Babak Falsafi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Jaggi" target="_blank">Martin Jaggi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Liu" target="_blank">Ming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunho Oh" target="_blank">Yunho Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suvinay Subramanian" target="_blank">Suvinay Subramanian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amir Yazdanbakhsh" target="_blank">Amir Yazdanbakhsh</a>
            </p>
            <p id="summary-wJv4AIt4sK@OpenReview" class="summary">The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy.</p>
            <p id="subjects-wJv4AIt4sK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wJv4AIt4sK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wJv4AIt4sK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wJv4AIt4sK@OpenReview" onclick="foldPdfKimi('wJv4AIt4sK@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="7xCSK9BLPy@OpenReview" class="panel paper" keywords="mbr,decoding,judges,llm,instruction,llms,bayes,reference,following,minimum">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7xCSK9BLPy" target="_blank" title="20/373"><span class="index notranslate">#20</span></a>
                <a id="title-7xCSK9BLPy@OpenReview" class="title-link" href="/venue/7xCSK9BLPy@OpenReview" target="_blank">Better Instruction-Following Through Minimum Bayes Risk</a>
                <a id="pdf-7xCSK9BLPy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7xCSK9BLPy@OpenReview', this)" data="https://openreview.net/pdf?id=7xCSK9BLPy">[PDF<sup id="pdf-stars-7xCSK9BLPy@OpenReview">15</sup>]</a>
                <a id="copy-7xCSK9BLPy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7xCSK9BLPy@OpenReview')">[Copy]</a>
                <a id="kimi-7xCSK9BLPy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7xCSK9BLPy@OpenReview', this)">[Kimi<sup id="kimi-stars-7xCSK9BLPy@OpenReview">10</sup>]</a>
                <a id="rel-7xCSK9BLPy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7xCSK9BLPy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7xCSK9BLPy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ian Wu" target="_blank">Ian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patrick Fernandes" target="_blank">Patrick Fernandes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amanda Bertsch" target="_blank">Amanda Bertsch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungone Kim" target="_blank">Seungone Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sina Pakazad" target="_blank">Sina Pakazad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Graham Neubig" target="_blank">Graham Neubig</a>
            </p>
            <p id="summary-7xCSK9BLPy@OpenReview" class="summary">General-purpose LLM judges capable of human-level evaluation provide not only a scalable and accurate way of evaluating instruction-following LLMs but also new avenues for supervising and improving their performance. One promising way of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR) decoding, which uses a reference-based evaluator to select a high-quality output from amongst a set of candidate outputs. In the first part of this work, we explore using MBR decoding as a method for improving the test-time performance of instruction-following LLMs. We find that MBR decoding with reference-based LLM judges substantially improves over greedy decoding, best-of-N decoding with reference-free judges and MBR decoding with lexical and embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent across LLMs with up to 70B parameters, demonstrating that smaller LLM judges can be used to supervise much larger LLMs. Then, seeking to retain the improvements from MBR decoding while mitigating additional test-time costs, we explore iterative self-training on MBR-decoded outputs. We find that self-training using Direct Preference Optimisation leads to significant performance gains, such that the self-trained models with greedy decoding generally match and sometimes exceed the performance of their base models with MBR decoding.</p>
            <p id="subjects-7xCSK9BLPy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-7xCSK9BLPy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7xCSK9BLPy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7xCSK9BLPy@OpenReview" onclick="foldPdfKimi('7xCSK9BLPy@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="qtTIP5Gjc5@OpenReview" class="panel paper" keywords="mamba,tokens,selective,scenario,ssm,diverge,refinements,convergent,demystifying,model">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qtTIP5Gjc5" target="_blank" title="21/373"><span class="index notranslate">#21</span></a>
                <a id="title-qtTIP5Gjc5@OpenReview" class="title-link" href="/venue/qtTIP5Gjc5@OpenReview" target="_blank">Demystifying the Token Dynamics of Deep Selective State Space Models</a>
                <a id="pdf-qtTIP5Gjc5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qtTIP5Gjc5@OpenReview', this)" data="https://openreview.net/pdf?id=qtTIP5Gjc5">[PDF<sup id="pdf-stars-qtTIP5Gjc5@OpenReview">9</sup>]</a>
                <a id="copy-qtTIP5Gjc5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qtTIP5Gjc5@OpenReview')">[Copy]</a>
                <a id="kimi-qtTIP5Gjc5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qtTIP5Gjc5@OpenReview', this)">[Kimi<sup id="kimi-stars-qtTIP5Gjc5@OpenReview">8</sup>]</a>
                <a id="rel-qtTIP5Gjc5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qtTIP5Gjc5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qtTIP5Gjc5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thieu Vo" target="_blank">Thieu Vo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Duy-Tung Pham" target="_blank">Duy-Tung Pham</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Tong" target="_blank">Xin Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tan Nguyen" target="_blank">Tan Nguyen</a>
            </p>
            <p id="summary-qtTIP5Gjc5@OpenReview" class="summary">Selective state space models (SSM), such as Mamba, have gained prominence for their effectiveness in modeling sequential data. Despite their outstanding empirical performance, a comprehensive theoretical understanding of deep selective SSM remains elusive, hindering their further development and adoption for applications that need high fidelity. In this paper, we investigate the dynamical properties of tokens in a pre-trained Mamba model. In particular, we derive the dynamical system governing the continuous-time limit of the Mamba model and characterize the asymptotic behavior of its solutions. In the one-dimensional case, we prove that only one of the following two scenarios happens: either all tokens converge to zero, or all tokens diverge to infinity. We provide criteria based on model parameters to determine when each scenario occurs. For the convergent scenario, we empirically verify that this scenario negatively impacts the model's performance. For the divergent scenario, we prove that different tokens will diverge to infinity at different rates, thereby contributing unequally to the updates during model training. Based on these investigations, we propose two refinements for the model: excluding the convergent scenario and reordering tokens based on their importance scores, both aimed at improving practical performance. Our experimental results validate these refinements, offering insights into enhancing Mamba's effectiveness in real-world applications.</p>
            <p id="subjects-qtTIP5Gjc5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qtTIP5Gjc5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qtTIP5Gjc5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qtTIP5Gjc5@OpenReview" onclick="foldPdfKimi('qtTIP5Gjc5@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="IwPXYk6BV9@OpenReview" class="panel paper" keywords="label,privacy,vector,flipping,labels,differential,approximation,privatized,protects,datasets">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IwPXYk6BV9" target="_blank" title="22/373"><span class="index notranslate">#22</span></a>
                <a id="title-IwPXYk6BV9@OpenReview" class="title-link" href="/venue/IwPXYk6BV9@OpenReview" target="_blank">Enhancing Learning with Label Differential Privacy by Vector Approximation</a>
                <a id="pdf-IwPXYk6BV9@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IwPXYk6BV9@OpenReview', this)" data="https://openreview.net/pdf?id=IwPXYk6BV9">[PDF<sup id="pdf-stars-IwPXYk6BV9@OpenReview">7</sup>]</a>
                <a id="copy-IwPXYk6BV9@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IwPXYk6BV9@OpenReview')">[Copy]</a>
                <a id="kimi-IwPXYk6BV9@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IwPXYk6BV9@OpenReview', this)">[Kimi<sup id="kimi-stars-IwPXYk6BV9@OpenReview">4</sup>]</a>
                <a id="rel-IwPXYk6BV9@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IwPXYk6BV9@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IwPXYk6BV9@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Puning Zhao" target="_blank">Puning Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiafei Wu" target="_blank">Jiafei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Liu" target="_blank">Zhe Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Shen" target="_blank">Li Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhikun Zhang" target="_blank">Zhikun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rongfei Fan" target="_blank">Rongfei Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Sun" target="_blank">Le Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingming Li" target="_blank">Qingming Li</a>
            </p>
            <p id="summary-IwPXYk6BV9@OpenReview" class="summary">Label differential privacy (DP) is a framework that protects the privacy of labels in training datasets, while the feature vectors are public. Existing approaches protect the privacy of labels by flipping them randomly, and then train a model to make the output approximate the privatized label. However, as the number of classes K increases, stronger randomization is needed, thus the performances of these methods become significantly worse. In this paper, we propose a vector approximation approach for learning with label local differential privacy, which is easy to implement and introduces little additional computational overhead. Instead of flipping each label into a single scalar, our method converts each label into a random vector with K components, whose expectations reflect class conditional probabilities. Intuitively, vector approximation retains more information than scalar labels. A brief theoretical analysis shows that the performance of our method only decays slightly with K. Finally, we conduct experiments on both synthesized and real datasets, which validate our theoretical analysis as well as the practical performance of our method.</p>
            <p id="subjects-IwPXYk6BV9@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IwPXYk6BV9@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IwPXYk6BV9@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IwPXYk6BV9@OpenReview" onclick="foldPdfKimi('IwPXYk6BV9@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="SgymXhOEA5@OpenReview" class="panel paper" keywords="camera,bias,reid,person,unsupervised,identification,models,unseen,normalization,biased">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SgymXhOEA5" target="_blank" title="23/373"><span class="index notranslate">#23</span></a>
                <a id="title-SgymXhOEA5@OpenReview" class="title-link" href="/venue/SgymXhOEA5@OpenReview" target="_blank">Exploring the Camera bias of Person Re-identification</a>
                <a id="pdf-SgymXhOEA5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SgymXhOEA5@OpenReview', this)" data="https://openreview.net/pdf?id=SgymXhOEA5">[PDF<sup id="pdf-stars-SgymXhOEA5@OpenReview">8</sup>]</a>
                <a id="copy-SgymXhOEA5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SgymXhOEA5@OpenReview')">[Copy]</a>
                <a id="kimi-SgymXhOEA5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SgymXhOEA5@OpenReview', this)">[Kimi<sup id="kimi-stars-SgymXhOEA5@OpenReview"></sup>]</a>
                <a id="rel-SgymXhOEA5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SgymXhOEA5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SgymXhOEA5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Myungseo Song" target="_blank">Myungseo Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin-Woo Park" target="_blank">Jin-Woo Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jong-Seok Lee" target="_blank">Jong-Seok Lee</a>
            </p>
            <p id="summary-SgymXhOEA5@OpenReview" class="summary">We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.</p>
            <p id="subjects-SgymXhOEA5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SgymXhOEA5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SgymXhOEA5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SgymXhOEA5@OpenReview" onclick="foldPdfKimi('SgymXhOEA5@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="rdAbEn5DZt@OpenReview" class="panel paper" keywords="ordering,optimization,objective,sample,jogba,balancing,sum,mgda,convergence,objectives">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rdAbEn5DZt" target="_blank" title="24/373"><span class="index notranslate">#24</span></a>
                <a id="title-rdAbEn5DZt@OpenReview" class="title-link" href="/venue/rdAbEn5DZt@OpenReview" target="_blank">Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization</a>
                <a id="pdf-rdAbEn5DZt@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rdAbEn5DZt@OpenReview', this)" data="https://openreview.net/pdf?id=rdAbEn5DZt">[PDF<sup id="pdf-stars-rdAbEn5DZt@OpenReview">7</sup>]</a>
                <a id="copy-rdAbEn5DZt@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rdAbEn5DZt@OpenReview')">[Copy]</a>
                <a id="kimi-rdAbEn5DZt@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rdAbEn5DZt@OpenReview', this)">[Kimi<sup id="kimi-stars-rdAbEn5DZt@OpenReview">2</sup>]</a>
                <a id="rel-rdAbEn5DZt@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rdAbEn5DZt@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rdAbEn5DZt@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hansi Yang" target="_blank">Hansi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Kwok" target="_blank">James Kwok</a>
            </p>
            <p id="summary-rdAbEn5DZt@OpenReview" class="summary">In finite-sum optimization problems, the sample orders for parameter updates can significantly influence the convergence rate of optimization algorithms. While numerous sample ordering techniques have been proposed in the context of single-objective optimization, the problem of sample ordering in finite-sum multi-objective optimization has not been thoroughly explored. To address this gap, we propose a sample ordering method called JoGBa, which finds the sample orders for multiple objectives by jointly performing online vector balancing on the gradients of all objectives. Our theoretical analysis demonstrates that this approach outperforms the standard baseline of random ordering and accelerates the convergence rate for the MGDA algorithm. Empirical evaluation across various datasets with different multi-objective optimization algorithms further demonstrates that JoGBa can achieve faster convergence and superior final performance than other data ordering strategies.</p>
            <p id="subjects-rdAbEn5DZt@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-rdAbEn5DZt@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rdAbEn5DZt@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rdAbEn5DZt@OpenReview" onclick="foldPdfKimi('rdAbEn5DZt@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="WcZLG8XxhD@OpenReview" class="panel paper" keywords="countmin,augmented,streaming,countsketch,hsu,frequency,learned,frequent,directions,estimation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WcZLG8XxhD" target="_blank" title="25/373"><span class="index notranslate">#25</span></a>
                <a id="title-WcZLG8XxhD@OpenReview" class="title-link" href="/venue/WcZLG8XxhD@OpenReview" target="_blank">Learning-Augmented Frequent Directions</a>
                <a id="pdf-WcZLG8XxhD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WcZLG8XxhD@OpenReview', this)" data="https://openreview.net/pdf?id=WcZLG8XxhD">[PDF<sup id="pdf-stars-WcZLG8XxhD@OpenReview">6</sup>]</a>
                <a id="copy-WcZLG8XxhD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WcZLG8XxhD@OpenReview')">[Copy]</a>
                <a id="kimi-WcZLG8XxhD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WcZLG8XxhD@OpenReview', this)">[Kimi<sup id="kimi-stars-WcZLG8XxhD@OpenReview">5</sup>]</a>
                <a id="rel-WcZLG8XxhD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WcZLG8XxhD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WcZLG8XxhD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anders Aamand" target="_blank">Anders Aamand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Justin Chen" target="_blank">Justin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddharth Gollapudi" target="_blank">Siddharth Gollapudi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sandeep Silwal" target="_blank">Sandeep Silwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao WU" target="_blank">Hao WU</a>
            </p>
            <p id="summary-WcZLG8XxhD@OpenReview" class="summary">An influential paper of Hsu et al. (ICLR'19) introduced the study of learning-augmented streaming algorithms in the context of frequency estimation. A fundamental problem in the streaming literature, the goal of frequency estimation is to approximate the number of occurrences of items appearing in a long stream of data using only a small amount of memory. Hsu et al. develop a natural framework to combine the worst-case guarantees of popular solutions such as CountMin and CountSketch with learned predictions of high frequency elements. They demonstrate that learning the underlying structure of data can be used to yield better streaming algorithms, both in theory and practice.We simplify and generalize past work on learning-augmented frequency estimation. Our first contribution is a learning-augmented variant of the Misra-Gries algorithm which improves upon the error of learned CountMin and learned CountSketch and achieves the state-of-the-art performance of randomized algorithms (Aamand et al., NeurIPS'23) with a simpler, deterministic algorithm. Our second contribution is to adapt learning-augmentation to a high-dimensional generalization of frequency estimation corresponding to finding important directions (top singular vectors) of a matrix given its rows one-by-one in a stream. We analyze a learning-augmented variant of the Frequent Directions algorithm, extending the theoretical and empirical understanding of learned predictions to matrix streaming.</p>
            <p id="subjects-WcZLG8XxhD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-WcZLG8XxhD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WcZLG8XxhD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WcZLG8XxhD@OpenReview" onclick="foldPdfKimi('WcZLG8XxhD@OpenReview', this)" class="hr hr-fold">
        </div>
    <div id="2o58Mbqkd2@OpenReview" class="panel paper" keywords="superdiff,diffusion,superposition,pre,trained,texttt,models,logical,cambrian,painless">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2o58Mbqkd2" target="_blank" title="26/373"><span class="index notranslate">#26</span></a>
                <a id="title-2o58Mbqkd2@OpenReview" class="title-link" href="/venue/2o58Mbqkd2@OpenReview" target="_blank">The Superposition of Diffusion Models</a>
                <a id="pdf-2o58Mbqkd2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2o58Mbqkd2@OpenReview', this)" data="https://openreview.net/pdf?id=2o58Mbqkd2">[PDF<sup id="pdf-stars-2o58Mbqkd2@OpenReview">18</sup>]</a>
                <a id="copy-2o58Mbqkd2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2o58Mbqkd2@OpenReview')">[Copy]</a>
                <a id="kimi-2o58Mbqkd2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2o58Mbqkd2@OpenReview', this)">[Kimi<sup id="kimi-stars-2o58Mbqkd2@OpenReview">11</sup>]</a>
                <a id="rel-2o58Mbqkd2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2o58Mbqkd2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2o58Mbqkd2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marta Skreta" target="_blank">Marta Skreta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lazar Atanackovic" target="_blank">Lazar Atanackovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joey Bose" target="_blank">Joey Bose</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Tong" target="_blank">Alexander Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kirill Neklyudov" target="_blank">Kirill Neklyudov</a>
            </p>
            <p id="summary-2o58Mbqkd2@OpenReview" class="summary">The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed *solely through composition during inference*, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;OR&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-22" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-23"><span class="texatom" id="MathJax-Span-24"><span class="mrow" id="MathJax-Span-25"><span class="mtext" id="MathJax-Span-26" style="font-family: MathJax_Typewriter;">OR</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">OR</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-6">\texttt{OR}</script> and the logical <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;AND&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-27" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.51em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-28"><span class="texatom" id="MathJax-Span-29"><span class="mrow" id="MathJax-Span-30"><span class="mtext" id="MathJax-Span-31" style="font-family: MathJax_Typewriter;">AND</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">AND</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-7">\texttt{AND}</script>. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional *de novo* structure design of proteins.</p>
            <p id="subjects-2o58Mbqkd2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2o58Mbqkd2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2o58Mbqkd2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2o58Mbqkd2@OpenReview" onclick="foldPdfKimi('2o58Mbqkd2@OpenReview', this)" class="hr hr-fold">
        </div><div id="xak8c9l1nu@OpenReview" class="panel paper" keywords="distance,mathsf,variation,explorations,computational,total,underexplored,distributions,arbitrary,alphabets">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xak8c9l1nu" target="_blank" title="27/373"><span class="index notranslate">#27</span></a>
                <a id="title-xak8c9l1nu@OpenReview" class="title-link" href="/venue/xak8c9l1nu@OpenReview" target="_blank">Computational Explorations of Total Variation Distance</a>
                <a id="pdf-xak8c9l1nu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xak8c9l1nu@OpenReview', this)" data="https://openreview.net/pdf?id=xak8c9l1nu">[PDF<sup id="pdf-stars-xak8c9l1nu@OpenReview">4</sup>]</a>
                <a id="copy-xak8c9l1nu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xak8c9l1nu@OpenReview')">[Copy]</a>
                <a id="kimi-xak8c9l1nu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xak8c9l1nu@OpenReview', this)">[Kimi<sup id="kimi-stars-xak8c9l1nu@OpenReview">7</sup>]</a>
                <a id="rel-xak8c9l1nu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xak8c9l1nu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xak8c9l1nu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Arnab Bhattacharyya" target="_blank">Arnab Bhattacharyya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sutanu Gayen" target="_blank">Sutanu Gayen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuldeep S. Meel" target="_blank">Kuldeep S. Meel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dimitrios Myrisiotis" target="_blank">Dimitrios Myrisiotis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=A. Pavan" target="_blank">A. Pavan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=N. V. Vinodchandran" target="_blank">N. V. Vinodchandran</a>
            </p>
            <p id="summary-xak8c9l1nu@OpenReview" class="summary">We investigate some previously unexplored (or underexplored) computational aspects of total variation (TV) distance.First, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets.This corresponds to a special case, whereby the TV distance between the two distributions is zero.Second, we prove that unless <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;N&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2286;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;R&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-32" style="width: 4.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1003.91em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-33"><span class="texatom" id="MathJax-Span-34"><span class="mrow" id="MathJax-Span-35"><span class="mi" id="MathJax-Span-36" style="font-family: MathJax_SansSerif;">N</span><span class="mi" id="MathJax-Span-37" style="font-family: MathJax_SansSerif;">P</span></span></span><span class="mo" id="MathJax-Span-38" style="font-family: MathJax_Main; padding-left: 0.263em;">⊆</span><span class="texatom" id="MathJax-Span-39" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-40"><span class="mi" id="MathJax-Span-41" style="font-family: MathJax_SansSerif;">R</span><span class="mi" id="MathJax-Span-42" style="font-family: MathJax_SansSerif;">P</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="sans-serif">N</mi><mi mathvariant="sans-serif">P</mi></mrow><mo>⊆</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="sans-serif">R</mi><mi mathvariant="sans-serif">P</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-8">\mathsf{NP} \subseteq \mathsf{RP}</script> it is impossible to efficiently estimate the TV distance between arbitrary Ising models, even in a bounded-error randomized setting.</p>
            <p id="subjects-xak8c9l1nu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xak8c9l1nu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xak8c9l1nu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xak8c9l1nu@OpenReview" onclick="foldPdfKimi('xak8c9l1nu@OpenReview', this)" class="hr hr-fold">
        </div><div id="hJ1BaJ5ELp@OpenReview" class="panel paper" keywords="sfpk,sparsity,pruning,pruner,parsity,neural,probabilistic,evolutionary,mask,unpruned">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hJ1BaJ5ELp" target="_blank" title="28/373"><span class="index notranslate">#28</span></a>
                <a id="title-hJ1BaJ5ELp@OpenReview" class="title-link" href="/venue/hJ1BaJ5ELp@OpenReview" target="_blank">Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation</a>
                <a id="pdf-hJ1BaJ5ELp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hJ1BaJ5ELp@OpenReview', this)" data="https://openreview.net/pdf?id=hJ1BaJ5ELp">[PDF<sup id="pdf-stars-hJ1BaJ5ELp@OpenReview">8</sup>]</a>
                <a id="copy-hJ1BaJ5ELp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hJ1BaJ5ELp@OpenReview')">[Copy]</a>
                <a id="kimi-hJ1BaJ5ELp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hJ1BaJ5ELp@OpenReview', this)">[Kimi<sup id="kimi-stars-hJ1BaJ5ELp@OpenReview">4</sup>]</a>
                <a id="rel-hJ1BaJ5ELp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hJ1BaJ5ELp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hJ1BaJ5ELp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanfeng Mo" target="_blank">Zhanfeng Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haosen Shi" target="_blank">Haosen Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sinno Pan" target="_blank">Sinno Pan</a>
            </p>
            <p id="summary-hJ1BaJ5ELp@OpenReview" class="summary">Neural pruning aims to compress and accelerate deep neural networks by identifying the optimal subnetwork within a specified sparsity budget. In this work, we study how to gradually sparsify the unpruned dense model to the target sparsity level with a minimal performance drop. Specifically, we analyze the evolution of the population of optimal subnetworks under continuous sparsity increments from a thermodynamics perspective. We first reformulate neural pruning as an expected loss minimization problem over the mask distributions. Then, we establish an effective approximation for the sparsity evolution of the optimal mask distribution, termed the **S**parsity Evolutionary **F**okker-**P**lanck-**K**olmogorov Equation (**SFPK**), which provides closed-form, mathematically tractable guidance on distributional transitions for minimizing the expected loss under an infinitesimal sparsity increment. On top of that, we propose SFPK-pruner, a particle simulation-based probabilistic pruning method, to sample performant masks with desired sparsity from the destination distribution of SFPK. In theory, we establish the convergence guarantee for the proposed SFPK-pruner. In practice, our SFPK-pruner exhibits competitive performance across various pruning scenarios.</p>
            <p id="subjects-hJ1BaJ5ELp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hJ1BaJ5ELp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hJ1BaJ5ELp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hJ1BaJ5ELp@OpenReview" onclick="foldPdfKimi('hJ1BaJ5ELp@OpenReview', this)" class="hr hr-fold">
        </div><div id="GjM61KRiTG@OpenReview" class="panel paper" keywords="helpfulness,bfpo,safety,rlhf,factorial,human,preference,tuning,optimization,fine">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GjM61KRiTG" target="_blank" title="29/373"><span class="index notranslate">#29</span></a>
                <a id="title-GjM61KRiTG@OpenReview" class="title-link" href="/venue/GjM61KRiTG@OpenReview" target="_blank">Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models</a>
                <a id="pdf-GjM61KRiTG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GjM61KRiTG@OpenReview', this)" data="https://openreview.net/pdf?id=GjM61KRiTG">[PDF<sup id="pdf-stars-GjM61KRiTG@OpenReview">13</sup>]</a>
                <a id="copy-GjM61KRiTG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GjM61KRiTG@OpenReview')">[Copy]</a>
                <a id="kimi-GjM61KRiTG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GjM61KRiTG@OpenReview', this)">[Kimi<sup id="kimi-stars-GjM61KRiTG@OpenReview">11</sup>]</a>
                <a id="rel-GjM61KRiTG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GjM61KRiTG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GjM61KRiTG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxuan Zhang" target="_blank">Wenxuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Torr" target="_blank">Philip Torr</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohamed Elhoseiny" target="_blank">Mohamed Elhoseiny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adel Bibi" target="_blank">Adel Bibi</a>
            </p>
            <p id="summary-GjM61KRiTG@OpenReview" class="summary">Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In the supervised optimization, a labeling function is used to capture global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark including comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO eliminates the need for human prompting and annotation in LLM fine-tuning while achieving the same level of safety as methods that heavily rely on human labor, with less than 10\% of the computational resources. The training recipes and models will be released.</p>
            <p id="subjects-GjM61KRiTG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GjM61KRiTG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GjM61KRiTG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GjM61KRiTG@OpenReview" onclick="foldPdfKimi('GjM61KRiTG@OpenReview', this)" class="hr hr-fold">
        </div><div id="D042vFwJAM@OpenReview" class="panel paper" keywords="palsb,physical,reconstruction,bridge,aligned,diffusion,constraints,boundary,field,physics">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=D042vFwJAM" target="_blank" title="30/373"><span class="index notranslate">#30</span></a>
                <a id="title-D042vFwJAM@OpenReview" class="title-link" href="/venue/D042vFwJAM@OpenReview" target="_blank">Physics-aligned field reconstruction with diffusion bridge</a>
                <a id="pdf-D042vFwJAM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('D042vFwJAM@OpenReview', this)" data="https://openreview.net/pdf?id=D042vFwJAM">[PDF<sup id="pdf-stars-D042vFwJAM@OpenReview">6</sup>]</a>
                <a id="copy-D042vFwJAM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('D042vFwJAM@OpenReview')">[Copy]</a>
                <a id="kimi-D042vFwJAM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('D042vFwJAM@OpenReview', this)">[Kimi<sup id="kimi-stars-D042vFwJAM@OpenReview">5</sup>]</a>
                <a id="rel-D042vFwJAM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('D042vFwJAM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-D042vFwJAM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Li" target="_blank">Zeyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongkun Dou" target="_blank">Hongkun Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shen Fang" target="_blank">Shen Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wang Han" target="_blank">Wang Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Deng" target="_blank">Yue Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lijun Yang" target="_blank">Lijun Yang</a>
            </p>
            <p id="summary-D042vFwJAM@OpenReview" class="summary">The reconstruction of physical fields from sparse measurements is pivotal in both scientific research and engineering applications. Traditional methods are increasingly supplemented by deep learning models due to their efficacy in extracting features from data. However, except for the low accuracy on complex physical systems, these models often fail to comply with essential physical constraints, such as governing equations and boundary conditions. To overcome this limitation, we introduce a novel data-driven field reconstruction framework, termed the Physics-aligned Schrödinger Bridge (PalSB). This framework leverages a diffusion bridge mechanism that is specifically tailored to align with physical constraints. The PalSB approach incorporates a dual-stage training process designed to address both local reconstruction mapping and global physical principles. Additionally, a boundary-aware sampling technique is implemented to ensure adherence to physical boundary conditions. We demonstrate the effectiveness of PalSB through its application to three complex nonlinear systems: cylinder flow from Particle Image Velocimetry experiments, two-dimensional turbulence, and a reaction-diffusion system. The results reveal that PalSB not only achieves higher accuracy but also exhibits enhanced compliance with physical constraints compared to existing methods. This highlights PalSB's capability to generate high-quality representations of intricate physical interactions, showcasing its potential for advancing field reconstruction techniques. The source code can be found at https://github.com/lzy12301/PalSB.</p>
            <p id="subjects-D042vFwJAM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-D042vFwJAM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-D042vFwJAM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-D042vFwJAM@OpenReview" onclick="foldPdfKimi('D042vFwJAM@OpenReview', this)" class="hr hr-fold">
        </div><div id="9WYMDgxDac@OpenReview" class="panel paper" keywords="tron,ended,risk,mllms,prediction,sets,multimodal,levels,language,isk">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9WYMDgxDac" target="_blank" title="31/373"><span class="index notranslate">#31</span></a>
                <a id="title-9WYMDgxDac@OpenReview" class="title-link" href="/venue/9WYMDgxDac@OpenReview" target="_blank">Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models</a>
                <a id="pdf-9WYMDgxDac@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9WYMDgxDac@OpenReview', this)" data="https://openreview.net/pdf?id=9WYMDgxDac">[PDF<sup id="pdf-stars-9WYMDgxDac@OpenReview">10</sup>]</a>
                <a id="copy-9WYMDgxDac@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9WYMDgxDac@OpenReview')">[Copy]</a>
                <a id="kimi-9WYMDgxDac@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9WYMDgxDac@OpenReview', this)">[Kimi<sup id="kimi-stars-9WYMDgxDac@OpenReview">12</sup>]</a>
                <a id="rel-9WYMDgxDac@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9WYMDgxDac@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9WYMDgxDac@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qingni Wang" target="_blank">Qingni Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiantian Geng" target="_blank">Tiantian Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Wang" target="_blank">Zhiyuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Teng Wang" target="_blank">Teng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Fu" target="_blank">Bo Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Zheng" target="_blank">Feng Zheng</a>
            </p>
            <p id="summary-9WYMDgxDac@OpenReview" class="summary">Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce *TRON*, a **t**wo-step framework for **r**isk c**o**ntrol and assessme**n**t, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. *TRON* comprises two main components: (1) a novel conformal score to **sample** response sets of minimum size, and (2) a nonconformity score to **identify** high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that *TRON* achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.</p>
            <p id="subjects-9WYMDgxDac@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9WYMDgxDac@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9WYMDgxDac@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9WYMDgxDac@OpenReview" onclick="foldPdfKimi('9WYMDgxDac@OpenReview', this)" class="hr hr-fold">
        </div><div id="xGs7Ch3Vyo@OpenReview" class="panel paper" keywords="regression,llms,raft,autoregressive,language,fine,perplexity,loss,finetuning,principled">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xGs7Ch3Vyo" target="_blank" title="32/373"><span class="index notranslate">#32</span></a>
                <a id="title-xGs7Ch3Vyo@OpenReview" class="title-link" href="/venue/xGs7Ch3Vyo@OpenReview" target="_blank">Better autoregressive regression with LLMs</a>
                <a id="pdf-xGs7Ch3Vyo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xGs7Ch3Vyo@OpenReview', this)" data="https://openreview.net/pdf?id=xGs7Ch3Vyo">[PDF<sup id="pdf-stars-xGs7Ch3Vyo@OpenReview">19</sup>]</a>
                <a id="copy-xGs7Ch3Vyo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xGs7Ch3Vyo@OpenReview')">[Copy]</a>
                <a id="kimi-xGs7Ch3Vyo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xGs7Ch3Vyo@OpenReview', this)">[Kimi<sup id="kimi-stars-xGs7Ch3Vyo@OpenReview">19</sup>]</a>
                <a id="rel-xGs7Ch3Vyo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xGs7Ch3Vyo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xGs7Ch3Vyo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michal Lukasik" target="_blank">Michal Lukasik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhao Meng" target="_blank">Zhao Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harikrishna Narasimhan" target="_blank">Harikrishna Narasimhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditya Krishna Menon" target="_blank">Aditya Krishna Menon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin-Wen Chang" target="_blank">Yin-Wen Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Yu" target="_blank">Felix Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjiv Kumar" target="_blank">Sanjiv Kumar</a>
            </p>
            <p id="summary-xGs7Ch3Vyo@OpenReview" class="summary">Large language models (LLMs) have proven successful on many machine learning tasks,including those that do not involve language generation. In specific, LLMs have been shown to be effective in solving regression, where the targets are real-numbers.One common approach is to fine tune the LLM based on the log-perplexity loss and use autoregressive sampling at the inference time. Another approach relies on adding a predictive head and finetuning it with a suitable loss. Despite the success, there has not been a study on the principled ways of using decoder LLMs for regression. In this work we compare different prior works under a unified view, and introduce RAFT, regression-aware fine-tuning, a novel approach based on the Bayes-optimal decision rule. We demonstrate how RAFT improves over established baselines on several benchmarks and model families.</p>
            <p id="subjects-xGs7Ch3Vyo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xGs7Ch3Vyo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xGs7Ch3Vyo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xGs7Ch3Vyo@OpenReview" onclick="foldPdfKimi('xGs7Ch3Vyo@OpenReview', this)" class="hr hr-fold">
        </div><div id="VeMC6Bn0ZB@OpenReview" class="panel paper" keywords="optimization,differential,citep,constrained,strategies,finance,equations,constraints,dynamic,problems">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VeMC6Bn0ZB" target="_blank" title="33/373"><span class="index notranslate">#33</span></a>
                <a id="title-VeMC6Bn0ZB@OpenReview" class="title-link" href="/venue/VeMC6Bn0ZB@OpenReview" target="_blank">Learning to Solve Differential Equation Constrained Optimization Problems</a>
                <a id="pdf-VeMC6Bn0ZB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VeMC6Bn0ZB@OpenReview', this)" data="https://openreview.net/pdf?id=VeMC6Bn0ZB">[PDF<sup id="pdf-stars-VeMC6Bn0ZB@OpenReview">4</sup>]</a>
                <a id="copy-VeMC6Bn0ZB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VeMC6Bn0ZB@OpenReview')">[Copy]</a>
                <a id="kimi-VeMC6Bn0ZB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VeMC6Bn0ZB@OpenReview', this)">[Kimi<sup id="kimi-stars-VeMC6Bn0ZB@OpenReview">3</sup>]</a>
                <a id="rel-VeMC6Bn0ZB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VeMC6Bn0ZB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VeMC6Bn0ZB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vincenzo Di Vito Francesco" target="_blank">Vincenzo Di Vito Francesco</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mostafa Mohammadian" target="_blank">Mostafa Mohammadian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyri Baker" target="_blank">Kyri Baker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ferdinando Fioretto" target="_blank">Ferdinando Fioretto</a>
            </p>
            <p id="summary-VeMC6Bn0ZB@OpenReview" class="summary">Differential equations (DE) constrained optimization plays a critical role in numerous scientific and engineering fields, including energy systems, aerospace engineering, ecology, and finance, where optimal configurations or control strategies must be determined for systems governed by ordinary or stochastic differential equations. Despite its significance, the computational challenges associated with these problems have limited their practical use. To address these limitations, this paper introduces a learning-based approach to DE-constrained optimization that combines techniques from proxy optimization \citep{kotary2021end} and neural differential equations \citep{chen2019neural}. The proposed approach uses a dual-network architecture, with one approximating the control strategies, focusing on steady-state constraints, and another solving the associated DEs. This combination enables the approximation of optimal strategies while accounting for dynamic constraints in near real-time.Experiments across problems in energy optimization and finance modeling show that this method provides full compliance with dynamic constraints and it produces results up to 25 times more precise than other methods which do not explicitly model the system's dynamic equations.</p>
            <p id="subjects-VeMC6Bn0ZB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-VeMC6Bn0ZB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VeMC6Bn0ZB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VeMC6Bn0ZB@OpenReview" onclick="foldPdfKimi('VeMC6Bn0ZB@OpenReview', this)" class="hr hr-fold">
        </div><div id="ofuLWn8DFZ@OpenReview" class="panel paper" keywords="poisoning,prediction,sets,reliability,conformal,calibration,reliable,data,quantification,training">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ofuLWn8DFZ" target="_blank" title="34/373"><span class="index notranslate">#34</span></a>
                <a id="title-ofuLWn8DFZ@OpenReview" class="title-link" href="/venue/ofuLWn8DFZ@OpenReview" target="_blank">Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning</a>
                <a id="pdf-ofuLWn8DFZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ofuLWn8DFZ@OpenReview', this)" data="https://openreview.net/pdf?id=ofuLWn8DFZ">[PDF<sup id="pdf-stars-ofuLWn8DFZ@OpenReview">3</sup>]</a>
                <a id="copy-ofuLWn8DFZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ofuLWn8DFZ@OpenReview')">[Copy]</a>
                <a id="kimi-ofuLWn8DFZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ofuLWn8DFZ@OpenReview', this)">[Kimi<sup id="kimi-stars-ofuLWn8DFZ@OpenReview">5</sup>]</a>
                <a id="rel-ofuLWn8DFZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ofuLWn8DFZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ofuLWn8DFZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Scholten" target="_blank">Yan Scholten</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Günnemann" target="_blank">Stephan Günnemann</a>
            </p>
            <p id="summary-ofuLWn8DFZ@OpenReview" class="summary">Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.</p>
            <p id="subjects-ofuLWn8DFZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ofuLWn8DFZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ofuLWn8DFZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ofuLWn8DFZ@OpenReview" onclick="foldPdfKimi('ofuLWn8DFZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="S85PP4xjFD@OpenReview" class="panel paper" keywords="contrastive,compositional,contrafusion,diffusion,images,t2i,contrastively,models,complex,pairs">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=S85PP4xjFD" target="_blank" title="35/373"><span class="index notranslate">#35</span></a>
                <a id="title-S85PP4xjFD@OpenReview" class="title-link" href="/venue/S85PP4xjFD@OpenReview" target="_blank">ContraFusion: Contrastively Improving Compositional Understanding in Diffusion Models via Fine-Grained Negative Images</a>
                <a id="pdf-S85PP4xjFD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('S85PP4xjFD@OpenReview', this)" data="https://openreview.net/pdf?id=S85PP4xjFD">[PDF<sup id="pdf-stars-S85PP4xjFD@OpenReview">18</sup>]</a>
                <a id="copy-S85PP4xjFD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('S85PP4xjFD@OpenReview')">[Copy]</a>
                <a id="kimi-S85PP4xjFD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('S85PP4xjFD@OpenReview', this)">[Kimi<sup id="kimi-stars-S85PP4xjFD@OpenReview">7</sup>]</a>
                <a id="rel-S85PP4xjFD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('S85PP4xjFD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-S85PP4xjFD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Han" target="_blank">Xu Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linghao Jin" target="_blank">Linghao Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofeng Liu" target="_blank">Xiaofeng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Pu Liang" target="_blank">Paul Pu Liang</a>
            </p>
            <p id="summary-S85PP4xjFD@OpenReview" class="summary">Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges through optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, COM-DIFF, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose CONTRAFUSION, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks. We will release our contrastive dataset to support the development of generative models.</p>
            <p id="subjects-S85PP4xjFD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-S85PP4xjFD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-S85PP4xjFD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-S85PP4xjFD@OpenReview" onclick="foldPdfKimi('S85PP4xjFD@OpenReview', this)" class="hr hr-fold">
        </div><div id="scI9307PLG@OpenReview" class="panel paper" keywords="bunns,message,squashing,diffusion,expressivity,bundle,graphs,bunn,neural,passing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=scI9307PLG" target="_blank" title="36/373"><span class="index notranslate">#36</span></a>
                <a id="title-scI9307PLG@OpenReview" class="title-link" href="/venue/scI9307PLG@OpenReview" target="_blank">Bundle Neural Network for message diffusion on graphs</a>
                <a id="pdf-scI9307PLG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('scI9307PLG@OpenReview', this)" data="https://openreview.net/pdf?id=scI9307PLG">[PDF<sup id="pdf-stars-scI9307PLG@OpenReview">11</sup>]</a>
                <a id="copy-scI9307PLG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('scI9307PLG@OpenReview')">[Copy]</a>
                <a id="kimi-scI9307PLG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('scI9307PLG@OpenReview', this)">[Kimi<sup id="kimi-stars-scI9307PLG@OpenReview">5</sup>]</a>
                <a id="rel-scI9307PLG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('scI9307PLG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-scI9307PLG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Bamberger" target="_blank">Jacob Bamberger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Barbero" target="_blank">Federico Barbero</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaowen Dong" target="_blank">Xiaowen Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Bronstein" target="_blank">Michael Bronstein</a>
            </p>
            <p id="summary-scI9307PLG@OpenReview" class="summary">The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a novel graph neural network architecture that operates via *message diffusion* on *flat vector bundles* — geometrically inspired structures that assign to each node a vector space and an orthogonal map. A BuNN layer evolves node features through a diffusion-type partial differential equation, where its discrete form acts as a special case of the recently introduced Sheaf Neural Network (SNN), effectively alleviating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate at larger scales, reducing over-squashing. We establish the universality of BuNNs in approximating feature transformations on infinite families of graphs with injective positional encodings, marking the first positive uniform expressivity result of its kind. We support our claims with formal analysis and synthetic experiments. Empirically, BuNNs perform strongly on heterophilic and long-range tasks, which demonstrates their robustness on a diverse range of challenging real-world tasks.</p>
            <p id="subjects-scI9307PLG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-scI9307PLG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-scI9307PLG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-scI9307PLG@OpenReview" onclick="foldPdfKimi('scI9307PLG@OpenReview', this)" class="hr hr-fold">
        </div><div id="sKYHBTAxVa@OpenReview" class="panel paper" keywords="livebench,contamination,llm,questions,benchmark,contains,tasks,release,benchmarks,harder">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sKYHBTAxVa" target="_blank" title="37/373"><span class="index notranslate">#37</span></a>
                <a id="title-sKYHBTAxVa@OpenReview" class="title-link" href="/venue/sKYHBTAxVa@OpenReview" target="_blank">LiveBench: A Challenging, Contamination-Free LLM Benchmark</a>
                <a id="pdf-sKYHBTAxVa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sKYHBTAxVa@OpenReview', this)" data="https://openreview.net/pdf?id=sKYHBTAxVa">[PDF<sup id="pdf-stars-sKYHBTAxVa@OpenReview">9</sup>]</a>
                <a id="copy-sKYHBTAxVa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sKYHBTAxVa@OpenReview')">[Copy]</a>
                <a id="kimi-sKYHBTAxVa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sKYHBTAxVa@OpenReview', this)">[Kimi<sup id="kimi-stars-sKYHBTAxVa@OpenReview">10</sup>]</a>
                <a id="rel-sKYHBTAxVa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sKYHBTAxVa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sKYHBTAxVa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Colin White" target="_blank">Colin White</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Dooley" target="_blank">Samuel Dooley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manley Roberts" target="_blank">Manley Roberts</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arka Pal" target="_blank">Arka Pal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Feuer" target="_blank">Benjamin Feuer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddhartha Jain" target="_blank">Siddhartha Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ravid Shwartz-Ziv" target="_blank">Ravid Shwartz-Ziv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neel Jain" target="_blank">Neel Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Khalid Saifullah" target="_blank">Khalid Saifullah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sreemanti Dey" target="_blank">Sreemanti Dey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shubh-Agrawal" target="_blank">Shubh-Agrawal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sandeep Sandha" target="_blank">Sandeep Sandha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddartha Naidu" target="_blank">Siddartha Naidu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chinmay Hegde" target="_blank">Chinmay Hegde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yann LeCun" target="_blank">Yann LeCun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Goldstein" target="_blank">Tom Goldstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Willie Neiswanger" target="_blank">Willie Neiswanger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Micah Goldblum" target="_blank">Micah Goldblum</a>
            </p>
            <p id="summary-sKYHBTAxVa@OpenReview" class="summary">Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.</p>
            <p id="subjects-sKYHBTAxVa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-sKYHBTAxVa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sKYHBTAxVa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sKYHBTAxVa@OpenReview" onclick="foldPdfKimi('sKYHBTAxVa@OpenReview', this)" class="hr hr-fold">
        </div><div id="JAMxRSXLFz@OpenReview" class="panel paper" keywords="disambiguation,task,questions,llm,llms,clarifying,agents,reasoning,viable,ambiguously">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=JAMxRSXLFz" target="_blank" title="38/373"><span class="index notranslate">#38</span></a>
                <a id="title-JAMxRSXLFz@OpenReview" class="title-link" href="/venue/JAMxRSXLFz@OpenReview" target="_blank">Active Task Disambiguation with LLMs</a>
                <a id="pdf-JAMxRSXLFz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('JAMxRSXLFz@OpenReview', this)" data="https://openreview.net/pdf?id=JAMxRSXLFz">[PDF<sup id="pdf-stars-JAMxRSXLFz@OpenReview">7</sup>]</a>
                <a id="copy-JAMxRSXLFz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('JAMxRSXLFz@OpenReview')">[Copy]</a>
                <a id="kimi-JAMxRSXLFz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('JAMxRSXLFz@OpenReview', this)">[Kimi<sup id="kimi-stars-JAMxRSXLFz@OpenReview">16</sup>]</a>
                <a id="rel-JAMxRSXLFz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('JAMxRSXLFz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-JAMxRSXLFz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Katarzyna Kobalczyk" target="_blank">Katarzyna Kobalczyk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolás Astorga" target="_blank">Nicolás Astorga</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tennison Liu" target="_blank">Tennison Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mihaela van der Schaar" target="_blank">Mihaela van der Schaar</a>
            </p>
            <p id="summary-JAMxRSXLFz@OpenReview" class="summary">Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems—frequent in real-world interactions—remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.</p>
            <p id="subjects-JAMxRSXLFz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-JAMxRSXLFz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-JAMxRSXLFz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-JAMxRSXLFz@OpenReview" onclick="foldPdfKimi('JAMxRSXLFz@OpenReview', this)" class="hr hr-fold">
        </div><div id="FxNNiUgtfa@OpenReview" class="panel paper" keywords="knowledge,language,capacity,laws,emph,bits,scaling,int8,washington,moe">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FxNNiUgtfa" target="_blank" title="39/373"><span class="index notranslate">#39</span></a>
                <a id="title-FxNNiUgtfa@OpenReview" class="title-link" href="/venue/FxNNiUgtfa@OpenReview" target="_blank">Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws</a>
                <a id="pdf-FxNNiUgtfa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FxNNiUgtfa@OpenReview', this)" data="https://openreview.net/pdf?id=FxNNiUgtfa">[PDF<sup id="pdf-stars-FxNNiUgtfa@OpenReview">16</sup>]</a>
                <a id="copy-FxNNiUgtfa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FxNNiUgtfa@OpenReview')">[Copy]</a>
                <a id="kimi-FxNNiUgtfa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FxNNiUgtfa@OpenReview', this)">[Kimi<sup id="kimi-stars-FxNNiUgtfa@OpenReview">9</sup>]</a>
                <a id="rel-FxNNiUgtfa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FxNNiUgtfa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FxNNiUgtfa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyuan Allen-Zhu" target="_blank">Zeyuan Allen-Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanzhi Li" target="_blank">Yuanzhi Li</a>
            </p>
            <p id="summary-FxNNiUgtfa@OpenReview" class="summary">Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \emph{bits} a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store \emph{2 bits of knowledge per parameter, even when quantized to int8}, and such knowledge can be flexibly extracted for downstream applications. More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity.</p>
            <p id="subjects-FxNNiUgtfa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FxNNiUgtfa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FxNNiUgtfa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FxNNiUgtfa@OpenReview" onclick="foldPdfKimi('FxNNiUgtfa@OpenReview', this)" class="hr hr-fold">
        </div><div id="ykuc5q381b@OpenReview" class="panel paper" keywords="retrieval,bright,queries,reasoning,documents,ndcg,intensive,challenging,benchmark,muennighoff">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ykuc5q381b" target="_blank" title="40/373"><span class="index notranslate">#40</span></a>
                <a id="title-ykuc5q381b@OpenReview" class="title-link" href="/venue/ykuc5q381b@OpenReview" target="_blank">BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</a>
                <a id="pdf-ykuc5q381b@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ykuc5q381b@OpenReview', this)" data="https://openreview.net/pdf?id=ykuc5q381b">[PDF<sup id="pdf-stars-ykuc5q381b@OpenReview">7</sup>]</a>
                <a id="copy-ykuc5q381b@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ykuc5q381b@OpenReview')">[Copy]</a>
                <a id="kimi-ykuc5q381b@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ykuc5q381b@OpenReview', this)">[Kimi<sup id="kimi-stars-ykuc5q381b@OpenReview">10</sup>]</a>
                <a id="rel-ykuc5q381b@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ykuc5q381b@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ykuc5q381b@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongjin SU" target="_blank">Hongjin SU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Howard Yen" target="_blank">Howard Yen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengzhou Xia" target="_blank">Mengzhou Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijia Shi" target="_blank">Weijia Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Muennighoff" target="_blank">Niklas Muennighoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han-yu Wang" target="_blank">Han-yu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liu Haisu" target="_blank">Liu Haisu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Shi" target="_blank">Quan Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zachary Siegel" target="_blank">Zachary Siegel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Tang" target="_blank">Michael Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoxi Sun" target="_blank">Ruoxi Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinsung Yoon" target="_blank">Jinsung Yoon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sercan Arik" target="_blank">Sercan Arik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danqi Chen" target="_blank">Danqi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Yu" target="_blank">Tao Yu</a>
            </p>
            <p id="summary-ykuc5q381b@OpenReview" class="summary">Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. Our dataset consists of 1,398 real-world queries spanning diverse domains such as economics, psychology, mathematics, coding, and more. These queries are drawn from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of 59.0 nDCG@10,1 produces a score of nDCG@10 of 18.0 on BRIGHT. We show that incorporating explicit reasoning about the query improves retrieval performance by up to 12.2 points. Moreover, incorporating retrieved documents from the top-performing retriever boosts question answering performance by over 6.6 points. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings.</p>
            <p id="subjects-ykuc5q381b@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ykuc5q381b@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ykuc5q381b@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ykuc5q381b@OpenReview" onclick="foldPdfKimi('ykuc5q381b@OpenReview', this)" class="hr hr-fold">
        </div><div id="yVQcr4qjD6@OpenReview" class="panel paper" keywords="calling,hammer,function,masking,device,models,robust,language,benchmarks,fitting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=yVQcr4qjD6" target="_blank" title="41/373"><span class="index notranslate">#41</span></a>
                <a id="title-yVQcr4qjD6@OpenReview" class="title-link" href="/venue/yVQcr4qjD6@OpenReview" target="_blank">Robust Function-Calling for On-Device Language Model via Function Masking</a>
                <a id="pdf-yVQcr4qjD6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('yVQcr4qjD6@OpenReview', this)" data="https://openreview.net/pdf?id=yVQcr4qjD6">[PDF<sup id="pdf-stars-yVQcr4qjD6@OpenReview">9</sup>]</a>
                <a id="copy-yVQcr4qjD6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('yVQcr4qjD6@OpenReview')">[Copy]</a>
                <a id="kimi-yVQcr4qjD6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('yVQcr4qjD6@OpenReview', this)">[Kimi<sup id="kimi-stars-yVQcr4qjD6@OpenReview">11</sup>]</a>
                <a id="rel-yVQcr4qjD6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('yVQcr4qjD6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-yVQcr4qjD6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Qiqiang" target="_blank">Lin Qiqiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muning Wen" target="_blank">Muning Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiuying Peng" target="_blank">Qiuying Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guanyu Nie" target="_blank">Guanyu Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junwei Liao" target="_blank">Junwei Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Wang" target="_blank">Jun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyun Mo" target="_blank">Xiaoyun Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiamu Zhou" target="_blank">Jiamu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Cheng" target="_blank">Cheng Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin Zhao" target="_blank">Yin Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Wang" target="_blank">Jun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weinan Zhang" target="_blank">Weinan Zhang</a>
            </p>
            <p id="summary-yVQcr4qjD6@OpenReview" class="summary">Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This paper identifies a critical gap in existing function-calling models, where performance varies significantly across benchmarks, often due to over-fitting to specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models’ sensitivity to irrelevant functions and incorporates function masking techniques to minimize over-fitting. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving state-of-the-art results. Our open-source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function-calling performance.</p>
            <p id="subjects-yVQcr4qjD6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-yVQcr4qjD6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-yVQcr4qjD6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-yVQcr4qjD6@OpenReview" onclick="foldPdfKimi('yVQcr4qjD6@OpenReview', this)" class="hr hr-fold">
        </div><div id="wmV4cIbgl6@OpenReview" class="panel paper" keywords="causal,causalrivers,discovery,bavaria,benchmarking,series,kit,river,eastern,areas">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wmV4cIbgl6" target="_blank" title="42/373"><span class="index notranslate">#42</span></a>
                <a id="title-wmV4cIbgl6@OpenReview" class="title-link" href="/venue/wmV4cIbgl6@OpenReview" target="_blank">CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series</a>
                <a id="pdf-wmV4cIbgl6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wmV4cIbgl6@OpenReview', this)" data="https://openreview.net/pdf?id=wmV4cIbgl6">[PDF<sup id="pdf-stars-wmV4cIbgl6@OpenReview">12</sup>]</a>
                <a id="copy-wmV4cIbgl6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wmV4cIbgl6@OpenReview')">[Copy]</a>
                <a id="kimi-wmV4cIbgl6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wmV4cIbgl6@OpenReview', this)">[Kimi<sup id="kimi-stars-wmV4cIbgl6@OpenReview">7</sup>]</a>
                <a id="rel-wmV4cIbgl6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wmV4cIbgl6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wmV4cIbgl6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gideon Stein" target="_blank">Gideon Stein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maha Shadaydeh" target="_blank">Maha Shadaydeh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Blunk" target="_blank">Jan Blunk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Penzel" target="_blank">Niklas Penzel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joachim Denzler" target="_blank">Joachim Denzler</a>
            </p>
            <p id="summary-wmV4cIbgl6@OpenReview" class="summary">Causal discovery, or identifying causal relationships from observational data, is a notoriously challenging task, with numerous methods proposed to tackle it.Despite this, in-the-wild evaluation is still lacking, as works frequently rely on synthetic data evaluation and sparse real-world examples under critical theoretical assumptions. Real-world causal structures, however, are often complex, evolving over time, non-linear, and influenced by unobserved factors, makingit hard for practitioners to select appropriate methods. To bridge this gap, we introduce CausalRivers, the largest in-the-wild causal discovery benchmarking kit for time series data to date.CausalRivers features an extensive dataset on river discharge that covers the complete eastern German territory (666 measurement stations) and the state of Bavaria (494 measurement stations). It spans the years 2019 to 2023 with a 15-minute temporal resolution. Further, we provide data from a recent flood around the Elbe River, as an event with a pronounced distributional shift. Leveraging multiple sources of information and time-series meta-data, we constructed two distinct causal ground truth graphs (Bavaria and eastern Germany).These graphs can be sampled to generate thousands of subgraphs to benchmark causal discovery across diverse and challenging settings.To demonstrate the utility of our benchmarking kit, we evaluate several causal discovery approaches through multiple experiments and introduce effective baselines, identifying several areas for enhancement.CausalRivers has the potential to facilitate robust evaluations and comparisons of causal discovery methods.Besides this primary purpose, we also expect that this dataset will be relevant for connected areas of research, such as time series forecasting and anomaly detection.Based on this, we hope to establish benchmark-driven method development that fosters advanced techniques for causal discovery, as is the case for many other areas of machine learning.</p>
            <p id="subjects-wmV4cIbgl6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wmV4cIbgl6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wmV4cIbgl6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wmV4cIbgl6@OpenReview" onclick="foldPdfKimi('wmV4cIbgl6@OpenReview', this)" class="hr hr-fold">
        </div><div id="wkHcXDv7cv@OpenReview" class="panel paper" keywords="bias,frequency,ssms,lti,initialization,tuning,tune,inborn,sequences,mechanisms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wkHcXDv7cv" target="_blank" title="43/373"><span class="index notranslate">#43</span></a>
                <a id="title-wkHcXDv7cv@OpenReview" class="title-link" href="/venue/wkHcXDv7cv@OpenReview" target="_blank">Tuning Frequency Bias of State Space Models</a>
                <a id="pdf-wkHcXDv7cv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wkHcXDv7cv@OpenReview', this)" data="https://openreview.net/pdf?id=wkHcXDv7cv">[PDF<sup id="pdf-stars-wkHcXDv7cv@OpenReview">9</sup>]</a>
                <a id="copy-wkHcXDv7cv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wkHcXDv7cv@OpenReview')">[Copy]</a>
                <a id="kimi-wkHcXDv7cv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wkHcXDv7cv@OpenReview', this)">[Kimi<sup id="kimi-stars-wkHcXDv7cv@OpenReview">4</sup>]</a>
                <a id="rel-wkHcXDv7cv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wkHcXDv7cv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wkHcXDv7cv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Annan Yu" target="_blank">Annan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongwei Lyu" target="_blank">Dongwei Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soon Hoe Lim" target="_blank">Soon Hoe Lim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael W Mahoney" target="_blank">Michael W Mahoney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=N. Benjamin Erichson" target="_blank">N. Benjamin Erichson</a>
            </p>
            <p id="summary-wkHcXDv7cv@OpenReview" class="summary">State space models (SSMs) leverage linear, time-invariant (LTI) systems to effectively learn sequences with long-range dependencies. By analyzing the transfer functions of LTI systems, we find that SSMs exhibit an implicit bias toward capturing low-frequency components more effectively than high-frequency ones. This behavior aligns with the broader notion of frequency bias in deep learning model training. We show that the initialization of an SSM assigns it an innate frequency bias and that training the model in a conventional way does not alter this bias. Based on our theory, we propose two mechanisms to tune frequency bias: either by scaling the initialization to tune the inborn frequency bias; or by applying a Sobolev-norm-based filter to adjust the sensitivity of the gradients to high-frequency inputs, which allows us to change the frequency bias via training. Using an image-denoising task, we empirically show that we can strengthen, weaken, or even reverse the frequency bias using both mechanisms. By tuning the frequency bias, we can also improve SSMs' performance on learning long-range sequences, averaging an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;88.26&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-43" style="width: 100%; display: inline-block; min-width: 2.763em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-44"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-45" style="font-family: MathJax_Main;">88.26</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-46" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>88.26</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-9">88.26\\%</script> accuracy on the Long-Range Arena (LRA) benchmark tasks.</p>
            <p id="subjects-wkHcXDv7cv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wkHcXDv7cv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wkHcXDv7cv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wkHcXDv7cv@OpenReview" onclick="foldPdfKimi('wkHcXDv7cv@OpenReview', this)" class="hr hr-fold">
        </div><div id="wg3rBImn3O@OpenReview" class="panel paper" keywords="shap,shapley,leverage,kernel,provably,value,score,agnostic,lundberg,accurate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wg3rBImn3O" target="_blank" title="44/373"><span class="index notranslate">#44</span></a>
                <a id="title-wg3rBImn3O@OpenReview" class="title-link" href="/venue/wg3rBImn3O@OpenReview" target="_blank">Provably Accurate Shapley Value Estimation via Leverage Score Sampling</a>
                <a id="pdf-wg3rBImn3O@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wg3rBImn3O@OpenReview', this)" data="https://openreview.net/pdf?id=wg3rBImn3O">[PDF<sup id="pdf-stars-wg3rBImn3O@OpenReview">5</sup>]</a>
                <a id="copy-wg3rBImn3O@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wg3rBImn3O@OpenReview')">[Copy]</a>
                <a id="kimi-wg3rBImn3O@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wg3rBImn3O@OpenReview', this)">[Kimi<sup id="kimi-stars-wg3rBImn3O@OpenReview">4</sup>]</a>
                <a id="rel-wg3rBImn3O@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wg3rBImn3O@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wg3rBImn3O@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Musco" target="_blank">Christopher Musco</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=R. Teal Witter" target="_blank">R. Teal Witter</a>
            </p>
            <p id="summary-wg3rBImn3O@OpenReview" class="summary">Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-47" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-48"><span class="mi" id="MathJax-Span-49" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-10">n</script> features, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-50" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-51"><span class="mi" id="MathJax-Span-52" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-53" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-54"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-55" style="font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.523em;"><span class="mi" id="MathJax-Span-56" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-57" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-11">O(2^n)</script> model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing *Leverage SHAP*, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-58" style="width: 5.211em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.22em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-59"><span class="mi" id="MathJax-Span-60" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-61" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-62" style="font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-63" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-64"></span><span class="mi" id="MathJax-Span-65" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">n</span><span class="mo" id="MathJax-Span-66" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-12">O(n\log n)</script> model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing *leverage score sampling*, a powerful regression tool. Beyond theoretical guarantees, we show that Leverage SHAP consistently outperforms even the highly optimized implementation of Kernel SHAP available in the ubiquitous SHAP library [Lundberg \&amp; Lee, 2017].</p>
            <p id="subjects-wg3rBImn3O@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wg3rBImn3O@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wg3rBImn3O@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wg3rBImn3O@OpenReview" onclick="foldPdfKimi('wg3rBImn3O@OpenReview', this)" class="hr hr-fold">
        </div><div id="wN3KaUXA5X@OpenReview" class="panel paper" keywords="syntax,graphics,programs,trees,diffusion,edit,write,program,anon,inverts">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wN3KaUXA5X" target="_blank" title="45/373"><span class="index notranslate">#45</span></a>
                <a id="title-wN3KaUXA5X@OpenReview" class="title-link" href="/venue/wN3KaUXA5X@OpenReview" target="_blank">Diffusion On Syntax Trees For Program Synthesis</a>
                <a id="pdf-wN3KaUXA5X@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wN3KaUXA5X@OpenReview', this)" data="https://openreview.net/pdf?id=wN3KaUXA5X">[PDF<sup id="pdf-stars-wN3KaUXA5X@OpenReview">6</sup>]</a>
                <a id="copy-wN3KaUXA5X@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wN3KaUXA5X@OpenReview')">[Copy]</a>
                <a id="kimi-wN3KaUXA5X@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wN3KaUXA5X@OpenReview', this)">[Kimi<sup id="kimi-stars-wN3KaUXA5X@OpenReview">3</sup>]</a>
                <a id="rel-wN3KaUXA5X@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wN3KaUXA5X@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wN3KaUXA5X@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shreyas Kapur" target="_blank">Shreyas Kapur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erik Jenner" target="_blank">Erik Jenner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stuart Russell" target="_blank">Stuart Russell</a>
            </p>
            <p id="summary-wN3KaUXA5X@OpenReview" class="summary">Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts "noise" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://td-anon.github.io.</p>
            <p id="subjects-wN3KaUXA5X@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wN3KaUXA5X@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wN3KaUXA5X@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wN3KaUXA5X@OpenReview" onclick="foldPdfKimi('wN3KaUXA5X@OpenReview', this)" class="hr hr-fold">
        </div><div id="wFD16gwpze@OpenReview" class="panel paper" keywords="law,covariance,laws,power,scaling,neural,data,spectra,teacher,student">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wFD16gwpze" target="_blank" title="46/373"><span class="index notranslate">#46</span></a>
                <a id="title-wFD16gwpze@OpenReview" class="title-link" href="/venue/wFD16gwpze@OpenReview" target="_blank">Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra</a>
                <a id="pdf-wFD16gwpze@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wFD16gwpze@OpenReview', this)" data="https://openreview.net/pdf?id=wFD16gwpze">[PDF<sup id="pdf-stars-wFD16gwpze@OpenReview">4</sup>]</a>
                <a id="copy-wFD16gwpze@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wFD16gwpze@OpenReview')">[Copy]</a>
                <a id="kimi-wFD16gwpze@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wFD16gwpze@OpenReview', this)">[Kimi<sup id="kimi-stars-wFD16gwpze@OpenReview">4</sup>]</a>
                <a id="rel-wFD16gwpze@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wFD16gwpze@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wFD16gwpze@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Roman Worschech" target="_blank">Roman Worschech</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernd Rosenow" target="_blank">Bernd Rosenow</a>
            </p>
            <p id="summary-wFD16gwpze@OpenReview" class="summary">Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra.For linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures.</p>
            <p id="subjects-wFD16gwpze@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wFD16gwpze@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wFD16gwpze@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wFD16gwpze@OpenReview" onclick="foldPdfKimi('wFD16gwpze@OpenReview', this)" class="hr hr-fold">
        </div><div id="vVCHWVBsLH@OpenReview" class="panel paper" keywords="cpwl,decompositions,polyhedron,piecewise,functions,decomposition,polyhedra,minimal,linear,convex">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vVCHWVBsLH" target="_blank" title="47/373"><span class="index notranslate">#47</span></a>
                <a id="title-vVCHWVBsLH@OpenReview" class="title-link" href="/venue/vVCHWVBsLH@OpenReview" target="_blank">Decomposition Polyhedra of Piecewise Linear Functions</a>
                <a id="pdf-vVCHWVBsLH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vVCHWVBsLH@OpenReview', this)" data="https://openreview.net/pdf?id=vVCHWVBsLH">[PDF<sup id="pdf-stars-vVCHWVBsLH@OpenReview">3</sup>]</a>
                <a id="copy-vVCHWVBsLH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vVCHWVBsLH@OpenReview')">[Copy]</a>
                <a id="kimi-vVCHWVBsLH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vVCHWVBsLH@OpenReview', this)">[Kimi<sup id="kimi-stars-vVCHWVBsLH@OpenReview">2</sup>]</a>
                <a id="rel-vVCHWVBsLH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vVCHWVBsLH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vVCHWVBsLH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marie-Charlotte Brandenburg" target="_blank">Marie-Charlotte Brandenburg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moritz Grillo" target="_blank">Moritz Grillo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Hertrich" target="_blank">Christoph Hertrich</a>
            </p>
            <p id="summary-vVCHWVBsLH@OpenReview" class="summary">In this paper we contribute to the frequently studied question of how to decompose a continuous piecewise linear (CPWL) function into a difference of two convex CPWL functions. Every CPWL function has infinitely many such decompositions, but for applications in optimization and neural network theory, it is crucial to find decompositions with as few linear pieces as possible. This is a highly challenging problem, as we further demonstrate by disproving a recently proposed approach by Tran and Wang [Minimal representations of tropical rational functions. Algebraic Statistics, 15(1):27–59, 2024]. To make the problem more tractable, we propose to fix an underlying polyhedral complex determining the possible locus of nonlinearity. Under this assumption, we prove that the set of decompositions forms a polyhedron that arises as intersection of two translated cones. We prove that irreducible decompositions correspond to the bounded faces of this polyhedron and minimal solutions must be vertices. We then identify cases with a unique minimal decomposition, and illustrate how our insights have consequences in the theory of submodular functions. Finally, we improve upon previous constructions of neural networks for a given convex CPWL function and apply our framework to obtain results in the nonconvex case.</p>
            <p id="subjects-vVCHWVBsLH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-vVCHWVBsLH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vVCHWVBsLH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vVCHWVBsLH@OpenReview" onclick="foldPdfKimi('vVCHWVBsLH@OpenReview', this)" class="hr hr-fold">
        </div><div id="v9EjwMM55Y@OpenReview" class="panel paper" keywords="unimatch,matching,molecular,meta,task,drug,hierarchical,molnet,universal,shot">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=v9EjwMM55Y" target="_blank" title="48/373"><span class="index notranslate">#48</span></a>
                <a id="title-v9EjwMM55Y@OpenReview" class="title-link" href="/venue/v9EjwMM55Y@OpenReview" target="_blank">UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery</a>
                <a id="pdf-v9EjwMM55Y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('v9EjwMM55Y@OpenReview', this)" data="https://openreview.net/pdf?id=v9EjwMM55Y">[PDF<sup id="pdf-stars-v9EjwMM55Y@OpenReview">10</sup>]</a>
                <a id="copy-v9EjwMM55Y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('v9EjwMM55Y@OpenReview')">[Copy]</a>
                <a id="kimi-v9EjwMM55Y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('v9EjwMM55Y@OpenReview', this)">[Kimi<sup id="kimi-stars-v9EjwMM55Y@OpenReview">7</sup>]</a>
                <a id="rel-v9EjwMM55Y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('v9EjwMM55Y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-v9EjwMM55Y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruifeng Li" target="_blank">Ruifeng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingqian Li" target="_blank">Mingqian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Liu" target="_blank">Wei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhua Zhou" target="_blank">Yuhua Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangxin Zhou" target="_blank">Xiangxin Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Yao" target="_blank">Yuan Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Zhang" target="_blank">Qiang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyang Chen" target="_blank">Hongyang Chen</a>
            </p>
            <p id="summary-v9EjwMM55Y@OpenReview" class="summary">Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels—atoms, substructures, and molecules—via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in ∆AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark.</p>
            <p id="subjects-v9EjwMM55Y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-v9EjwMM55Y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-v9EjwMM55Y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-v9EjwMM55Y@OpenReview" onclick="foldPdfKimi('v9EjwMM55Y@OpenReview', this)" class="hr hr-fold">
        </div><div id="twEvvkQqPS@OpenReview" class="panel paper" keywords="waloss,sham,kohn,dft,molecular,scf,scalability,hamiltonians,hamiltonian,physical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=twEvvkQqPS" target="_blank" title="49/373"><span class="index notranslate">#49</span></a>
                <a id="title-twEvvkQqPS@OpenReview" class="title-link" href="/venue/twEvvkQqPS@OpenReview" target="_blank">Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems</a>
                <a id="pdf-twEvvkQqPS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('twEvvkQqPS@OpenReview', this)" data="https://openreview.net/pdf?id=twEvvkQqPS">[PDF<sup id="pdf-stars-twEvvkQqPS@OpenReview">4</sup>]</a>
                <a id="copy-twEvvkQqPS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('twEvvkQqPS@OpenReview')">[Copy]</a>
                <a id="kimi-twEvvkQqPS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('twEvvkQqPS@OpenReview', this)">[Kimi<sup id="kimi-stars-twEvvkQqPS@OpenReview">5</sup>]</a>
                <a id="rel-twEvvkQqPS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('twEvvkQqPS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-twEvvkQqPS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yunyang Li" target="_blank">Yunyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaishuo Xia" target="_blank">Zaishuo Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Huang" target="_blank">Lin Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinran Wei" target="_blank">Xinran Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Harshe" target="_blank">Samuel Harshe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Yang" target="_blank">Han Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erpai Luo" target="_blank">Erpai Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zun Wang" target="_blank">Zun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jia Zhang" target="_blank">Jia Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Liu" target="_blank">Chang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Shao" target="_blank">Bin Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Gerstein" target="_blank">Mark Gerstein</a>
            </p>
            <p id="summary-twEvvkQqPS@OpenReview" class="summary">Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources required to construct the Kohn-Sham Hamiltonian. In response to these limitations, current research has employed deep-learning models to efficiently predict molecular and solid Hamiltonians, with roto-translational symmetries encoded in their neural networks. However, the scalability of prior models may be problematic when applied to large molecules, resulting in non-physical predictions of ground-state properties. In this study, we generate a substantially larger training set (PubChemQH) than used previously and use it to create a scalable model for DFT calculations with physical accuracy. For our model, we introduce a loss function derived from physical principles, which we call Wavefunction Alignment Loss (WALoss). WALoss involves performing a basis change on the predicted Hamiltonian to align it with the observed one; thus, the resulting differences can serve as a surrogate for orbital energy differences, allowing models to make better predictions for molecular orbitals and total energies than previously possible. WALoss also substantially accelerates self-consistent-field (SCF) DFT calculations. Here, we show it achieves a reduction in total energy prediction error by a factor of 1347 and an SCF calculation speed-up by a factor of 18\%. These substantial improvements set new benchmarks for achieving accurate and applicable predictions in larger molecular systems.</p>
            <p id="subjects-twEvvkQqPS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-twEvvkQqPS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-twEvvkQqPS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-twEvvkQqPS@OpenReview" onclick="foldPdfKimi('twEvvkQqPS@OpenReview', this)" class="hr hr-fold">
        </div><div id="sZQRUrvLn4@OpenReview" class="panel paper" keywords="gnns,count,graph,subgraph,subgraphs,substructures,message,passing,isomorphism,able">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sZQRUrvLn4" target="_blank" title="50/373"><span class="index notranslate">#50</span></a>
                <a id="title-sZQRUrvLn4@OpenReview" class="title-link" href="/venue/sZQRUrvLn4@OpenReview" target="_blank">Graph Neural Networks Can (Often) Count Substructures</a>
                <a id="pdf-sZQRUrvLn4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sZQRUrvLn4@OpenReview', this)" data="https://openreview.net/pdf?id=sZQRUrvLn4">[PDF<sup id="pdf-stars-sZQRUrvLn4@OpenReview">12</sup>]</a>
                <a id="copy-sZQRUrvLn4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sZQRUrvLn4@OpenReview')">[Copy]</a>
                <a id="kimi-sZQRUrvLn4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sZQRUrvLn4@OpenReview', this)">[Kimi<sup id="kimi-stars-sZQRUrvLn4@OpenReview">2</sup>]</a>
                <a id="rel-sZQRUrvLn4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sZQRUrvLn4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sZQRUrvLn4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Paolo Pellizzoni" target="_blank">Paolo Pellizzoni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Till Schulz" target="_blank">Till Schulz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karsten Borgwardt" target="_blank">Karsten Borgwardt</a>
            </p>
            <p id="summary-sZQRUrvLn4@OpenReview" class="summary">Message passing graph neural networks (GNNs) are known to have limited expressive power in their ability to distinguish some non-isomorphic graphs.Because of this, it is well known that they are unable to detect or count arbitrary graph substructures (i.e., solving the subgraph isomorphism problem), a task that is of great importance for several types of graph-structured data. However, we observe that GNNs are in fact able to count graph patterns quite accurately across several real-world graph datasets.Motivated by this observation, we provide an analysis of the subgraph-counting capabilities of GNNs beyond the worst case, deriving several sufficient conditions for GNNs to be able to count subgraphs and, more importantly, to be able to sample-efficiently learn to count subgraphs. Moreover, we develop novel dynamic programming algorithms for solving the subgraph isomorphism problem on restricted classes of pattern and target graphs, and show that message-passing GNNs can efficiently simulate these dynamic programs. Finally, we empirically validate that our sufficient conditions for GNNs to count subgraphs hold on many real-world datasets, providing a theoretically-grounded explanation to our motivating observations.</p>
            <p id="subjects-sZQRUrvLn4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-sZQRUrvLn4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sZQRUrvLn4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sZQRUrvLn4@OpenReview" onclick="foldPdfKimi('sZQRUrvLn4@OpenReview', this)" class="hr hr-fold">
        </div><div id="qxRoo7ULCo@OpenReview" class="panel paper" keywords="panoramic,immersive,dynamic,360,4k4dgen,circ,panorama,resolution,textbf,lifting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qxRoo7ULCo" target="_blank" title="51/373"><span class="index notranslate">#51</span></a>
                <a id="title-qxRoo7ULCo@OpenReview" class="title-link" href="/venue/qxRoo7ULCo@OpenReview" target="_blank">4K4DGen: Panoramic 4D Generation at 4K Resolution</a>
                <a id="pdf-qxRoo7ULCo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qxRoo7ULCo@OpenReview', this)" data="https://openreview.net/pdf?id=qxRoo7ULCo">[PDF<sup id="pdf-stars-qxRoo7ULCo@OpenReview">4</sup>]</a>
                <a id="copy-qxRoo7ULCo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qxRoo7ULCo@OpenReview')">[Copy]</a>
                <a id="kimi-qxRoo7ULCo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qxRoo7ULCo@OpenReview', this)">[Kimi<sup id="kimi-stars-qxRoo7ULCo@OpenReview">4</sup>]</a>
                <a id="rel-qxRoo7ULCo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qxRoo7ULCo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qxRoo7ULCo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Renjie Li" target="_blank">Renjie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Panwang Pan" target="_blank">Panwang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bangbang Yang" target="_blank">Bangbang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dejia Xu" target="_blank">Dejia Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijie Zhou" target="_blank">Shijie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=zhang xuanyang" target="_blank">zhang xuanyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeming Li" target="_blank">Zeming Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Achuta Kadambi" target="_blank">Achuta Kadambi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhangyang Wang" target="_blank">Zhangyang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengzhong Tu" target="_blank">Zhengzhong Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiwen Fan" target="_blank">Zhiwen Fan</a>
            </p>
            <p id="summary-qxRoo7ULCo@OpenReview" class="summary">The blooming of virtual reality and augmented reality (VR/AR) technologies has driven an increasing demand for the creation of high-quality, immersive, and dynamic environments. However, existing generative techniques either focus solely on dynamic objects or perform outpainting from a single perspective image, failing to meet the requirements of VR/AR applications that need free-viewpoint, 360<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2218;&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-67" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-68"><span class="msubsup" id="MathJax-Span-69"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-70"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="texatom" id="MathJax-Span-71"><span class="mrow" id="MathJax-Span-72"><span class="mo" id="MathJax-Span-73" style="font-size: 70.7%; font-family: MathJax_Main;">∘</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo>∘</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-13">^{\circ}</script> virtual views where users can move in all directions. In this work, we tackle the challenging task of elevating a single panorama to an immersive 4D experience. For the first time, we demonstrate the capability to generate omnidirectional dynamic scenes with 360<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2218;&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-74" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-75"><span class="msubsup" id="MathJax-Span-76"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-77"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="texatom" id="MathJax-Span-78"><span class="mrow" id="MathJax-Span-79"><span class="mo" id="MathJax-Span-80" style="font-size: 70.7%; font-family: MathJax_Main;">∘</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo>∘</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-14">^{\circ}</script> views at 4K (4096 <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-81" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-82"><span class="mo" id="MathJax-Span-83" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-15">\times</script> 2048) resolution, thereby providing an immersive user experience. Our method introduces a pipeline that facilitates natural scene animations and optimizes a set of dynamic Gaussians using efficient splatting techniques for real-time exploration. To overcome the lack of scene-scale annotated 4D data and models, especially in panoramic formats, we propose a novel \textbf{Panoramic Denoiser} that adapts generic 2D diffusion priors to animate consistently in 360<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2218;&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-84" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-85"><span class="msubsup" id="MathJax-Span-86"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-87"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="texatom" id="MathJax-Span-88"><span class="mrow" id="MathJax-Span-89"><span class="mo" id="MathJax-Span-90" style="font-size: 70.7%; font-family: MathJax_Main;">∘</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo>∘</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-16">^{\circ}</script> images, transforming them into panoramic videos with dynamic scenes at targeted regions. Subsequently, we propose \textbf{Dynamic Panoramic Lifting} to elevate the panoramic video into a 4D immersive environment while preserving spatial and temporal consistency. By transferring prior knowledge from 2D models in the perspective domain to the panoramic domain and the 4D lifting with spatial appearance and geometry regularization, we achieve high-quality Panorama-to-4D generation at a resolution of 4K for the first time.</p>
            <p id="subjects-qxRoo7ULCo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qxRoo7ULCo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qxRoo7ULCo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qxRoo7ULCo@OpenReview" onclick="foldPdfKimi('qxRoo7ULCo@OpenReview', this)" class="hr hr-fold">
        </div><div id="qPx3i9sMxv@OpenReview" class="panel paper" keywords="audio,spatial,generation,soundscapes,stereo,guidance,immersive,ears,bewo,spatialsonic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qPx3i9sMxv" target="_blank" title="52/373"><span class="index notranslate">#52</span></a>
                <a id="title-qPx3i9sMxv@OpenReview" class="title-link" href="/venue/qPx3i9sMxv@OpenReview" target="_blank">Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation</a>
                <a id="pdf-qPx3i9sMxv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qPx3i9sMxv@OpenReview', this)" data="https://openreview.net/pdf?id=qPx3i9sMxv">[PDF<sup id="pdf-stars-qPx3i9sMxv@OpenReview">5</sup>]</a>
                <a id="copy-qPx3i9sMxv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qPx3i9sMxv@OpenReview')">[Copy]</a>
                <a id="kimi-qPx3i9sMxv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qPx3i9sMxv@OpenReview', this)">[Kimi<sup id="kimi-stars-qPx3i9sMxv@OpenReview">3</sup>]</a>
                <a id="rel-qPx3i9sMxv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qPx3i9sMxv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qPx3i9sMxv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peiwen Sun" target="_blank">Peiwen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sitong Cheng" target="_blank">Sitong Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangtai Li" target="_blank">Xiangtai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Ye" target="_blank">Zhen Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huadai Liu" target="_blank">Huadai Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Honggang Zhang" target="_blank">Honggang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Xue" target="_blank">Wei Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yike Guo" target="_blank">Yike Guo</a>
            </p>
            <p id="summary-qPx3i9sMxv@OpenReview" class="summary">Recently, diffusion models have achieved great success in mono-channel audio generation.However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions.Controlling stereo audio with spatial contexts remains challenging due to high data costs and unstable generative models. To the best of our knowledge, this work represents the first attempt to address these issues.We first construct a large-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant soundscapes and descriptions even including moving and multiple sources.Beyond text modality, we have also acquired a set of images and rationally paired stereo audios through retrieval to advance multimodal generation. Existing audio generation models tend to generate rather random spatial audio. To provide accurate guidance for Latent Diffusion Models, we introduce the SpatialSonic model utilizing spatial-aware encoders and azimuth state matrices to reveal reasonable spatial guidance. By leveraging spatial guidance, our unified model not only achieves the objective of generating immersive and controllable spatial audio from text and image but also enables interactive audio generation during inference.Finally, under fair settings, we conduct subjective and objective evaluations on simulated and real-world data to compare our approach with prevailing methods. The results demonstrate the effectiveness of our method, highlighting its capability to generate spatial audio that adheres to physical rules.Our demos are available at https://immersive-audio.github.io/. Our code, model, and dataset will be released soon.</p>
            <p id="subjects-qPx3i9sMxv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qPx3i9sMxv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qPx3i9sMxv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qPx3i9sMxv@OpenReview" onclick="foldPdfKimi('qPx3i9sMxv@OpenReview', this)" class="hr hr-fold">
        </div><div id="q3EbOXb4y1@OpenReview" class="panel paper" keywords="retri3d,retrieval,3dngrs,graphics,stores,ngrs,representation,viewing,neural,ngr">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=q3EbOXb4y1" target="_blank" title="53/373"><span class="index notranslate">#53</span></a>
                <a id="title-q3EbOXb4y1@OpenReview" class="title-link" href="/venue/q3EbOXb4y1@OpenReview" target="_blank">Retri3D: 3D Neural Graphics Representation Retrieval</a>
                <a id="pdf-q3EbOXb4y1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('q3EbOXb4y1@OpenReview', this)" data="https://openreview.net/pdf?id=q3EbOXb4y1">[PDF<sup id="pdf-stars-q3EbOXb4y1@OpenReview">6</sup>]</a>
                <a id="copy-q3EbOXb4y1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('q3EbOXb4y1@OpenReview')">[Copy]</a>
                <a id="kimi-q3EbOXb4y1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('q3EbOXb4y1@OpenReview', this)">[Kimi<sup id="kimi-stars-q3EbOXb4y1@OpenReview">2</sup>]</a>
                <a id="rel-q3EbOXb4y1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('q3EbOXb4y1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-q3EbOXb4y1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yushi Guan" target="_blank">Yushi Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Kwan" target="_blank">Daniel Kwan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jean Dandurand" target="_blank">Jean Dandurand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Yan" target="_blank">Xi Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruofan Liang" target="_blank">Ruofan Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Zhang" target="_blank">Yuxuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nilesh Jain" target="_blank">Nilesh Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nilesh Ahuja" target="_blank">Nilesh Ahuja</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Selvakumar Panneer" target="_blank">Selvakumar Panneer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nandita Vijaykumar" target="_blank">Nandita Vijaykumar</a>
            </p>
            <p id="summary-q3EbOXb4y1@OpenReview" class="summary">Learnable 3D Neural Graphics Representations (3DNGR) have emerged as promising 3D representations for reconstructing 3D scenes from 2D images. Numerous works, including Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and their variants, have significantly enhanced the quality of these representations. The ease of construction from 2D images, suitability for online viewing/sharing, and applications in game/art design downstream tasks make it a vital 3D representation, with potential creation of large numbers of such 3D models. This necessitates large data stores, local or online, to save 3D visual data in these formats. However, no existing framework enables accurate retrieval of stored 3DNGRs. In this work, we propose, Retri3D, a framework that enables accurate and efficient retrieval of 3D scenes represented as NGRs from large data stores using text queries. We introduce a novel Neural Field Artifact Analysis technique, combined with a Smart Camera Movement Module, to select clean views and navigate pre-trained 3DNGRs. These techniques enable accurate retrieval by selecting the best viewing directions in the 3D scene for high-quality visual feature embeddings. We demonstrate that Retri3D is compatible with any NGR representation. On the LERF and ScanNet++ datasets, we show significant improvement in retrieval accuracy compared to existing techniques, while being orders of magnitude faster and storage efficient.</p>
            <p id="subjects-q3EbOXb4y1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-q3EbOXb4y1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-q3EbOXb4y1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-q3EbOXb4y1@OpenReview" onclick="foldPdfKimi('q3EbOXb4y1@OpenReview', this)" class="hr hr-fold">
        </div><div id="p4cLtzk4oe@OpenReview" class="panel paper" keywords="memorization,ending,diffusion,attention,memorized,bright,task,patches,models,local">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=p4cLtzk4oe" target="_blank" title="54/373"><span class="index notranslate">#54</span></a>
                <a id="title-p4cLtzk4oe@OpenReview" class="title-link" href="/venue/p4cLtzk4oe@OpenReview" target="_blank">Exploring Local Memorization in Diffusion Models via Bright Ending Attention</a>
                <a id="pdf-p4cLtzk4oe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('p4cLtzk4oe@OpenReview', this)" data="https://openreview.net/pdf?id=p4cLtzk4oe">[PDF<sup id="pdf-stars-p4cLtzk4oe@OpenReview">6</sup>]</a>
                <a id="copy-p4cLtzk4oe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('p4cLtzk4oe@OpenReview')">[Copy]</a>
                <a id="kimi-p4cLtzk4oe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('p4cLtzk4oe@OpenReview', this)">[Kimi<sup id="kimi-stars-p4cLtzk4oe@OpenReview">4</sup>]</a>
                <a id="rel-p4cLtzk4oe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('p4cLtzk4oe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-p4cLtzk4oe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Chen" target="_blank">Chen Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daochang Liu" target="_blank">Daochang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mubarak Shah" target="_blank">Mubarak Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Xu" target="_blank">Chang Xu</a>
            </p>
            <p id="summary-p4cLtzk4oe@OpenReview" class="summary">In this paper, we identify and leverage a novel `bright ending' (BE) anomaly in diffusion models prone to memorizing training images to address a new task: locating localized memorization regions within these models. BE refers to a distinct cross-attention pattern observed in text-to-image generations using diffusion models. Specifically, memorized image patches exhibit significantly greater attention to the end token during the final inference step compared to non-memorized patches. This attention map effectively highlights regions where the generated image replicates training data. Furthermore, driven by our observation that local memorization significantly underperforms in existing tasks of measuring, detecting, and mitigating memorization in diffusion models compared to global memorization, we propose a simple yet effective method to integrate BE and the results of the new localization task into these existing frameworks. This integration effectively improves their performances by narrowing the performance gap caused by local memorization. Our results not only demonstrate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon.</p>
            <p id="subjects-p4cLtzk4oe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-p4cLtzk4oe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-p4cLtzk4oe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-p4cLtzk4oe@OpenReview" onclick="foldPdfKimi('p4cLtzk4oe@OpenReview', this)" class="hr hr-fold">
        </div><div id="o1Et3MogPw@OpenReview" class="panel paper" keywords="agents,ioa,internet,agent,frameworks,weaving,heterogeneous,intelligence,collaboration,capable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=o1Et3MogPw" target="_blank" title="55/373"><span class="index notranslate">#55</span></a>
                <a id="title-o1Et3MogPw@OpenReview" class="title-link" href="/venue/o1Et3MogPw@OpenReview" target="_blank">Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a>
                <a id="pdf-o1Et3MogPw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('o1Et3MogPw@OpenReview', this)" data="https://openreview.net/pdf?id=o1Et3MogPw">[PDF<sup id="pdf-stars-o1Et3MogPw@OpenReview">13</sup>]</a>
                <a id="copy-o1Et3MogPw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('o1Et3MogPw@OpenReview')">[Copy]</a>
                <a id="kimi-o1Et3MogPw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('o1Et3MogPw@OpenReview', this)">[Kimi<sup id="kimi-stars-o1Et3MogPw@OpenReview">19</sup>]</a>
                <a id="rel-o1Et3MogPw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('o1Et3MogPw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-o1Et3MogPw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weize Chen" target="_blank">Weize Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziming You" target="_blank">Ziming You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Li" target="_blank">Ran Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=yitong guan" target="_blank">yitong guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Qian" target="_blank">Chen Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenyang Zhao" target="_blank">Chenyang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Yang" target="_blank">Cheng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruobing Xie" target="_blank">Ruobing Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Liu" target="_blank">Zhiyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maosong Sun" target="_blank">Maosong Sun</a>
            </p>
            <p id="summary-o1Et3MogPw@OpenReview" class="summary">The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research.</p>
            <p id="subjects-o1Et3MogPw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-o1Et3MogPw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-o1Et3MogPw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-o1Et3MogPw@OpenReview" onclick="foldPdfKimi('o1Et3MogPw@OpenReview', this)" class="hr hr-fold">
        </div><div id="n9PDaFNi8t@OpenReview" class="panel paper" keywords="gui,grounding,atlas,vlms,agentic,source,foundation,geminiprovision,ood,innovations">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=n9PDaFNi8t" target="_blank" title="56/373"><span class="index notranslate">#56</span></a>
                <a id="title-n9PDaFNi8t@OpenReview" class="title-link" href="/venue/n9PDaFNi8t@OpenReview" target="_blank">OS-ATLAS: Foundation Action Model for Generalist GUI Agents</a>
                <a id="pdf-n9PDaFNi8t@OpenReview" class="title-pdf notranslate" onclick="togglePdf('n9PDaFNi8t@OpenReview', this)" data="https://openreview.net/pdf?id=n9PDaFNi8t">[PDF<sup id="pdf-stars-n9PDaFNi8t@OpenReview">11</sup>]</a>
                <a id="copy-n9PDaFNi8t@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('n9PDaFNi8t@OpenReview')">[Copy]</a>
                <a id="kimi-n9PDaFNi8t@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('n9PDaFNi8t@OpenReview', this)">[Kimi<sup id="kimi-stars-n9PDaFNi8t@OpenReview">9</sup>]</a>
                <a id="rel-n9PDaFNi8t@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('n9PDaFNi8t@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-n9PDaFNi8t@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyong Wu" target="_blank">Zhiyong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu Wu" target="_blank">Zhenyu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangzhi Xu" target="_blank">Fangzhi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yian Wang" target="_blank">Yian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiushi Sun" target="_blank">Qiushi Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengyou Jia" target="_blank">Chengyou Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kanzhi Cheng" target="_blank">Kanzhi Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Ding" target="_blank">Zichen Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liheng Chen" target="_blank">Liheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Pu Liang" target="_blank">Paul Pu Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Qiao" target="_blank">Yu Qiao</a>
            </p>
            <p id="summary-n9PDaFNi8t@OpenReview" class="summary">Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas —a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested substantial engineering effort into developing a toolkit for synthesizing multi-platform GUI grounding data. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs. All our data, code, and models will be made publicly available.</p>
            <p id="subjects-n9PDaFNi8t@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-n9PDaFNi8t@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-n9PDaFNi8t@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-n9PDaFNi8t@OpenReview" onclick="foldPdfKimi('n9PDaFNi8t@OpenReview', this)" class="hr hr-fold">
        </div><div id="n0OtGl6VGb@OpenReview" class="panel paper" keywords="cache,kivi,think,pruning,memory,thinner,query,eviction,long,inefficiencies">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=n0OtGl6VGb" target="_blank" title="57/373"><span class="index notranslate">#57</span></a>
                <a id="title-n0OtGl6VGb@OpenReview" class="title-link" href="/venue/n0OtGl6VGb@OpenReview" target="_blank">ThinK: Thinner Key Cache by Query-Driven Pruning</a>
                <a id="pdf-n0OtGl6VGb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('n0OtGl6VGb@OpenReview', this)" data="https://openreview.net/pdf?id=n0OtGl6VGb">[PDF<sup id="pdf-stars-n0OtGl6VGb@OpenReview">10</sup>]</a>
                <a id="copy-n0OtGl6VGb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('n0OtGl6VGb@OpenReview')">[Copy]</a>
                <a id="kimi-n0OtGl6VGb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('n0OtGl6VGb@OpenReview', this)">[Kimi<sup id="kimi-stars-n0OtGl6VGb@OpenReview">10</sup>]</a>
                <a id="rel-n0OtGl6VGb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('n0OtGl6VGb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-n0OtGl6VGb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhui Xu" target="_blank">Yuhui Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanming Jie" target="_blank">Zhanming Jie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanze Dong" target="_blank">Hanze Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Wang" target="_blank">Lei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xudong Lu" target="_blank">Xudong Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aojun Zhou" target="_blank">Aojun Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amrita Saha" target="_blank">Amrita Saha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caiming Xiong" target="_blank">Caiming Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Doyen Sahoo" target="_blank">Doyen Sahoo</a>
            </p>
            <p id="summary-n0OtGl6VGb@OpenReview" class="summary">Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences.This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights.In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve 2.8x peak memory reduction while maintaining nearly the same quality, enabling a batch size increase from 4x (with KIVI alone) to 5x when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of \our, establishing a new baseline algorithm for efficient LLM deployment without compromising performance.</p>
            <p id="subjects-n0OtGl6VGb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-n0OtGl6VGb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-n0OtGl6VGb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-n0OtGl6VGb@OpenReview" onclick="foldPdfKimi('n0OtGl6VGb@OpenReview', this)" class="hr hr-fold">
        </div><div id="m9RNBZewW2@OpenReview" class="panel paper" keywords="restoration,mgfr,facial,face,quality,attribute,modal,images,false,illusions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=m9RNBZewW2" target="_blank" title="58/373"><span class="index notranslate">#58</span></a>
                <a id="title-m9RNBZewW2@OpenReview" class="title-link" href="/venue/m9RNBZewW2@OpenReview" target="_blank">Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model</a>
                <a id="pdf-m9RNBZewW2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('m9RNBZewW2@OpenReview', this)" data="https://openreview.net/pdf?id=m9RNBZewW2">[PDF<sup id="pdf-stars-m9RNBZewW2@OpenReview">7</sup>]</a>
                <a id="copy-m9RNBZewW2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('m9RNBZewW2@OpenReview')">[Copy]</a>
                <a id="kimi-m9RNBZewW2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('m9RNBZewW2@OpenReview', this)">[Kimi<sup id="kimi-stars-m9RNBZewW2@OpenReview">4</sup>]</a>
                <a id="rel-m9RNBZewW2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('m9RNBZewW2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-m9RNBZewW2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Keda TAO" target="_blank">Keda TAO</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinjin Gu" target="_blank">Jinjin Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulun Zhang" target="_blank">Yulun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiucheng Wang" target="_blank">Xiucheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Cheng" target="_blank">Nan Cheng</a>
            </p>
            <p id="summary-m9RNBZewW2@OpenReview" class="summary">We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR) technique designed to improve the quality of facial image restoration from low-quality inputs. Leveraging a blend of attribute text prompts, high-quality reference images, and identity information, MGFR can mitigate the generation of false facial attributes and identities often associated with generative face restoration methods. By incorporating a dual-control adapter and a two-stage training strategy, our method effectively utilizes multi-modal prior information for targeted restoration tasks. We also present the Reface-HQ dataset, comprising over 23,000 high-resolution facial images across 5,000 identities, to address the need for reference face training images. Our approach achieves superior visual quality in restoring facial details under severe degradation and allows for controlled restoration processes, enhancing the accuracy of identity preservation and attribute correction. Including negative quality samples and attribute prompts in the training further refines the model's ability to generate detailed and perceptually accurate images.</p>
            <p id="subjects-m9RNBZewW2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-m9RNBZewW2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-m9RNBZewW2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-m9RNBZewW2@OpenReview" onclick="foldPdfKimi('m9RNBZewW2@OpenReview', this)" class="hr hr-fold">
        </div><div id="lzdFImKK8w@OpenReview" class="panel paper" keywords="protein,delta,folding,skempi,inverse,boltzmann,conformational,mutational,5134,4324">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=lzdFImKK8w" target="_blank" title="59/373"><span class="index notranslate">#59</span></a>
                <a id="title-lzdFImKK8w@OpenReview" class="title-link" href="/venue/lzdFImKK8w@OpenReview" target="_blank">Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions</a>
                <a id="pdf-lzdFImKK8w@OpenReview" class="title-pdf notranslate" onclick="togglePdf('lzdFImKK8w@OpenReview', this)" data="https://openreview.net/pdf?id=lzdFImKK8w">[PDF<sup id="pdf-stars-lzdFImKK8w@OpenReview">6</sup>]</a>
                <a id="copy-lzdFImKK8w@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('lzdFImKK8w@OpenReview')">[Copy]</a>
                <a id="kimi-lzdFImKK8w@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('lzdFImKK8w@OpenReview', this)">[Kimi<sup id="kimi-stars-lzdFImKK8w@OpenReview">4</sup>]</a>
                <a id="rel-lzdFImKK8w@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('lzdFImKK8w@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-lzdFImKK8w@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoran Jiao" target="_blank">Xiaoran Jiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weian Mao" target="_blank">Weian Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wengong Jin" target="_blank">Wengong Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiyuan Yang" target="_blank">Peiyuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chen" target="_blank">Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunhua Shen" target="_blank">Chunhua Shen</a>
            </p>
            <p id="summary-lzdFImKK8w@OpenReview" class="summary">Predicting the change in binding free energy (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-91" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-92"><span class="mi" id="MathJax-Span-93" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-94" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-95" style="font-family: MathJax_Math-italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi mathvariant="normal">Δ</mi><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-17">\Delta \Delta G</script>) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design.Due to the scarcity of experimental <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-96" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-97"><span class="mi" id="MathJax-Span-98" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-99" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-100" style="font-family: MathJax_Math-italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi mathvariant="normal">Δ</mi><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-18">\Delta\Delta G</script> data, existing methods focus on pre-training, while alignment receives less attention.In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-101" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-102"><span class="mi" id="MathJax-Span-103" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-104" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-105" style="font-family: MathJax_Math-italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi mathvariant="normal">Δ</mi><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-19">\Delta\Delta G</script> prediction.We begin by analyzing the thermodynamic definition of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-106" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-107"><span class="mi" id="MathJax-Span-108" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-109" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-110" style="font-family: MathJax_Math-italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi mathvariant="normal">Δ</mi><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-20">\Delta\Delta G</script> and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes’ theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-111" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-112"><span class="mi" id="MathJax-Span-113" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-114" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-115" style="font-family: MathJax_Math-italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi mathvariant="normal">Δ</mi><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-21">\Delta\Delta G</script> estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0394;&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-116" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-117"><span class="mi" id="MathJax-Span-118" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-119" style="font-family: MathJax_Main;">Δ</span><span class="mi" id="MathJax-Span-120" style="font-family: MathJax_Math-italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi mathvariant="normal">Δ</mi><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-22">\Delta \Delta G</script> thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance.Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively.Futhermore, we demonstrate the capability of our method on bindingenergy prediction, protein-protein docking and antibody optimization tasks.</p>
            <p id="subjects-lzdFImKK8w@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-lzdFImKK8w@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-lzdFImKK8w@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-lzdFImKK8w@OpenReview" onclick="foldPdfKimi('lzdFImKK8w@OpenReview', this)" class="hr hr-fold">
        </div><div id="l4fMj4Vnly@OpenReview" class="panel paper" keywords="audio,explanations,adiff,language,explaining,captioning,baseline,differences,events,difference">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=l4fMj4Vnly" target="_blank" title="60/373"><span class="index notranslate">#60</span></a>
                <a id="title-l4fMj4Vnly@OpenReview" class="title-link" href="/venue/l4fMj4Vnly@OpenReview" target="_blank">ADIFF: Explaining audio difference using natural language</a>
                <a id="pdf-l4fMj4Vnly@OpenReview" class="title-pdf notranslate" onclick="togglePdf('l4fMj4Vnly@OpenReview', this)" data="https://openreview.net/pdf?id=l4fMj4Vnly">[PDF<sup id="pdf-stars-l4fMj4Vnly@OpenReview">4</sup>]</a>
                <a id="copy-l4fMj4Vnly@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('l4fMj4Vnly@OpenReview')">[Copy]</a>
                <a id="kimi-l4fMj4Vnly@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('l4fMj4Vnly@OpenReview', this)">[Kimi<sup id="kimi-stars-l4fMj4Vnly@OpenReview">1</sup>]</a>
                <a id="rel-l4fMj4Vnly@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('l4fMj4Vnly@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-l4fMj4Vnly@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Soham Deshmukh" target="_blank">Soham Deshmukh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuo Han" target="_blank">Shuo Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rita Singh" target="_blank">Rita Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bhiksha Raj" target="_blank">Bhiksha Raj</a>
            </p>
            <p id="summary-l4fMj4Vnly@OpenReview" class="summary">Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model’s ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences.</p>
            <p id="subjects-l4fMj4Vnly@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-l4fMj4Vnly@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-l4fMj4Vnly@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-l4fMj4Vnly@OpenReview" onclick="foldPdfKimi('l4fMj4Vnly@OpenReview', this)" class="hr hr-fold">
        </div><div id="q5EZ7gKcnW@OpenReview" class="panel paper" keywords="sft,supervision,rlhf,unreliable,ilr,feedback,refinement,demonstrations,lms,human">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=q5EZ7gKcnW" target="_blank" title="61/373"><span class="index notranslate">#61</span></a>
                <a id="title-q5EZ7gKcnW@OpenReview" class="title-link" href="/venue/q5EZ7gKcnW@OpenReview" target="_blank">Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision</a>
                <a id="pdf-q5EZ7gKcnW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('q5EZ7gKcnW@OpenReview', this)" data="https://openreview.net/pdf?id=q5EZ7gKcnW">[PDF<sup id="pdf-stars-q5EZ7gKcnW@OpenReview">5</sup>]</a>
                <a id="copy-q5EZ7gKcnW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('q5EZ7gKcnW@OpenReview')">[Copy]</a>
                <a id="kimi-q5EZ7gKcnW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('q5EZ7gKcnW@OpenReview', this)">[Kimi<sup id="kimi-stars-q5EZ7gKcnW@OpenReview">6</sup>]</a>
                <a id="rel-q5EZ7gKcnW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('q5EZ7gKcnW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-q5EZ7gKcnW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yaowen Ye" target="_blank">Yaowen Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cassidy Laidlaw" target="_blank">Cassidy Laidlaw</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Steinhardt" target="_blank">Jacob Steinhardt</a>
            </p>
            <p id="summary-q5EZ7gKcnW@OpenReview" class="summary">Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF). As LMs become more capable, the tasks they are given become harder to supervise. Will post-training remain effective under unreliable supervision? To test this, we simulate unreliable demonstrations and comparison feedback using small LMs and time-constrained humans. We find that in the presence of unreliable supervision, SFT still retains some effectiveness, but DPO (a common RLHF algorithm) fails to improve the model beyond SFT. To address this, we propose *iterative label refinement* (ILR) as an alternative to RLHF. ILR improves the SFT data by using comparison feedback to decide whether human demonstrations should be replaced by model-generated alternatives, then retrains the model via SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with unreliable supervision (math, coding, and safe instruction-following). Our findings suggest that as LMs are used for complex tasks where human supervision is unreliable, RLHF may no longer be the best use of human comparison feedback; instead, it is better to direct feedback towards improving the training *data* rather than continually training the *model*. Our code and data are available at https://github.com/helloelwin/iterative-label-refinement.</p>
            <p id="subjects-q5EZ7gKcnW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-q5EZ7gKcnW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-q5EZ7gKcnW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-q5EZ7gKcnW@OpenReview" onclick="foldPdfKimi('q5EZ7gKcnW@OpenReview', this)" class="hr hr-fold">
        </div><div id="kxFtMHItrf@OpenReview" class="panel paper" keywords="reti,retinex,idir,rgformer,illumination,diff,rldm,degradation,priors,restoration">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kxFtMHItrf" target="_blank" title="62/373"><span class="index notranslate">#62</span></a>
                <a id="title-kxFtMHItrf@OpenReview" class="title-link" href="/venue/kxFtMHItrf@OpenReview" target="_blank">Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model</a>
                <a id="pdf-kxFtMHItrf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kxFtMHItrf@OpenReview', this)" data="https://openreview.net/pdf?id=kxFtMHItrf">[PDF<sup id="pdf-stars-kxFtMHItrf@OpenReview">7</sup>]</a>
                <a id="copy-kxFtMHItrf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kxFtMHItrf@OpenReview')">[Copy]</a>
                <a id="kimi-kxFtMHItrf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kxFtMHItrf@OpenReview', this)">[Kimi<sup id="kimi-stars-kxFtMHItrf@OpenReview">1</sup>]</a>
                <a id="rel-kxFtMHItrf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kxFtMHItrf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kxFtMHItrf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chunming He" target="_blank">Chunming He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengyu Fang" target="_blank">Chengyu Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulun Zhang" target="_blank">Yulun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longxiang Tang" target="_blank">Longxiang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinfa Huang" target="_blank">Jinfa Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Li" target="_blank">Kai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=zhenhua guo" target="_blank">zhenhua guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiu Li" target="_blank">Xiu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sina Farsiu" target="_blank">Sina Farsiu</a>
            </p>
            <p id="summary-kxFtMHItrf@OpenReview" class="summary">Illumination degradation image restoration (IDIR) techniques aim to improve the visibility of degraded images and mitigate the adverse effects of deteriorated illumination. Among these algorithms, diffusion-based models (DM) have shown promising performance but are often burdened by heavy computational demands and pixel misalignment issues when predicting the image-level distribution. To tackle these problems, we propose to leverage DM within a compact latent space to generate concise guidance priors and introduce a novel solution called Reti-Diff for the IDIR task. Specifically, Reti-Diff comprises two significant components: the Retinex-based latent DM (RLDM) and the Retinex-guided transformer (RGformer). RLDM is designed to acquire Retinex knowledge, extracting reflectance and illumination priors to facilitate detailed reconstruction and illumination correction. RGformer subsequently utilizes these compact priors to guide the decomposition of image features into their respective reflectance and illumination components. Following this, RGformer further enhances and consolidates these decomposed features, resulting in the production of refined images with consistent content and robustness to handle complex degradation scenarios. Extensive experiments demonstrate that Reti-Diff outperforms existing methods on three IDIR tasks, as well as downstream applications.</p>
            <p id="subjects-kxFtMHItrf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-kxFtMHItrf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kxFtMHItrf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kxFtMHItrf@OpenReview" onclick="foldPdfKimi('kxFtMHItrf@OpenReview', this)" class="hr hr-fold">
        </div><div id="kpq3IIjUD3@OpenReview" class="panel paper" keywords="slem,equivariant,quantum,operators,overlap,dft,efficiency,representations,predicting,matrices">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kpq3IIjUD3" target="_blank" title="63/373"><span class="index notranslate">#63</span></a>
                <a id="title-kpq3IIjUD3@OpenReview" class="title-link" href="/venue/kpq3IIjUD3@OpenReview" target="_blank">Learning local equivariant representations for quantum operators</a>
                <a id="pdf-kpq3IIjUD3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kpq3IIjUD3@OpenReview', this)" data="https://openreview.net/pdf?id=kpq3IIjUD3">[PDF<sup id="pdf-stars-kpq3IIjUD3@OpenReview">4</sup>]</a>
                <a id="copy-kpq3IIjUD3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kpq3IIjUD3@OpenReview')">[Copy]</a>
                <a id="kimi-kpq3IIjUD3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kpq3IIjUD3@OpenReview', this)">[Kimi<sup id="kimi-stars-kpq3IIjUD3@OpenReview">2</sup>]</a>
                <a id="rel-kpq3IIjUD3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kpq3IIjUD3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kpq3IIjUD3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanghao Zhouyin" target="_blank">Zhanghao Zhouyin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixi Gan" target="_blank">Zixi Gan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shishir Pandey" target="_blank">Shishir Pandey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Zhang" target="_blank">Linfeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiangqiang Gu" target="_blank">Qiangqiang Gu</a>
            </p>
            <p id="summary-kpq3IIjUD3@OpenReview" class="summary">Predicting quantum operator matrices such as Hamiltonian, overlap, and density matrices in the density functional theory (DFT) framework is crucial for understanding material properties. Current methods often focus on individual operators and struggle with efficiency and scalability for large systems. Here we introduce a novel deep learning model, SLEM (strictly localized equivariant message-passing) for predicting multiple quantum operators, that achieves state-of-the-art accuracy while dramatically improving computational efficiency. SLEM's key innovation is its strict locality-based design, constructing local, equivariant representations for quantum tensors while preserving physical symmetries. This enables complex many-body dependence without expanding the effective receptive field, leading to superior data efficiency and transferability. Using an innovative SO(2) convolution and invariant overlap parameterization, SLEM reduces the computational complexity of high-order tensor products and is therefore capable of handling systems requiring the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-121" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-122"><span class="mi" id="MathJax-Span-123" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-23">f</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-124" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-125"><span class="mi" id="MathJax-Span-126" style="font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi></math></span></span><script type="math/tex" id="MathJax-Element-24">g</script> orbitals in their basis sets. We demonstrate SLEM's capabilities across diverse 2D and 3D materials, achieving high accuracy even with limited training data. SLEM's design facilitates efficient parallelization, potentially extending DFT simulations to systems with device-level sizes, opening new possibilities for large-scale quantum simulations and high-throughput materials discovery.</p>
            <p id="subjects-kpq3IIjUD3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-kpq3IIjUD3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kpq3IIjUD3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kpq3IIjUD3@OpenReview" onclick="foldPdfKimi('kpq3IIjUD3@OpenReview', this)" class="hr hr-fold">
        </div><div id="xsx3Fpo3UD@OpenReview" class="panel paper" keywords="adpa,alignment,slms,dckd,teacher,distillation,student,aligned,advantage,preference">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xsx3Fpo3UD" target="_blank" title="64/373"><span class="index notranslate">#64</span></a>
                <a id="title-xsx3Fpo3UD@OpenReview" class="title-link" href="/venue/xsx3Fpo3UD@OpenReview" target="_blank">Advantage-Guided Distillation for Preference Alignment in Small Language Models</a>
                <a id="pdf-xsx3Fpo3UD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xsx3Fpo3UD@OpenReview', this)" data="https://openreview.net/pdf?id=xsx3Fpo3UD">[PDF<sup id="pdf-stars-xsx3Fpo3UD@OpenReview">13</sup>]</a>
                <a id="copy-xsx3Fpo3UD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xsx3Fpo3UD@OpenReview')">[Copy]</a>
                <a id="kimi-xsx3Fpo3UD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xsx3Fpo3UD@OpenReview', this)">[Kimi<sup id="kimi-stars-xsx3Fpo3UD@OpenReview">11</sup>]</a>
                <a id="rel-xsx3Fpo3UD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xsx3Fpo3UD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xsx3Fpo3UD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shiping Gao" target="_blank">Shiping Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fanqi Wan" target="_blank">Fanqi Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajian Guo" target="_blank">Jiajian Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojun Quan" target="_blank">Xiaojun Quan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qifan Wang" target="_blank">Qifan Wang</a>
            </p>
            <p id="summary-xsx3Fpo3UD@OpenReview" class="summary">Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to SLMs, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the student's ability to distinguish between preferred and dispreferred responses, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results show that these two approaches appreciably improve the alignment of SLMs and narrow the performance gap with larger counterparts. Among them, ADPA demonstrates superior performance and achieves even greater effectiveness when integrated with DCKD. Our code is available at \url{https://github.com/SLIT-AI/ADPA}.</p>
            <p id="subjects-xsx3Fpo3UD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xsx3Fpo3UD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xsx3Fpo3UD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xsx3Fpo3UD@OpenReview" onclick="foldPdfKimi('xsx3Fpo3UD@OpenReview', this)" class="hr hr-fold">
        </div><div id="jXvwJ51vcK@OpenReview" class="panel paper" keywords="multimodal,pcs,fss,shot,cloud,unimodal,semantic,multimodality,segmentation,modalities">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jXvwJ51vcK" target="_blank" title="65/373"><span class="index notranslate">#65</span></a>
                <a id="title-jXvwJ51vcK@OpenReview" class="title-link" href="/venue/jXvwJ51vcK@OpenReview" target="_blank">Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation</a>
                <a id="pdf-jXvwJ51vcK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jXvwJ51vcK@OpenReview', this)" data="https://openreview.net/pdf?id=jXvwJ51vcK">[PDF<sup id="pdf-stars-jXvwJ51vcK@OpenReview">8</sup>]</a>
                <a id="copy-jXvwJ51vcK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jXvwJ51vcK@OpenReview')">[Copy]</a>
                <a id="kimi-jXvwJ51vcK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jXvwJ51vcK@OpenReview', this)">[Kimi<sup id="kimi-stars-jXvwJ51vcK@OpenReview">1</sup>]</a>
                <a id="rel-jXvwJ51vcK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jXvwJ51vcK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jXvwJ51vcK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaochong An" target="_blank">Zhaochong An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guolei Sun" target="_blank">Guolei Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Liu" target="_blank">Yun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runjia Li" target="_blank">Runjia Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Wu" target="_blank">Min Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Ming Cheng" target="_blank">Ming-Ming Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ender Konukoglu" target="_blank">Ender Konukoglu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Serge Belongie" target="_blank">Serge Belongie</a>
            </p>
            <p id="summary-jXvwJ51vcK@OpenReview" class="summary">Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a cost-free multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code will be released.</p>
            <p id="subjects-jXvwJ51vcK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jXvwJ51vcK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jXvwJ51vcK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jXvwJ51vcK@OpenReview" onclick="foldPdfKimi('jXvwJ51vcK@OpenReview', this)" class="hr hr-fold">
        </div><div id="jXLiDKsuDo@OpenReview" class="panel paper" keywords="simba,simplicity,scaling,bias,deep,overfitting,humanoidbench,parameters,myosuite,integrating">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jXLiDKsuDo" target="_blank" title="66/373"><span class="index notranslate">#66</span></a>
                <a id="title-jXLiDKsuDo@OpenReview" class="title-link" href="/venue/jXLiDKsuDo@OpenReview" target="_blank">SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning</a>
                <a id="pdf-jXLiDKsuDo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jXLiDKsuDo@OpenReview', this)" data="https://openreview.net/pdf?id=jXLiDKsuDo">[PDF<sup id="pdf-stars-jXLiDKsuDo@OpenReview">5</sup>]</a>
                <a id="copy-jXLiDKsuDo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jXLiDKsuDo@OpenReview')">[Copy]</a>
                <a id="kimi-jXLiDKsuDo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jXLiDKsuDo@OpenReview', this)">[Kimi<sup id="kimi-stars-jXLiDKsuDo@OpenReview">1</sup>]</a>
                <a id="rel-jXLiDKsuDo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jXLiDKsuDo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jXLiDKsuDo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hojoon Lee" target="_blank">Hojoon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongyoon Hwang" target="_blank">Dongyoon Hwang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Donghu Kim" target="_blank">Donghu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunseung Kim" target="_blank">Hyunseung Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Jet Tai" target="_blank">Jun Jet Tai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaushik Subramanian" target="_blank">Kaushik Subramanian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Wurman" target="_blank">Peter Wurman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaegul Choo" target="_blank">Jaegul Choo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Stone" target="_blank">Peter Stone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Takuma Seno" target="_blank">Takuma Seno</a>
            </p>
            <p id="summary-jXLiDKsuDo@OpenReview" class="summary">Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting.These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored.Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms—including off-policy, on-policy, and unsupervised methods—is consistently improved.Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench.These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.</p>
            <p id="subjects-jXLiDKsuDo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jXLiDKsuDo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jXLiDKsuDo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jXLiDKsuDo@OpenReview" onclick="foldPdfKimi('jXLiDKsuDo@OpenReview', this)" class="hr hr-fold">
        </div><div id="iuxaCU3DI7@OpenReview" class="panel paper" keywords="surgical,raso,tag,map,recognize,object,supervised,unleashing,weakly,procedures">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=iuxaCU3DI7" target="_blank" title="67/373"><span class="index notranslate">#67</span></a>
                <a id="title-iuxaCU3DI7@OpenReview" class="title-link" href="/venue/iuxaCU3DI7@OpenReview" target="_blank">Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data</a>
                <a id="pdf-iuxaCU3DI7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('iuxaCU3DI7@OpenReview', this)" data="https://openreview.net/pdf?id=iuxaCU3DI7">[PDF<sup id="pdf-stars-iuxaCU3DI7@OpenReview">8</sup>]</a>
                <a id="copy-iuxaCU3DI7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('iuxaCU3DI7@OpenReview')">[Copy]</a>
                <a id="kimi-iuxaCU3DI7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('iuxaCU3DI7@OpenReview', this)">[Kimi<sup id="kimi-stars-iuxaCU3DI7@OpenReview">1</sup>]</a>
                <a id="rel-iuxaCU3DI7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('iuxaCU3DI7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-iuxaCU3DI7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajie Li" target="_blank">Jiajie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Quaranto" target="_blank">Brian Quaranto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenhui Xu" target="_blank">Chenhui Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ishan Mishra" target="_blank">Ishan Mishra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiyang Qin" target="_blank">Ruiyang Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dancheng Liu" target="_blank">Dancheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Kim" target="_blank">Peter Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinjun Xiong" target="_blank">Jinjun Xiong</a>
            </p>
            <p id="summary-iuxaCU3DI7@OpenReview" class="summary">We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gatherers to 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks respectively in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. We will open-source our code, model, and dataset to facilitate further research.</p>
            <p id="subjects-iuxaCU3DI7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-iuxaCU3DI7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-iuxaCU3DI7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-iuxaCU3DI7@OpenReview" onclick="foldPdfKimi('iuxaCU3DI7@OpenReview', this)" class="hr hr-fold">
        </div><div id="ikkvC1UnnE@OpenReview" class="panel paper" keywords="sosp,privately,fosp,finding,frac,epsilon,stationary,batch,tilde,ganesh">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ikkvC1UnnE" target="_blank" title="68/373"><span class="index notranslate">#68</span></a>
                <a id="title-ikkvC1UnnE@OpenReview" class="title-link" href="/venue/ikkvC1UnnE@OpenReview" target="_blank">Adaptive Batch Size for Privately Finding Second-Order Stationary Points</a>
                <a id="pdf-ikkvC1UnnE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ikkvC1UnnE@OpenReview', this)" data="https://openreview.net/pdf?id=ikkvC1UnnE">[PDF<sup id="pdf-stars-ikkvC1UnnE@OpenReview">4</sup>]</a>
                <a id="copy-ikkvC1UnnE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ikkvC1UnnE@OpenReview')">[Copy]</a>
                <a id="kimi-ikkvC1UnnE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ikkvC1UnnE@OpenReview', this)">[Kimi<sup id="kimi-stars-ikkvC1UnnE@OpenReview"></sup>]</a>
                <a id="rel-ikkvC1UnnE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ikkvC1UnnE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ikkvC1UnnE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daogao Liu" target="_blank">Daogao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kunal Talwar" target="_blank">Kunal Talwar</a>
            </p>
            <p id="summary-ikkvC1UnnE@OpenReview" class="summary">There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) demonstrated that an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-127" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-128"><span class="mi" id="MathJax-Span-129" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-25">\alpha</script>-SOSP can be found with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext mathcolor=&quot;red&quot;&gt;\Tilde&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;msqrt&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msqrt&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;7&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-130" style="width: 14.534em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.888em, 1011.98em, 2.763em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-131"><span class="mi" id="MathJax-Span-132" style="font-family: MathJax_Math-italic;">α</span><span class="mo" id="MathJax-Span-133" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mtext" id="MathJax-Span-134" style="font-family: MathJax_Main; padding-left: 0.263em; color: red;">\Tilde</span><span class="texatom" id="MathJax-Span-135"><span class="mrow" id="MathJax-Span-136"><span class="mi" id="MathJax-Span-137" style="font-family: MathJax_Math-italic;">O</span></span></span><span class="mo" id="MathJax-Span-138" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-139"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-140" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.284em, 1001.2em, 4.169em, -999.997em); top: -3.539em; left: 50%; margin-left: -0.622em;"><span class="msubsup" id="MathJax-Span-141"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-142" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.341em; left: 0.419em;"><span class="texatom" id="MathJax-Span-143"><span class="mrow" id="MathJax-Span-144"><span class="mn" id="MathJax-Span-145" style="font-size: 50%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-146"><span class="mrow" id="MathJax-Span-147"><span class="mo" id="MathJax-Span-148" style="font-size: 50%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-149" style="font-size: 50%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.3em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.305em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-150" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mo" id="MathJax-Span-151" style="font-family: MathJax_Main; padding-left: 0.211em;">(</span><span class="mfrac" id="MathJax-Span-152"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.284em, 1000.94em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.466em;"><span class="msqrt" id="MathJax-Span-153"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.37em, 2.294em, -999.997em); top: -2.133em; left: 0.576em;"><span class="mrow" id="MathJax-Span-154"><span class="mi" id="MathJax-Span-155" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.94em, 1000.37em, 1.253em, -999.997em); top: -1.664em; left: 0.576em;"><span style="display: inline-block; overflow: hidden; vertical-align: -0.049em; border-top: 1.2px solid; width: 0.367em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span><span style="position: absolute; clip: rect(3.284em, 1000.58em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span><span style="font-size: 70.7%; font-family: MathJax_Main;">√</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.73em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.362em;"><span class="mrow" id="MathJax-Span-156"><span class="mi" id="MathJax-Span-157" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-158" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-159"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.26em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-160" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.367em;"><span class="texatom" id="MathJax-Span-161"><span class="mrow" id="MathJax-Span-162"><span class="mn" id="MathJax-Span-163" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span class="texatom" id="MathJax-Span-164"><span class="mrow" id="MathJax-Span-165"><span class="mo" id="MathJax-Span-166" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-167" style="font-size: 70.7%; font-family: MathJax_Main;">7</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-168" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.622em; border-left: 0px solid; width: 0px; height: 2.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>=</mo><mtext mathcolor="red">\Tilde</mtext><mrow class="MJX-TeXAtom-ORD"><mi>O</mi></mrow><mo stretchy="false">(</mo><mfrac><mn>1</mn><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup></mfrac><mo>+</mo><mo stretchy="false">(</mo><mfrac><msqrt><mi>d</mi></msqrt><mrow><mi>n</mi><mi>ϵ</mi></mrow></mfrac><msup><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mn>3</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>7</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-26">\alpha=\Tilde{O}(\frac{1}{n^{1/3}}+(\frac{\sqrt{d}}{n\epsilon})^{3/7})</script>, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-169" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-170"><span class="mi" id="MathJax-Span-171" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-27">n</script> is the dataset size, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-172" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-173"><span class="mi" id="MathJax-Span-174" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-28">d</script> is the dimension, and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-175" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-176"><span class="mi" id="MathJax-Span-177" style="font-family: MathJax_Math-italic;">ϵ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></span></span><script type="math/tex" id="MathJax-Element-29">\epsilon</script> is the differential privacy parameter. Building on the SpiderBoost algorithm framework, we propose a new approach that uses adaptive batch sizes and incorporates the binary tree mechanism. Our method improves the results for privately finding an SOSP, achieving <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext mathcolor=&quot;red&quot;&gt;\Tilde&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;msqrt&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msqrt&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-178" style="width: 14.534em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.888em, 1011.98em, 2.763em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-179"><span class="mi" id="MathJax-Span-180" style="font-family: MathJax_Math-italic;">α</span><span class="mo" id="MathJax-Span-181" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mtext" id="MathJax-Span-182" style="font-family: MathJax_Main; padding-left: 0.263em; color: red;">\Tilde</span><span class="texatom" id="MathJax-Span-183"><span class="mrow" id="MathJax-Span-184"><span class="mi" id="MathJax-Span-185" style="font-family: MathJax_Math-italic;">O</span></span></span><span class="mo" id="MathJax-Span-186" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-187"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-188" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.284em, 1001.2em, 4.169em, -999.997em); top: -3.539em; left: 50%; margin-left: -0.622em;"><span class="msubsup" id="MathJax-Span-189"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-190" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.341em; left: 0.419em;"><span class="texatom" id="MathJax-Span-191"><span class="mrow" id="MathJax-Span-192"><span class="mn" id="MathJax-Span-193" style="font-size: 50%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-194"><span class="mrow" id="MathJax-Span-195"><span class="mo" id="MathJax-Span-196" style="font-size: 50%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-197" style="font-size: 50%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.3em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.305em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-198" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mo" id="MathJax-Span-199" style="font-family: MathJax_Main; padding-left: 0.211em;">(</span><span class="mfrac" id="MathJax-Span-200"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.284em, 1000.94em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.466em;"><span class="msqrt" id="MathJax-Span-201"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.37em, 2.294em, -999.997em); top: -2.133em; left: 0.576em;"><span class="mrow" id="MathJax-Span-202"><span class="mi" id="MathJax-Span-203" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.94em, 1000.37em, 1.253em, -999.997em); top: -1.664em; left: 0.576em;"><span style="display: inline-block; overflow: hidden; vertical-align: -0.049em; border-top: 1.2px solid; width: 0.367em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span><span style="position: absolute; clip: rect(3.284em, 1000.58em, 4.326em, -999.997em); top: -4.008em; left: 0em;"><span><span style="font-size: 70.7%; font-family: MathJax_Main;">√</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.73em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.362em;"><span class="mrow" id="MathJax-Span-204"><span class="mi" id="MathJax-Span-205" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-206" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-207"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.26em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-208" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.367em;"><span class="texatom" id="MathJax-Span-209"><span class="mrow" id="MathJax-Span-210"><span class="mn" id="MathJax-Span-211" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-212"><span class="mrow" id="MathJax-Span-213"><span class="mo" id="MathJax-Span-214" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-215" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-216" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.622em; border-left: 0px solid; width: 0px; height: 2.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>=</mo><mtext mathcolor="red">\Tilde</mtext><mrow class="MJX-TeXAtom-ORD"><mi>O</mi></mrow><mo stretchy="false">(</mo><mfrac><mn>1</mn><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup></mfrac><mo>+</mo><mo stretchy="false">(</mo><mfrac><msqrt><mi>d</mi></msqrt><mrow><mi>n</mi><mi>ϵ</mi></mrow></mfrac><msup><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-30">\alpha=\Tilde{O}(\frac{1}{n^{1/3}}+(\frac{\sqrt{d}}{n\epsilon})^{1/2})</script>. This improved bound matches the state-of-the-art for finding an FOSP, suggesting that privately finding an SOSP may be achievable at no additional cost.</p>
            <p id="subjects-ikkvC1UnnE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ikkvC1UnnE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ikkvC1UnnE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ikkvC1UnnE@OpenReview" onclick="foldPdfKimi('ikkvC1UnnE@OpenReview', this)" class="hr hr-fold">
        </div><div id="S5Yo6w3n3f@OpenReview" class="panel paper" keywords="smode,ode,smooth,network,control,actions,lipsnet,neuron,reinforcement,lipschitz">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=S5Yo6w3n3f" target="_blank" title="69/373"><span class="index notranslate">#69</span></a>
                <a id="title-S5Yo6w3n3f@OpenReview" class="title-link" href="/venue/S5Yo6w3n3f@OpenReview" target="_blank">ODE-based Smoothing Neural Network for Reinforcement Learning Tasks</a>
                <a id="pdf-S5Yo6w3n3f@OpenReview" class="title-pdf notranslate" onclick="togglePdf('S5Yo6w3n3f@OpenReview', this)" data="https://openreview.net/pdf?id=S5Yo6w3n3f">[PDF<sup id="pdf-stars-S5Yo6w3n3f@OpenReview">5</sup>]</a>
                <a id="copy-S5Yo6w3n3f@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('S5Yo6w3n3f@OpenReview')">[Copy]</a>
                <a id="kimi-S5Yo6w3n3f@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('S5Yo6w3n3f@OpenReview', this)">[Kimi<sup id="kimi-stars-S5Yo6w3n3f@OpenReview">1</sup>]</a>
                <a id="rel-S5Yo6w3n3f@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('S5Yo6w3n3f@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-S5Yo6w3n3f@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yinuo Wang" target="_blank">Yinuo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxuan Wang" target="_blank">Wenxuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xujie Song" target="_blank">Xujie Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Liu" target="_blank">Tong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuming Yin" target="_blank">Yuming Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangfa Chen" target="_blank">Liangfa Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Likun Wang" target="_blank">Likun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingliang Duan" target="_blank">Jingliang Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengbo Li" target="_blank">Shengbo Li</a>
            </p>
            <p id="summary-S5Yo6w3n3f@OpenReview" class="summary">The smoothness of control actions is a significant challenge faced by deep reinforcement learning (RL) techniques in solving optimal control problems. Existing RL-trained policies tend to produce non-smooth actions due to high-frequency input noise and unconstrained Lipschitz constants in neural networks. This article presents a Smooth ODE (SmODE) network capable of simultaneously addressing both causes of unsmooth control actions, thereby enhancing policy performance and robustness under noise condition. We first design a smooth ODE neuron with first-order low-pass filtering expression, which can dynamically filter out high frequency noises of hidden state by a learnable state-based system time constant. Additionally, we construct a state-based mapping function, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-31-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-217" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-218"><span class="mi" id="MathJax-Span-219" style="font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi></math></span></span><script type="math/tex" id="MathJax-Element-31">g</script>, and theoretically demonstrate its capacity to control the ODE neuron's Lipschitz constant. Then, based on the above neuronal structure design, we further advanced the SmODE network serving as RL policy approximators. This network is compatible with most existing RL algorithms, offering improved adaptability compared to prior approaches. Various experiments show that our SmODE network demonstrates superior anti-interference capabilities and smoother action outputs than the multi-layer perception and smooth network architectures like LipsNet.</p>
            <p id="subjects-S5Yo6w3n3f@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-S5Yo6w3n3f@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-S5Yo6w3n3f@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-S5Yo6w3n3f@OpenReview" onclick="foldPdfKimi('S5Yo6w3n3f@OpenReview', this)" class="hr hr-fold">
        </div><div id="hpCfPEvBsr@OpenReview" class="panel paper" keywords="mixeval,evaluations,world,real,mixture,modalities,effectively,benchmark,diverse,modal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hpCfPEvBsr" target="_blank" title="70/373"><span class="index notranslate">#70</span></a>
                <a id="title-hpCfPEvBsr@OpenReview" class="title-link" href="/venue/hpCfPEvBsr@OpenReview" target="_blank">MixEval-X: Any-to-any Evaluations from Real-world Data Mixture</a>
                <a id="pdf-hpCfPEvBsr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hpCfPEvBsr@OpenReview', this)" data="https://openreview.net/pdf?id=hpCfPEvBsr">[PDF<sup id="pdf-stars-hpCfPEvBsr@OpenReview">3</sup>]</a>
                <a id="copy-hpCfPEvBsr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hpCfPEvBsr@OpenReview')">[Copy]</a>
                <a id="kimi-hpCfPEvBsr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hpCfPEvBsr@OpenReview', this)">[Kimi<sup id="kimi-stars-hpCfPEvBsr@OpenReview">2</sup>]</a>
                <a id="rel-hpCfPEvBsr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hpCfPEvBsr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hpCfPEvBsr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinjie Ni" target="_blank">Jinjie Ni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Song" target="_blank">Yifan Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deepanway Ghosal" target="_blank">Deepanway Ghosal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Li" target="_blank">Bo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Junhao Zhang" target="_blank">David Junhao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Yue" target="_blank">Xiang Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fuzhao Xue" target="_blank">Fuzhao Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuntian Deng" target="_blank">Yuntian Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andy Zheng" target="_blank">Andy Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaichen Zhang" target="_blank">Kaichen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahir Shah" target="_blank">Mahir Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kabir Jain" target="_blank">Kabir Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang You" target="_blank">Yang You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Qizhe Shieh" target="_blank">Michael Qizhe Shieh</a>
            </p>
            <p id="summary-hpCfPEvBsr@OpenReview" class="summary">Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any, real-world benchmark designed to optimize and standardize evaluations across diverse input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions. Meanwhile, MixEval-X's model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98) while being much more efficient. We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research.</p>
            <p id="subjects-hpCfPEvBsr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hpCfPEvBsr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hpCfPEvBsr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hpCfPEvBsr@OpenReview" onclick="foldPdfKimi('hpCfPEvBsr@OpenReview', this)" class="hr hr-fold">
        </div><div id="hUb2At2DsQ@OpenReview" class="panel paper" keywords="autoformalization,beq,formal,mapsto,statement,dependency,rautoformalizer,faithful,retrieval,con">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hUb2At2DsQ" target="_blank" title="71/373"><span class="index notranslate">#71</span></a>
                <a id="title-hUb2At2DsQ@OpenReview" class="title-link" href="/venue/hUb2At2DsQ@OpenReview" target="_blank">Rethinking and improving autoformalization: towards a faithful metric and a Dependency Retrieval-based approach</a>
                <a id="pdf-hUb2At2DsQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hUb2At2DsQ@OpenReview', this)" data="https://openreview.net/pdf?id=hUb2At2DsQ">[PDF<sup id="pdf-stars-hUb2At2DsQ@OpenReview">4</sup>]</a>
                <a id="copy-hUb2At2DsQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hUb2At2DsQ@OpenReview')">[Copy]</a>
                <a id="kimi-hUb2At2DsQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hUb2At2DsQ@OpenReview', this)">[Kimi<sup id="kimi-stars-hUb2At2DsQ@OpenReview">4</sup>]</a>
                <a id="rel-hUb2At2DsQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hUb2At2DsQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hUb2At2DsQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Liu" target="_blank">Qi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinhao Zheng" target="_blank">Xinhao Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xudong Lu" target="_blank">Xudong Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qinxiang Cao" target="_blank">Qinxiang Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junchi Yan" target="_blank">Junchi Yan</a>
            </p>
            <p id="summary-hUb2At2DsQ@OpenReview" class="summary">As a central component in formal verification, statement autoformalization has been widely studied including the recent efforts from machine learning community, but still remains a widely-recognized difficult and open problem. In this paper, we delve into two critical yet under-explored gaps: 1) absence of faithful and universal automated evaluation for autoformalization results; 2) agnosia of contextural information, inducing severe hallucination of formal definitions and theorems.To address the first issue, we propose **BEq** (_**B**idirectional **E**xtended Definitional E**q**uivalence_), an automated neuro-symbolic method to determine the equivalence between two formal statements, which is formal-grounded and well-aligned with human intuition.For the second, we propose **RAutoformalizer** (_**R**etrieval-augmented **Autoformalizer**_), augmenting statement autoformalization by _Dependency Retrieval_, retrieving potentially dependent objects from formal libraries.We parse the dependencies of libraries and propose to _structurally informalise_ formal objects by the topological order of dependencies. To evaluate OOD generalization and research-level capabilities, we build a novel benchmark, _Con-NF_, consisting of 961 informal-formal statement pairs from frontier mathematical researches.Extensive experiments validate the effectiveness of our proposed approaches. In particular, BEq is evaluated on 200 diverse formal statement pairs with expert-annotated equivalence label, exhibiting significantly improved accuracy (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-32-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;82.50&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-220" style="width: 100%; display: inline-block; min-width: 2.763em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-221"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-222" style="font-family: MathJax_Main;">82.50</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-223" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>82.50</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-32">82.50\\% \mapsto 90.50\\%</script>) and precision (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-33-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;70.59&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-224" style="width: 100%; display: inline-block; min-width: 2.763em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-225"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-226" style="font-family: MathJax_Main;">70.59</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-227" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>70.59</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-33">70.59\\% \mapsto 100.0\\%</script>).For dependency retrieval, a baseline with excellent performance is established.The proposed RAutoformalizer substantially outperforms SOTA baselines in both in-distribution ProofNet benchmark (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-34-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;12.83&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-228" style="width: 100%; display: inline-block; min-width: 2.763em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-229"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1002.24em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-230" style="font-family: MathJax_Main;">12.83</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-231" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>12.83</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-34">12.83\\% \mapsto 18.18\\%</script>, BEq@8) and OOD Con-NF scenario (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-35-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;4.58&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-232" style="width: 100%; display: inline-block; min-width: 2.138em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1001.72em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-233"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1001.72em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-234" style="font-family: MathJax_Main;">4.58</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-235" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4.58</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-35">4.58\\%\mapsto 16.86\\%</script>, BEq@8). Code, data, and models will be available.</p>
            <p id="subjects-hUb2At2DsQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hUb2At2DsQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hUb2At2DsQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hUb2At2DsQ@OpenReview" onclick="foldPdfKimi('hUb2At2DsQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="gYWqxXE5RJ@OpenReview" class="panel paper" keywords="implicitness,impscore,language,metric,implicit,sentence,learnable,operationalize,580,quantifying">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gYWqxXE5RJ" target="_blank" title="72/373"><span class="index notranslate">#72</span></a>
                <a id="title-gYWqxXE5RJ@OpenReview" class="title-link" href="/venue/gYWqxXE5RJ@OpenReview" target="_blank">ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language</a>
                <a id="pdf-gYWqxXE5RJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gYWqxXE5RJ@OpenReview', this)" data="https://openreview.net/pdf?id=gYWqxXE5RJ">[PDF<sup id="pdf-stars-gYWqxXE5RJ@OpenReview">4</sup>]</a>
                <a id="copy-gYWqxXE5RJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gYWqxXE5RJ@OpenReview')">[Copy]</a>
                <a id="kimi-gYWqxXE5RJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gYWqxXE5RJ@OpenReview', this)">[Kimi<sup id="kimi-stars-gYWqxXE5RJ@OpenReview">5</sup>]</a>
                <a id="rel-gYWqxXE5RJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gYWqxXE5RJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gYWqxXE5RJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxin Wang" target="_blank">Yuxin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaomeng Zhu" target="_blank">Xiaomeng Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weimin Lyu" target="_blank">Weimin Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saeed Hassanpour" target="_blank">Saeed Hassanpour</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soroush Vosoughi" target="_blank">Soroush Vosoughi</a>
            </p>
            <p id="summary-gYWqxXE5RJ@OpenReview" class="summary">Handling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a robust metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define "implicitness" as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a novel, reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset comprising <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-36-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;112&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;580&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-236" style="width: 4.169em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1003.39em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-237"><span class="mn" id="MathJax-Span-238" style="font-family: MathJax_Main;">112</span><span class="mo" id="MathJax-Span-239" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-240" style="font-family: MathJax_Main; padding-left: 0.159em;">580</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>112</mn><mo>,</mo><mn>580</mn></math></span></span><script type="math/tex" id="MathJax-Element-36">112,580</script> (implicit sentence, explicit sentence) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content.</p>
            <p id="subjects-gYWqxXE5RJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gYWqxXE5RJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gYWqxXE5RJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gYWqxXE5RJ@OpenReview" onclick="foldPdfKimi('gYWqxXE5RJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="gWgaypDBs8@OpenReview" class="panel paper" keywords="representative,guidance,repg,sampling,diffusion,target,discernment,coherent,classifier,sacrificed">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gWgaypDBs8" target="_blank" title="73/373"><span class="index notranslate">#73</span></a>
                <a id="title-gWgaypDBs8@OpenReview" class="title-link" href="/venue/gWgaypDBs8@OpenReview" target="_blank">Representative Guidance: Diffusion Model Sampling with Consistency</a>
                <a id="pdf-gWgaypDBs8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gWgaypDBs8@OpenReview', this)" data="https://openreview.net/pdf?id=gWgaypDBs8">[PDF<sup id="pdf-stars-gWgaypDBs8@OpenReview">16</sup>]</a>
                <a id="copy-gWgaypDBs8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gWgaypDBs8@OpenReview')">[Copy]</a>
                <a id="kimi-gWgaypDBs8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gWgaypDBs8@OpenReview', this)">[Kimi<sup id="kimi-stars-gWgaypDBs8@OpenReview">3</sup>]</a>
                <a id="rel-gWgaypDBs8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gWgaypDBs8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gWgaypDBs8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anh-Dung Dinh" target="_blank">Anh-Dung Dinh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daochang Liu" target="_blank">Daochang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Xu" target="_blank">Chang Xu</a>
            </p>
            <p id="summary-gWgaypDBs8@OpenReview" class="summary">The diffusion sampling process faces a persistent challenge stemming from its incoherence, attributable to varying noise directions across different time steps. Our Representative Guidance (RepG) offers a new perspective to handle this issue by reformulating the sampling process with a coherent direction towards a representative target.In this formulation, while the classic classifier guidance improves feature discernment by steering the model away from ambiguous features, it fails to provide a favorable representative target, since the class label is overly compact and leads to sacrificed diversity and the adversarial generation problem.In contrast, we leverage self-supervised representations as the coherent target and treat sampling as a downstream task, which refines image details and corrects errors rather than settling for simpler samples.Our representative guidance achieves superior performance and also illustrates the potential of pre-trained self-supervised models in image sampling. Our findings demonstrate that RepG not only substantially enhances vanilla diffusion sampling but also surpasses state-of-the-art benchmarks when combined with the classifier-free guidance. Our code will be released.</p>
            <p id="subjects-gWgaypDBs8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gWgaypDBs8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gWgaypDBs8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gWgaypDBs8@OpenReview" onclick="foldPdfKimi('gWgaypDBs8@OpenReview', this)" class="hr hr-fold">
        </div><div id="friHAl5ofG@OpenReview" class="panel paper" keywords="gvl,value,temporal,progress,frames,tasks,vlms,task,context,vision">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=friHAl5ofG" target="_blank" title="74/373"><span class="index notranslate">#74</span></a>
                <a id="title-friHAl5ofG@OpenReview" class="title-link" href="/venue/friHAl5ofG@OpenReview" target="_blank">Vision Language Models are In-Context Value Learners</a>
                <a id="pdf-friHAl5ofG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('friHAl5ofG@OpenReview', this)" data="https://openreview.net/pdf?id=friHAl5ofG">[PDF<sup id="pdf-stars-friHAl5ofG@OpenReview">10</sup>]</a>
                <a id="copy-friHAl5ofG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('friHAl5ofG@OpenReview')">[Copy]</a>
                <a id="kimi-friHAl5ofG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('friHAl5ofG@OpenReview', this)">[Kimi<sup id="kimi-stars-friHAl5ofG@OpenReview">9</sup>]</a>
                <a id="rel-friHAl5ofG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('friHAl5ofG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-friHAl5ofG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yecheng Jason Ma" target="_blank">Yecheng Jason Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joey Hejna" target="_blank">Joey Hejna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuyuan Fu" target="_blank">Chuyuan Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dhruv Shah" target="_blank">Dhruv Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacky Liang" target="_blank">Jacky Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuo Xu" target="_blank">Zhuo Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sean Kirmani" target="_blank">Sean Kirmani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Xu" target="_blank">Peng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danny Driess" target="_blank">Danny Driess</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ted Xiao" target="_blank">Ted Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Osbert Bastani" target="_blank">Osbert Bastani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dinesh Jayaraman" target="_blank">Dinesh Jayaraman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Yu" target="_blank">Wenhao Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tingnan Zhang" target="_blank">Tingnan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dorsa Sadigh" target="_blank">Dorsa Sadigh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Xia" target="_blank">Fei Xia</a>
            </p>
            <p id="summary-friHAl5ofG@OpenReview" class="summary">Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and value-weighted regression -- all without any model training or finetuning.</p>
            <p id="subjects-friHAl5ofG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-friHAl5ofG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-friHAl5ofG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-friHAl5ofG@OpenReview" onclick="foldPdfKimi('friHAl5ofG@OpenReview', this)" class="hr hr-fold">
        </div><div id="eW4yh6HKz4@OpenReview" class="panel paper" keywords="cbq,quantization,ptq,llms,block,dependencies,layer,cross,w2a16,llama1">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eW4yh6HKz4" target="_blank" title="75/373"><span class="index notranslate">#75</span></a>
                <a id="title-eW4yh6HKz4@OpenReview" class="title-link" href="/venue/eW4yh6HKz4@OpenReview" target="_blank">CBQ: Cross-Block Quantization for Large Language Models</a>
                <a id="pdf-eW4yh6HKz4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eW4yh6HKz4@OpenReview', this)" data="https://openreview.net/pdf?id=eW4yh6HKz4">[PDF<sup id="pdf-stars-eW4yh6HKz4@OpenReview">6</sup>]</a>
                <a id="copy-eW4yh6HKz4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eW4yh6HKz4@OpenReview')">[Copy]</a>
                <a id="kimi-eW4yh6HKz4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eW4yh6HKz4@OpenReview', this)">[Kimi<sup id="kimi-stars-eW4yh6HKz4@OpenReview">6</sup>]</a>
                <a id="rel-eW4yh6HKz4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eW4yh6HKz4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eW4yh6HKz4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Ding" target="_blank">Xin Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Liu" target="_blank">Xiaoyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijun Tu" target="_blank">Zhijun Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Zhang" target="_blank">Yun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Li" target="_blank">Wei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Hu" target="_blank">Jie Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanting Chen" target="_blank">Hanting Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yehui Tang" target="_blank">Yehui Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiwei Xiong" target="_blank">Zhiwei Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoqun Yin" target="_blank">Baoqun Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhe Wang" target="_blank">Yunhe Wang</a>
            </p>
            <p id="summary-eW4yh6HKz4@OpenReview" class="summary">Post-training quantization (PTQ) has played a pivotal role in compressing large language models (LLMs) at ultra-low costs. Although current PTQ methods have achieved promising results by addressing outliers and employing layer- or block-wise loss optimization techniques, they still suffer from significant performance degradation at ultra-low bits precision. To dissect this issue, we conducted an in-depth analysis of quantization errors specific to LLMs and surprisingly discovered that, unlike traditional sources of quantization errors, the growing number of model parameters, combined with the reduction in quantization bits, intensifies inter-layer and intra-layer dependencies, which severely impact quantization accuracy. This finding highlights a critical challenge in quantizing LLMs. To address this, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ leverages a cross-block dependency to establish long-range dependencies across multiple blocks and integrates an adaptive LoRA-Rounding technique to manage intra-layer dependencies. To further enhance performance, CBQ incorporates a coarse-to-fine pre-processing mechanism for processing weights and activations. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across various LLMs and datasets. Notably, CBQ only takes 4.3 hours to quantize a weight-only quantization of a 4-bit LLAMA1-65B model, achieving a commendable trade off between performance and efficiency.</p>
            <p id="subjects-eW4yh6HKz4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-eW4yh6HKz4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eW4yh6HKz4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eW4yh6HKz4@OpenReview" onclick="foldPdfKimi('eW4yh6HKz4@OpenReview', this)" class="hr hr-fold">
        </div><div id="dRXxFEY8ZE@OpenReview" class="panel paper" keywords="birdset,texttt,bioacoustics,audio,classification,avian,audioset,dataset,accessibility,benchmark">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dRXxFEY8ZE" target="_blank" title="76/373"><span class="index notranslate">#76</span></a>
                <a id="title-dRXxFEY8ZE@OpenReview" class="title-link" href="/venue/dRXxFEY8ZE@OpenReview" target="_blank"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-37-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;BirdSet&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-241" style="width: 4.411em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.682em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.425em, 1003.61em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-242"><span class="texatom" id="MathJax-Span-243"><span class="mrow" id="MathJax-Span-244"><span class="mtext" id="MathJax-Span-245" style="font-family: MathJax_Typewriter;">BirdSet</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.04em; border-left: 0px solid; width: 0px; height: 0.835em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">BirdSet</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-37">\texttt{BirdSet}</script>: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics</a>
                <a id="pdf-dRXxFEY8ZE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dRXxFEY8ZE@OpenReview', this)" data="https://openreview.net/pdf?id=dRXxFEY8ZE">[PDF<sup id="pdf-stars-dRXxFEY8ZE@OpenReview">3</sup>]</a>
                <a id="copy-dRXxFEY8ZE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dRXxFEY8ZE@OpenReview')">[Copy]</a>
                <a id="kimi-dRXxFEY8ZE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dRXxFEY8ZE@OpenReview', this)">[Kimi<sup id="kimi-stars-dRXxFEY8ZE@OpenReview">3</sup>]</a>
                <a id="rel-dRXxFEY8ZE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dRXxFEY8ZE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dRXxFEY8ZE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lukas Rauch" target="_blank">Lukas Rauch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Raphael Schwinger" target="_blank">Raphael Schwinger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moritz Wirth" target="_blank">Moritz Wirth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=René Heinrich" target="_blank">René Heinrich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Denis Huseljic" target="_blank">Denis Huseljic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marek Herde" target="_blank">Marek Herde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Lange" target="_blank">Jonas Lange</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Kahl" target="_blank">Stefan Kahl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernhard Sick" target="_blank">Bernhard Sick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sven Tomforde" target="_blank">Sven Tomforde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Scholz" target="_blank">Christoph Scholz</a>
            </p>
            <p id="summary-dRXxFEY8ZE@OpenReview" class="summary">Deep learning (DL) has greatly advanced audio classification, yet the field is limited by the scarcity of large-scale benchmark datasets that have propelled progress in other domains. While AudioSet is a pivotal step to bridge this gap as a universal-domain dataset, its restricted accessibility and limited range of evaluation use cases challenge its role as the sole resource. Therefore, we introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-38-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;BirdSet&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-246" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-247"><span class="texatom" id="MathJax-Span-248"><span class="mrow" id="MathJax-Span-249"><span class="mtext" id="MathJax-Span-250" style="font-family: MathJax_Typewriter;">BirdSet</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">BirdSet</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-38">\texttt{BirdSet}</script>, a large-scale benchmark data set for audio classification focusing on avian bioacoustics. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-39-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;BirdSet&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-251" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-252"><span class="texatom" id="MathJax-Span-253"><span class="mrow" id="MathJax-Span-254"><span class="mtext" id="MathJax-Span-255" style="font-family: MathJax_Typewriter;">BirdSet</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">BirdSet</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-39">\texttt{BirdSet}</script> surpasses AudioSet with over 6,800 recording hours (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-40-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2191;&lt;/mo&gt;&lt;mn&gt;17&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-256" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.55em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-257"><span class="mo" id="MathJax-Span-258" style="font-family: MathJax_Main;">↑</span><span class="mn" id="MathJax-Span-259" style="font-family: MathJax_Main; padding-left: 0.263em;">17</span><span class="mi" id="MathJax-Span-260" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo><mn>17</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-40">\uparrow17\%</script>) from nearly 10,000 classes (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-41-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2191;&lt;/mo&gt;&lt;mn&gt;18&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-261" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.4em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-262"><span class="mo" id="MathJax-Span-263" style="font-family: MathJax_Main;">↑</span><span class="mn" id="MathJax-Span-264" style="font-family: MathJax_Main; padding-left: 0.263em;">18</span><span class="mo" id="MathJax-Span-265" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo><mn>18</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-41">\uparrow18\times</script>) for training and more than 400 hours (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-42-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2191;&lt;/mo&gt;&lt;mn&gt;7&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-266" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.88em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-267"><span class="mo" id="MathJax-Span-268" style="font-family: MathJax_Main;">↑</span><span class="mn" id="MathJax-Span-269" style="font-family: MathJax_Main; padding-left: 0.263em;">7</span><span class="mo" id="MathJax-Span-270" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo><mn>7</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-42">\uparrow7\times</script>) across eight strongly labeled evaluation datasets. It serves as a versatile resource for use cases such as multi-label classification, covariate shift or self-supervised learning. We benchmark six well-known DL models in multi-label classification across three distinct training scenarios and outline further evaluation use cases in audio classification. We host our dataset on Hugging Face for easy accessibility and offer an extensive codebase to reproduce our results.</p>
            <p id="subjects-dRXxFEY8ZE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dRXxFEY8ZE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dRXxFEY8ZE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dRXxFEY8ZE@OpenReview" onclick="foldPdfKimi('dRXxFEY8ZE@OpenReview', this)" class="hr hr-fold">
        </div><div id="dDpB23VbVa@OpenReview" class="panel paper" keywords="patch,training,llms,level,costs,language,370m,token,unit,next">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dDpB23VbVa" target="_blank" title="77/373"><span class="index notranslate">#77</span></a>
                <a id="title-dDpB23VbVa@OpenReview" class="title-link" href="/venue/dDpB23VbVa@OpenReview" target="_blank">Patch-Level Training for Large Language Models</a>
                <a id="pdf-dDpB23VbVa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dDpB23VbVa@OpenReview', this)" data="https://openreview.net/pdf?id=dDpB23VbVa">[PDF<sup id="pdf-stars-dDpB23VbVa@OpenReview">6</sup>]</a>
                <a id="copy-dDpB23VbVa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dDpB23VbVa@OpenReview')">[Copy]</a>
                <a id="kimi-dDpB23VbVa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dDpB23VbVa@OpenReview', this)">[Kimi<sup id="kimi-stars-dDpB23VbVa@OpenReview">8</sup>]</a>
                <a id="rel-dDpB23VbVa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dDpB23VbVa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dDpB23VbVa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenze Shao" target="_blank">Chenze Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fandong Meng" target="_blank">Fandong Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Zhou" target="_blank">Jie Zhou</a>
            </p>
            <p id="summary-dDpB23VbVa@OpenReview" class="summary">The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-43-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-271" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-272"><span class="mo" id="MathJax-Span-273" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-43">\times</script>, without compromising the model performance compared to token-level training.</p>
            <p id="subjects-dDpB23VbVa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dDpB23VbVa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dDpB23VbVa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dDpB23VbVa@OpenReview" onclick="foldPdfKimi('dDpB23VbVa@OpenReview', this)" class="hr hr-fold">
        </div><div id="hNjCVVm0EQ@OpenReview" class="panel paper" keywords="koopman,mamko,mamba,control,modeling,systems,operator,model,predictive,nonlinear">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hNjCVVm0EQ" target="_blank" title="78/373"><span class="index notranslate">#78</span></a>
                <a id="title-hNjCVVm0EQ@OpenReview" class="title-link" href="/venue/hNjCVVm0EQ@OpenReview" target="_blank">MamKO: Mamba-based Koopman operator for modeling and predictive control</a>
                <a id="pdf-hNjCVVm0EQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hNjCVVm0EQ@OpenReview', this)" data="https://openreview.net/pdf?id=hNjCVVm0EQ">[PDF<sup id="pdf-stars-hNjCVVm0EQ@OpenReview">6</sup>]</a>
                <a id="copy-hNjCVVm0EQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hNjCVVm0EQ@OpenReview')">[Copy]</a>
                <a id="kimi-hNjCVVm0EQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hNjCVVm0EQ@OpenReview', this)">[Kimi<sup id="kimi-stars-hNjCVVm0EQ@OpenReview">4</sup>]</a>
                <a id="rel-hNjCVVm0EQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hNjCVVm0EQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hNjCVVm0EQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyang Li" target="_blank">Zhaoyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minghao Han" target="_blank">Minghao Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xunyuan Yin" target="_blank">Xunyuan Yin</a>
            </p>
            <p id="summary-hNjCVVm0EQ@OpenReview" class="summary">The Koopman theory, which enables the transformation of nonlinear systems into linear representations, is a powerful and efficient tool to model and control nonlinear systems. However, the ability of the Koopman operator to model complex systems, particularly time-varying systems, is limited by the fixed linear state-space representation. To address the limitation, the large language model, Mamba, is considered a promising strategy for enhancing modeling capabilities while preserving the linear state-space structure.In this paper, we propose a new framework, the Mamba-based Koopman operator (MamKO), which provides enhanced model prediction capability and adaptability, as compared to Koopman models with constant Koopman operators. Inspired by the Mamba structure, MamKO generates Koopman operators from online data; this enables the model to effectively capture the dynamic behaviors of the nonlinear system over time. A model predictive control system is then developed based on the proposed MamKO model. The modeling and control performance of the proposed method is evaluated through experiments on benchmark time-invariant and time-varying systems. The experimental results demonstrate the superiority of the proposed approach. Additionally, we perform ablation experiments to test the effectiveness of individual components of MamKO. This approach unlocks new possibilities for integrating large language models with control frameworks, and it achieves a good balance between advanced modeling capabilities and real-time control implementation efficiency.</p>
            <p id="subjects-hNjCVVm0EQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hNjCVVm0EQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hNjCVVm0EQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hNjCVVm0EQ@OpenReview" onclick="foldPdfKimi('hNjCVVm0EQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="y5einmJ0Yx@OpenReview" class="panel paper" keywords="ood,graph,exposure,gold,generative,embeddings,data,implicit,adversarial,instances">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=y5einmJ0Yx" target="_blank" title="79/373"><span class="index notranslate">#79</span></a>
                <a id="title-y5einmJ0Yx@OpenReview" class="title-link" href="/venue/y5einmJ0Yx@OpenReview" target="_blank">GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation</a>
                <a id="pdf-y5einmJ0Yx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('y5einmJ0Yx@OpenReview', this)" data="https://openreview.net/pdf?id=y5einmJ0Yx">[PDF<sup id="pdf-stars-y5einmJ0Yx@OpenReview">5</sup>]</a>
                <a id="copy-y5einmJ0Yx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('y5einmJ0Yx@OpenReview')">[Copy]</a>
                <a id="kimi-y5einmJ0Yx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('y5einmJ0Yx@OpenReview', this)">[Kimi<sup id="kimi-stars-y5einmJ0Yx@OpenReview">4</sup>]</a>
                <a id="rel-y5einmJ0Yx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('y5einmJ0Yx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-y5einmJ0Yx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Danny Wang" target="_blank">Danny Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruihong Qiu" target="_blank">Ruihong Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangdong Bai" target="_blank">Guangdong Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zi Huang" target="_blank">Zi Huang</a>
            </p>
            <p id="summary-y5einmJ0Yx@OpenReview" class="summary">Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines.</p>
            <p id="subjects-y5einmJ0Yx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-y5einmJ0Yx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-y5einmJ0Yx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-y5einmJ0Yx@OpenReview" onclick="foldPdfKimi('y5einmJ0Yx@OpenReview', this)" class="hr hr-fold">
        </div><div id="fU8H4lzkIm@OpenReview" class="panel paper" keywords="phympgn,spatiotemporal,pde,encoded,message,passing,graph,meshes,gnn,irregular">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fU8H4lzkIm" target="_blank" title="80/373"><span class="index notranslate">#80</span></a>
                <a id="title-fU8H4lzkIm@OpenReview" class="title-link" href="/venue/fU8H4lzkIm@OpenReview" target="_blank">PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems</a>
                <a id="pdf-fU8H4lzkIm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fU8H4lzkIm@OpenReview', this)" data="https://openreview.net/pdf?id=fU8H4lzkIm">[PDF<sup id="pdf-stars-fU8H4lzkIm@OpenReview">10</sup>]</a>
                <a id="copy-fU8H4lzkIm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fU8H4lzkIm@OpenReview')">[Copy]</a>
                <a id="kimi-fU8H4lzkIm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fU8H4lzkIm@OpenReview', this)">[Kimi<sup id="kimi-stars-fU8H4lzkIm@OpenReview">2</sup>]</a>
                <a id="rel-fU8H4lzkIm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fU8H4lzkIm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fU8H4lzkIm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bocheng Zeng" target="_blank">Bocheng Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Wang" target="_blank">Qi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengtao Yan" target="_blank">Mengtao Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Liu" target="_blank">Yang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruizhi Chengze" target="_blank">Ruizhi Chengze</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhang" target="_blank">Yi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongsheng Liu" target="_blank">Hongsheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zidong Wang" target="_blank">Zidong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Sun" target="_blank">Hao Sun</a>
            </p>
            <p id="summary-fU8H4lzkIm@OpenReview" class="summary">Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains.</p>
            <p id="subjects-fU8H4lzkIm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-fU8H4lzkIm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fU8H4lzkIm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fU8H4lzkIm@OpenReview" onclick="foldPdfKimi('fU8H4lzkIm@OpenReview', this)" class="hr hr-fold">
        </div><div id="d9aWa875kj@OpenReview" class="panel paper" keywords="flipping,poisoning,label,certificates,gnns,certification,robustness,ntk,exact,neural">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=d9aWa875kj" target="_blank" title="81/373"><span class="index notranslate">#81</span></a>
                <a id="title-d9aWa875kj@OpenReview" class="title-link" href="/venue/d9aWa875kj@OpenReview" target="_blank">Exact Certification of (Graph) Neural Networks Against Label Poisoning</a>
                <a id="pdf-d9aWa875kj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('d9aWa875kj@OpenReview', this)" data="https://openreview.net/pdf?id=d9aWa875kj">[PDF<sup id="pdf-stars-d9aWa875kj@OpenReview">3</sup>]</a>
                <a id="copy-d9aWa875kj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('d9aWa875kj@OpenReview')">[Copy]</a>
                <a id="kimi-d9aWa875kj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('d9aWa875kj@OpenReview', this)">[Kimi<sup id="kimi-stars-d9aWa875kj@OpenReview">2</sup>]</a>
                <a id="rel-d9aWa875kj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('d9aWa875kj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-d9aWa875kj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mahalakshmi Sabanayagam" target="_blank">Mahalakshmi Sabanayagam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lukas Gosch" target="_blank">Lukas Gosch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Günnemann" target="_blank">Stephan Günnemann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Debarghya Ghoshdastidar" target="_blank">Debarghya Ghoshdastidar</a>
            </p>
            <p id="summary-d9aWa875kj@OpenReview" class="summary">Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case robustness behavior. However, for Graph Neural Networks (GNNs), the problem of certifying label flipping has so far been unsolved. We change this by introducing an exact certification method, deriving both sample-wise and collective certificates. Our method leverages the Neural Tangent Kernel (NTK) to capture the training dynamics of wide networks enabling us to reformulate the bilevel optimization problem representing label flipping into a Mixed-Integer Linear Program (MILP). We apply our method to certify a broad range of GNN architectures in node classification tasks. Thereby, concerning the worst-case robustness to label flipping: <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-44-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-274" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.04em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-275"><span class="mo" id="MathJax-Span-276" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-277" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-278" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-44">(i)</script> we establish hierarchies of GNNs on different benchmark graphs; <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-45-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-279" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.36em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-280"><span class="mo" id="MathJax-Span-281" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-282" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-283" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-284" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-45">(ii)</script> quantify the effect of architectural choices such as activations, depth and skip-connections; and surprisingly, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-46-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-285" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.72em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-286"><span class="mo" id="MathJax-Span-287" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-288" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-289" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-290" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-291" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mi>i</mi><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-46">(iii)</script> uncover a novel phenomenon of the robustness plateauing for intermediate perturbation budgets across all investigated datasets and architectures. While we focus on GNNs, our certificates are applicable to sufficiently wide NNs in general through their NTK. Thus, our work presents the first exact certificate to a poisoning attack ever derived for neural networks, which could be of independent interest.</p>
            <p id="subjects-d9aWa875kj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-d9aWa875kj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-d9aWa875kj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-d9aWa875kj@OpenReview" onclick="foldPdfKimi('d9aWa875kj@OpenReview', this)" class="hr hr-fold">
        </div><div id="lqTILjL6lP@OpenReview" class="panel paper" keywords="resum,nldbd,rare,event,detector,design,surrogate,red,physics,designs">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=lqTILjL6lP" target="_blank" title="82/373"><span class="index notranslate">#82</span></a>
                <a id="title-lqTILjL6lP@OpenReview" class="title-link" href="/venue/lqTILjL6lP@OpenReview" target="_blank">RESuM: A Rare Event Surrogate Model for Physics Detector Design</a>
                <a id="pdf-lqTILjL6lP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('lqTILjL6lP@OpenReview', this)" data="https://openreview.net/pdf?id=lqTILjL6lP">[PDF<sup id="pdf-stars-lqTILjL6lP@OpenReview">2</sup>]</a>
                <a id="copy-lqTILjL6lP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('lqTILjL6lP@OpenReview')">[Copy]</a>
                <a id="kimi-lqTILjL6lP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('lqTILjL6lP@OpenReview', this)">[Kimi<sup id="kimi-stars-lqTILjL6lP@OpenReview">2</sup>]</a>
                <a id="rel-lqTILjL6lP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('lqTILjL6lP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-lqTILjL6lP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ann-Kathrin Schuetz" target="_blank">Ann-Kathrin Schuetz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Poon" target="_blank">Alan Poon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aobo Li" target="_blank">Aobo Li</a>
            </p>
            <p id="summary-lqTILjL6lP@OpenReview" class="summary">The experimental discovery of neutrinoless double-beta decay (NLDBD) would answer one of the most important questions in physics: Why is there more matter than antimatter in our universe? To maximize the chances of discovery, NLDBD experiments must optimize their detector designs to minimize the probability of background events contaminating the detector. Given that this probability is inherently low, design optimization either requires extremely costly simulations to generate sufficient background counts or contending with significant variance. In this work, we formalize this dilemma as a Rare Event Design (RED) problem: identifying optimal design parameters when the design metric to be minimized is inherently small. We then designed the Rare Event Surrogate Model (RESuM) for physics detector design optimization under RED conditions. RESuM uses a pre-trained Conditional Neural Process (CNP) model to incorporate additional prior knowledge into a Multi-Fidelity Gaussian Process model. We applied RESuM to optimize neutron shielding designs for the LEGEND NLDBD experiment, identifying an optimal design that reduces the neutron background by <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-47-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;66.5&lt;/mn&gt;&lt;mo&gt;&amp;#x00B1;&lt;/mo&gt;&lt;mn&gt;3.5&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-292" style="width: 6.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.055em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.95em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-293"><span class="mo" id="MathJax-Span-294" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-295" style="font-family: MathJax_Main;">66.5</span><span class="mo" id="MathJax-Span-296" style="font-family: MathJax_Main; padding-left: 0.211em;">±</span><span class="mn" id="MathJax-Span-297" style="font-family: MathJax_Main; padding-left: 0.211em;">3.5</span><span class="mo" id="MathJax-Span-298" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>66.5</mn><mo>±</mo><mn>3.5</mn><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-47">(66.5 \pm 3.5)</script>% while using only 3.3% of the computational resources compared to traditional methods. Given the prevalence of RED problems in other fields of physical sciences, especially in rare-event searches, the RESuM algorithm has broad potential for accelerating simulation-intensive applications.</p>
            <p id="subjects-lqTILjL6lP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-lqTILjL6lP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-lqTILjL6lP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-lqTILjL6lP@OpenReview" onclick="foldPdfKimi('lqTILjL6lP@OpenReview', this)" class="hr hr-fold">
        </div><div id="cv2iMNWCsh@OpenReview" class="panel paper" keywords="credal,wrapper,cifar10,uncertainty,bnns,imagenet,cifar100,averaging,estimation,des">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cv2iMNWCsh" target="_blank" title="83/373"><span class="index notranslate">#83</span></a>
                <a id="title-cv2iMNWCsh@OpenReview" class="title-link" href="/venue/cv2iMNWCsh@OpenReview" target="_blank">Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification</a>
                <a id="pdf-cv2iMNWCsh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cv2iMNWCsh@OpenReview', this)" data="https://openreview.net/pdf?id=cv2iMNWCsh">[PDF<sup id="pdf-stars-cv2iMNWCsh@OpenReview">7</sup>]</a>
                <a id="copy-cv2iMNWCsh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cv2iMNWCsh@OpenReview')">[Copy]</a>
                <a id="kimi-cv2iMNWCsh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cv2iMNWCsh@OpenReview', this)">[Kimi<sup id="kimi-stars-cv2iMNWCsh@OpenReview">2</sup>]</a>
                <a id="rel-cv2iMNWCsh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cv2iMNWCsh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cv2iMNWCsh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaizheng Wang" target="_blank">Kaizheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabio Cuzzolin" target="_blank">Fabio Cuzzolin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keivan Shariatmadar" target="_blank">Keivan Shariatmadar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Moens" target="_blank">David Moens</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hans Hallez" target="_blank">Hans Hallez</a>
            </p>
            <p id="summary-cv2iMNWCsh@OpenReview" class="summary">This paper presents an innovative approach, called credal wrapper, to formulating a credal set representation of model averaging for Bayesian neural networks (BNNs) and deep ensembles (DEs), capable of improving uncertainty estimation in classification tasks. Given a finite collection of single predictive distributions derived from BNNs or DEs, the proposed credal wrapper approach extracts an upper and a lower probability bound per class, acknowledging the epistemic uncertainty due to the availability of a limited amount of distributions. Such probability intervals over classes can be mapped on a convex set of probabilities (a credal set) from which, in turn, a unique prediction can be obtained using a transformation called intersection probability transformation. In this article, we conduct extensive experiments on several out-of-distribution (OOD) detection benchmarks, encompassing various dataset pairs (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, CIFAR100 vs CIFAR100-C and ImageNet vs ImageNet-O) and using different network architectures (such as VGG16, ResNet-18/50, EfficientNet B2, and ViT Base). Compared to the BNN and DE baselines, the proposed credal wrapper method exhibits superior performance in uncertainty estimation and achieves a lower expected calibration error on corrupted data.</p>
            <p id="subjects-cv2iMNWCsh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cv2iMNWCsh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cv2iMNWCsh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cv2iMNWCsh@OpenReview" onclick="foldPdfKimi('cv2iMNWCsh@OpenReview', this)" class="hr hr-fold">
        </div><div id="cqsw28DuMW@OpenReview" class="panel paper" keywords="taid,distillation,teacher,student,interpolated,mode,knowledge,language,temporally,texttt">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cqsw28DuMW" target="_blank" title="84/373"><span class="index notranslate">#84</span></a>
                <a id="title-cqsw28DuMW@OpenReview" class="title-link" href="/venue/cqsw28DuMW@OpenReview" target="_blank">TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models</a>
                <a id="pdf-cqsw28DuMW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cqsw28DuMW@OpenReview', this)" data="https://openreview.net/pdf?id=cqsw28DuMW">[PDF<sup id="pdf-stars-cqsw28DuMW@OpenReview">4</sup>]</a>
                <a id="copy-cqsw28DuMW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cqsw28DuMW@OpenReview')">[Copy]</a>
                <a id="kimi-cqsw28DuMW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cqsw28DuMW@OpenReview', this)">[Kimi<sup id="kimi-stars-cqsw28DuMW@OpenReview">8</sup>]</a>
                <a id="rel-cqsw28DuMW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cqsw28DuMW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cqsw28DuMW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Makoto Shing" target="_blank">Makoto Shing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kou Misaki" target="_blank">Kou Misaki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Bao" target="_blank">Han Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sho Yokoi" target="_blank">Sho Yokoi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Takuya Akiba" target="_blank">Takuya Akiba</a>
            </p>
            <p id="summary-cqsw28DuMW@OpenReview" class="summary">Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression.A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation.To address these issues, we introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-48-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;Temporally Adaptive Interpolated Distillation (TAID)&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-299" style="width: 27.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 23.284em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1023.28em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-300"><span class="texatom" id="MathJax-Span-301"><span class="mrow" id="MathJax-Span-302"><span class="mtext" id="MathJax-Span-303" style="font-family: MathJax_Main-italic;">Temporally Adaptive Interpolated Distillation (TAID)</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">Temporally Adaptive Interpolated Distillation (TAID)</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-48">\textit{Temporally Adaptive Interpolated Distillation (TAID)}</script>, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse.Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-49-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;TAID-LLM-1.5B&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-304" style="width: 8.232em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1006.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-305"><span class="texatom" id="MathJax-Span-306"><span class="mrow" id="MathJax-Span-307"><span class="mtext" id="MathJax-Span-308" style="font-family: MathJax_Typewriter;">TAID-LLM-1.5B</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">TAID-LLM-1.5B</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-49">\texttt{TAID-LLM-1.5B}</script> for language tasks and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-50-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;TAID-VLM-2B&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-309" style="width: 6.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-310"><span class="texatom" id="MathJax-Span-311"><span class="mrow" id="MathJax-Span-312"><span class="mtext" id="MathJax-Span-313" style="font-family: MathJax_Typewriter;">TAID-VLM-2B</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">TAID-VLM-2B</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-50">\texttt{TAID-VLM-2B}</script> for vision-language tasks.These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.</p>
            <p id="subjects-cqsw28DuMW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cqsw28DuMW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cqsw28DuMW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cqsw28DuMW@OpenReview" onclick="foldPdfKimi('cqsw28DuMW@OpenReview', this)" class="hr hr-fold">
        </div><div id="cWHonXThtM@OpenReview" class="panel paper" keywords="mipkd,distillation,teacher,knowledge,student,mixer,granularity,feature,mixture,super">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cWHonXThtM" target="_blank" title="85/373"><span class="index notranslate">#85</span></a>
                <a id="title-cWHonXThtM@OpenReview" class="title-link" href="/venue/cWHonXThtM@OpenReview" target="_blank">Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution</a>
                <a id="pdf-cWHonXThtM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cWHonXThtM@OpenReview', this)" data="https://openreview.net/pdf?id=cWHonXThtM">[PDF<sup id="pdf-stars-cWHonXThtM@OpenReview">15</sup>]</a>
                <a id="copy-cWHonXThtM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cWHonXThtM@OpenReview')">[Copy]</a>
                <a id="kimi-cWHonXThtM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cWHonXThtM@OpenReview', this)">[Kimi<sup id="kimi-stars-cWHonXThtM@OpenReview">6</sup>]</a>
                <a id="rel-cWHonXThtM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cWHonXThtM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cWHonXThtM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Simiao Li" target="_blank">Simiao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Zhang" target="_blank">Yun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Li" target="_blank">Wei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanting Chen" target="_blank">Hanting Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjia Wang" target="_blank">Wenjia Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingyi Jing" target="_blank">Bingyi Jing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohui Lin" target="_blank">Shaohui Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Hu" target="_blank">Jie Hu</a>
            </p>
            <p id="summary-cWHonXThtM@OpenReview" class="summary">Knowledge distillation (KD) is a promising yet challenging model compression technique that transfers rich learning representations from a well-performing but cumbersome teacher model to a compact student model. Previous methods for image super-resolution (SR) mostly are tailored to the specific teacher-student architectures. And the potential for improvement is limited, which hinders their wide applications. This work presents a novel KD framework for SR models, the multi-granularity mixture of prior knowledge distillation (MiPKD), that is universally applicable to a wide array of architectures at feature and block levels. The teacher’s knowledge is effectively integrated with the student's feature via the Feature Prior Mixer, and the reconstructed feature propagates dynamically in the training phase with the Block Prior Mixer. Extensive experiments demonstrate the effectiveness of the proposed MiPKD method.</p>
            <p id="subjects-cWHonXThtM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cWHonXThtM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cWHonXThtM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cWHonXThtM@OpenReview" onclick="foldPdfKimi('cWHonXThtM@OpenReview', this)" class="hr hr-fold">
        </div><div id="jCPak79Kev@OpenReview" class="panel paper" keywords="analog,ics,analoggenie,textbf,underline,generative,circuit,topologies,design,foundational">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jCPak79Kev" target="_blank" title="86/373"><span class="index notranslate">#86</span></a>
                <a id="title-jCPak79Kev@OpenReview" class="title-link" href="/venue/jCPak79Kev@OpenReview" target="_blank">AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies</a>
                <a id="pdf-jCPak79Kev@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jCPak79Kev@OpenReview', this)" data="https://openreview.net/pdf?id=jCPak79Kev">[PDF<sup id="pdf-stars-jCPak79Kev@OpenReview">6</sup>]</a>
                <a id="copy-jCPak79Kev@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jCPak79Kev@OpenReview')">[Copy]</a>
                <a id="kimi-jCPak79Kev@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jCPak79Kev@OpenReview', this)">[Kimi<sup id="kimi-stars-jCPak79Kev@OpenReview">2</sup>]</a>
                <a id="rel-jCPak79Kev@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jCPak79Kev@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jCPak79Kev@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Gao" target="_blank">Jian Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weidong Cao" target="_blank">Weidong Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyi Yang" target="_blank">Junyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Zhang" target="_blank">Xuan Zhang</a>
            </p>
            <p id="summary-jCPak79Kev@OpenReview" class="summary">The massive and large-scale design of foundational semiconductor integrated circuits (ICs) is crucial to sustaining the advancement of many emerging and future technologies, such as generative AI, 5G/6G, and quantum computing.Excitingly, recent studies have shown the great capabilities of foundational models in expediting the design of digital ICs.Yet, applying generative AI techniques to accelerate the design of analog ICs remains a significant challenge due to critical domain-specific issues, such as the lack of a comprehensive dataset and effective representation methods for analog circuits.This paper proposes, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-51-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;AnalogGenie&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-314" style="width: 7.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1006.41em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-315"><span class="texatom" id="MathJax-Span-316"><span class="mrow" id="MathJax-Span-317"><span class="mtext" id="MathJax-Span-318" style="font-family: MathJax_Main-bold;">AnalogGenie</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">AnalogGenie</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-51">\textbf{AnalogGenie}</script>, a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-52-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;Gen&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-319" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.09em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-320"><span class="munderover" id="MathJax-Span-321"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px;"><span style="position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0.003em;"><span class="texatom" id="MathJax-Span-322"><span class="mrow" id="MathJax-Span-323"><span class="mtext" id="MathJax-Span-324" style="font-family: MathJax_Main-bold;">Gen</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; clip: rect(1.721em, 1002.09em, 2.034em, -999.997em); top: -1.664em; left: 0em;"><span class="mo" id="MathJax-Span-325" style=""><span style="display: inline-block; position: relative; width: 2.086em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.617em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.367em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.784em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.201em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">Gen</mtext></mrow><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-52">\underline{\textbf{Gen}}</script>erat<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-53-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;i&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-326" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-327"><span class="munderover" id="MathJax-Span-328"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; clip: rect(1.357em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0.107em;"><span class="texatom" id="MathJax-Span-329"><span class="mrow" id="MathJax-Span-330"><span class="mtext" id="MathJax-Span-331" style="font-family: MathJax_Main-bold;">i</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; clip: rect(1.721em, 1000.52em, 2.034em, -999.997em); top: -1.664em; left: 0em;"><span class="mo" id="MathJax-Span-332" style=""><span style="font-family: MathJax_Main;">–</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">i</mtext></mrow><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-53">\underline{\textbf{i}}</script>ve <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-54-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;e&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-333" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-334"><span class="munderover" id="MathJax-Span-335"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0.003em;"><span class="texatom" id="MathJax-Span-336"><span class="mrow" id="MathJax-Span-337"><span class="mtext" id="MathJax-Span-338" style="font-family: MathJax_Main-bold;">e</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; clip: rect(1.721em, 1000.52em, 2.034em, -999.997em); top: -1.664em; left: 0em;"><span class="mo" id="MathJax-Span-339" style=""><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.055em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">e</mtext></mrow><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-54">\underline{\textbf{e}}</script>ngine for automatic design/discovery of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-55-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;Analog&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-340" style="width: 4.273em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.544em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1003.54em, 2.711em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-341"><span class="munderover" id="MathJax-Span-342"><span style="display: inline-block; position: relative; width: 3.544em; height: 0px;"><span style="position: absolute; clip: rect(1.357em, 1003.54em, 2.555em, -999.997em); top: -2.185em; left: 0.003em;"><span class="texatom" id="MathJax-Span-343"><span class="mrow" id="MathJax-Span-344"><span class="mtext" id="MathJax-Span-345" style="font-family: MathJax_Main-bold;">Analog</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; clip: rect(1.721em, 1003.54em, 2.034em, -999.997em); top: -1.456em; left: 0em;"><span class="mo" id="MathJax-Span-346" style=""><span style="display: inline-block; position: relative; width: 3.544em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 3.076em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.419em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.836em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.305em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.721em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 2.19em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 2.607em;">–<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">Analog</mtext></mrow><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-55">\underline{\textbf{Analog}}</script> circuit topologies--the most challenging and creative task in the conventional manual design flow of analog ICs.AnalogGenie addresses two key gaps in the field: building a foundational comprehensive dataset of analog circuit topology and developing a scalable sequence-based graph representation universal to analog circuits.Experimental results show the remarkable generation performance of AnalogGenie in broadening the variety of analog ICs, increasing the number of devices within a single design, and discovering unseen circuit topologies far beyond any prior arts.Our work paves the way to transform the longstanding time-consuming manual design flow of analog ICs to an automatic and massive manner powered by generative AI.</p>
            <p id="subjects-jCPak79Kev@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jCPak79Kev@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jCPak79Kev@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jCPak79Kev@OpenReview" onclick="foldPdfKimi('jCPak79Kev@OpenReview', this)" class="hr hr-fold">
        </div><div id="cD1kl2QKv1@OpenReview" class="panel paper" keywords="textit,prompt,1prompt1story,t2i,generation,consistent,identity,story,preserving,text">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cD1kl2QKv1" target="_blank" title="87/373"><span class="index notranslate">#87</span></a>
                <a id="title-cD1kl2QKv1@OpenReview" class="title-link" href="/venue/cD1kl2QKv1@OpenReview" target="_blank">One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt</a>
                <a id="pdf-cD1kl2QKv1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cD1kl2QKv1@OpenReview', this)" data="https://openreview.net/pdf?id=cD1kl2QKv1">[PDF<sup id="pdf-stars-cD1kl2QKv1@OpenReview">6</sup>]</a>
                <a id="copy-cD1kl2QKv1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cD1kl2QKv1@OpenReview')">[Copy]</a>
                <a id="kimi-cD1kl2QKv1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cD1kl2QKv1@OpenReview', this)">[Kimi<sup id="kimi-stars-cD1kl2QKv1@OpenReview">10</sup>]</a>
                <a id="rel-cD1kl2QKv1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cD1kl2QKv1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cD1kl2QKv1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Liu" target="_blank">Tao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Wang" target="_blank">Kai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Senmao Li" target="_blank">Senmao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joost van de Weijer" target="_blank">Joost van de Weijer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fahad Khan" target="_blank">Fahad Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiqi Yang" target="_blank">Shiqi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaxing Wang" target="_blank">Yaxing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Yang" target="_blank">Jian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Ming Cheng" target="_blank">Ming-Ming Cheng</a>
            </p>
            <p id="summary-cD1kl2QKv1@OpenReview" class="summary">Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-56-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;context consistency&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-347" style="width: 10.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.336em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1008.39em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-348"><span class="texatom" id="MathJax-Span-349"><span class="mrow" id="MathJax-Span-350"><span class="mtext" id="MathJax-Span-351" style="font-family: MathJax_Main-italic;">context consistency</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">context consistency</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-56">\textit{context consistency}</script>, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-57-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;context consistency&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-352" style="width: 10.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.336em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1008.39em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-353"><span class="texatom" id="MathJax-Span-354"><span class="mrow" id="MathJax-Span-355"><span class="mtext" id="MathJax-Span-356" style="font-family: MathJax_Main-italic;">context consistency</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">context consistency</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-57">\textit{context consistency}</script>, we propose a novel <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-58-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;training-free&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-357" style="width: 6.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-358"><span class="texatom" id="MathJax-Span-359"><span class="mrow" id="MathJax-Span-360"><span class="mtext" id="MathJax-Span-361" style="font-family: MathJax_Main-italic;">training-free</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">training-free</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-58">\textit{training-free}</script> method for consistent text-to-image (T2I) generation, termed "One-Prompt-One-Story" (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-59-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;1Prompt1Story&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-362" style="width: 7.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1006.67em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-363"><span class="texatom" id="MathJax-Span-364"><span class="mrow" id="MathJax-Span-365"><span class="mtext" id="MathJax-Span-366" style="font-family: MathJax_Main-italic;">1Prompt1Story</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">1Prompt1Story</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-59">\textit{1Prompt1Story}</script>). Our approach <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-60-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;1Prompt1Story&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-367" style="width: 7.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1006.67em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-368"><span class="texatom" id="MathJax-Span-369"><span class="mrow" id="MathJax-Span-370"><span class="mtext" id="MathJax-Span-371" style="font-family: MathJax_Main-italic;">1Prompt1Story</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">1Prompt1Story</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-60">\textit{1Prompt1Story}</script> concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-61-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;Singular-ValueReweighting&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-372" style="width: 14.065em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1011.77em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-373"><span class="texatom" id="MathJax-Span-374"><span class="mrow" id="MathJax-Span-375"><span class="mtext" id="MathJax-Span-376" style="font-family: MathJax_Main-italic;">Singular-ValueReweighting</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">Singular-ValueReweighting</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-61">\textit{Singular-ValueReweighting}</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-62-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;Identity-Preserving Cross-Attention&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-377" style="width: 18.701em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1015.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-378"><span class="texatom" id="MathJax-Span-379"><span class="mrow" id="MathJax-Span-380"><span class="mtext" id="MathJax-Span-381" style="font-family: MathJax_Main-italic;">Identity-Preserving Cross-Attention</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">Identity-Preserving Cross-Attention</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-62">\textit{Identity-Preserving Cross-Attention}</script>, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness, through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story.</p>
            <p id="subjects-cD1kl2QKv1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cD1kl2QKv1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cD1kl2QKv1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cD1kl2QKv1@OpenReview" onclick="foldPdfKimi('cD1kl2QKv1@OpenReview', this)" class="hr hr-fold">
        </div><div id="bjxuqI4KwU@OpenReview" class="panel paper" keywords="confounders,scm,gaussian,scms,identifiable,noise,emph,confounding,variables,presence">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bjxuqI4KwU" target="_blank" title="88/373"><span class="index notranslate">#88</span></a>
                <a id="title-bjxuqI4KwU@OpenReview" class="title-link" href="/venue/bjxuqI4KwU@OpenReview" target="_blank">Linear SCM Identification in the Presence of Confounders and Gaussian Noise</a>
                <a id="pdf-bjxuqI4KwU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bjxuqI4KwU@OpenReview', this)" data="https://openreview.net/pdf?id=bjxuqI4KwU">[PDF<sup id="pdf-stars-bjxuqI4KwU@OpenReview">3</sup>]</a>
                <a id="copy-bjxuqI4KwU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bjxuqI4KwU@OpenReview')">[Copy]</a>
                <a id="kimi-bjxuqI4KwU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bjxuqI4KwU@OpenReview', this)">[Kimi<sup id="kimi-stars-bjxuqI4KwU@OpenReview">4</sup>]</a>
                <a id="rel-bjxuqI4KwU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bjxuqI4KwU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bjxuqI4KwU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vahideh Sanjaroonpouri" target="_blank">Vahideh Sanjaroonpouri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pouria Ramazi" target="_blank">Pouria Ramazi</a>
            </p>
            <p id="summary-bjxuqI4KwU@OpenReview" class="summary">Noisy linear structural causal models (SCMs) in the presence of confounding variables are known to be identifiable if all confounding and noise variables are non-Gaussian and unidentifiable if all are Gaussian. The identifiability when only some are Gaussian remains concealed. We show that, in the presence of Gaussian noise, a linear SCM is uniquely identifiable provided that \emph{(i)} the number of confounders is at most the number of the observed variables, \emph{(ii)} the confounders do not have a Gaussian component, and \emph{(iii)} the causal structure of the SCM is known. If the third condition is relaxed, the SCM becomes finitely identifiable; more specifically, it belongs to a set of at most <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-63-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-382" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-383"><span class="mi" id="MathJax-Span-384" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-385" style="font-family: MathJax_Main;">!</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>!</mo></math></span></span><script type="math/tex" id="MathJax-Element-63">n!</script> linear SCMS, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-64-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-386" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-387"><span class="mi" id="MathJax-Span-388" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-64">n</script> is the number of observed variables. The confounders in all of these <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-65-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-389" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-390"><span class="mi" id="MathJax-Span-391" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-392" style="font-family: MathJax_Main;">!</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>!</mo></math></span></span><script type="math/tex" id="MathJax-Element-65">n!</script> SCMs share the same joint probability distribution function (PDF), which we obtain analytically. For the case where both the noise and confounders are Gaussian, we provide further insight into the existing counter-example-based unidentifiability result and demonstrate that every SCM with confounders can be represented as an SCM without confounders but with the same joint PDF.</p>
            <p id="subjects-bjxuqI4KwU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-bjxuqI4KwU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bjxuqI4KwU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bjxuqI4KwU@OpenReview" onclick="foldPdfKimi('bjxuqI4KwU@OpenReview', this)" class="hr hr-fold">
        </div><div id="bcTjW5kS4W@OpenReview" class="panel paper" keywords="netformer,connectivity,nonstationary,neuronal,activity,neural,interpretable,recordings,plasticity,key">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bcTjW5kS4W" target="_blank" title="89/373"><span class="index notranslate">#89</span></a>
                <a id="title-bcTjW5kS4W@OpenReview" class="title-link" href="/venue/bcTjW5kS4W@OpenReview" target="_blank">NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics</a>
                <a id="pdf-bcTjW5kS4W@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bcTjW5kS4W@OpenReview', this)" data="https://openreview.net/pdf?id=bcTjW5kS4W">[PDF<sup id="pdf-stars-bcTjW5kS4W@OpenReview">4</sup>]</a>
                <a id="copy-bcTjW5kS4W@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bcTjW5kS4W@OpenReview')">[Copy]</a>
                <a id="kimi-bcTjW5kS4W@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bcTjW5kS4W@OpenReview', this)">[Kimi<sup id="kimi-stars-bcTjW5kS4W@OpenReview">4</sup>]</a>
                <a id="rel-bcTjW5kS4W@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bcTjW5kS4W@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bcTjW5kS4W@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wuwei Zhang" target="_blank">Wuwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyu Lu" target="_blank">Ziyu Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Trung Le" target="_blank">Trung Le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Wang" target="_blank">Hao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Uygar Sümbül" target="_blank">Uygar Sümbül</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric SheaBrown" target="_blank">Eric SheaBrown</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Mi" target="_blank">Lu Mi</a>
            </p>
            <p id="summary-bcTjW5kS4W@OpenReview" class="summary">Neuronal dynamics are highly nonlinear and nonstationary. Traditional methods for extracting the underlying network structure from neuronal activity recordings mainly concentrate on modeling static connectivity, without accounting for key nonstationary aspects of biological neural systems, such as ongoing synaptic plasticity and neuronal modulation. To bridge this gap, we introduce the NetFormer model, an interpretable approach applicable to such systems. In NetFormer, the activity of each neuron across a series of historical time steps is defined as a token. These tokens are then linearly mapped through a query and key mechanism to generate a state- (and hence time-) dependent attention matrix that directly encodes nonstationary connectivity structures. We analyze our formulation from the perspective of nonstationary and nonlinear networked dynamical systems, and show both via an analytical expansion and targeted simulations how it can approximate the underlying ground truth. Next, we demonstrate NetFormer's ability to model a key feature of biological networks, spike-timing-dependent plasticity, whereby connection strengths continually change in response to local activity patterns. We further demonstrate that NetFormer can capture task-induced connectivity patterns on activity generated by task-trained recurrent neural networks. Thus informed, we apply NetFormer to a multi-modal dataset of real neural recordings, which contains neural activity, cell type, and behavioral state information. We show that the NetFormer effectively predicts neural dynamics and identifies cell-type specific, state-dependent dynamic connectivity that matches patterns measured in separate ground-truth physiology experiments, demonstrating its ability to help decode complex neural interactions based on population activity observations alone.</p>
            <p id="subjects-bcTjW5kS4W@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-bcTjW5kS4W@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bcTjW5kS4W@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bcTjW5kS4W@OpenReview" onclick="foldPdfKimi('bcTjW5kS4W@OpenReview', this)" class="hr hr-fold">
        </div><div id="bW9fGYo44s@OpenReview" class="panel paper" keywords="video,motionaura,spatiotemporal,videos,quality,inpainting,generation,sota,denoising,mbq">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bW9fGYo44s" target="_blank" title="90/373"><span class="index notranslate">#90</span></a>
                <a id="title-bW9fGYo44s@OpenReview" class="title-link" href="/venue/bW9fGYo44s@OpenReview" target="_blank">MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion</a>
                <a id="pdf-bW9fGYo44s@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bW9fGYo44s@OpenReview', this)" data="https://openreview.net/pdf?id=bW9fGYo44s">[PDF<sup id="pdf-stars-bW9fGYo44s@OpenReview">10</sup>]</a>
                <a id="copy-bW9fGYo44s@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bW9fGYo44s@OpenReview')">[Copy]</a>
                <a id="kimi-bW9fGYo44s@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bW9fGYo44s@OpenReview', this)">[Kimi<sup id="kimi-stars-bW9fGYo44s@OpenReview">6</sup>]</a>
                <a id="rel-bW9fGYo44s@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bW9fGYo44s@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bW9fGYo44s@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Onkar Susladkar" target="_blank">Onkar Susladkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jishu Sen Gupta" target="_blank">Jishu Sen Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chirag Sehgal" target="_blank">Chirag Sehgal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sparsh Mittal" target="_blank">Sparsh Mittal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rekha Singhal" target="_blank">Rekha Singhal</a>
            </p>
            <p id="summary-bW9fGYo44s@OpenReview" class="summary">The spatio-temporal complexity of video data presents significant challenges in tasks such as compression, generation, and inpainting. We present four key contributions to address the challenges of spatiotemporal video processing. First, we introduce the 3D Mobile Inverted Vector-Quantization Variational Autoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with masked modeling to enhance spatiotemporal video compression. The model achieves superior temporal consistency and state-of-the-art (SOTA) reconstruction quality by employing a novel training strategy with full frame masking. Second, we present MotionAura, a text-to-video generation framework that utilizes vector-quantized diffusion models to discretize the latent space and capture complex motion dynamics, producing temporally coherent videos aligned with text prompts. Third, we propose a spectral transformer-based denoising network that processes video data in the frequency domain using the Fourier Transform. This method effectively captures global context and long-range dependencies for high-quality video generation and denoising. Lastly, we introduce a downstream task of Sketch Guided Video Inpainting. This task leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. Our models achieve SOTA performance on a range of benchmarks. Our work offers robust frameworks for spatiotemporal modeling and user-driven video content manipulation. We will release the code, dataset, and models in open-source.</p>
            <p id="subjects-bW9fGYo44s@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-bW9fGYo44s@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bW9fGYo44s@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bW9fGYo44s@OpenReview" onclick="foldPdfKimi('bW9fGYo44s@OpenReview', this)" class="hr hr-fold">
        </div><div id="bMC1t7eLRc@OpenReview" class="panel paper" keywords="instances,influence,diversity,pretraining,texttt,data,quad,quality,clusters,selection">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bMC1t7eLRc" target="_blank" title="91/373"><span class="index notranslate">#91</span></a>
                <a id="title-bMC1t7eLRc@OpenReview" class="title-link" href="/venue/bMC1t7eLRc@OpenReview" target="_blank">Harnessing Diversity for Important Data Selection in Pretraining Large Language Models</a>
                <a id="pdf-bMC1t7eLRc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bMC1t7eLRc@OpenReview', this)" data="https://openreview.net/pdf?id=bMC1t7eLRc">[PDF<sup id="pdf-stars-bMC1t7eLRc@OpenReview">5</sup>]</a>
                <a id="copy-bMC1t7eLRc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bMC1t7eLRc@OpenReview')">[Copy]</a>
                <a id="kimi-bMC1t7eLRc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bMC1t7eLRc@OpenReview', this)">[Kimi<sup id="kimi-stars-bMC1t7eLRc@OpenReview">12</sup>]</a>
                <a id="rel-bMC1t7eLRc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bMC1t7eLRc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bMC1t7eLRc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chi Zhang" target="_blank">Chi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaping Zhong" target="_blank">Huaping Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuan Zhang" target="_blank">Kuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengliang Chai" target="_blank">Chengliang Chai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Wang" target="_blank">Rui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlin Zhuang" target="_blank">Xinlin Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Bai" target="_blank">Tianyi Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiu Jiantao" target="_blank">Qiu Jiantao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Cao" target="_blank">Lei Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ju Fan" target="_blank">Ju Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Yuan" target="_blank">Ye Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guoren Wang" target="_blank">Guoren Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Conghui He" target="_blank">Conghui He</a>
            </p>
            <p id="summary-bMC1t7eLRc@OpenReview" class="summary">Data selection is of great significance in pretraining large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-66-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-393" style="width: 2.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.93em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-394"><span class="mi" id="MathJax-Span-395" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-396" style="font-family: MathJax_Main;">.</span><span class="mi" id="MathJax-Span-397" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">e</span><span class="mo" id="MathJax-Span-398" style="font-family: MathJax_Main;">.</span><span class="mo" id="MathJax-Span-399" style="font-family: MathJax_Main; padding-left: 0.159em;">,</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mo>.</mo><mi>e</mi><mo>.</mo><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-66">i.e.,</script> a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-67-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-400" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-401"><span class="mi" id="MathJax-Span-402" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-67">k</script> instances with the highest scores. However, this approach has several limitations. (1) Calculating the accurate influence of all available data is time-consuming.(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.In this paper, we introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-68-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;Quad&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-403" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.09em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-404"><span class="texatom" id="MathJax-Span-405"><span class="mrow" id="MathJax-Span-406"><span class="mtext" id="MathJax-Span-407" style="font-family: MathJax_Typewriter;">Quad</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">Quad</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-68">\texttt{Quad}</script>, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.To compute the influence (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-69-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-408" style="width: 2.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.93em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-409"><span class="mi" id="MathJax-Span-410" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-411" style="font-family: MathJax_Main;">.</span><span class="mi" id="MathJax-Span-412" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">e</span><span class="mo" id="MathJax-Span-413" style="font-family: MathJax_Main;">.</span><span class="mo" id="MathJax-Span-414" style="font-family: MathJax_Main; padding-left: 0.159em;">,</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mo>.</mo><mi>e</mi><mo>.</mo><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-69">i.e.,</script> the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. For the diversity, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-70-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;Quad&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-415" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.09em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-416"><span class="texatom" id="MathJax-Span-417"><span class="mrow" id="MathJax-Span-418"><span class="mtext" id="MathJax-Span-419" style="font-family: MathJax_Typewriter;">Quad</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">Quad</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-70">\texttt{Quad}</script> clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity. Experiments on Slimpajama and FineWeb over 7B large language models demonstrate that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-71-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;Quad&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-420" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.09em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-421"><span class="texatom" id="MathJax-Span-422"><span class="mrow" id="MathJax-Span-423"><span class="mtext" id="MathJax-Span-424" style="font-family: MathJax_Typewriter;">Quad</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">Quad</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-71">\texttt{Quad}</script> significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation.</p>
            <p id="subjects-bMC1t7eLRc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-bMC1t7eLRc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bMC1t7eLRc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bMC1t7eLRc@OpenReview" onclick="foldPdfKimi('bMC1t7eLRc@OpenReview', this)" class="hr hr-fold">
        </div><div id="auZZ2gN0ZN@OpenReview" class="panel paper" keywords="captioning,task,video,vidstg,vln,disjoint,supervision,grounding,dense,scenic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=auZZ2gN0ZN" target="_blank" title="92/373"><span class="index notranslate">#92</span></a>
                <a id="title-auZZ2gN0ZN@OpenReview" class="title-link" href="/venue/auZZ2gN0ZN@OpenReview" target="_blank">Dense Video Object Captioning from Disjoint Supervision</a>
                <a id="pdf-auZZ2gN0ZN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('auZZ2gN0ZN@OpenReview', this)" data="https://openreview.net/pdf?id=auZZ2gN0ZN">[PDF<sup id="pdf-stars-auZZ2gN0ZN@OpenReview">3</sup>]</a>
                <a id="copy-auZZ2gN0ZN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('auZZ2gN0ZN@OpenReview')">[Copy]</a>
                <a id="kimi-auZZ2gN0ZN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('auZZ2gN0ZN@OpenReview', this)">[Kimi<sup id="kimi-stars-auZZ2gN0ZN@OpenReview">2</sup>]</a>
                <a id="rel-auZZ2gN0ZN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('auZZ2gN0ZN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-auZZ2gN0ZN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyi Zhou" target="_blank">Xingyi Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anurag Arnab" target="_blank">Anurag Arnab</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Sun" target="_blank">Chen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cordelia Schmid" target="_blank">Cordelia Schmid</a>
            </p>
            <p id="summary-auZZ2gN0ZN@OpenReview" class="summary">We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task, and show how we can repurpose existing video grounding datasets (e.g. VidSTG and VLN) for our new task. We show that our model improves upon a number of strong baselines for this new task. Furthermore, we can apply our model to the task of spatial grounding, outperforming prior state-of-the-art on VidSTG and VLN, without explicitly training for it. Our code is available at https://github.com/google-research/scenic.</p>
            <p id="subjects-auZZ2gN0ZN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-auZZ2gN0ZN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-auZZ2gN0ZN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-auZZ2gN0ZN@OpenReview" onclick="foldPdfKimi('auZZ2gN0ZN@OpenReview', this)" class="hr hr-fold">
        </div><div id="agHddsQhsL@OpenReview" class="panel paper" keywords="customization,unauthorized,attacks,targeted,diffusion,protection,protections,untargeted,attack,poisoning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=agHddsQhsL" target="_blank" title="93/373"><span class="index notranslate">#93</span></a>
                <a id="title-agHddsQhsL@OpenReview" class="title-link" href="/venue/agHddsQhsL@OpenReview" target="_blank">Targeted Attack Improves Protection against Unauthorized Diffusion Customization</a>
                <a id="pdf-agHddsQhsL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('agHddsQhsL@OpenReview', this)" data="https://openreview.net/pdf?id=agHddsQhsL">[PDF<sup id="pdf-stars-agHddsQhsL@OpenReview">8</sup>]</a>
                <a id="copy-agHddsQhsL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('agHddsQhsL@OpenReview')">[Copy]</a>
                <a id="kimi-agHddsQhsL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('agHddsQhsL@OpenReview', this)">[Kimi<sup id="kimi-stars-agHddsQhsL@OpenReview">6</sup>]</a>
                <a id="rel-agHddsQhsL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('agHddsQhsL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-agHddsQhsL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Boyang Zheng" target="_blank">Boyang Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chumeng Liang" target="_blank">Chumeng Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Wu" target="_blank">Xiaoyu Wu</a>
            </p>
            <p id="summary-agHddsQhsL@OpenReview" class="summary">Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve into the mechanism of attack-based protections and propose a hypothesis based on our observation, which enhances the comprehension of attack-based protections. To the best of our knowledge, we are the first to both reveal the vulnerability of diffusion models to targeted attacks and leverage targeted attacks to enhance protection against unauthorized diffusion customization.</p>
            <p id="subjects-agHddsQhsL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-agHddsQhsL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-agHddsQhsL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-agHddsQhsL@OpenReview" onclick="foldPdfKimi('agHddsQhsL@OpenReview', this)" class="hr hr-fold">
        </div><div id="aZ1gNJu8wO@OpenReview" class="panel paper" keywords="memorization,mmh,memorized,manifold,generative,framework,geometric,hypothesis,dimensionalities,datapoint">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aZ1gNJu8wO" target="_blank" title="94/373"><span class="index notranslate">#94</span></a>
                <a id="title-aZ1gNJu8wO@OpenReview" class="title-link" href="/venue/aZ1gNJu8wO@OpenReview" target="_blank">A Geometric Framework for Understanding Memorization in Generative Models</a>
                <a id="pdf-aZ1gNJu8wO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aZ1gNJu8wO@OpenReview', this)" data="https://openreview.net/pdf?id=aZ1gNJu8wO">[PDF<sup id="pdf-stars-aZ1gNJu8wO@OpenReview">7</sup>]</a>
                <a id="copy-aZ1gNJu8wO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aZ1gNJu8wO@OpenReview')">[Copy]</a>
                <a id="kimi-aZ1gNJu8wO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aZ1gNJu8wO@OpenReview', this)">[Kimi<sup id="kimi-stars-aZ1gNJu8wO@OpenReview">8</sup>]</a>
                <a id="rel-aZ1gNJu8wO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aZ1gNJu8wO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aZ1gNJu8wO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Brendan Ross" target="_blank">Brendan Ross</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hamidreza Kamkari" target="_blank">Hamidreza Kamkari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tongzi Wu" target="_blank">Tongzi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rasa Hosseinzadeh" target="_blank">Rasa Hosseinzadeh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyan Liu" target="_blank">Zhaoyan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=George Stein" target="_blank">George Stein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jesse Cresswell" target="_blank">Jesse Cresswell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Loaiza-Ganem" target="_blank">Gabriel Loaiza-Ganem</a>
            </p>
            <p id="summary-aZ1gNJu8wO@OpenReview" class="summary">As deep generative models have progressed, recent work has shown them to be capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by memorization. To better understand this phenomenon, we propose the *manifold memorization hypothesis* (MMH), a geometric framework which leverages the manifold hypothesis into a clear language in which to reason about memorization. We propose to analyze memorization in terms of the relationship between the dimensionalities of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-72-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-425" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.04em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-426"><span class="mo" id="MathJax-Span-427" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-428" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-429" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-72">(i)</script> the ground truth data manifold and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-73-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-430" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.36em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-431"><span class="mo" id="MathJax-Span-432" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-433" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-434" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-435" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>i</mi><mi>i</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-73">(ii)</script> the manifold learned by the model. This framework provides a formal standard for "how memorized" a datapoint is and systematically categorizes memorized data into two types: memorization driven by overfitting and memorization driven by the underlying data distribution. By analyzing prior work in the context of the MMH, we explain and unify assorted observations in the literature. We empirically validate the MMH using synthetic data and image datasets up to the scale of Stable Diffusion, developing new tools for detecting and preventing generation of memorized samples in the process.</p>
            <p id="subjects-aZ1gNJu8wO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-aZ1gNJu8wO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aZ1gNJu8wO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aZ1gNJu8wO@OpenReview" onclick="foldPdfKimi('aZ1gNJu8wO@OpenReview', this)" class="hr hr-fold">
        </div><div id="aX7X9z3vQS@OpenReview" class="panel paper" keywords="ollivier,manifold,manl,orc,ricci,nearest,neighbor,curvature,edges,recovering">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aX7X9z3vQS" target="_blank" title="95/373"><span class="index notranslate">#95</span></a>
                <a id="title-aX7X9z3vQS@OpenReview" class="title-link" href="/venue/aX7X9z3vQS@OpenReview" target="_blank">Recovering Manifold Structure Using Ollivier Ricci Curvature</a>
                <a id="pdf-aX7X9z3vQS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aX7X9z3vQS@OpenReview', this)" data="https://openreview.net/pdf?id=aX7X9z3vQS">[PDF<sup id="pdf-stars-aX7X9z3vQS@OpenReview">4</sup>]</a>
                <a id="copy-aX7X9z3vQS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aX7X9z3vQS@OpenReview')">[Copy]</a>
                <a id="kimi-aX7X9z3vQS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aX7X9z3vQS@OpenReview', this)">[Kimi<sup id="kimi-stars-aX7X9z3vQS@OpenReview">2</sup>]</a>
                <a id="rel-aX7X9z3vQS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aX7X9z3vQS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aX7X9z3vQS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tristan L. Saidi" target="_blank">Tristan L. Saidi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abigail Hickok" target="_blank">Abigail Hickok</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew J Blumberg" target="_blank">Andrew J Blumberg</a>
            </p>
            <p id="summary-aX7X9z3vQS@OpenReview" class="summary">We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings.</p>
            <p id="subjects-aX7X9z3vQS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-aX7X9z3vQS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aX7X9z3vQS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aX7X9z3vQS@OpenReview" onclick="foldPdfKimi('aX7X9z3vQS@OpenReview', this)" class="hr hr-fold">
        </div><div id="aMBSY2ebPw@OpenReview" class="panel paper" keywords="xlr,grammar,book,translation,grammatical,linguistic,parallel,llms,resource,books">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aMBSY2ebPw" target="_blank" title="96/373"><span class="index notranslate">#96</span></a>
                <a id="title-aMBSY2ebPw@OpenReview" class="title-link" href="/venue/aMBSY2ebPw@OpenReview" target="_blank">Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?</a>
                <a id="pdf-aMBSY2ebPw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aMBSY2ebPw@OpenReview', this)" data="https://openreview.net/pdf?id=aMBSY2ebPw">[PDF<sup id="pdf-stars-aMBSY2ebPw@OpenReview">1</sup>]</a>
                <a id="copy-aMBSY2ebPw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aMBSY2ebPw@OpenReview')">[Copy]</a>
                <a id="kimi-aMBSY2ebPw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aMBSY2ebPw@OpenReview', this)">[Kimi<sup id="kimi-stars-aMBSY2ebPw@OpenReview">11</sup>]</a>
                <a id="rel-aMBSY2ebPw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aMBSY2ebPw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aMBSY2ebPw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seth Aycock" target="_blank">Seth Aycock</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Stap" target="_blank">David Stap</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Wu" target="_blank">Di Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christof Monz" target="_blank">Christof Monz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Khalil Simaan" target="_blank">Khalil Simaan</a>
            </p>
            <p id="summary-aMBSY2ebPw@OpenReview" class="summary">Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests prompting long-context LLMs with one grammar book enables English–Kalamang translation, an unseen XLR language—a noteworthy case of linguistic knowledge helping an NLP task. We investigate whether the book's grammatical explanations or its parallel examples are most effective for learning XLR translation, finding almost all improvement stems from the parallel examples. We find similar results for Nepali and Guarani, seen low-resource languages, and achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate *where* grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what *kind* of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we suggest data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.</p>
            <p id="subjects-aMBSY2ebPw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-aMBSY2ebPw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aMBSY2ebPw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aMBSY2ebPw@OpenReview" onclick="foldPdfKimi('aMBSY2ebPw@OpenReview', this)" class="hr hr-fold">
        </div><div id="t8qcGXaepr@OpenReview" class="panel paper" keywords="editing,knowledge,overfit,edit,lti,overfitting,edited,llms,uncovering,unedited">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=t8qcGXaepr" target="_blank" title="97/373"><span class="index notranslate">#97</span></a>
                <a id="title-t8qcGXaepr@OpenReview" class="title-link" href="/venue/t8qcGXaepr@OpenReview" target="_blank">Uncovering Overfitting in Large Language Model Editing</a>
                <a id="pdf-t8qcGXaepr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('t8qcGXaepr@OpenReview', this)" data="https://openreview.net/pdf?id=t8qcGXaepr">[PDF<sup id="pdf-stars-t8qcGXaepr@OpenReview">15</sup>]</a>
                <a id="copy-t8qcGXaepr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('t8qcGXaepr@OpenReview')">[Copy]</a>
                <a id="kimi-t8qcGXaepr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('t8qcGXaepr@OpenReview', this)">[Kimi<sup id="kimi-stars-t8qcGXaepr@OpenReview">14</sup>]</a>
                <a id="rel-t8qcGXaepr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('t8qcGXaepr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-t8qcGXaepr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mengqi Zhang" target="_blank">Mengqi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaotian Ye" target="_blank">Xiaotian Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Liu" target="_blank">Qiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=shu wu" target="_blank">shu wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengjie Ren" target="_blank">Pengjie Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhumin Chen" target="_blank">Zhumin Chen</a>
            </p>
            <p id="summary-t8qcGXaepr@OpenReview" class="summary">Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of **Editing Overfit**, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs’ knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn to Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.</p>
            <p id="subjects-t8qcGXaepr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-t8qcGXaepr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-t8qcGXaepr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-t8qcGXaepr@OpenReview" onclick="foldPdfKimi('t8qcGXaepr@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZGkfoufDaU@OpenReview" class="panel paper" keywords="min,training,reference,pre,maxima,detection,wikimia,insightfully,mimir,improved">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZGkfoufDaU" target="_blank" title="98/373"><span class="index notranslate">#98</span></a>
                <a id="title-ZGkfoufDaU@OpenReview" class="title-link" href="/venue/ZGkfoufDaU@OpenReview" target="_blank">Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models</a>
                <a id="pdf-ZGkfoufDaU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZGkfoufDaU@OpenReview', this)" data="https://openreview.net/pdf?id=ZGkfoufDaU">[PDF<sup id="pdf-stars-ZGkfoufDaU@OpenReview">2</sup>]</a>
                <a id="copy-ZGkfoufDaU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZGkfoufDaU@OpenReview')">[Copy]</a>
                <a id="kimi-ZGkfoufDaU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZGkfoufDaU@OpenReview', this)">[Kimi<sup id="kimi-stars-ZGkfoufDaU@OpenReview">5</sup>]</a>
                <a id="rel-ZGkfoufDaU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZGkfoufDaU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZGkfoufDaU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyang Zhang" target="_blank">Jingyang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingwei Sun" target="_blank">Jingwei Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Yeats" target="_blank">Eric Yeats</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Ouyang" target="_blank">Yang Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Kuo" target="_blank">Martin Kuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyi Zhang" target="_blank">Jianyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Yang" target="_blank">Hao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hai Li" target="_blank">Hai Li</a>
            </p>
            <p id="summary-ZGkfoufDaU@OpenReview" class="summary">The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings (evaluated with 5 families of 10 models and 2 benchmarks). On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.</p>
            <p id="subjects-ZGkfoufDaU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ZGkfoufDaU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZGkfoufDaU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZGkfoufDaU@OpenReview" onclick="foldPdfKimi('ZGkfoufDaU@OpenReview', this)" class="hr hr-fold">
        </div><div id="YwJkv2YqBq@OpenReview" class="panel paper" keywords="nag,nesterov,convex,benignly,optimization,additive,landscapes,overparametrized,notoriously,historically">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YwJkv2YqBq" target="_blank" title="99/373"><span class="index notranslate">#99</span></a>
                <a id="title-YwJkv2YqBq@OpenReview" class="title-link" href="/venue/YwJkv2YqBq@OpenReview" target="_blank">Nesterov acceleration in benignly non-convex landscapes</a>
                <a id="pdf-YwJkv2YqBq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YwJkv2YqBq@OpenReview', this)" data="https://openreview.net/pdf?id=YwJkv2YqBq">[PDF<sup id="pdf-stars-YwJkv2YqBq@OpenReview">4</sup>]</a>
                <a id="copy-YwJkv2YqBq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YwJkv2YqBq@OpenReview')">[Copy]</a>
                <a id="kimi-YwJkv2YqBq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YwJkv2YqBq@OpenReview', this)">[Kimi<sup id="kimi-stars-YwJkv2YqBq@OpenReview">4</sup>]</a>
                <a id="rel-YwJkv2YqBq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YwJkv2YqBq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YwJkv2YqBq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kanan Gupta" target="_blank">Kanan Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Wojtowytsch" target="_blank">Stephan Wojtowytsch</a>
            </p>
            <p id="summary-YwJkv2YqBq@OpenReview" class="summary">While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a `benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling.</p>
            <p id="subjects-YwJkv2YqBq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-YwJkv2YqBq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YwJkv2YqBq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YwJkv2YqBq@OpenReview" onclick="foldPdfKimi('YwJkv2YqBq@OpenReview', this)" class="hr hr-fold">
        </div><div id="e1wDDFmlVu@OpenReview" class="panel paper" keywords="moe,forecasting,time,series,billion,foundation,models,scale,pre,experts">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=e1wDDFmlVu" target="_blank" title="100/373"><span class="index notranslate">#100</span></a>
                <a id="title-e1wDDFmlVu@OpenReview" class="title-link" href="/venue/e1wDDFmlVu@OpenReview" target="_blank">Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts</a>
                <a id="pdf-e1wDDFmlVu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('e1wDDFmlVu@OpenReview', this)" data="https://openreview.net/pdf?id=e1wDDFmlVu">[PDF<sup id="pdf-stars-e1wDDFmlVu@OpenReview">10</sup>]</a>
                <a id="copy-e1wDDFmlVu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('e1wDDFmlVu@OpenReview')">[Copy]</a>
                <a id="kimi-e1wDDFmlVu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('e1wDDFmlVu@OpenReview', this)">[Kimi<sup id="kimi-stars-e1wDDFmlVu@OpenReview">10</sup>]</a>
                <a id="rel-e1wDDFmlVu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('e1wDDFmlVu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-e1wDDFmlVu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoming Shi" target="_blank">Xiaoming Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyu Wang" target="_blank">Shiyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqi Nie" target="_blank">Yuqi Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dianqi Li" target="_blank">Dianqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhou Ye" target="_blank">Zhou Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingsong Wen" target="_blank">Qingsong Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Jin" target="_blank">Ming Jin</a>
            </p>
            <p id="summary-e1wDDFmlVu@OpenReview" class="summary">Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.</p>
            <p id="subjects-e1wDDFmlVu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-e1wDDFmlVu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-e1wDDFmlVu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-e1wDDFmlVu@OpenReview" onclick="foldPdfKimi('e1wDDFmlVu@OpenReview', this)" class="hr hr-fold">
        </div><div id="Y2RW9EVwhT@OpenReview" class="panel paper" keywords="encoders,mllms,vision,eagle,mixture,multimodal,visual,tokens,design,strategies">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Y2RW9EVwhT" target="_blank" title="101/373"><span class="index notranslate">#101</span></a>
                <a id="title-Y2RW9EVwhT@OpenReview" class="title-link" href="/venue/Y2RW9EVwhT@OpenReview" target="_blank">Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</a>
                <a id="pdf-Y2RW9EVwhT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Y2RW9EVwhT@OpenReview', this)" data="https://openreview.net/pdf?id=Y2RW9EVwhT">[PDF<sup id="pdf-stars-Y2RW9EVwhT@OpenReview">10</sup>]</a>
                <a id="copy-Y2RW9EVwhT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Y2RW9EVwhT@OpenReview')">[Copy]</a>
                <a id="kimi-Y2RW9EVwhT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Y2RW9EVwhT@OpenReview', this)">[Kimi<sup id="kimi-stars-Y2RW9EVwhT@OpenReview">8</sup>]</a>
                <a id="rel-Y2RW9EVwhT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Y2RW9EVwhT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Y2RW9EVwhT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Min Shi" target="_blank">Min Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fuxiao Liu" target="_blank">Fuxiao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shihao Wang" target="_blank">Shihao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijia Liao" target="_blank">Shijia Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Subhashree Radhakrishnan" target="_blank">Subhashree Radhakrishnan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilin Zhao" target="_blank">Yilin Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=De-An Huang" target="_blank">De-An Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongxu Yin" target="_blank">Hongxu Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karan Sapra" target="_blank">Karan Sapra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaser Yacoob" target="_blank">Yaser Yacoob</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Humphrey Shi" target="_blank">Humphrey Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bryan Catanzaro" target="_blank">Bryan Catanzaro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Tao" target="_blank">Andrew Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Kautz" target="_blank">Jan Kautz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiding Yu" target="_blank">Zhiding Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guilin Liu" target="_blank">Guilin Liu</a>
            </p>
            <p id="summary-Y2RW9EVwhT@OpenReview" class="summary">The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks.</p>
            <p id="subjects-Y2RW9EVwhT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Y2RW9EVwhT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Y2RW9EVwhT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Y2RW9EVwhT@OpenReview" onclick="foldPdfKimi('Y2RW9EVwhT@OpenReview', this)" class="hr hr-fold">
        </div><div id="awvJBtB2op@OpenReview" class="panel paper" keywords="endoskeletal,robots,bodies,jointed,rigid,freeform,soft,jointless,embodied,terrestrial">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=awvJBtB2op" target="_blank" title="102/373"><span class="index notranslate">#102</span></a>
                <a id="title-awvJBtB2op@OpenReview" class="title-link" href="/venue/awvJBtB2op@OpenReview" target="_blank">Generating Freeform Endoskeletal Robots</a>
                <a id="pdf-awvJBtB2op@OpenReview" class="title-pdf notranslate" onclick="togglePdf('awvJBtB2op@OpenReview', this)" data="https://openreview.net/pdf?id=awvJBtB2op">[PDF<sup id="pdf-stars-awvJBtB2op@OpenReview">1</sup>]</a>
                <a id="copy-awvJBtB2op@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('awvJBtB2op@OpenReview')">[Copy]</a>
                <a id="kimi-awvJBtB2op@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('awvJBtB2op@OpenReview', this)">[Kimi<sup id="kimi-stars-awvJBtB2op@OpenReview">2</sup>]</a>
                <a id="rel-awvJBtB2op@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('awvJBtB2op@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-awvJBtB2op@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Muhan Li" target="_blank">Muhan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingji Kong" target="_blank">Lingji Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sam Kriegman" target="_blank">Sam Kriegman</a>
            </p>
            <p id="summary-awvJBtB2op@OpenReview" class="summary">The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless bodies. Here we bridge these two extremes with the open ended creation of terrestrial endoskeletal robots: deformable soft bodies that leverage jointed internal skeletons to move efficiently across land. Simultaneous de novo generation of external and internal structures is achieved by (i) modeling 3D endoskeletal body plans as integrated collections of elastic and rigid cells that directly attach to form soft tissues anchored to compound rigid bodies; (ii) encoding these discrete mechanical subsystems into a continuous yet coherent latent embedding; (iii) optimizing the sensorimotor coordination of each decoded design using model-free reinforcement learning; and (iv) navigating this smooth yet highly non-convex latent manifold using evolutionary strategies. This yields an endless stream of novel species of ``higher robots'' that, like all higher animals, harness the mechanical advantages of both elastic tissues and skeletal levers for terrestrial travel. It also provides a plug-and-play experimental platform for benchmarking evolutionary design and representation learning algorithms in complex hierarchical embodied systems.</p>
            <p id="subjects-awvJBtB2op@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-awvJBtB2op@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-awvJBtB2op@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-awvJBtB2op@OpenReview" onclick="foldPdfKimi('awvJBtB2op@OpenReview', this)" class="hr hr-fold">
        </div><div id="YhfrKB3Ah7@OpenReview" class="panel paper" keywords="pbo,preferential,amortized,pabbo,surrogate,acquisition,amortizing,optimization,latent,feedback">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YhfrKB3Ah7" target="_blank" title="103/373"><span class="index notranslate">#103</span></a>
                <a id="title-YhfrKB3Ah7@OpenReview" class="title-link" href="/venue/YhfrKB3Ah7@OpenReview" target="_blank">PABBO: Preferential Amortized Black-Box Optimization</a>
                <a id="pdf-YhfrKB3Ah7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YhfrKB3Ah7@OpenReview', this)" data="https://openreview.net/pdf?id=YhfrKB3Ah7">[PDF<sup id="pdf-stars-YhfrKB3Ah7@OpenReview">3</sup>]</a>
                <a id="copy-YhfrKB3Ah7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YhfrKB3Ah7@OpenReview')">[Copy]</a>
                <a id="kimi-YhfrKB3Ah7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YhfrKB3Ah7@OpenReview', this)">[Kimi<sup id="kimi-stars-YhfrKB3Ah7@OpenReview">3</sup>]</a>
                <a id="rel-YhfrKB3Ah7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YhfrKB3Ah7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YhfrKB3Ah7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Zhang" target="_blank">Xinyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daolang Huang" target="_blank">Daolang Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julien Martinelli" target="_blank">Julien Martinelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Kaski" target="_blank">Samuel Kaski</a>
            </p>
            <p id="summary-YhfrKB3Ah7@OpenReview" class="summary">Preferential Bayesian Optimization (PBO) is a sample-efficient method to learn latent user utilities from preferential feedback over a pair of designs. It relies on a statistical surrogate model for the latent function, usually a Gaussian process, and an acquisition strategy to select the next candidate pair to get user feedback on. Due to the non-conjugacy of the associated likelihood, every PBO step requires a significant amount of computations with various approximate inference techniques. This computational overhead is incompatible with the way humans interact with computers, hindering the use of PBO in real-world cases. Building on the recent advances of amortized BO, we propose to circumvent this issue by fully amortizing PBO, meta-learning both the surrogate and the acquisition function. Our method comprises a novel transformer neural process architecture, trained using reinforcement learning and tailored auxiliary losses.On a benchmark composed of synthetic and real-world datasets, our method is several orders of magnitude faster than the usual Gaussian process-based strategies and often outperforms them in accuracy.</p>
            <p id="subjects-YhfrKB3Ah7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-YhfrKB3Ah7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YhfrKB3Ah7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YhfrKB3Ah7@OpenReview" onclick="foldPdfKimi('YhfrKB3Ah7@OpenReview', this)" class="hr hr-fold">
        </div><div id="WzCEiBILHu@OpenReview" class="panel paper" keywords="topological,emph,matching,mathcal,bridge,schrödinger,unknowns,distributions,process,reference">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WzCEiBILHu" target="_blank" title="104/373"><span class="index notranslate">#104</span></a>
                <a id="title-WzCEiBILHu@OpenReview" class="title-link" href="/venue/WzCEiBILHu@OpenReview" target="_blank">Topological Schrödinger Bridge Matching</a>
                <a id="pdf-WzCEiBILHu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WzCEiBILHu@OpenReview', this)" data="https://openreview.net/pdf?id=WzCEiBILHu">[PDF<sup id="pdf-stars-WzCEiBILHu@OpenReview">7</sup>]</a>
                <a id="copy-WzCEiBILHu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WzCEiBILHu@OpenReview')">[Copy]</a>
                <a id="kimi-WzCEiBILHu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WzCEiBILHu@OpenReview', this)">[Kimi<sup id="kimi-stars-WzCEiBILHu@OpenReview">4</sup>]</a>
                <a id="rel-WzCEiBILHu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WzCEiBILHu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WzCEiBILHu@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Maosheng Yang" target="_blank">Maosheng Yang</a>
            </p>
            <p id="summary-WzCEiBILHu@OpenReview" class="summary">Given two boundary distributions, the \emph{Schrödinger Bridge} (SB) problem seeks the “most likely” random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows.In this work, we propose the \emph{Topological Schrödinger Bridge problem} (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-74-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-436" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-437"><span class="texatom" id="MathJax-Span-438"><span class="mrow" id="MathJax-Span-439"><span class="mi" id="MathJax-Span-440" style="font-family: MathJax_Caligraphic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.315em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">T</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-74">\mathcal{T}</script>SBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable \emph{topology-aware} stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a \emph{closed-form} topological SB (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-75-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-441" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-442"><span class="texatom" id="MathJax-Span-443"><span class="mrow" id="MathJax-Span-444"><span class="mi" id="MathJax-Span-445" style="font-family: MathJax_Caligraphic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.315em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">T</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-75">\mathcal{T}</script>SB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns.Building on these results, we develop <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-76-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-446" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-447"><span class="texatom" id="MathJax-Span-448"><span class="mrow" id="MathJax-Span-449"><span class="mi" id="MathJax-Span-450" style="font-family: MathJax_Caligraphic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.315em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">T</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-76">\mathcal{T}</script>SB-based models for matching topological signals by parameterizing the unknowns in the optimal process as \emph{(topological) neural networks} and learning them through \emph{likelihood training}. We validate the theoretical results and demonstrate the practical applications of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-77-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-451" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-452"><span class="texatom" id="MathJax-Span-453"><span class="mrow" id="MathJax-Span-454"><span class="mi" id="MathJax-Span-455" style="font-family: MathJax_Caligraphic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.315em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">T</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-77">\mathcal{T}</script>SB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-78-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-456" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-457"><span class="texatom" id="MathJax-Span-458"><span class="mrow" id="MathJax-Span-459"><span class="mi" id="MathJax-Span-460" style="font-family: MathJax_Caligraphic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.315em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">T</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-78">\mathcal{T}</script>SB-based models to other emerging models, and outline future directions for topological signal matching.</p>
            <p id="subjects-WzCEiBILHu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-WzCEiBILHu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WzCEiBILHu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WzCEiBILHu@OpenReview" onclick="foldPdfKimi('WzCEiBILHu@OpenReview', this)" class="hr hr-fold">
        </div><div id="z8sxoCYgmd@OpenReview" class="panel paper" keywords="loki,lmms,synthetic,multimodal,data,benchmark,comprehensive,inundated,modalities,18k">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=z8sxoCYgmd" target="_blank" title="105/373"><span class="index notranslate">#105</span></a>
                <a id="title-z8sxoCYgmd@OpenReview" class="title-link" href="/venue/z8sxoCYgmd@OpenReview" target="_blank">LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models</a>
                <a id="pdf-z8sxoCYgmd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('z8sxoCYgmd@OpenReview', this)" data="https://openreview.net/pdf?id=z8sxoCYgmd">[PDF<sup id="pdf-stars-z8sxoCYgmd@OpenReview">6</sup>]</a>
                <a id="copy-z8sxoCYgmd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('z8sxoCYgmd@OpenReview')">[Copy]</a>
                <a id="kimi-z8sxoCYgmd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('z8sxoCYgmd@OpenReview', this)">[Kimi<sup id="kimi-stars-z8sxoCYgmd@OpenReview">4</sup>]</a>
                <a id="rel-z8sxoCYgmd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('z8sxoCYgmd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-z8sxoCYgmd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junyan Ye" target="_blank">Junyan Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baichuan Zhou" target="_blank">Baichuan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zilong Huang" target="_blank">Zilong Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junan Zhang" target="_blank">Junan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Bai" target="_blank">Tianyi Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengrui Kang" target="_blank">Hengrui Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun He" target="_blank">Jun He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Honglin Lin" target="_blank">Honglin Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihao Wang" target="_blank">Zihao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Wu" target="_blank">Tong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhizheng Wu" target="_blank">Zhizheng Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiping Chen" target="_blank">Yiping Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dahua Lin" target="_blank">Dahua Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Conghui He" target="_blank">Conghui He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijia Li" target="_blank">Weijia Li</a>
            </p>
            <p id="summary-z8sxoCYgmd@OpenReview" class="summary">With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://loki102.github.io/LOKI.github.io/.</p>
            <p id="subjects-z8sxoCYgmd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-z8sxoCYgmd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-z8sxoCYgmd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-z8sxoCYgmd@OpenReview" onclick="foldPdfKimi('z8sxoCYgmd@OpenReview', this)" class="hr hr-fold">
        </div><div id="uqWM9hBDAE@OpenReview" class="panel paper" keywords="mse,estimator,estimators,chiefly,missing,turing,classes,sample,unknown,classifier">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uqWM9hBDAE" target="_blank" title="106/373"><span class="index notranslate">#106</span></a>
                <a id="title-uqWM9hBDAE@OpenReview" class="title-link" href="/venue/uqWM9hBDAE@OpenReview" target="_blank">How Much is Unseen Depends Chiefly on Information About the Seen</a>
                <a id="pdf-uqWM9hBDAE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uqWM9hBDAE@OpenReview', this)" data="https://openreview.net/pdf?id=uqWM9hBDAE">[PDF<sup id="pdf-stars-uqWM9hBDAE@OpenReview">6</sup>]</a>
                <a id="copy-uqWM9hBDAE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uqWM9hBDAE@OpenReview')">[Copy]</a>
                <a id="kimi-uqWM9hBDAE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uqWM9hBDAE@OpenReview', this)">[Kimi<sup id="kimi-stars-uqWM9hBDAE@OpenReview">6</sup>]</a>
                <a id="rel-uqWM9hBDAE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uqWM9hBDAE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uqWM9hBDAE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seongmin Lee" target="_blank">Seongmin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marcel Boehme" target="_blank">Marcel Boehme</a>
            </p>
            <p id="summary-uqWM9hBDAE@OpenReview" class="summary">The *missing mass* refers to the proportion of data points in an *unknown* population of classifier inputs that belong to classes *not* present in the classifier's training data, which is assumed to be a random sample from that unknown population.We find that *in expectation* the missing mass is entirely determined by the number <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-79-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-461" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-462"><span class="msubsup" id="MathJax-Span-463"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-464" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.471em;"><span class="mi" id="MathJax-Span-465" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>k</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-79">f_k</script> of classes that *do* appear in the training data the same number of times *and an exponentially decaying error*.While this is the first precise characterization of the expected missing mass in terms of the sample, the induced estimator suffers from an impractically high variance. However, our theory suggests a large search space of nearly unbiased estimators that can be searched effectively and efficiently. Hence, we cast distribution-free estimation as an optimization problem to find a distribution-specific estimator with a minimized mean-squared error (MSE), given only the sample.In our experiments, our search algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 93\% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80\% of the Good-Turing estimator's.</p>
            <p id="subjects-uqWM9hBDAE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-uqWM9hBDAE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uqWM9hBDAE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uqWM9hBDAE@OpenReview" onclick="foldPdfKimi('uqWM9hBDAE@OpenReview', this)" class="hr hr-fold">
        </div><div id="Tv36j85SqR@OpenReview" class="panel paper" keywords="coding,ltc,quantization,compression,distortion,lattice,compressors,optimal,transform,neural">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Tv36j85SqR" target="_blank" title="107/373"><span class="index notranslate">#107</span></a>
                <a id="title-Tv36j85SqR@OpenReview" class="title-link" href="/venue/Tv36j85SqR@OpenReview" target="_blank">Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding</a>
                <a id="pdf-Tv36j85SqR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Tv36j85SqR@OpenReview', this)" data="https://openreview.net/pdf?id=Tv36j85SqR">[PDF<sup id="pdf-stars-Tv36j85SqR@OpenReview">6</sup>]</a>
                <a id="copy-Tv36j85SqR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Tv36j85SqR@OpenReview')">[Copy]</a>
                <a id="kimi-Tv36j85SqR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Tv36j85SqR@OpenReview', this)">[Kimi<sup id="kimi-stars-Tv36j85SqR@OpenReview">3</sup>]</a>
                <a id="rel-Tv36j85SqR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Tv36j85SqR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Tv36j85SqR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Lei" target="_blank">Eric Lei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hamed Hassani" target="_blank">Hamed Hassani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shirin Saeedi Bidokhti" target="_blank">Shirin Saeedi Bidokhti</a>
            </p>
            <p id="summary-Tv36j85SqR@OpenReview" class="summary">Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding.</p>
            <p id="subjects-Tv36j85SqR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Tv36j85SqR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tv36j85SqR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tv36j85SqR@OpenReview" onclick="foldPdfKimi('Tv36j85SqR@OpenReview', this)" class="hr hr-fold">
        </div><div id="TtUh0TOlGX@OpenReview" class="panel paper" keywords="treg,inverse,diffusion,ambiguities,regularization,latent,descriptions,preconceptions,texts,solvers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TtUh0TOlGX" target="_blank" title="108/373"><span class="index notranslate">#108</span></a>
                <a id="title-TtUh0TOlGX@OpenReview" class="title-link" href="/venue/TtUh0TOlGX@OpenReview" target="_blank">Regularization by Texts for Latent Diffusion Inverse Solvers</a>
                <a id="pdf-TtUh0TOlGX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TtUh0TOlGX@OpenReview', this)" data="https://openreview.net/pdf?id=TtUh0TOlGX">[PDF<sup id="pdf-stars-TtUh0TOlGX@OpenReview">2</sup>]</a>
                <a id="copy-TtUh0TOlGX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TtUh0TOlGX@OpenReview')">[Copy]</a>
                <a id="kimi-TtUh0TOlGX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TtUh0TOlGX@OpenReview', this)">[Kimi<sup id="kimi-stars-TtUh0TOlGX@OpenReview">1</sup>]</a>
                <a id="rel-TtUh0TOlGX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TtUh0TOlGX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TtUh0TOlGX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jeongsol Kim" target="_blank">Jeongsol Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Geon Yeong Park" target="_blank">Geon Yeong Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyungjin Chung" target="_blank">Hyungjin Chung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jong Chul YE" target="_blank">Jong Chul YE</a>
            </p>
            <p id="summary-TtUh0TOlGX@OpenReview" class="summary">The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency.</p>
            <p id="subjects-TtUh0TOlGX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TtUh0TOlGX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TtUh0TOlGX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TtUh0TOlGX@OpenReview" onclick="foldPdfKimi('TtUh0TOlGX@OpenReview', this)" class="hr hr-fold">
        </div><div id="TlAdgeoDTo@OpenReview" class="panel paper" keywords="gender,chatbots,chats,fairness,hasshown,andalso,name,race,introducea,languagemodel">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TlAdgeoDTo" target="_blank" title="109/373"><span class="index notranslate">#109</span></a>
                <a id="title-TlAdgeoDTo@OpenReview" class="title-link" href="/venue/TlAdgeoDTo@OpenReview" target="_blank">First-Person Fairness in Chatbots</a>
                <a id="pdf-TlAdgeoDTo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TlAdgeoDTo@OpenReview', this)" data="https://openreview.net/pdf?id=TlAdgeoDTo">[PDF<sup id="pdf-stars-TlAdgeoDTo@OpenReview">1</sup>]</a>
                <a id="copy-TlAdgeoDTo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TlAdgeoDTo@OpenReview')">[Copy]</a>
                <a id="kimi-TlAdgeoDTo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TlAdgeoDTo@OpenReview', this)">[Kimi<sup id="kimi-stars-TlAdgeoDTo@OpenReview">4</sup>]</a>
                <a id="rel-TlAdgeoDTo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TlAdgeoDTo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TlAdgeoDTo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tyna Eloundou" target="_blank">Tyna Eloundou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Beutel" target="_blank">Alex Beutel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Robinson" target="_blank">David Robinson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keren Gu" target="_blank">Keren Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna-Luisa Brakman" target="_blank">Anna-Luisa Brakman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pamela Mishkin" target="_blank">Pamela Mishkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meghan Shah" target="_blank">Meghan Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johannes Heidecke" target="_blank">Johannes Heidecke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lilian Weng" target="_blank">Lilian Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Tauman Kalai" target="_blank">Adam Tauman Kalai</a>
            </p>
            <p id="summary-TlAdgeoDTo@OpenReview" class="summary">Some chatbots have access to a user’s name when responding. Prior work hasshown that large language model outputs can change based on the demographictraits correlated with a name, such as gender or race. In this study, we introducea scalable method for studying one form of first-personfairness—fairness towards the user based on their demographic information—across a large and heterogeneous corpus of actual chats. We leverage a languagemodel as an AI “research assistant” (AI RA) that can privately and scalably analyzechat data, surfacing broader trends without exposing specific examples to theresearchers. We corroborate the labels of the AI RA with independent humanannotations, finding it highly consistent with human ratings of gender bias (less sofor racial bias). We apply this methodology to a large set of chats with a commercialchatbot. We assess overall quality of responses conditional on different names andalso subtle differences in similar-quality responses that may in aggregate reinforceharmful stereotypes based on gender or race. The largest detected biases are genderbiases in older generations of models and in open-ended tasks, like writing a story.Finally, evaluations like ours are important for monitoring and reducing biases.</p>
            <p id="subjects-TlAdgeoDTo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TlAdgeoDTo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TlAdgeoDTo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TlAdgeoDTo@OpenReview" onclick="foldPdfKimi('TlAdgeoDTo@OpenReview', this)" class="hr hr-fold">
        </div><div id="uREg3OHjLL@OpenReview" class="panel paper" keywords="relu,hertrich,networks,depth,lceil,rceil,conjecture,layers,weights,skutella">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uREg3OHjLL" target="_blank" title="110/373"><span class="index notranslate">#110</span></a>
                <a id="title-uREg3OHjLL@OpenReview" class="title-link" href="/venue/uREg3OHjLL@OpenReview" target="_blank">On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth</a>
                <a id="pdf-uREg3OHjLL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uREg3OHjLL@OpenReview', this)" data="https://openreview.net/pdf?id=uREg3OHjLL">[PDF<sup id="pdf-stars-uREg3OHjLL@OpenReview">3</sup>]</a>
                <a id="copy-uREg3OHjLL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uREg3OHjLL@OpenReview')">[Copy]</a>
                <a id="kimi-uREg3OHjLL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uREg3OHjLL@OpenReview', this)">[Kimi<sup id="kimi-stars-uREg3OHjLL@OpenReview">5</sup>]</a>
                <a id="rel-uREg3OHjLL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uREg3OHjLL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uREg3OHjLL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gennadiy Averkov" target="_blank">Gennadiy Averkov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Hojny" target="_blank">Christopher Hojny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maximilian Merkert" target="_blank">Maximilian Merkert</a>
            </p>
            <p id="summary-uREg3OHjLL@OpenReview" class="summary">To confirm that the expressive power of ReLU neural networks grows with their depth, the function <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-80-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;&amp;#x2026;&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-466" style="width: 12.294em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.211em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1010.11em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-467"><span class="msubsup" id="MathJax-Span-468"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-469" style="font-family: MathJax_Math-italic;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-470" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-471" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mo" id="MathJax-Span-472" style="font-family: MathJax_Main; padding-left: 0.263em;">max</span><span class="mo" id="MathJax-Span-473" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-474" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-475" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-476" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-477" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.576em;"><span class="mn" id="MathJax-Span-478" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-479" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-480" style="font-family: MathJax_Main; padding-left: 0.159em;">…</span><span class="mo" id="MathJax-Span-481" style="font-family: MathJax_Main; padding-left: 0.159em;">,</span><span class="msubsup" id="MathJax-Span-482" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-483" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.576em;"><span class="mi" id="MathJax-Span-484" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-485" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>F</mi><mi>n</mi></msub><mo>=</mo><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-80">F_n = \max (0,x_1,\ldots,x_n )</script> has been considered in the literature. A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-81-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-486" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-487"><span class="msubsup" id="MathJax-Span-488"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-489" style="font-family: MathJax_Math-italic;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-490" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>F</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-81">F_n</script> has at least <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-82-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2308;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2309;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-491" style="width: 6.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.52em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-492"><span class="mo" id="MathJax-Span-493" style="font-family: MathJax_Main;">⌈</span><span class="msubsup" id="MathJax-Span-494"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-495" style="font-family: MathJax_Main;">log</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.872em; left: 1.305em;"><span class="mn" id="MathJax-Span-496" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-497"></span><span class="mo" id="MathJax-Span-498" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-499" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-500" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mn" id="MathJax-Span-501" style="font-family: MathJax_Main; padding-left: 0.211em;">1</span><span class="mo" id="MathJax-Span-502" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-503" style="font-family: MathJax_Main;">⌉</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">⌈</mo><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo fence="false" stretchy="false">⌉</mo></math></span></span><script type="math/tex" id="MathJax-Element-82">\lceil \log_2 (n+1) \rceil</script> hidden layers. The conjecture has recently been confirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR 2023]. We follow up on this line of research and show that, within ReLU networks whose weights are decimal fractions, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-83-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-504" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-505"><span class="msubsup" id="MathJax-Span-506"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-507" style="font-family: MathJax_Math-italic;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-508" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>F</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-83">F_n</script> can only be represented by networks with at least <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-84-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2308;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2309;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-509" style="width: 6.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.52em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-510"><span class="mo" id="MathJax-Span-511" style="font-family: MathJax_Main;">⌈</span><span class="msubsup" id="MathJax-Span-512"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-513" style="font-family: MathJax_Main;">log</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.872em; left: 1.305em;"><span class="mn" id="MathJax-Span-514" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-515"></span><span class="mo" id="MathJax-Span-516" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-517" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-518" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mn" id="MathJax-Span-519" style="font-family: MathJax_Main; padding-left: 0.211em;">1</span><span class="mo" id="MathJax-Span-520" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-521" style="font-family: MathJax_Main;">⌉</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">⌈</mo><msub><mi>log</mi><mn>3</mn></msub><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo fence="false" stretchy="false">⌉</mo></math></span></span><script type="math/tex" id="MathJax-Element-84">\lceil \log_3 (n+1) \rceil</script> hidden layers. Moreover, if all weights are <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-85-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-522" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-523"><span class="mi" id="MathJax-Span-524" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-85">N</script>-ary fractions, then <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-86-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-525" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-526"><span class="msubsup" id="MathJax-Span-527"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-528" style="font-family: MathJax_Math-italic;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-529" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>F</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-86">F_n</script> can only be represented by networks with at least <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-87-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-530" style="width: 4.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.86em, 2.711em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-531"><span class="mi" id="MathJax-Span-532" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-533" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-534"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1001.2em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.57em;"><span class="mrow" id="MathJax-Span-535"><span class="mi" id="MathJax-Span-536" style="font-size: 70.7%; font-family: MathJax_Main;">ln</span><span class="mo" id="MathJax-Span-537" style="font-size: 70.7%;"></span><span class="mi" id="MathJax-Span-538" style="font-size: 70.7%; font-family: MathJax_Math-italic; padding-left: 0.263em;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1002.14em, 2.294em, -999.997em); top: -1.716em; left: 50%; margin-left: -1.091em;"><span class="mrow" id="MathJax-Span-539"><span class="mi" id="MathJax-Span-540" style="font-size: 70.7%; font-family: MathJax_Main;">ln</span><span class="mo" id="MathJax-Span-541" style="font-size: 70.7%;"></span><span class="mi" id="MathJax-Span-542" style="font-size: 70.7%; font-family: MathJax_Main; padding-left: 0.263em;">ln</span><span class="mo" id="MathJax-Span-543" style="font-size: 70.7%;"></span><span class="mi" id="MathJax-Span-544" style="font-size: 70.7%; font-family: MathJax_Math-italic; padding-left: 0.263em;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1002.24em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.242em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-545" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>ln</mi><mo>⁡</mo><mi>n</mi></mrow><mrow><mi>ln</mi><mo>⁡</mo><mi>ln</mi><mo>⁡</mo><mi>N</mi></mrow></mfrac><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-87">\Omega( \frac{\ln n}{\ln \ln N})</script> layers. These results are a partial confirmation of the above conjecture for rational ReLU networks, and provide the first non-constant lower bound on the depth of practically relevant ReLU networks.</p>
            <p id="subjects-uREg3OHjLL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-uREg3OHjLL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uREg3OHjLL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uREg3OHjLL@OpenReview" onclick="foldPdfKimi('uREg3OHjLL@OpenReview', this)" class="hr hr-fold">
        </div><div id="SqZ0KY4qBD@OpenReview" class="panel paper" keywords="transformers,markov,bigram,layer,minima,curious,unigram,single,chains,attention">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SqZ0KY4qBD" target="_blank" title="111/373"><span class="index notranslate">#111</span></a>
                <a id="title-SqZ0KY4qBD@OpenReview" class="title-link" href="/venue/SqZ0KY4qBD@OpenReview" target="_blank">Attention with Markov: A Curious Case of Single-layer Transformers</a>
                <a id="pdf-SqZ0KY4qBD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SqZ0KY4qBD@OpenReview', this)" data="https://openreview.net/pdf?id=SqZ0KY4qBD">[PDF<sup id="pdf-stars-SqZ0KY4qBD@OpenReview">7</sup>]</a>
                <a id="copy-SqZ0KY4qBD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SqZ0KY4qBD@OpenReview')">[Copy]</a>
                <a id="kimi-SqZ0KY4qBD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SqZ0KY4qBD@OpenReview', this)">[Kimi<sup id="kimi-stars-SqZ0KY4qBD@OpenReview">9</sup>]</a>
                <a id="rel-SqZ0KY4qBD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SqZ0KY4qBD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SqZ0KY4qBD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ashok Makkuva" target="_blank">Ashok Makkuva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Bondaschi" target="_blank">Marco Bondaschi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alliot Nagle" target="_blank">Alliot Nagle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adway Girish" target="_blank">Adway Girish</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyeji Kim" target="_blank">Hyeji Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Jaggi" target="_blank">Martin Jaggi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Gastpar" target="_blank">Michael Gastpar</a>
            </p>
            <p id="summary-SqZ0KY4qBD@OpenReview" class="summary">Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena. Code is available at \url{https://anonymous.4open.science/r/Attention-with-Markov-A617/}.</p>
            <p id="subjects-SqZ0KY4qBD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SqZ0KY4qBD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SqZ0KY4qBD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SqZ0KY4qBD@OpenReview" onclick="foldPdfKimi('SqZ0KY4qBD@OpenReview', this)" class="hr hr-fold">
        </div><div id="rDLgnYLM5b@OpenReview" class="panel paper" keywords="isg,interleaved,image,text,bench,vision,holistic,scene,assessment,responses">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rDLgnYLM5b" target="_blank" title="112/373"><span class="index notranslate">#112</span></a>
                <a id="title-rDLgnYLM5b@OpenReview" class="title-link" href="/venue/rDLgnYLM5b@OpenReview" target="_blank">Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment</a>
                <a id="pdf-rDLgnYLM5b@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rDLgnYLM5b@OpenReview', this)" data="https://openreview.net/pdf?id=rDLgnYLM5b">[PDF<sup id="pdf-stars-rDLgnYLM5b@OpenReview">5</sup>]</a>
                <a id="copy-rDLgnYLM5b@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rDLgnYLM5b@OpenReview')">[Copy]</a>
                <a id="kimi-rDLgnYLM5b@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rDLgnYLM5b@OpenReview', this)">[Kimi<sup id="kimi-stars-rDLgnYLM5b@OpenReview">6</sup>]</a>
                <a id="rel-rDLgnYLM5b@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rDLgnYLM5b@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rDLgnYLM5b@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongping Chen" target="_blank">Dongping Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoxi Chen" target="_blank">Ruoxi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shu Pu" target="_blank">Shu Pu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyi Liu" target="_blank">Zhaoyi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanru Wu" target="_blank">Yanru Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caixi Chen" target="_blank">Caixi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benlin Liu" target="_blank">Benlin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Huang" target="_blank">Yue Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Wan" target="_blank">Yao Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pan Zhou" target="_blank">Pan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ranjay Krishna" target="_blank">Ranjay Krishna</a>
            </p>
            <p id="summary-rDLgnYLM5b@OpenReview" class="summary">Many real-world user queries (e.g. *"How do to make egg fried rice?"*) could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook.Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities.To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback.In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels.To facilitate future work, we develop ISG-Agent, a baseline agent employing a *"plan-execute-refine"* pipeline to invoke tools, achieving a 122% performance improvement.</p>
            <p id="subjects-rDLgnYLM5b@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-rDLgnYLM5b@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rDLgnYLM5b@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rDLgnYLM5b@OpenReview" onclick="foldPdfKimi('rDLgnYLM5b@OpenReview', this)" class="hr hr-fold">
        </div><div id="SRpq5OBpED@OpenReview" class="panel paper" keywords="recordings,neural,meta,integrative,learning,across,tasks,dynamics,dynamical,shared">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SRpq5OBpED" target="_blank" title="113/373"><span class="index notranslate">#113</span></a>
                <a id="title-SRpq5OBpED@OpenReview" class="title-link" href="/venue/SRpq5OBpED@OpenReview" target="_blank">Meta-Dynamical State Space Models for Integrative Neural Data Analysis</a>
                <a id="pdf-SRpq5OBpED@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SRpq5OBpED@OpenReview', this)" data="https://openreview.net/pdf?id=SRpq5OBpED">[PDF<sup id="pdf-stars-SRpq5OBpED@OpenReview">2</sup>]</a>
                <a id="copy-SRpq5OBpED@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SRpq5OBpED@OpenReview')">[Copy]</a>
                <a id="kimi-SRpq5OBpED@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SRpq5OBpED@OpenReview', this)">[Kimi<sup id="kimi-stars-SRpq5OBpED@OpenReview">2</sup>]</a>
                <a id="rel-SRpq5OBpED@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SRpq5OBpED@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SRpq5OBpED@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ayesha Vermani" target="_blank">Ayesha Vermani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Josue Nassar" target="_blank">Josue Nassar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyungju Jeon" target="_blank">Hyungju Jeon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Dowling" target="_blank">Matthew Dowling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Il Memming Park" target="_blank">Il Memming Park</a>
            </p>
            <p id="summary-SRpq5OBpED@OpenReview" class="summary">Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings.Existing approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family ofrelated solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given new recordings. We demonstrate the efficacy of our approach onfew-shot reconstruction and forecasting of synthetic dynamical systems, and neural recordings from the motor cortex during different arm reaching tasks.</p>
            <p id="subjects-SRpq5OBpED@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SRpq5OBpED@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SRpq5OBpED@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SRpq5OBpED@OpenReview" onclick="foldPdfKimi('SRpq5OBpED@OpenReview', this)" class="hr hr-fold">
        </div><div id="SOd07Qxkw4@OpenReview" class="panel paper" keywords="varepsilon,diffusion,log,convergence,lot,models,score,shen2019therandomized,gupta2024faster,midpoint">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SOd07Qxkw4" target="_blank" title="114/373"><span class="index notranslate">#114</span></a>
                <a id="title-SOd07Qxkw4@OpenReview" class="title-link" href="/venue/SOd07Qxkw4@OpenReview" target="_blank">Improved Convergence Rate for Diffusion Probabilistic Models</a>
                <a id="pdf-SOd07Qxkw4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SOd07Qxkw4@OpenReview', this)" data="https://openreview.net/pdf?id=SOd07Qxkw4">[PDF<sup id="pdf-stars-SOd07Qxkw4@OpenReview">3</sup>]</a>
                <a id="copy-SOd07Qxkw4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SOd07Qxkw4@OpenReview')">[Copy]</a>
                <a id="kimi-SOd07Qxkw4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SOd07Qxkw4@OpenReview', this)">[Kimi<sup id="kimi-stars-SOd07Qxkw4@OpenReview">1</sup>]</a>
                <a id="rel-SOd07Qxkw4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SOd07Qxkw4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SOd07Qxkw4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gen Li" target="_blank">Gen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Jiao" target="_blank">Yuchen Jiao</a>
            </p>
            <p id="summary-SOd07Qxkw4@OpenReview" class="summary">Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly convergence analysis for such models, has attracted a lot of interests. Despite a lot of theoretical attempts, there still exists significant gap between theory and practice. Towards to close this gap, we establish an iteration complexity at the order of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-88-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-546" style="width: 4.586em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.8em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-547"><span class="msubsup" id="MathJax-Span-548"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-549" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-550"><span class="mrow" id="MathJax-Span-551"><span class="mn" id="MathJax-Span-552" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-553"><span class="mrow" id="MathJax-Span-554"><span class="mo" id="MathJax-Span-555" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-556" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-557"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-558" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-559"><span class="mrow" id="MathJax-Span-560"><span class="mo" id="MathJax-Span-561" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-562" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-563"><span class="mrow" id="MathJax-Span-564"><span class="mo" id="MathJax-Span-565" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-566" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-88">d^{1/3}\varepsilon^{-2/3}</script>, which is better than <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-89-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-567" style="width: 4.169em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.44em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-568"><span class="msubsup" id="MathJax-Span-569"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-570" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-571"><span class="mrow" id="MathJax-Span-572"><span class="mn" id="MathJax-Span-573" style="font-size: 70.7%; font-family: MathJax_Main;">5</span><span class="texatom" id="MathJax-Span-574"><span class="mrow" id="MathJax-Span-575"><span class="mo" id="MathJax-Span-576" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-577" style="font-size: 70.7%; font-family: MathJax_Main;">12</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-578"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-579" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-580"><span class="mrow" id="MathJax-Span-581"><span class="mo" id="MathJax-Span-582" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-583" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mn>5</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>12</mn></mrow></msup><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-89">d^{5/12}\varepsilon^{-1}</script>, the best known complexity achieved before our work. This convergence analysis is based on a randomized midpoint method, which is first proposed for log-concave sampling \citep{Shen2019TheRandomized}, and then extended to diffusion models by \citet{Gupta2024Faster}. Our theory accommodates <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-90-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-584" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-585"><span class="mi" id="MathJax-Span-586" style="font-family: MathJax_Math-italic;">ε</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi></math></span></span><script type="math/tex" id="MathJax-Element-90">\varepsilon</script>-accurate score estimates, and does not require log-concavity on the target distribution. Moreover, the algorithm can also be parallelized to run in only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-91-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-587" style="width: 6.669em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1005.42em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-588"><span class="mi" id="MathJax-Span-589" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-590" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-591"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-592" style="font-family: MathJax_Main;">log</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 1.305em;"><span class="mn" id="MathJax-Span-593" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-594"></span><span class="mo" id="MathJax-Span-595" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-596" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-597"><span class="mrow" id="MathJax-Span-598"><span class="mo" id="MathJax-Span-599" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-600" style="font-family: MathJax_Math-italic;">ε</span><span class="mo" id="MathJax-Span-601" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-602" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>log</mi><mn>2</mn></msup><mo>⁡</mo><mo stretchy="false">(</mo><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>ε</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-91">O(\log^2(d/\varepsilon))</script> parallel rounds in a similar way to prior works.</p>
            <p id="subjects-SOd07Qxkw4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SOd07Qxkw4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SOd07Qxkw4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SOd07Qxkw4@OpenReview" onclick="foldPdfKimi('SOd07Qxkw4@OpenReview', this)" class="hr hr-fold">
        </div><div id="SG1R2H3fa1@OpenReview" class="panel paper" keywords="rwnns,record,random,walk,graph,walks,graphs,message,passing,revisiting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SG1R2H3fa1" target="_blank" title="115/373"><span class="index notranslate">#115</span></a>
                <a id="title-SG1R2H3fa1@OpenReview" class="title-link" href="/venue/SG1R2H3fa1@OpenReview" target="_blank">Revisiting Random Walks for Learning on Graphs</a>
                <a id="pdf-SG1R2H3fa1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SG1R2H3fa1@OpenReview', this)" data="https://openreview.net/pdf?id=SG1R2H3fa1">[PDF<sup id="pdf-stars-SG1R2H3fa1@OpenReview">7</sup>]</a>
                <a id="copy-SG1R2H3fa1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SG1R2H3fa1@OpenReview')">[Copy]</a>
                <a id="kimi-SG1R2H3fa1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SG1R2H3fa1@OpenReview', this)">[Kimi<sup id="kimi-stars-SG1R2H3fa1@OpenReview">6</sup>]</a>
                <a id="rel-SG1R2H3fa1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SG1R2H3fa1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SG1R2H3fa1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwoo Kim" target="_blank">Jinwoo Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Olga Zaghen" target="_blank">Olga Zaghen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayhan Suleymanzade" target="_blank">Ayhan Suleymanzade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youngmin Ryou" target="_blank">Youngmin Ryou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seunghoon Hong" target="_blank">Seunghoon Hong</a>
            </p>
            <p id="summary-SG1R2H3fa1@OpenReview" class="summary">We revisit a recent model class for machine learning on graphs, where a random walk on a graph produces a machine-readable record, and this record is processed by a deep neural network to directly make vertex-level or graph-level predictions. We refer to these stochastic machines as random walk neural networks (RWNNs), and through principled analysis, show that we can design them to be isomorphism invariant while capable of universal approximation of graph functions in probability. A useful finding is that almost any kind of record of random walk guarantees probabilistic invariance as long as the vertices are anonymized. This enables us, for example, to record random walks in plain text and adopt a language model to read these text records to solve graph tasks. We further establish a parallelism to message passing neural networks using tools from Markov chain theory, and show that over-smoothing in message passing is alleviated by construction in RWNNs, while over-squashing manifests as probabilistic under-reaching. We empirically demonstrate RWNNs on a range of problems, verifying our theoretical analysis and demonstrating the use of language models for separating strongly regular graphs where the 3-WL test fails, and transductive classification on arXiv citation network.</p>
            <p id="subjects-SG1R2H3fa1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SG1R2H3fa1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SG1R2H3fa1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SG1R2H3fa1@OpenReview" onclick="foldPdfKimi('SG1R2H3fa1@OpenReview', this)" class="hr hr-fold">
        </div><div id="SyVPiehSbg@OpenReview" class="panel paper" keywords="kst,actnet,kolmogorov,kans,superposition,formulation,original,deep,theorem,alternatives">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SyVPiehSbg" target="_blank" title="116/373"><span class="index notranslate">#116</span></a>
                <a id="title-SyVPiehSbg@OpenReview" class="title-link" href="/venue/SyVPiehSbg@OpenReview" target="_blank">Deep Learning Alternatives Of The Kolmogorov Superposition Theorem</a>
                <a id="pdf-SyVPiehSbg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SyVPiehSbg@OpenReview', this)" data="https://openreview.net/pdf?id=SyVPiehSbg">[PDF<sup id="pdf-stars-SyVPiehSbg@OpenReview">4</sup>]</a>
                <a id="copy-SyVPiehSbg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SyVPiehSbg@OpenReview')">[Copy]</a>
                <a id="kimi-SyVPiehSbg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SyVPiehSbg@OpenReview', this)">[Kimi<sup id="kimi-stars-SyVPiehSbg@OpenReview">3</sup>]</a>
                <a id="rel-SyVPiehSbg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SyVPiehSbg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SyVPiehSbg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Leonardo Ferreira Guilhoto" target="_blank">Leonardo Ferreira Guilhoto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paris Perdikaris" target="_blank">Paris Perdikaris</a>
            </p>
            <p id="summary-SyVPiehSbg@OpenReview" class="summary">This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks.</p>
            <p id="subjects-SyVPiehSbg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SyVPiehSbg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SyVPiehSbg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SyVPiehSbg@OpenReview" onclick="foldPdfKimi('SyVPiehSbg@OpenReview', this)" class="hr hr-fold">
        </div><div id="vi3DjUhFVm@OpenReview" class="panel paper" keywords="reward,diffusion,optimization,krafton,aligning,rewards,objectives,target,test,tuning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vi3DjUhFVm" target="_blank" title="117/373"><span class="index notranslate">#117</span></a>
                <a id="title-vi3DjUhFVm@OpenReview" class="title-link" href="/venue/vi3DjUhFVm@OpenReview" target="_blank">Test-time Alignment of Diffusion Models without Reward Over-optimization</a>
                <a id="pdf-vi3DjUhFVm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vi3DjUhFVm@OpenReview', this)" data="https://openreview.net/pdf?id=vi3DjUhFVm">[PDF<sup id="pdf-stars-vi3DjUhFVm@OpenReview">4</sup>]</a>
                <a id="copy-vi3DjUhFVm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vi3DjUhFVm@OpenReview')">[Copy]</a>
                <a id="kimi-vi3DjUhFVm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vi3DjUhFVm@OpenReview', this)">[Kimi<sup id="kimi-stars-vi3DjUhFVm@OpenReview">8</sup>]</a>
                <a id="rel-vi3DjUhFVm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vi3DjUhFVm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vi3DjUhFVm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sunwoo Kim" target="_blank">Sunwoo Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minkyu Kim" target="_blank">Minkyu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongmin Park" target="_blank">Dongmin Park</a>
            </p>
            <p id="summary-vi3DjUhFVm@OpenReview" class="summary">Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively. Addressing these limitations, we propose a training-free, test-time method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. Our approach, tailored for diffusion sampling and incorporating tempering techniques, achieves comparable or superior target rewards to fine-tuning methods while preserving diversity and cross-reward generalization. We demonstrate its effectiveness in single-reward optimization, multi-objective scenarios, and online black-box optimization. This work offers a robust solution for aligning diffusion models with diverse downstream objectives without compromising their general capabilities. Code is available at https://github.com/krafton-ai/DAS.</p>
            <p id="subjects-vi3DjUhFVm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-vi3DjUhFVm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vi3DjUhFVm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vi3DjUhFVm@OpenReview" onclick="foldPdfKimi('vi3DjUhFVm@OpenReview', this)" class="hr hr-fold">
        </div><div id="QOfswj7hij@OpenReview" class="panel paper" keywords="symbolic,invention,neuro,predicate,predicates,online,planning,inventing,abstracting,sensorimotor">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QOfswj7hij" target="_blank" title="118/373"><span class="index notranslate">#118</span></a>
                <a id="title-QOfswj7hij@OpenReview" class="title-link" href="/venue/QOfswj7hij@OpenReview" target="_blank">Online Neuro-Symbolic Predicate Invention for High-Level Planning</a>
                <a id="pdf-QOfswj7hij@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QOfswj7hij@OpenReview', this)" data="https://openreview.net/pdf?id=QOfswj7hij">[PDF<sup id="pdf-stars-QOfswj7hij@OpenReview">6</sup>]</a>
                <a id="copy-QOfswj7hij@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QOfswj7hij@OpenReview')">[Copy]</a>
                <a id="kimi-QOfswj7hij@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QOfswj7hij@OpenReview', this)">[Kimi<sup id="kimi-stars-QOfswj7hij@OpenReview">6</sup>]</a>
                <a id="rel-QOfswj7hij@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QOfswj7hij@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QOfswj7hij@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yichao Liang" target="_blank">Yichao Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nishanth Kumar" target="_blank">Nishanth Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tang" target="_blank">Hao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adrian Weller" target="_blank">Adrian Weller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua B Tenenbaum" target="_blank">Joshua B Tenenbaum</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Silver" target="_blank">Tom Silver</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joao F. Henriques" target="_blank">Joao F. Henriques</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Ellis" target="_blank">Kevin Ellis</a>
            </p>
            <p id="summary-QOfswj7hij@OpenReview" class="summary">Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.</p>
            <p id="subjects-QOfswj7hij@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-QOfswj7hij@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QOfswj7hij@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QOfswj7hij@OpenReview" onclick="foldPdfKimi('QOfswj7hij@OpenReview', this)" class="hr hr-fold">
        </div><div id="tIBAOcAvn4@OpenReview" class="panel paper" keywords="priors,gradient,ray,search,hard,label,query,attacks,estimators,transfer">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tIBAOcAvn4" target="_blank" title="119/373"><span class="index notranslate">#119</span></a>
                <a id="title-tIBAOcAvn4@OpenReview" class="title-link" href="/venue/tIBAOcAvn4@OpenReview" target="_blank">Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors</a>
                <a id="pdf-tIBAOcAvn4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tIBAOcAvn4@OpenReview', this)" data="https://openreview.net/pdf?id=tIBAOcAvn4">[PDF<sup id="pdf-stars-tIBAOcAvn4@OpenReview">5</sup>]</a>
                <a id="copy-tIBAOcAvn4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tIBAOcAvn4@OpenReview')">[Copy]</a>
                <a id="kimi-tIBAOcAvn4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tIBAOcAvn4@OpenReview', this)">[Kimi<sup id="kimi-stars-tIBAOcAvn4@OpenReview">3</sup>]</a>
                <a id="rel-tIBAOcAvn4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tIBAOcAvn4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tIBAOcAvn4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Ma" target="_blank">Chen Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinjie Xu" target="_blank">Xinjie Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuyu Cheng" target="_blank">Shuyu Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Xuan" target="_blank">Qi Xuan</a>
            </p>
            <p id="summary-tIBAOcAvn4@OpenReview" class="summary">One of the most practical and challenging types of black-box adversarial attacks is the hard-label attack, where only top-1 predicted labels are available. One effective approach is to search for the optimal ray direction from the benign image that minimizes the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-92-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-603" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-604"><span class="msubsup" id="MathJax-Span-605"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-606" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-607" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-92">\ell_p</script> norm distance to the adversarial region. The unique advantage of this approach is that it transforms the hard-label attack into a continuous optimization problem. The objective function value is the ray's radius and can be obtained through a binary search with high query cost. Existing methods use a "sign trick" in gradient estimation to reduce queries. In this paper, we theoretically analyze the quality of this gradient estimation, proposing a novel prior-guided approach to improve ray search efficiency, based on theoretical and experimental analysis. Specifically, we utilize the transfer-based priors from surrogate models, and our gradient estimators appropriately integrate them by approximating the projection of the true gradient onto the subspace spanned by these priors and some random directions, in a query-efficient way. We theoretically derive the expected cosine similarity between the obtained gradient estimators and the true gradient, and demonstrate the improvement brought by using priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that our approach significantly outperforms 11 state-of-the-art methods in query efficiency. Code will be released.</p>
            <p id="subjects-tIBAOcAvn4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tIBAOcAvn4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tIBAOcAvn4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tIBAOcAvn4@OpenReview" onclick="foldPdfKimi('tIBAOcAvn4@OpenReview', this)" class="hr hr-fold">
        </div><div id="Q0zmmNNePz@OpenReview" class="panel paper" keywords="topological,topograph,segmentation,graph,topology,loss,topologically,image,guarantees,fivefold">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Q0zmmNNePz" target="_blank" title="120/373"><span class="index notranslate">#120</span></a>
                <a id="title-Q0zmmNNePz@OpenReview" class="title-link" href="/venue/Q0zmmNNePz@OpenReview" target="_blank">Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation</a>
                <a id="pdf-Q0zmmNNePz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Q0zmmNNePz@OpenReview', this)" data="https://openreview.net/pdf?id=Q0zmmNNePz">[PDF<sup id="pdf-stars-Q0zmmNNePz@OpenReview">5</sup>]</a>
                <a id="copy-Q0zmmNNePz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Q0zmmNNePz@OpenReview')">[Copy]</a>
                <a id="kimi-Q0zmmNNePz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Q0zmmNNePz@OpenReview', this)">[Kimi<sup id="kimi-stars-Q0zmmNNePz@OpenReview">4</sup>]</a>
                <a id="rel-Q0zmmNNePz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Q0zmmNNePz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Q0zmmNNePz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Laurin Lux" target="_blank">Laurin Lux</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander H Berger" target="_blank">Alexander H Berger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Weers" target="_blank">Alexander Weers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nico Stucki" target="_blank">Nico Stucki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Rueckert" target="_blank">Daniel Rueckert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ulrich Bauer" target="_blank">Ulrich Bauer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johannes Paetzold" target="_blank">Johannes Paetzold</a>
            </p>
            <p id="summary-Q0zmmNNePz@OpenReview" class="summary">Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets, demonstrating state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods.</p>
            <p id="subjects-Q0zmmNNePz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Q0zmmNNePz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Q0zmmNNePz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Q0zmmNNePz@OpenReview" onclick="foldPdfKimi('Q0zmmNNePz@OpenReview', this)" class="hr hr-fold">
        </div><div id="PpYy0dR3Qw@OpenReview" class="panel paper" keywords="locodl,communication,mpression,istributed,compression,bitstreams,floats,training,earning,compressors">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PpYy0dR3Qw" target="_blank" title="121/373"><span class="index notranslate">#121</span></a>
                <a id="title-PpYy0dR3Qw@OpenReview" class="title-link" href="/venue/PpYy0dR3Qw@OpenReview" target="_blank">LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression</a>
                <a id="pdf-PpYy0dR3Qw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PpYy0dR3Qw@OpenReview', this)" data="https://openreview.net/pdf?id=PpYy0dR3Qw">[PDF<sup id="pdf-stars-PpYy0dR3Qw@OpenReview">5</sup>]</a>
                <a id="copy-PpYy0dR3Qw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PpYy0dR3Qw@OpenReview')">[Copy]</a>
                <a id="kimi-PpYy0dR3Qw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PpYy0dR3Qw@OpenReview', this)">[Kimi<sup id="kimi-stars-PpYy0dR3Qw@OpenReview">4</sup>]</a>
                <a id="rel-PpYy0dR3Qw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PpYy0dR3Qw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PpYy0dR3Qw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Laurent Condat" target="_blank">Laurent Condat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Artavazd Maranjyan" target="_blank">Artavazd Maranjyan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Richtarik" target="_blank">Peter Richtarik</a>
            </p>
            <p id="summary-PpYy0dR3Qw@OpenReview" class="summary">In <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-93-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-608" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.84em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-609"><span class="mi" id="MathJax-Span-610" style="font-family: MathJax_Math-italic;">D</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi></math></span></span><script type="math/tex" id="MathJax-Element-93">D</script>istributed optimization and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-94-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-611" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-612"><span class="mi" id="MathJax-Span-613" style="font-family: MathJax_Math-italic;">L</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></span></span><script type="math/tex" id="MathJax-Element-94">L</script>earning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-95-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-614" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-615"><span class="mi" id="MathJax-Span-616" style="font-family: MathJax_Math-italic;">L</span><span class="mi" id="MathJax-Span-617" style="font-family: MathJax_Math-italic;">o</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi><mi>o</mi></math></span></span><script type="math/tex" id="MathJax-Element-95">Lo</script>cal training, which reduces the communication frequency, and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-96-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-618" style="width: 1.513em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.25em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-619"><span class="mi" id="MathJax-Span-620" style="font-family: MathJax_Math-italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-621" style="font-family: MathJax_Math-italic;">o</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi><mi>o</mi></math></span></span><script type="math/tex" id="MathJax-Element-96">Co</script>mpression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogeneous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.</p>
            <p id="subjects-PpYy0dR3Qw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PpYy0dR3Qw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PpYy0dR3Qw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PpYy0dR3Qw@OpenReview" onclick="foldPdfKimi('PpYy0dR3Qw@OpenReview', this)" class="hr hr-fold">
        </div><div id="PkpNRmBZ32@OpenReview" class="panel paper" keywords="ssm,blocks,contractions,centaurus,convolutions,tensor,ssms,convnets,network,asr">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PkpNRmBZ32" target="_blank" title="122/373"><span class="index notranslate">#122</span></a>
                <a id="title-PkpNRmBZ32@OpenReview" class="title-link" href="/venue/PkpNRmBZ32@OpenReview" target="_blank">Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions</a>
                <a id="pdf-PkpNRmBZ32@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PkpNRmBZ32@OpenReview', this)" data="https://openreview.net/pdf?id=PkpNRmBZ32">[PDF<sup id="pdf-stars-PkpNRmBZ32@OpenReview">8</sup>]</a>
                <a id="copy-PkpNRmBZ32@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PkpNRmBZ32@OpenReview')">[Copy]</a>
                <a id="kimi-PkpNRmBZ32@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PkpNRmBZ32@OpenReview', this)">[Kimi<sup id="kimi-stars-PkpNRmBZ32@OpenReview">5</sup>]</a>
                <a id="rel-PkpNRmBZ32@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PkpNRmBZ32@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PkpNRmBZ32@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Ru Pei" target="_blank">Yan Ru Pei</a>
            </p>
            <p id="summary-PkpNRmBZ32@OpenReview" class="summary">We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism.</p>
            <p id="subjects-PkpNRmBZ32@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PkpNRmBZ32@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PkpNRmBZ32@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PkpNRmBZ32@OpenReview" onclick="foldPdfKimi('PkpNRmBZ32@OpenReview', this)" class="hr hr-fold">
        </div><div id="wHebuIb6IH@OpenReview" class="panel paper" keywords="procedural,material,vlm,vlmaterial,vision,programs,language,materials,photorealistic,perform">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wHebuIb6IH" target="_blank" title="123/373"><span class="index notranslate">#123</span></a>
                <a id="title-wHebuIb6IH@OpenReview" class="title-link" href="/venue/wHebuIb6IH@OpenReview" target="_blank">VLMaterial: Procedural Material Generation with Large Vision-Language Models</a>
                <a id="pdf-wHebuIb6IH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wHebuIb6IH@OpenReview', this)" data="https://openreview.net/pdf?id=wHebuIb6IH">[PDF<sup id="pdf-stars-wHebuIb6IH@OpenReview">3</sup>]</a>
                <a id="copy-wHebuIb6IH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wHebuIb6IH@OpenReview')">[Copy]</a>
                <a id="kimi-wHebuIb6IH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wHebuIb6IH@OpenReview', this)">[Kimi<sup id="kimi-stars-wHebuIb6IH@OpenReview">2</sup>]</a>
                <a id="rel-wHebuIb6IH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wHebuIb6IH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wHebuIb6IH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Beichen Li" target="_blank">Beichen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rundi Wu" target="_blank">Rundi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Armando Solar-Lezama" target="_blank">Armando Solar-Lezama</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Shi" target="_blank">Liang Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changxi Zheng" target="_blank">Changxi Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernd Bickel" target="_blank">Bernd Bickel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wojciech Matusik" target="_blank">Wojciech Matusik</a>
            </p>
            <p id="summary-wHebuIb6IH@OpenReview" class="summary">Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another VLM. Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples.</p>
            <p id="subjects-wHebuIb6IH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wHebuIb6IH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wHebuIb6IH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wHebuIb6IH@OpenReview" onclick="foldPdfKimi('wHebuIb6IH@OpenReview', this)" class="hr hr-fold">
        </div><div id="PUnD86UEK5@OpenReview" class="panel paper" keywords="adam,sgd,ell,loss,smoothness,infty,geometry,adaptivity,convergence,analysis">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PUnD86UEK5" target="_blank" title="124/373"><span class="index notranslate">#124</span></a>
                <a id="title-PUnD86UEK5@OpenReview" class="title-link" href="/venue/PUnD86UEK5@OpenReview" target="_blank">Adam Exploits <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-97-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-622" style="width: 1.425em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.182em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.356em, 1001.18em, 2.432em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-623"><span class="msubsup" id="MathJax-Span-624"><span style="display: inline-block; position: relative; width: 1.182em; height: 0px;"><span style="position: absolute; clip: rect(1.391em, 1000.38em, 2.328em, -999.998em); top: -2.186em; left: 0em;"><span class="mi" id="MathJax-Span-625" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.189em;"></span></span><span style="position: absolute; top: -2.012em; left: 0.418em;"><span class="mi" id="MathJax-Span-626" style="font-size: 70.7%; font-family: MathJax_Main;">∞</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.248em; border-left: 0px solid; width: 0px; height: 1.127em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi mathvariant="normal">∞</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-97">\ell_\infty</script>-geometry of Loss Landscape via Coordinate-wise Adaptivity</a>
                <a id="pdf-PUnD86UEK5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PUnD86UEK5@OpenReview', this)" data="https://openreview.net/pdf?id=PUnD86UEK5">[PDF<sup id="pdf-stars-PUnD86UEK5@OpenReview">6</sup>]</a>
                <a id="copy-PUnD86UEK5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PUnD86UEK5@OpenReview')">[Copy]</a>
                <a id="kimi-PUnD86UEK5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PUnD86UEK5@OpenReview', this)">[Kimi<sup id="kimi-stars-PUnD86UEK5@OpenReview">6</sup>]</a>
                <a id="rel-PUnD86UEK5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PUnD86UEK5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PUnD86UEK5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuo Xie" target="_blank">Shuo Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohamad Amin Mohamadi" target="_blank">Mohamad Amin Mohamadi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Li" target="_blank">Zhiyuan Li</a>
            </p>
            <p id="summary-PUnD86UEK5@OpenReview" class="summary">Adam outperforms SGD when training language models. Yet such benefits are not well-understood theoretically -- previous convergence analysis for Adam and SGD mainly focuses on the number of steps <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-98-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-627" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-628"><span class="mi" id="MathJax-Span-629" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-98">T</script> and is already minimax-optimal in non-convex cases, which are both <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-99-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-630" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.91em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-631"><span class="mi" id="MathJax-Span-632" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-633" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-634"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-635" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="texatom" id="MathJax-Span-636"><span class="mrow" id="MathJax-Span-637"><span class="mo" id="MathJax-Span-638" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-639" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-640"><span class="mrow" id="MathJax-Span-641"><span class="mo" id="MathJax-Span-642" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-643" style="font-size: 70.7%; font-family: MathJax_Main;">4</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-644" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>T</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>4</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-99">O(T^{-1/4})</script>. In this work, we argue that the better dependence on the loss smoothness is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-100-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-645" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-646"><span class="msubsup" id="MathJax-Span-647"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-648" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-649" style="font-size: 70.7%; font-family: MathJax_Main;">∞</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi mathvariant="normal">∞</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-100">\ell_\infty</script> geometry rather than the more common <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-101-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-650" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-651"><span class="msubsup" id="MathJax-Span-652"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-653" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mn" id="MathJax-Span-654" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-101">\ell_2</script> geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Moreover, we show that if we rotate the training loss randomly, Adam can be outperformed by some variants of SGD which is invariant to rotations. This implies that any practically relevant explanation of Adam's optimization benefit must involve non-rotational invariant properties of loss, such as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-102-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-655" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-656"><span class="msubsup" id="MathJax-Span-657"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-658" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-659" style="font-size: 70.7%; font-family: MathJax_Main;">∞</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi mathvariant="normal">∞</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-102">\ell_\infty</script> smoothness as used in our analysis. We also extend the convergence analysis to blockwise Adam, which is a generalization of standard Adam.</p>
            <p id="subjects-PUnD86UEK5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PUnD86UEK5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PUnD86UEK5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PUnD86UEK5@OpenReview" onclick="foldPdfKimi('PUnD86UEK5@OpenReview', this)" class="hr hr-fold">
        </div><div id="j4LITBSUjs@OpenReview" class="panel paper" keywords="hallucinations,perturbollava,multimodal,language,captions,agranular,reliance,reducing,visual,dense">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=j4LITBSUjs" target="_blank" title="125/373"><span class="index notranslate">#125</span></a>
                <a id="title-j4LITBSUjs@OpenReview" class="title-link" href="/venue/j4LITBSUjs@OpenReview" target="_blank">PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training</a>
                <a id="pdf-j4LITBSUjs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('j4LITBSUjs@OpenReview', this)" data="https://openreview.net/pdf?id=j4LITBSUjs">[PDF<sup id="pdf-stars-j4LITBSUjs@OpenReview">15</sup>]</a>
                <a id="copy-j4LITBSUjs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('j4LITBSUjs@OpenReview')">[Copy]</a>
                <a id="kimi-j4LITBSUjs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('j4LITBSUjs@OpenReview', this)">[Kimi<sup id="kimi-stars-j4LITBSUjs@OpenReview">12</sup>]</a>
                <a id="rel-j4LITBSUjs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('j4LITBSUjs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-j4LITBSUjs@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cong Chen" target="_blank">Cong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyu Liu" target="_blank">Mingyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenchen Jing" target="_blank">Chenchen Jing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhou Zhou" target="_blank">Yizhou Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengyun Rao" target="_blank">Fengyun Rao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chen" target="_blank">Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zhang" target="_blank">Bo Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunhua Shen" target="_blank">Chunhua Shen</a>
            </p>
            <p id="summary-j4LITBSUjs@OpenReview" class="summary">This paper aims to address the challenge of hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks. To tackle the challenge, we identify the current lack of a metric that finely measures the caption quality in concept level. We hereby introduce HalFscore, a novel metric built upon the language graph and is designed to evaluate both the accuracy and completeness of dense captions at agranular level. Additionally, we identify the root cause of hallucination as the model's over-reliance on its language prior. To address this, we propose PerturboLLaVA, which reduces the model's reliance on the language prior by incorporating adversarially perturbed text during training. This method enhances the model's focus on visual inputs, effectively reducing hallucinations and producing accurate, image-grounded descriptions without incurring additional computational overhead. PerturboLLaVA significantly improves the fidelity of generated captions, outperforming existing approaches in handling multimodal hallucinations and achieving improved performance across general multimodal benchmarks.</p>
            <p id="subjects-j4LITBSUjs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-j4LITBSUjs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-j4LITBSUjs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-j4LITBSUjs@OpenReview" onclick="foldPdfKimi('j4LITBSUjs@OpenReview', this)" class="hr hr-fold">
        </div><div id="qgsXsqahMq@OpenReview" class="panel paper" keywords="calibration,gets,graph,ensemble,temperature,scaling,gnn,underconfidence,input,experts">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qgsXsqahMq" target="_blank" title="126/373"><span class="index notranslate">#126</span></a>
                <a id="title-qgsXsqahMq@OpenReview" class="title-link" href="/venue/qgsXsqahMq@OpenReview" target="_blank">GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks</a>
                <a id="pdf-qgsXsqahMq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qgsXsqahMq@OpenReview', this)" data="https://openreview.net/pdf?id=qgsXsqahMq">[PDF<sup id="pdf-stars-qgsXsqahMq@OpenReview">9</sup>]</a>
                <a id="copy-qgsXsqahMq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qgsXsqahMq@OpenReview')">[Copy]</a>
                <a id="kimi-qgsXsqahMq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qgsXsqahMq@OpenReview', this)">[Kimi<sup id="kimi-stars-qgsXsqahMq@OpenReview">2</sup>]</a>
                <a id="rel-qgsXsqahMq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qgsXsqahMq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qgsXsqahMq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dingyi Zhuang" target="_blank">Dingyi Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chonghe Jiang" target="_blank">Chonghe Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhan Zheng" target="_blank">Yunhan Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shenhao Wang" target="_blank">Shenhao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinhua Zhao" target="_blank">Jinhua Zhao</a>
            </p>
            <p id="summary-qgsXsqahMq@OpenReview" class="summary">Graph Neural Networks (GNNs) deliver strong classification results but often suffer from poor calibration performance, leading to overconfidence or underconfidence. This is particularly problematic in high-stakes applications where accurate uncertainty estimates are essential. Existing post-hoc methods, such as temperature scaling, fail to effectively utilize graph structures, while current GNN calibration methods often overlook the potential of leveraging diverse input information and model ensembles jointly. In the paper, we propose Graph Ensemble Temperature Scaling (GETS), a novel calibration framework that combines input and model ensemble strategies within a Graph Mixture-of-Experts (MoE) architecture. GETS integrates diverse inputs, including logits, node features, and degree embeddings, and adaptively selects the most relevant experts for each node’s calibration procedure. Our method outperforms state-of-the-art calibration techniques, reducing expected calibration error (ECE) by <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-103-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x2265;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-660" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-661"><span class="mo" id="MathJax-Span-662" style="font-family: MathJax_Main;">≥</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>≥</mo></math></span></span><script type="math/tex" id="MathJax-Element-103">\geq</script> 25% across 10 GNN benchmark datasets. Additionally, GETS is computationally efficient, scalable, and capable of selecting effective input combinations for improved calibration performance.</p>
            <p id="subjects-qgsXsqahMq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qgsXsqahMq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qgsXsqahMq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qgsXsqahMq@OpenReview" onclick="foldPdfKimi('qgsXsqahMq@OpenReview', this)" class="hr hr-fold">
        </div><div id="ujpAYpFDEA@OpenReview" class="panel paper" keywords="watermarked,watermark,imperceptibility,llms,watermarking,prompts,watermarks,llm,key,water">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ujpAYpFDEA" target="_blank" title="127/373"><span class="index notranslate">#127</span></a>
                <a id="title-ujpAYpFDEA@OpenReview" class="title-link" href="/venue/ujpAYpFDEA@OpenReview" target="_blank">Can Watermarked LLMs be Identified by Users via Crafted Prompts?</a>
                <a id="pdf-ujpAYpFDEA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ujpAYpFDEA@OpenReview', this)" data="https://openreview.net/pdf?id=ujpAYpFDEA">[PDF<sup id="pdf-stars-ujpAYpFDEA@OpenReview">7</sup>]</a>
                <a id="copy-ujpAYpFDEA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ujpAYpFDEA@OpenReview')">[Copy]</a>
                <a id="kimi-ujpAYpFDEA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ujpAYpFDEA@OpenReview', this)">[Kimi<sup id="kimi-stars-ujpAYpFDEA@OpenReview">7</sup>]</a>
                <a id="rel-ujpAYpFDEA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ujpAYpFDEA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ujpAYpFDEA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aiwei Liu" target="_blank">Aiwei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Guan" target="_blank">Sheng Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Liu" target="_blank">Yiming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leyi Pan" target="_blank">Leyi Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Zhang" target="_blank">Yifei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liancheng Fang" target="_blank">Liancheng Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lijie Wen" target="_blank">Lijie Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Yu" target="_blank">Philip Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuming Hu" target="_blank">Xuming Hu</a>
            </p>
            <p id="summary-ujpAYpFDEA@OpenReview" class="summary">Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services. This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs. Finally, we propose that the key to enhancing the imperceptibility of watermarked LLMs is to increase the randomness of watermark key selection. Based on this, we introduce the Water-Bag strategy, which significantly improves watermark imperceptibility by merging multiple watermark keys.</p>
            <p id="subjects-ujpAYpFDEA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ujpAYpFDEA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ujpAYpFDEA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ujpAYpFDEA@OpenReview" onclick="foldPdfKimi('ujpAYpFDEA@OpenReview', this)" class="hr hr-fold">
        </div><div id="P7KRIiLM8T@OpenReview" class="panel paper" keywords="unit,parametrization,update,activations,maximal,size,ensures,fp8,hps,scaled">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=P7KRIiLM8T" target="_blank" title="128/373"><span class="index notranslate">#128</span></a>
                <a id="title-P7KRIiLM8T@OpenReview" class="title-link" href="/venue/P7KRIiLM8T@OpenReview" target="_blank">u-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-104-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-663" style="width: 0.731em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.592em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.599em, 1000.56em, 2.467em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-664"><span class="mi" id="MathJax-Span-665" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.29em; border-left: 0px solid; width: 0px; height: 0.877em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-104">\mu</script>P: The Unit-Scaled Maximal Update Parametrization</a>
                <a id="pdf-P7KRIiLM8T@OpenReview" class="title-pdf notranslate" onclick="togglePdf('P7KRIiLM8T@OpenReview', this)" data="https://openreview.net/pdf?id=P7KRIiLM8T">[PDF<sup id="pdf-stars-P7KRIiLM8T@OpenReview">2</sup>]</a>
                <a id="copy-P7KRIiLM8T@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('P7KRIiLM8T@OpenReview')">[Copy]</a>
                <a id="kimi-P7KRIiLM8T@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('P7KRIiLM8T@OpenReview', this)">[Kimi<sup id="kimi-stars-P7KRIiLM8T@OpenReview">5</sup>]</a>
                <a id="rel-P7KRIiLM8T@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('P7KRIiLM8T@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-P7KRIiLM8T@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Blake" target="_blank">Charles Blake</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Constantin Eichenberg" target="_blank">Constantin Eichenberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Josef Dean" target="_blank">Josef Dean</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lukas Balles" target="_blank">Lukas Balles</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luke Prince" target="_blank">Luke Prince</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Björn Deiseroth" target="_blank">Björn Deiseroth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andres Felipe Cruz Salinas" target="_blank">Andres Felipe Cruz Salinas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carlo Luschi" target="_blank">Carlo Luschi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Weinbach" target="_blank">Samuel Weinbach</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Douglas Orr" target="_blank">Douglas Orr</a>
            </p>
            <p id="summary-P7KRIiLM8T@OpenReview" class="summary">The Maximal Update Parametrization (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-105-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-666" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-667"><span class="mi" id="MathJax-Span-668" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-105">\mu</script>P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-106-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-669" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-670"><span class="mi" id="MathJax-Span-671" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-106">\mu</script>P, which improves upon <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-107-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-672" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-673"><span class="mi" id="MathJax-Span-674" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-107">\mu</script>P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-108-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-675" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-676"><span class="mi" id="MathJax-Span-677" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-108">\mu</script>P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-109-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-678" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-679"><span class="mi" id="MathJax-Span-680" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-109">\mu</script>P models reaching a lower loss than comparable <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-110-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-681" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-682"><span class="mi" id="MathJax-Span-683" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-110">\mu</script>P models and working out-of-the-box in FP8.</p>
            <p id="subjects-P7KRIiLM8T@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-P7KRIiLM8T@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-P7KRIiLM8T@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-P7KRIiLM8T@OpenReview" onclick="foldPdfKimi('P7KRIiLM8T@OpenReview', this)" class="hr hr-fold">
        </div><div id="P42DbV2nuV@OpenReview" class="panel paper" keywords="stopping,instance,ies,mastered,early,backpropagation,instances,training,loss,dependent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=P42DbV2nuV" target="_blank" title="129/373"><span class="index notranslate">#129</span></a>
                <a id="title-P42DbV2nuV@OpenReview" class="title-link" href="/venue/P42DbV2nuV@OpenReview" target="_blank">Instance-dependent Early Stopping</a>
                <a id="pdf-P42DbV2nuV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('P42DbV2nuV@OpenReview', this)" data="https://openreview.net/pdf?id=P42DbV2nuV">[PDF<sup id="pdf-stars-P42DbV2nuV@OpenReview">8</sup>]</a>
                <a id="copy-P42DbV2nuV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('P42DbV2nuV@OpenReview')">[Copy]</a>
                <a id="kimi-P42DbV2nuV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('P42DbV2nuV@OpenReview', this)">[Kimi<sup id="kimi-stars-P42DbV2nuV@OpenReview">8</sup>]</a>
                <a id="rel-P42DbV2nuV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('P42DbV2nuV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-P42DbV2nuV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Suqin Yuan" target="_blank">Suqin Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runqi Lin" target="_blank">Runqi Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Feng" target="_blank">Lei Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Han" target="_blank">Bo Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tongliang Liu" target="_blank">Tongliang Liu</a>
            </p>
            <p id="summary-P42DbV2nuV@OpenReview" class="summary">In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model.</p>
            <p id="subjects-P42DbV2nuV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-P42DbV2nuV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-P42DbV2nuV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-P42DbV2nuV@OpenReview" onclick="foldPdfKimi('P42DbV2nuV@OpenReview', this)" class="hr hr-fold">
        </div><div id="oCHsDpyawq@OpenReview" class="panel paper" keywords="zebrafish,brain,zapbench,activity,benchmark,forecasting,larval,prediction,progress,vertebrate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oCHsDpyawq" target="_blank" title="130/373"><span class="index notranslate">#130</span></a>
                <a id="title-oCHsDpyawq@OpenReview" class="title-link" href="/venue/oCHsDpyawq@OpenReview" target="_blank">ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish</a>
                <a id="pdf-oCHsDpyawq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oCHsDpyawq@OpenReview', this)" data="https://openreview.net/pdf?id=oCHsDpyawq">[PDF<sup id="pdf-stars-oCHsDpyawq@OpenReview">3</sup>]</a>
                <a id="copy-oCHsDpyawq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oCHsDpyawq@OpenReview')">[Copy]</a>
                <a id="kimi-oCHsDpyawq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oCHsDpyawq@OpenReview', this)">[Kimi<sup id="kimi-stars-oCHsDpyawq@OpenReview">4</sup>]</a>
                <a id="rel-oCHsDpyawq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oCHsDpyawq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oCHsDpyawq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jan-Matthis Lueckmann" target="_blank">Jan-Matthis Lueckmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Immer" target="_blank">Alexander Immer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Chen" target="_blank">Alex Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Li" target="_blank">Peter Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mariela Petkova" target="_blank">Mariela Petkova</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nirmala Iyer" target="_blank">Nirmala Iyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luuk Hesselink" target="_blank">Luuk Hesselink</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aparna Dev" target="_blank">Aparna Dev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gudrun Ihrke" target="_blank">Gudrun Ihrke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Woohyun Park" target="_blank">Woohyun Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alyson Petruncio" target="_blank">Alyson Petruncio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aubrey Weigel" target="_blank">Aubrey Weigel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wyatt Korff" target="_blank">Wyatt Korff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Engert" target="_blank">Florian Engert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeff Lichtman" target="_blank">Jeff Lichtman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Misha Ahrens" target="_blank">Misha Ahrens</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michal Januszewski" target="_blank">Michal Januszewski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viren Jain" target="_blank">Viren Jain</a>
            </p>
            <p id="summary-oCHsDpyawq@OpenReview" class="summary">Data-driven benchmarks have led to significant progress in key scientific modeling domains including weather and structural biology. Here, we introduce the Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on the problem of predicting cellular-resolution neural activity throughout an entire vertebrate brain. The benchmark is based on a novel dataset containing 4d light-sheet microscopy recordings of over 70,000 neurons in a larval zebrafish brain, along with motion stabilized and voxel-level cell segmentations of these data that facilitate development of a variety of forecasting methods. Initial results from a selection of time series and volumetric video modeling approaches achieve better performance than naive baseline methods, but also show room for further improvement. The specific brain used in the activity recording is also undergoing synaptic-level anatomical mapping, which will enable future integration of detailed structural information into forecasting methods.</p>
            <p id="subjects-oCHsDpyawq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-oCHsDpyawq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oCHsDpyawq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oCHsDpyawq@OpenReview" onclick="foldPdfKimi('oCHsDpyawq@OpenReview', this)" class="hr hr-fold">
        </div><div id="U3PBITXNG6@OpenReview" class="panel paper" keywords="inversebench,inverse,plug,scientific,diffusion,play,problems,textsc,algorithms,benchmarking">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=U3PBITXNG6" target="_blank" title="131/373"><span class="index notranslate">#131</span></a>
                <a id="title-U3PBITXNG6@OpenReview" class="title-link" href="/venue/U3PBITXNG6@OpenReview" target="_blank">InverseBench: Benchmarking Plug-and-Play Diffusion Models for Scientific Inverse Problems</a>
                <a id="pdf-U3PBITXNG6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('U3PBITXNG6@OpenReview', this)" data="https://openreview.net/pdf?id=U3PBITXNG6">[PDF<sup id="pdf-stars-U3PBITXNG6@OpenReview">7</sup>]</a>
                <a id="copy-U3PBITXNG6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('U3PBITXNG6@OpenReview')">[Copy]</a>
                <a id="kimi-U3PBITXNG6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('U3PBITXNG6@OpenReview', this)">[Kimi<sup id="kimi-stars-U3PBITXNG6@OpenReview">3</sup>]</a>
                <a id="rel-U3PBITXNG6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('U3PBITXNG6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-U3PBITXNG6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongkai Zheng" target="_blank">Hongkai Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenda Chu" target="_blank">Wenda Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingliang Zhang" target="_blank">Bingliang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihui Wu" target="_blank">Zihui Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Austin Wang" target="_blank">Austin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Berthy Feng" target="_blank">Berthy Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caifeng Zou" target="_blank">Caifeng Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Sun" target="_blank">Yu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikola Kovachki" target="_blank">Nikola Kovachki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zachary Ross" target="_blank">Zachary Ross</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katherine Bouman" target="_blank">Katherine Bouman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yisong Yue" target="_blank">Yisong Yue</a>
            </p>
            <p id="summary-U3PBITXNG6@OpenReview" class="summary">Plug-and-play diffusion prior methods have emerged as a promising research direction for solving inverse problems. However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \textsc{InverseBench}, a unified framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as black hole imaging, seismology, optical tomography, medical imaging, and fluid dynamics. With \textsc{InverseBench}, we benchmark 15 inverse problem algorithms that use plug-and-play diffusion prior methods against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. We open-source the datasets, pre-trained models, and the codebase to facilitate future research and development.</p>
            <p id="subjects-U3PBITXNG6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-U3PBITXNG6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-U3PBITXNG6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-U3PBITXNG6@OpenReview" onclick="foldPdfKimi('U3PBITXNG6@OpenReview', this)" class="hr hr-fold">
        </div><div id="Oi47wc10sm@OpenReview" class="panel paper" keywords="activation,steering,content,refuse,llm,responses,refusal,cast,conditional,behavior">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Oi47wc10sm" target="_blank" title="132/373"><span class="index notranslate">#132</span></a>
                <a id="title-Oi47wc10sm@OpenReview" class="title-link" href="/venue/Oi47wc10sm@OpenReview" target="_blank">Programming Refusal with Conditional Activation Steering</a>
                <a id="pdf-Oi47wc10sm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Oi47wc10sm@OpenReview', this)" data="https://openreview.net/pdf?id=Oi47wc10sm">[PDF<sup id="pdf-stars-Oi47wc10sm@OpenReview">3</sup>]</a>
                <a id="copy-Oi47wc10sm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Oi47wc10sm@OpenReview')">[Copy]</a>
                <a id="kimi-Oi47wc10sm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Oi47wc10sm@OpenReview', this)">[Kimi<sup id="kimi-stars-Oi47wc10sm@OpenReview">7</sup>]</a>
                <a id="rel-Oi47wc10sm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Oi47wc10sm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Oi47wc10sm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bruce Lee" target="_blank">Bruce Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Inkit Padhi" target="_blank">Inkit Padhi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karthikeyan Natesan Ramamurthy" target="_blank">Karthikeyan Natesan Ramamurthy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erik Miehling" target="_blank">Erik Miehling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre Dognin" target="_blank">Pierre Dognin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manish Nagireddy" target="_blank">Manish Nagireddy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Dhurandhar" target="_blank">Amit Dhurandhar</a>
            </p>
            <p id="summary-Oi47wc10sm@OpenReview" class="summary">LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging.Existing activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants.In this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context.Our method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states.Using CAST, one can systematically control LLM behavior with rules like "if input is about hate speech or adult content, then refuse" or "if input is not about legal advice, then refuse."This allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization.We release an open-source implementation of our framework at &lt;placeholder: open-source GitHub link&gt;.</p>
            <p id="subjects-Oi47wc10sm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Oi47wc10sm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Oi47wc10sm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Oi47wc10sm@OpenReview" onclick="foldPdfKimi('Oi47wc10sm@OpenReview', this)" class="hr hr-fold">
        </div><div id="qtWjSboqfe@OpenReview" class="panel paper" keywords="deem,robustvqa,perception,lmms,mmvp,image,models,eyes,pope,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qtWjSboqfe" target="_blank" title="133/373"><span class="index notranslate">#133</span></a>
                <a id="title-qtWjSboqfe@OpenReview" class="title-link" href="/venue/qtWjSboqfe@OpenReview" target="_blank">DEEM: Diffusion models serve as the eyes of large language models for image perception</a>
                <a id="pdf-qtWjSboqfe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qtWjSboqfe@OpenReview', this)" data="https://openreview.net/pdf?id=qtWjSboqfe">[PDF<sup id="pdf-stars-qtWjSboqfe@OpenReview">11</sup>]</a>
                <a id="copy-qtWjSboqfe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qtWjSboqfe@OpenReview')">[Copy]</a>
                <a id="kimi-qtWjSboqfe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qtWjSboqfe@OpenReview', this)">[Kimi<sup id="kimi-stars-qtWjSboqfe@OpenReview">8</sup>]</a>
                <a id="rel-qtWjSboqfe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qtWjSboqfe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qtWjSboqfe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Run Luo" target="_blank">Run Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunshui Li" target="_blank">Yunshui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longze Chen" target="_blank">Longze Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanwei He" target="_blank">Wanwei He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ting-En Lin" target="_blank">Ting-En Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqiang Liu" target="_blank">Ziqiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Zhang" target="_blank">Lei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zikai Song" target="_blank">Zikai Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hamid Alinejad-Rokny" target="_blank">Hamid Alinejad-Rokny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaobo Xia" target="_blank">Xiaobo Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tongliang Liu" target="_blank">Tongliang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Yang" target="_blank">Min Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Binyuan Hui" target="_blank">Binyuan Hui</a>
            </p>
            <p id="summary-qtWjSboqfe@OpenReview" class="summary">The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like CLIP-ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and other well-known benchmarks, POPE and MMVP, for visual hallucination and perception. In particular, DEEM improves LMM's visual perception performance to a large extent (e.g., 4\% ↑ on RobustVQA, 6.5\% ↑ on MMVP and 12.8 \% ↑ on POPE ). Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10\%), and a smaller base model size. Extensive experiments demonstrate that DEEM enhances the performance of LMMs on various downstream tasks without inferior performance in the long term, including visual question answering, image captioning, and text-conditioned image synthesis.</p>
            <p id="subjects-qtWjSboqfe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qtWjSboqfe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qtWjSboqfe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qtWjSboqfe@OpenReview" onclick="foldPdfKimi('qtWjSboqfe@OpenReview', this)" class="hr hr-fold">
        </div><div id="yVeNBxwL5W@OpenReview" class="panel paper" keywords="diffusion,sde,sampler,nfes,mrs,reverting,controllable,ode,generation,quality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=yVeNBxwL5W" target="_blank" title="134/373"><span class="index notranslate">#134</span></a>
                <a id="title-yVeNBxwL5W@OpenReview" class="title-link" href="/venue/yVeNBxwL5W@OpenReview" target="_blank">MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers</a>
                <a id="pdf-yVeNBxwL5W@OpenReview" class="title-pdf notranslate" onclick="togglePdf('yVeNBxwL5W@OpenReview', this)" data="https://openreview.net/pdf?id=yVeNBxwL5W">[PDF<sup id="pdf-stars-yVeNBxwL5W@OpenReview">3</sup>]</a>
                <a id="copy-yVeNBxwL5W@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('yVeNBxwL5W@OpenReview')">[Copy]</a>
                <a id="kimi-yVeNBxwL5W@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('yVeNBxwL5W@OpenReview', this)">[Kimi<sup id="kimi-stars-yVeNBxwL5W@OpenReview">2</sup>]</a>
                <a id="rel-yVeNBxwL5W@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('yVeNBxwL5W@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-yVeNBxwL5W@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ao Li" target="_blank">Ao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Fang" target="_blank">Wei Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongbo Zhao" target="_blank">Hongbo Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Lu" target="_blank">Le Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minfeng Xu" target="_blank">Minfeng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ge Yang" target="_blank">Ge Yang</a>
            </p>
            <p id="summary-yVeNBxwL5W@OpenReview" class="summary">In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.</p>
            <p id="subjects-yVeNBxwL5W@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-yVeNBxwL5W@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-yVeNBxwL5W@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-yVeNBxwL5W@OpenReview" onclick="foldPdfKimi('yVeNBxwL5W@OpenReview', this)" class="hr hr-fold">
        </div><div id="OZbFRNhpwr@OpenReview" class="panel paper" keywords="smartphone,spa,agents,bench,agent,evaluation,comprehensive,benchmark,contenders,generalisable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OZbFRNhpwr" target="_blank" title="135/373"><span class="index notranslate">#135</span></a>
                <a id="title-OZbFRNhpwr@OpenReview" class="title-link" href="/venue/OZbFRNhpwr@OpenReview" target="_blank">SPA-BENCH: A COMPREHENSIVE BENCHMARK FOR SMARTPHONE AGENT EVALUATION</a>
                <a id="pdf-OZbFRNhpwr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OZbFRNhpwr@OpenReview', this)" data="https://openreview.net/pdf?id=OZbFRNhpwr">[PDF<sup id="pdf-stars-OZbFRNhpwr@OpenReview">8</sup>]</a>
                <a id="copy-OZbFRNhpwr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OZbFRNhpwr@OpenReview')">[Copy]</a>
                <a id="kimi-OZbFRNhpwr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OZbFRNhpwr@OpenReview', this)">[Kimi<sup id="kimi-stars-OZbFRNhpwr@OpenReview">7</sup>]</a>
                <a id="rel-OZbFRNhpwr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OZbFRNhpwr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OZbFRNhpwr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingxuan Chen" target="_blank">Jingxuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Derek Yuen" target="_blank">Derek Yuen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Xie" target="_blank">Bin Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhao Yang" target="_blank">Yuhao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gongwei Chen" target="_blank">Gongwei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Wu" target="_blank">Zhihao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Yixing" target="_blank">Li Yixing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xurui Zhou" target="_blank">Xurui Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiwen Liu" target="_blank">Weiwen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Wang" target="_blank">Shuai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiwen Zhou" target="_blank">Kaiwen Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Shao" target="_blank">Rui Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liqiang Nie" target="_blank">Liqiang Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yasheng Wang" target="_blank">Yasheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianye HAO" target="_blank">Jianye HAO</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Wang" target="_blank">Jun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Shao" target="_blank">Kun Shao</a>
            </p>
            <p id="summary-OZbFRNhpwr@OpenReview" class="summary">Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications.</p>
            <p id="subjects-OZbFRNhpwr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-OZbFRNhpwr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OZbFRNhpwr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OZbFRNhpwr@OpenReview" onclick="foldPdfKimi('OZbFRNhpwr@OpenReview', this)" class="hr hr-fold">
        </div><div id="k3gCieTXeY@OpenReview" class="panel paper" keywords="multilingual,regional,languages,llms,language,knowledge,english,include,bottlenecked,resources">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=k3gCieTXeY" target="_blank" title="136/373"><span class="index notranslate">#136</span></a>
                <a id="title-k3gCieTXeY@OpenReview" class="title-link" href="/venue/k3gCieTXeY@OpenReview" target="_blank">INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</a>
                <a id="pdf-k3gCieTXeY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('k3gCieTXeY@OpenReview', this)" data="https://openreview.net/pdf?id=k3gCieTXeY">[PDF<sup id="pdf-stars-k3gCieTXeY@OpenReview">4</sup>]</a>
                <a id="copy-k3gCieTXeY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('k3gCieTXeY@OpenReview')">[Copy]</a>
                <a id="kimi-k3gCieTXeY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('k3gCieTXeY@OpenReview', this)">[Kimi<sup id="kimi-stars-k3gCieTXeY@OpenReview">5</sup>]</a>
                <a id="rel-k3gCieTXeY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('k3gCieTXeY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-k3gCieTXeY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Angelika Romanou" target="_blank">Angelika Romanou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Negar Foroutan" target="_blank">Negar Foroutan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Sotnikova" target="_blank">Anna Sotnikova</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sree Harsha Nelaturu" target="_blank">Sree Harsha Nelaturu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shivalika Singh" target="_blank">Shivalika Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Maheshwary" target="_blank">Rishabh Maheshwary</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Micol Altomare" target="_blank">Micol Altomare</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeming Chen" target="_blank">Zeming Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohamed Haggag" target="_blank">Mohamed Haggag</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Snegha A" target="_blank">Snegha A</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alfonso Amayuelas" target="_blank">Alfonso Amayuelas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Azril Hafizi Amirudin" target="_blank">Azril Hafizi Amirudin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danylo Boiko" target="_blank">Danylo Boiko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Chang" target="_blank">Michael Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jenny Chim" target="_blank">Jenny Chim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gal Cohen" target="_blank">Gal Cohen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditya K Dalmia" target="_blank">Aditya K Dalmia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abraham Diress" target="_blank">Abraham Diress</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sharad Duwal" target="_blank">Sharad Duwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniil Dzenhaliou" target="_blank">Daniil Dzenhaliou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Florez" target="_blank">Daniel Florez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Farestam" target="_blank">Fabian Farestam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joseph Marvin Imperial" target="_blank">Joseph Marvin Imperial</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shayekh Islam" target="_blank">Shayekh Islam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Perttu Isotalo" target="_blank">Perttu Isotalo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maral Jabbarishiviari" target="_blank">Maral Jabbarishiviari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Börje Karlsson" target="_blank">Börje Karlsson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eldar Khalilov" target="_blank">Eldar Khalilov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Klamm" target="_blank">Christopher Klamm</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fajri Koto" target="_blank">Fajri Koto</a><span style="display:none">,
                <a class="author notranslate" href="https://www.google.com/search?q=Dominik Krzemiński" target="_blank">Dominik Krzemiński</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel de Melo" target="_blank">Gabriel de Melo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Syrielle Montariol" target="_blank">Syrielle Montariol</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiyang Nan" target="_blank">Yiyang Nan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joel Niklaus" target="_blank">Joel Niklaus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jekaterina Novikova" target="_blank">Jekaterina Novikova</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johan S Obando Ceron" target="_blank">Johan S Obando Ceron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Debjit Paul" target="_blank">Debjit Paul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Esther Ploeger" target="_blank">Esther Ploeger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jebish Purbey" target="_blank">Jebish Purbey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Swati Rajwal" target="_blank">Swati Rajwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Selvan Sunitha Ravi" target="_blank">Selvan Sunitha Ravi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sara Rydell" target="_blank">Sara Rydell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roshan Santhosh" target="_blank">Roshan Santhosh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Drishti Sharma" target="_blank">Drishti Sharma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marjana Prifti Skenduli" target="_blank">Marjana Prifti Skenduli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arshia Soltani Moakhar" target="_blank">Arshia Soltani Moakhar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bardia moakhar" target="_blank">Bardia moakhar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayush Tarun" target="_blank">Ayush Tarun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Azmine Toushik Wasi" target="_blank">Azmine Toushik Wasi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thenuka Weerasinghe" target="_blank">Thenuka Weerasinghe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Serhan Yilmaz" target="_blank">Serhan Yilmaz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mike Zhang" target="_blank">Mike Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Imanol Schlag" target="_blank">Imanol Schlag</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marzieh Fadaee" target="_blank">Marzieh Fadaee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sara Hooker" target="_blank">Sara Hooker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antoine Bosselut" target="_blank">Antoine Bosselut</a></span>
                <a class="notranslate" onclick="toggleAuthorList(this, 'et al. (27 additional authors not shown)')">et al. (27 additional authors not shown)</a>
            </p>
            <p id="summary-k3gCieTXeY@OpenReview" class="summary">The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts.Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.</p>
            <p id="subjects-k3gCieTXeY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-k3gCieTXeY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-k3gCieTXeY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-k3gCieTXeY@OpenReview" onclick="foldPdfKimi('k3gCieTXeY@OpenReview', this)" class="hr hr-fold">
        </div><div id="jkUp3lybXf@OpenReview" class="panel paper" keywords="reasoning,preference,pseudo,feedback,optimization,tasks,surpassing,livecodebench,haiku,coding">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jkUp3lybXf" target="_blank" title="137/373"><span class="index notranslate">#137</span></a>
                <a id="title-jkUp3lybXf@OpenReview" class="title-link" href="/venue/jkUp3lybXf@OpenReview" target="_blank">Preference Optimization for Reasoning with Pseudo Feedback</a>
                <a id="pdf-jkUp3lybXf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jkUp3lybXf@OpenReview', this)" data="https://openreview.net/pdf?id=jkUp3lybXf">[PDF<sup id="pdf-stars-jkUp3lybXf@OpenReview">10</sup>]</a>
                <a id="copy-jkUp3lybXf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jkUp3lybXf@OpenReview')">[Copy]</a>
                <a id="kimi-jkUp3lybXf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jkUp3lybXf@OpenReview', this)">[Kimi<sup id="kimi-stars-jkUp3lybXf@OpenReview">9</sup>]</a>
                <a id="rel-jkUp3lybXf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jkUp3lybXf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jkUp3lybXf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fangkai Jiao" target="_blank">Fangkai Jiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Geyang Guo" target="_blank">Geyang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingxing Zhang" target="_blank">Xingxing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nancy F Chen" target="_blank">Nancy F Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shafiq Joty" target="_blank">Shafiq Joty</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Furu Wei" target="_blank">Furu Wei</a>
            </p>
            <p id="summary-jkUp3lybXf@OpenReview" class="summary">Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited.In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated \emph{test cases}. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case.We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.3 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.</p>
            <p id="subjects-jkUp3lybXf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jkUp3lybXf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jkUp3lybXf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jkUp3lybXf@OpenReview" onclick="foldPdfKimi('jkUp3lybXf@OpenReview', this)" class="hr hr-fold">
        </div><div id="Nx4PMtJ1ER@OpenReview" class="panel paper" keywords="causal,discovery,sde,signature,independence,stochastic,processes,kernel,conditional,ancestral">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Nx4PMtJ1ER" target="_blank" title="138/373"><span class="index notranslate">#138</span></a>
                <a id="title-Nx4PMtJ1ER@OpenReview" class="title-link" href="/venue/Nx4PMtJ1ER@OpenReview" target="_blank">Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes</a>
                <a id="pdf-Nx4PMtJ1ER@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Nx4PMtJ1ER@OpenReview', this)" data="https://openreview.net/pdf?id=Nx4PMtJ1ER">[PDF<sup id="pdf-stars-Nx4PMtJ1ER@OpenReview">4</sup>]</a>
                <a id="copy-Nx4PMtJ1ER@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Nx4PMtJ1ER@OpenReview')">[Copy]</a>
                <a id="kimi-Nx4PMtJ1ER@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Nx4PMtJ1ER@OpenReview', this)">[Kimi<sup id="kimi-stars-Nx4PMtJ1ER@OpenReview">2</sup>]</a>
                <a id="rel-Nx4PMtJ1ER@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Nx4PMtJ1ER@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Nx4PMtJ1ER@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Georg Manten" target="_blank">Georg Manten</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cecilia Casolo" target="_blank">Cecilia Casolo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emilio Ferrucci" target="_blank">Emilio Ferrucci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Søren Mogensen" target="_blank">Søren Mogensen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cristopher Salvi" target="_blank">Cristopher Salvi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niki Kilbertus" target="_blank">Niki Kilbertus</a>
            </p>
            <p id="summary-Nx4PMtJ1ER@OpenReview" class="summary">Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via `which variables enter the differential of which other variables'. In this paper, we develop conditional independence (CI) constraints on coordinate processes over selected intervals that are Markov with respect to the acyclic dependence graph (allowing self-loops) induced by a general SDE model. We then provide a sound and complete causal discovery algorithm, capable of handling both fully and partially observed data, and uniquely recovering the underlying or induced ancestral graph by exploiting time directionality assuming a CI oracle. Finally, to make our algorithm practically usable, we also propose a flexible, consistent signature kernel-based CI test to infer these constraints from data. We extensively benchmark the CI test in isolation and as part of our causal discovery algorithms, outperforming existing approaches in SDE models and beyond.</p>
            <p id="subjects-Nx4PMtJ1ER@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Nx4PMtJ1ER@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Nx4PMtJ1ER@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Nx4PMtJ1ER@OpenReview" onclick="foldPdfKimi('Nx4PMtJ1ER@OpenReview', this)" class="hr hr-fold">
        </div><div id="nGiGXLnKhl@OpenReview" class="panel paper" keywords="rwkv,vrwkv,vision,processing,perception,vit,resolution,tasks,window,speeds">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=nGiGXLnKhl" target="_blank" title="139/373"><span class="index notranslate">#139</span></a>
                <a id="title-nGiGXLnKhl@OpenReview" class="title-link" href="/venue/nGiGXLnKhl@OpenReview" target="_blank">Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures</a>
                <a id="pdf-nGiGXLnKhl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('nGiGXLnKhl@OpenReview', this)" data="https://openreview.net/pdf?id=nGiGXLnKhl">[PDF<sup id="pdf-stars-nGiGXLnKhl@OpenReview">12</sup>]</a>
                <a id="copy-nGiGXLnKhl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('nGiGXLnKhl@OpenReview')">[Copy]</a>
                <a id="kimi-nGiGXLnKhl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('nGiGXLnKhl@OpenReview', this)">[Kimi<sup id="kimi-stars-nGiGXLnKhl@OpenReview">8</sup>]</a>
                <a id="rel-nGiGXLnKhl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('nGiGXLnKhl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-nGiGXLnKhl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Duan" target="_blank">Yuchen Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiyun Wang" target="_blank">Weiyun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Chen" target="_blank">Zhe Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xizhou Zhu" target="_blank">Xizhou Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lewei Lu" target="_blank">Lewei Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Lu" target="_blank">Tong Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Qiao" target="_blank">Yu Qiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongsheng Li" target="_blank">Hongsheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jifeng Dai" target="_blank">Jifeng Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhai Wang" target="_blank">Wenhai Wang</a>
            </p>
            <p id="summary-nGiGXLnKhl@OpenReview" class="summary">Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model that builds upon the RWKV architecture from the NLP field with key modifications tailored specifically for vision tasks. Similar to the Vision Transformer (ViT), our model demonstrates robust global processing capabilities, efficiently handles sparse inputs like masked images, and can scale up to accommodate both large-scale parameters and extensive datasets. Its distinctive advantage is its reduced spatial aggregation complexity, enabling seamless processing of high-resolution images without the need for window operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code and models are available at~\url{https://github.com/OpenGVLab/Vision-RWKV}.</p>
            <p id="subjects-nGiGXLnKhl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-nGiGXLnKhl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-nGiGXLnKhl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-nGiGXLnKhl@OpenReview" onclick="foldPdfKimi('nGiGXLnKhl@OpenReview', this)" class="hr hr-fold">
        </div><div id="N1L5TgtkAw@OpenReview" class="panel paper" keywords="draft,token,speculative,sampling,scheme,step,acceptance,tokens,probability,theoretical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=N1L5TgtkAw" target="_blank" title="140/373"><span class="index notranslate">#140</span></a>
                <a id="title-N1L5TgtkAw@OpenReview" class="title-link" href="/venue/N1L5TgtkAw@OpenReview" target="_blank">Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</a>
                <a id="pdf-N1L5TgtkAw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('N1L5TgtkAw@OpenReview', this)" data="https://openreview.net/pdf?id=N1L5TgtkAw">[PDF<sup id="pdf-stars-N1L5TgtkAw@OpenReview">4</sup>]</a>
                <a id="copy-N1L5TgtkAw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('N1L5TgtkAw@OpenReview')">[Copy]</a>
                <a id="kimi-N1L5TgtkAw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('N1L5TgtkAw@OpenReview', this)">[Kimi<sup id="kimi-stars-N1L5TgtkAw@OpenReview">5</sup>]</a>
                <a id="rel-N1L5TgtkAw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('N1L5TgtkAw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-N1L5TgtkAw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ashish Khisti" target="_blank">Ashish Khisti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=MohammadReza Ebrahimi" target="_blank">MohammadReza Ebrahimi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hassan Dbouk" target="_blank">Hassan Dbouk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arash Behboodi" target="_blank">Arash Behboodi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roland Memisevic" target="_blank">Roland Memisevic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christos Louizos" target="_blank">Christos Louizos</a>
            </p>
            <p id="summary-N1L5TgtkAw@OpenReview" class="summary">We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection scheme based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.</p>
            <p id="subjects-N1L5TgtkAw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-N1L5TgtkAw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-N1L5TgtkAw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-N1L5TgtkAw@OpenReview" onclick="foldPdfKimi('N1L5TgtkAw@OpenReview', this)" class="hr hr-fold">
        </div><div id="hBGavkf61a@OpenReview" class="panel paper" keywords="mathbf,dbae,endpoint,diffusion,representation,bridge,latent,auteencoders,generation,auxiliary">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hBGavkf61a" target="_blank" title="141/373"><span class="index notranslate">#141</span></a>
                <a id="title-hBGavkf61a@OpenReview" class="title-link" href="/venue/hBGavkf61a@OpenReview" target="_blank">Diffusion Bridge AutoEncoders for Unsupervised Representation Learning</a>
                <a id="pdf-hBGavkf61a@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hBGavkf61a@OpenReview', this)" data="https://openreview.net/pdf?id=hBGavkf61a">[PDF<sup id="pdf-stars-hBGavkf61a@OpenReview">8</sup>]</a>
                <a id="copy-hBGavkf61a@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hBGavkf61a@OpenReview')">[Copy]</a>
                <a id="kimi-hBGavkf61a@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hBGavkf61a@OpenReview', this)">[Kimi<sup id="kimi-stars-hBGavkf61a@OpenReview">4</sup>]</a>
                <a id="rel-hBGavkf61a@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hBGavkf61a@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hBGavkf61a@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yeongmin Kim" target="_blank">Yeongmin Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwanghyeon Lee" target="_blank">Kwanghyeon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minsang Park" target="_blank">Minsang Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Byeonghu Na" target="_blank">Byeonghu Na</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Il-chul Moon" target="_blank">Il-chul Moon</a>
            </p>
            <p id="summary-hBGavkf61a@OpenReview" class="summary">Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a latent variable <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-111-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-684" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-685"><span class="texatom" id="MathJax-Span-686"><span class="mrow" id="MathJax-Span-687"><span class="mi" id="MathJax-Span-688" style="font-family: MathJax_Main-bold;">z</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">z</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-111">\mathbf{z}</script>. Meanwhile, this auxiliary structure invokes an *information split problem*; the information of each data instance <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-112-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-689" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-690"><span class="msubsup" id="MathJax-Span-691"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="texatom" id="MathJax-Span-692"><span class="mrow" id="MathJax-Span-693"><span class="mi" id="MathJax-Span-694" style="font-family: MathJax_Main-bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mn" id="MathJax-Span-695" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-112">\mathbf{x}_0</script> is divided into diffusion endpoint <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-113-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-696" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-697"><span class="msubsup" id="MathJax-Span-698"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="texatom" id="MathJax-Span-699"><span class="mrow" id="MathJax-Span-700"><span class="mi" id="MathJax-Span-701" style="font-family: MathJax_Main-bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-702" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mi>T</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-113">\mathbf{x}_T</script> and encoded <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-114-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-703" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-704"><span class="texatom" id="MathJax-Span-705"><span class="mrow" id="MathJax-Span-706"><span class="mi" id="MathJax-Span-707" style="font-family: MathJax_Main-bold;">z</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">z</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-114">\mathbf{z}</script> because there exist two inference paths starting from the data. The latent variable modeled by diffusion endpoint <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-115-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-708" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-709"><span class="msubsup" id="MathJax-Span-710"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="texatom" id="MathJax-Span-711"><span class="mrow" id="MathJax-Span-712"><span class="mi" id="MathJax-Span-713" style="font-family: MathJax_Main-bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-714" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mi>T</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-115">\mathbf{x}_T</script> has some disadvantages. The diffusion endpoint <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-116-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-715" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-716"><span class="msubsup" id="MathJax-Span-717"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="texatom" id="MathJax-Span-718"><span class="mrow" id="MathJax-Span-719"><span class="mi" id="MathJax-Span-720" style="font-family: MathJax_Main-bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-721" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mi>T</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-116">\mathbf{x}_T</script> is computationally expensive to obtain and inflexible in dimensionality. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enables <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-117-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-722" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-723"><span class="texatom" id="MathJax-Span-724"><span class="mrow" id="MathJax-Span-725"><span class="mi" id="MathJax-Span-726" style="font-family: MathJax_Main-bold;">z</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">z</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-117">\mathbf{z}</script>-dependent endpoint <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-118-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-727" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-728"><span class="msubsup" id="MathJax-Span-729"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="texatom" id="MathJax-Span-730"><span class="mrow" id="MathJax-Span-731"><span class="mi" id="MathJax-Span-732" style="font-family: MathJax_Main-bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-733" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mi>T</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-118">\mathbf{x}_T</script> inference through a feed-forward architecture. This structure creates an information bottleneck at <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-119-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-734" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-735"><span class="texatom" id="MathJax-Span-736"><span class="mrow" id="MathJax-Span-737"><span class="mi" id="MathJax-Span-738" style="font-family: MathJax_Main-bold;">z</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">z</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-119">\mathbf{z}</script>, so <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-120-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-739" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-740"><span class="msubsup" id="MathJax-Span-741"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="texatom" id="MathJax-Span-742"><span class="mrow" id="MathJax-Span-743"><span class="mi" id="MathJax-Span-744" style="font-family: MathJax_Main-bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-745" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mi>T</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-120">\mathbf{x}_T</script> becomes dependent on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-121-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-746" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-747"><span class="texatom" id="MathJax-Span-748"><span class="mrow" id="MathJax-Span-749"><span class="mi" id="MathJax-Span-750" style="font-family: MathJax_Main-bold;">z</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">z</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-121">\mathbf{z}</script> in its generation. This results in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-122-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-751" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-752"><span class="texatom" id="MathJax-Span-753"><span class="mrow" id="MathJax-Span-754"><span class="mi" id="MathJax-Span-755" style="font-family: MathJax_Main-bold;">z</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">z</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-122">\mathbf{z}</script> holding the full information of data. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation.</p>
            <p id="subjects-hBGavkf61a@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hBGavkf61a@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hBGavkf61a@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hBGavkf61a@OpenReview" onclick="foldPdfKimi('hBGavkf61a@OpenReview', this)" class="hr hr-fold">
        </div><div id="NTHMw8S1Ow@OpenReview" class="panel paper" keywords="informed,meta,knowledge,learning,inductive,automated,machine,biases,representations,principles">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NTHMw8S1Ow" target="_blank" title="142/373"><span class="index notranslate">#142</span></a>
                <a id="title-NTHMw8S1Ow@OpenReview" class="title-link" href="/venue/NTHMw8S1Ow@OpenReview" target="_blank">Towards Automated Knowledge Integration From Human-Interpretable Representations</a>
                <a id="pdf-NTHMw8S1Ow@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NTHMw8S1Ow@OpenReview', this)" data="https://openreview.net/pdf?id=NTHMw8S1Ow">[PDF<sup id="pdf-stars-NTHMw8S1Ow@OpenReview">7</sup>]</a>
                <a id="copy-NTHMw8S1Ow@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NTHMw8S1Ow@OpenReview')">[Copy]</a>
                <a id="kimi-NTHMw8S1Ow@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NTHMw8S1Ow@OpenReview', this)">[Kimi<sup id="kimi-stars-NTHMw8S1Ow@OpenReview">6</sup>]</a>
                <a id="rel-NTHMw8S1Ow@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NTHMw8S1Ow@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NTHMw8S1Ow@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Katarzyna Kobalczyk" target="_blank">Katarzyna Kobalczyk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mihaela van der Schaar" target="_blank">Mihaela van der Schaar</a>
            </p>
            <p id="summary-NTHMw8S1Ow@OpenReview" class="summary">In noisy and low-data environments, a significant challenge in machine learning lies in effectively incorporating inductive biases that enhance data efficiency and robustness. Despite many success of informed machine learning methods, designing algorithms with explicit inductive biases based on prior expert knowledge remains largely a manual process. In this work, we explore how prior knowledge represented in its native formats, e.g. in natural language, can be integrated into machine learning models in an automated manner. Inspired by the learning to learn principles of meta-learning, we consider an approach of learning to integrate knowledge via conditional meta-learning, a paradigm we refer to as informed meta-learning. We introduce and motivate theoretically the principles of informed meta-learning enabling automated and controllable inductive bias selection. To illustrate our claims, we implement an instantiation of informed meta-learning--the Informed Neural Process, and empirically demonstrate the potential benefits and limitations of informed meta-learning in improving data efficiency and generalizing to novel knowledge representations.</p>
            <p id="subjects-NTHMw8S1Ow@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-NTHMw8S1Ow@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NTHMw8S1Ow@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NTHMw8S1Ow@OpenReview" onclick="foldPdfKimi('NTHMw8S1Ow@OpenReview', this)" class="hr hr-fold">
        </div><div id="PiZtlzMWUj@OpenReview" class="panel paper" keywords="softcvi,variational,inference,contrastive,posterior,unnormalized,objectives,soft,normalizing,labels">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PiZtlzMWUj" target="_blank" title="143/373"><span class="index notranslate">#143</span></a>
                <a id="title-PiZtlzMWUj@OpenReview" class="title-link" href="/venue/PiZtlzMWUj@OpenReview" target="_blank">SoftCVI: Contrastive variational inference with self-generated soft labels</a>
                <a id="pdf-PiZtlzMWUj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PiZtlzMWUj@OpenReview', this)" data="https://openreview.net/pdf?id=PiZtlzMWUj">[PDF<sup id="pdf-stars-PiZtlzMWUj@OpenReview">4</sup>]</a>
                <a id="copy-PiZtlzMWUj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PiZtlzMWUj@OpenReview')">[Copy]</a>
                <a id="kimi-PiZtlzMWUj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PiZtlzMWUj@OpenReview', this)">[Kimi<sup id="kimi-stars-PiZtlzMWUj@OpenReview">4</sup>]</a>
                <a id="rel-PiZtlzMWUj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PiZtlzMWUj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PiZtlzMWUj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Ward" target="_blank">Daniel Ward</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Beaumont" target="_blank">Mark Beaumont</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Fasiolo" target="_blank">Matteo Fasiolo</a>
            </p>
            <p id="summary-PiZtlzMWUj@OpenReview" class="summary">Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both are often challenging to apply reliably, particularly when the posterior has complex geometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI), which allows a family of variational objectives to be derived through a contrastive estimation framework. The approach parameterizes a classifier in terms of a variational distribution, reframing the inference task as a contrastive estimation problem aiming to identify a single true posterior sample among a set of samples. Despite this framing, we do not require positive or negative samples, but rather learn by sampling the variational distribution and computing ground truth soft classification labels from the unnormalized posterior itself. The objectives have zero variance gradient when the variational approximation is exact, without the need for specialized gradient estimators. We empirically investigate the performance on a variety of Bayesian inference tasks, using both simple (e.g. normal) and expressive (normalizing flow) variational distributions. We find that SoftCVI can be used to form objectives which are stable to train and mass-covering, frequently outperforming inference with other variational approaches.</p>
            <p id="subjects-PiZtlzMWUj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PiZtlzMWUj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PiZtlzMWUj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PiZtlzMWUj@OpenReview" onclick="foldPdfKimi('PiZtlzMWUj@OpenReview', this)" class="hr hr-fold">
        </div><div id="wXSshrxlP4@OpenReview" class="panel paper" keywords="gops,priors,object,pretrained,unsupervised,segmentation,objects,stage,generative,objectness">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wXSshrxlP4" target="_blank" title="144/373"><span class="index notranslate">#144</span></a>
                <a id="title-wXSshrxlP4@OpenReview" class="title-link" href="/venue/wXSshrxlP4@OpenReview" target="_blank">GOPS: Learning Generative Object Priors for Unsupervised 3D Instance Segmentation</a>
                <a id="pdf-wXSshrxlP4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wXSshrxlP4@OpenReview', this)" data="https://openreview.net/pdf?id=wXSshrxlP4">[PDF<sup id="pdf-stars-wXSshrxlP4@OpenReview">2</sup>]</a>
                <a id="copy-wXSshrxlP4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wXSshrxlP4@OpenReview')">[Copy]</a>
                <a id="kimi-wXSshrxlP4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wXSshrxlP4@OpenReview', this)">[Kimi<sup id="kimi-stars-wXSshrxlP4@OpenReview">3</sup>]</a>
                <a id="rel-wXSshrxlP4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wXSshrxlP4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wXSshrxlP4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zihui Zhang" target="_blank">Zihui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yafei YANG" target="_blank">Yafei YANG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongtao Wen" target="_blank">Hongtao Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Yang" target="_blank">Bo Yang</a>
            </p>
            <p id="summary-wXSshrxlP4@OpenReview" class="summary">We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GOPS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then to learn multiple objects by querying against the pretrained priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods.</p>
            <p id="subjects-wXSshrxlP4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wXSshrxlP4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wXSshrxlP4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wXSshrxlP4@OpenReview" onclick="foldPdfKimi('wXSshrxlP4@OpenReview', this)" class="hr hr-fold">
        </div><div id="gcouwCx7dG@OpenReview" class="panel paper" keywords="snns,sparse,rewiring,efficiency,pruning,compression,spiking,synaptic,training,stage">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gcouwCx7dG" target="_blank" title="145/373"><span class="index notranslate">#145</span></a>
                <a id="title-gcouwCx7dG@OpenReview" class="title-link" href="/venue/gcouwCx7dG@OpenReview" target="_blank">Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency</a>
                <a id="pdf-gcouwCx7dG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gcouwCx7dG@OpenReview', this)" data="https://openreview.net/pdf?id=gcouwCx7dG">[PDF<sup id="pdf-stars-gcouwCx7dG@OpenReview">3</sup>]</a>
                <a id="copy-gcouwCx7dG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gcouwCx7dG@OpenReview')">[Copy]</a>
                <a id="kimi-gcouwCx7dG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gcouwCx7dG@OpenReview', this)">[Kimi<sup id="kimi-stars-gcouwCx7dG@OpenReview">4</sup>]</a>
                <a id="rel-gcouwCx7dG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gcouwCx7dG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gcouwCx7dG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangrong Shen" target="_blank">Jiangrong Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Xu" target="_blank">Qi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gang Pan" target="_blank">Gang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Badong Chen" target="_blank">Badong Chen</a>
            </p>
            <p id="summary-gcouwCx7dG@OpenReview" class="summary">The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.In this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. The first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.Our experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware.</p>
            <p id="subjects-gcouwCx7dG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gcouwCx7dG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gcouwCx7dG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gcouwCx7dG@OpenReview" onclick="foldPdfKimi('gcouwCx7dG@OpenReview', this)" class="hr hr-fold">
        </div><div id="MKEHCx25xp@OpenReview" class="panel paper" keywords="wildbench,worse,evaluation,reward,slightly,llms,chatbot,tie,win,score">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MKEHCx25xp" target="_blank" title="146/373"><span class="index notranslate">#146</span></a>
                <a id="title-MKEHCx25xp@OpenReview" class="title-link" href="/venue/MKEHCx25xp@OpenReview" target="_blank">WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</a>
                <a id="pdf-MKEHCx25xp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MKEHCx25xp@OpenReview', this)" data="https://openreview.net/pdf?id=MKEHCx25xp">[PDF<sup id="pdf-stars-MKEHCx25xp@OpenReview">3</sup>]</a>
                <a id="copy-MKEHCx25xp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MKEHCx25xp@OpenReview')">[Copy]</a>
                <a id="kimi-MKEHCx25xp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MKEHCx25xp@OpenReview', this)">[Kimi<sup id="kimi-stars-MKEHCx25xp@OpenReview">3</sup>]</a>
                <a id="rel-MKEHCx25xp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MKEHCx25xp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MKEHCx25xp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bill Yuchen Lin" target="_blank">Bill Yuchen Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuntian Deng" target="_blank">Yuntian Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Khyathi Chandu" target="_blank">Khyathi Chandu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhilasha Ravichander" target="_blank">Abhilasha Ravichander</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Valentina Pyatkin" target="_blank">Valentina Pyatkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nouha Dziri" target="_blank">Nouha Dziri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronan Le Bras" target="_blank">Ronan Le Bras</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yejin Choi" target="_blank">Yejin Choi</a>
            </p>
            <p id="summary-MKEHCx25xp@OpenReview" class="summary">We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of “slightly better/worse” to “tie” if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard’s 0.91 and AlpacaEval2.0’s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.</p>
            <p id="subjects-MKEHCx25xp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-MKEHCx25xp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MKEHCx25xp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MKEHCx25xp@OpenReview" onclick="foldPdfKimi('MKEHCx25xp@OpenReview', this)" class="hr hr-fold">
        </div><div id="MGKDBuyv4p@OpenReview" class="panel paper" keywords="memorization,tinymem,methods,unlearning,mitigation,curbing,lms,regularizer,balancedsubnet,based">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MGKDBuyv4p" target="_blank" title="147/373"><span class="index notranslate">#147</span></a>
                <a id="title-MGKDBuyv4p@OpenReview" class="title-link" href="/venue/MGKDBuyv4p@OpenReview" target="_blank">Mitigating Memorization in Language Models</a>
                <a id="pdf-MGKDBuyv4p@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MGKDBuyv4p@OpenReview', this)" data="https://openreview.net/pdf?id=MGKDBuyv4p">[PDF<sup id="pdf-stars-MGKDBuyv4p@OpenReview">7</sup>]</a>
                <a id="copy-MGKDBuyv4p@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MGKDBuyv4p@OpenReview')">[Copy]</a>
                <a id="kimi-MGKDBuyv4p@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MGKDBuyv4p@OpenReview', this)">[Kimi<sup id="kimi-stars-MGKDBuyv4p@OpenReview">11</sup>]</a>
                <a id="rel-MGKDBuyv4p@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MGKDBuyv4p@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MGKDBuyv4p@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mansi Sakarvadia" target="_blank">Mansi Sakarvadia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aswathy Ajith" target="_blank">Aswathy Ajith</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arham Khan" target="_blank">Arham Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathaniel Hudson" target="_blank">Nathaniel Hudson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caleb Geniesse" target="_blank">Caleb Geniesse</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyle Chard" target="_blank">Kyle Chard</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaoqing Yang" target="_blank">Yaoqing Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ian Foster" target="_blank">Ian Foster</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael W Mahoney" target="_blank">Michael W Mahoney</a>
            </p>
            <p id="summary-MGKDBuyv4p@OpenReview" class="summary">Language models (LMs) can “memorize” information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methodsare effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removingmemorized information while preserving performance on target tasks.</p>
            <p id="subjects-MGKDBuyv4p@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-MGKDBuyv4p@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MGKDBuyv4p@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MGKDBuyv4p@OpenReview" onclick="foldPdfKimi('MGKDBuyv4p@OpenReview', this)" class="hr hr-fold">
        </div><div id="cnKhHxN3xj@OpenReview" class="panel paper" keywords="wasserstein,neurons,polysemantic,disentangling,neuron,neuronal,output,sparsity,mixture,entanglement">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cnKhHxN3xj" target="_blank" title="148/373"><span class="index notranslate">#148</span></a>
                <a id="title-cnKhHxN3xj@OpenReview" class="title-link" href="/venue/cnKhHxN3xj@OpenReview" target="_blank">Wasserstein Distances, Neuronal Entanglement, and Sparsity</a>
                <a id="pdf-cnKhHxN3xj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cnKhHxN3xj@OpenReview', this)" data="https://openreview.net/pdf?id=cnKhHxN3xj">[PDF<sup id="pdf-stars-cnKhHxN3xj@OpenReview">5</sup>]</a>
                <a id="copy-cnKhHxN3xj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cnKhHxN3xj@OpenReview')">[Copy]</a>
                <a id="kimi-cnKhHxN3xj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cnKhHxN3xj@OpenReview', this)">[Kimi<sup id="kimi-stars-cnKhHxN3xj@OpenReview">3</sup>]</a>
                <a id="rel-cnKhHxN3xj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cnKhHxN3xj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cnKhHxN3xj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shashata Sawmya" target="_blank">Shashata Sawmya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linghao Kong" target="_blank">Linghao Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ilia Markov" target="_blank">Ilia Markov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Alistarh" target="_blank">Dan Alistarh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nir Shavit" target="_blank">Nir Shavit</a>
            </p>
            <p id="summary-cnKhHxN3xj@OpenReview" class="summary">Disentangling polysemantic neurons is at the core of many current approaches to interpretability of large language models. Here we attempt to study how disentanglement can be used to understand performance, in particular under weight sparsity, one of today's leading post-training optimization techniques. We suggest a novel measure for estimating neuronal entanglement: the Wasserstein distance of a neuron’s output distribution to a Gaussian. Moreover, we show the existence of a small number of highly entangled "Wasserstein Neurons" in each linear layer of an LLM, characterized by their highly non-Gaussian output distributions and their significant impact on model accuracy. To study this phenomena, we propose a new experimental framework for disentangling polysemantic neurons. Our framework separates each layer’s inputs to create a mixture of experts where each neuron's output is computed by a mixture of neurons of lower Wasserstein distance, each better at maintaining accuracy when sparsified without retraining. We provide strong evidence that this is because the mixture of sparse experts is effectively disentangling the input-output relationship of every individual neuron, in particular the difficult Wasserstein neurons.</p>
            <p id="subjects-cnKhHxN3xj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cnKhHxN3xj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cnKhHxN3xj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cnKhHxN3xj@OpenReview" onclick="foldPdfKimi('cnKhHxN3xj@OpenReview', this)" class="hr hr-fold">
        </div><div id="MFZjrTFE7h@OpenReview" class="panel paper" keywords="fine,localization,detrs,detr,fdr,regression,grained,refinement,redefine,distillation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MFZjrTFE7h" target="_blank" title="149/373"><span class="index notranslate">#149</span></a>
                <a id="title-MFZjrTFE7h@OpenReview" class="title-link" href="/venue/MFZjrTFE7h@OpenReview" target="_blank">D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement</a>
                <a id="pdf-MFZjrTFE7h@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MFZjrTFE7h@OpenReview', this)" data="https://openreview.net/pdf?id=MFZjrTFE7h">[PDF<sup id="pdf-stars-MFZjrTFE7h@OpenReview">3</sup>]</a>
                <a id="copy-MFZjrTFE7h@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MFZjrTFE7h@OpenReview')">[Copy]</a>
                <a id="kimi-MFZjrTFE7h@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MFZjrTFE7h@OpenReview', this)">[Kimi<sup id="kimi-stars-MFZjrTFE7h@OpenReview">2</sup>]</a>
                <a id="rel-MFZjrTFE7h@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MFZjrTFE7h@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MFZjrTFE7h@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yansong Peng" target="_blank">Yansong Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hebei Li" target="_blank">Hebei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peixi Wu" target="_blank">Peixi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueyi Zhang" target="_blank">Yueyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyan Sun" target="_blank">Xiaoyan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Wu" target="_blank">Feng Wu</a>
            </p>
            <p id="summary-MFZjrTFE7h@OpenReview" class="summary">We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). FDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GOLSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying the residual prediction tasks for deeper layers. Additionally, D-FINE incorporates lightweight optimizations in computationally intensive modules and operations, achieving a better balance between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8% AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing real-time detectors. Furthermore, our method significantly enhances the performance of a wide range of DETR models by up to 5.3% AP with negligible extra parameters and training costs. Our code and models will be made publicly available.</p>
            <p id="subjects-MFZjrTFE7h@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-MFZjrTFE7h@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MFZjrTFE7h@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MFZjrTFE7h@OpenReview" onclick="foldPdfKimi('MFZjrTFE7h@OpenReview', this)" class="hr hr-fold">
        </div><div id="M8OGl34Pmg@OpenReview" class="panel paper" keywords="social,human,navigation,trajectories,sda,robot,thread,humans,dynamics,following">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=M8OGl34Pmg" target="_blank" title="150/373"><span class="index notranslate">#150</span></a>
                <a id="title-M8OGl34Pmg@OpenReview" class="title-link" href="/venue/M8OGl34Pmg@OpenReview" target="_blank">Following the Human Thread in Social Navigation</a>
                <a id="pdf-M8OGl34Pmg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('M8OGl34Pmg@OpenReview', this)" data="https://openreview.net/pdf?id=M8OGl34Pmg">[PDF<sup id="pdf-stars-M8OGl34Pmg@OpenReview">2</sup>]</a>
                <a id="copy-M8OGl34Pmg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('M8OGl34Pmg@OpenReview')">[Copy]</a>
                <a id="kimi-M8OGl34Pmg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('M8OGl34Pmg@OpenReview', this)">[Kimi<sup id="kimi-stars-M8OGl34Pmg@OpenReview">5</sup>]</a>
                <a id="rel-M8OGl34Pmg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('M8OGl34Pmg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-M8OGl34Pmg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Scofano" target="_blank">Luca Scofano</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessio Sampieri" target="_blank">Alessio Sampieri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommaso Campari" target="_blank">Tommaso Campari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Valentino Sacco" target="_blank">Valentino Sacco</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Indro Spinelli" target="_blank">Indro Spinelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lamberto Ballan" target="_blank">Lamberto Ballan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabio Galasso" target="_blank">Fabio Galasso</a>
            </p>
            <p id="summary-M8OGl34Pmg@OpenReview" class="summary">The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time.Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans. The code will be released upon acceptance.</p>
            <p id="subjects-M8OGl34Pmg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-M8OGl34Pmg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-M8OGl34Pmg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-M8OGl34Pmg@OpenReview" onclick="foldPdfKimi('M8OGl34Pmg@OpenReview', this)" class="hr hr-fold">
        </div><div id="M2SsqpxGtc@OpenReview" class="panel paper" keywords="panorama,generation,cubediff,cubemaps,diffusion,repurposing,cubemap,panoramas,equirectangular,models">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=M2SsqpxGtc" target="_blank" title="151/373"><span class="index notranslate">#151</span></a>
                <a id="title-M2SsqpxGtc@OpenReview" class="title-link" href="/venue/M2SsqpxGtc@OpenReview" target="_blank">CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation</a>
                <a id="pdf-M2SsqpxGtc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('M2SsqpxGtc@OpenReview', this)" data="https://openreview.net/pdf?id=M2SsqpxGtc">[PDF<sup id="pdf-stars-M2SsqpxGtc@OpenReview">6</sup>]</a>
                <a id="copy-M2SsqpxGtc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('M2SsqpxGtc@OpenReview')">[Copy]</a>
                <a id="kimi-M2SsqpxGtc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('M2SsqpxGtc@OpenReview', this)">[Kimi<sup id="kimi-stars-M2SsqpxGtc@OpenReview">3</sup>]</a>
                <a id="rel-M2SsqpxGtc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('M2SsqpxGtc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-M2SsqpxGtc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nikolai Kalischek" target="_blank">Nikolai Kalischek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Oechsle" target="_blank">Michael Oechsle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Manhardt" target="_blank">Fabian Manhardt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Henzler" target="_blank">Philipp Henzler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Konrad Schindler" target="_blank">Konrad Schindler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Tombari" target="_blank">Federico Tombari</a>
            </p>
            <p id="summary-M2SsqpxGtc@OpenReview" class="summary">We introduce a novel method for generating 360° panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular projections or autoregressive generation, our method treats each face as a standard perspective image, simplifying the generation process and enabling the use of existing multi-view diffusion models. We demonstrate that these models can be adapted to produce high-quality cubemaps without requiring correspondence-aware attention layers. Our model allows for fine-grained text control, generates high resolution panorama images and generalizes well beyond its training set, whilst achieving state-of-the-art results, both qualitatively and quantitatively.</p>
            <p id="subjects-M2SsqpxGtc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-M2SsqpxGtc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-M2SsqpxGtc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-M2SsqpxGtc@OpenReview" onclick="foldPdfKimi('M2SsqpxGtc@OpenReview', this)" class="hr hr-fold">
        </div><div id="Lz0XW99tE0@OpenReview" class="panel paper" keywords="crysbfn,crystal,periodic,bayesian,flow,modeling,generation,conditioning,bfn,euclidean">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Lz0XW99tE0" target="_blank" title="152/373"><span class="index notranslate">#152</span></a>
                <a id="title-Lz0XW99tE0@OpenReview" class="title-link" href="/venue/Lz0XW99tE0@OpenReview" target="_blank">A Periodic Bayesian Flow for Material Generation</a>
                <a id="pdf-Lz0XW99tE0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Lz0XW99tE0@OpenReview', this)" data="https://openreview.net/pdf?id=Lz0XW99tE0">[PDF<sup id="pdf-stars-Lz0XW99tE0@OpenReview">2</sup>]</a>
                <a id="copy-Lz0XW99tE0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Lz0XW99tE0@OpenReview')">[Copy]</a>
                <a id="kimi-Lz0XW99tE0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Lz0XW99tE0@OpenReview', this)">[Kimi<sup id="kimi-stars-Lz0XW99tE0@OpenReview">1</sup>]</a>
                <a id="rel-Lz0XW99tE0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Lz0XW99tE0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Lz0XW99tE0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanlin Wu" target="_blank">Hanlin Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Song" target="_blank">Yuxuan Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingjing Gong" target="_blank">Jingjing Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyao Cao" target="_blank">Ziyao Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yawen Ouyang" target="_blank">Yawen Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianbing Zhang" target="_blank">Jianbing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhou" target="_blank">Hao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Ying Ma" target="_blank">Wei-Ying Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingjing Liu" target="_blank">Jingjing Liu</a>
            </p>
            <p id="summary-Lz0XW99tE0@OpenReview" class="summary">Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song, et al.,2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~ 100x speedup (10 v.s. 2000 steps network forwards) compared with previous Diffusion-based methods on MP-20 dataset.</p>
            <p id="subjects-Lz0XW99tE0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Lz0XW99tE0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lz0XW99tE0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lz0XW99tE0@OpenReview" onclick="foldPdfKimi('Lz0XW99tE0@OpenReview', this)" class="hr hr-fold">
        </div><div id="LiUfN9h0Lx@OpenReview" class="panel paper" keywords="cte,explanation,compression,approximation,estimation,distribution,fewer,improves,relies,attributions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LiUfN9h0Lx" target="_blank" title="153/373"><span class="index notranslate">#153</span></a>
                <a id="title-LiUfN9h0Lx@OpenReview" class="title-link" href="/venue/LiUfN9h0Lx@OpenReview" target="_blank">Efficient and Accurate Explanation Estimation with Distribution Compression</a>
                <a id="pdf-LiUfN9h0Lx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LiUfN9h0Lx@OpenReview', this)" data="https://openreview.net/pdf?id=LiUfN9h0Lx">[PDF<sup id="pdf-stars-LiUfN9h0Lx@OpenReview">3</sup>]</a>
                <a id="copy-LiUfN9h0Lx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LiUfN9h0Lx@OpenReview')">[Copy]</a>
                <a id="kimi-LiUfN9h0Lx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LiUfN9h0Lx@OpenReview', this)">[Kimi<sup id="kimi-stars-LiUfN9h0Lx@OpenReview">3</sup>]</a>
                <a id="rel-LiUfN9h0Lx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LiUfN9h0Lx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LiUfN9h0Lx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hubert Baniecki" target="_blank">Hubert Baniecki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giuseppe Casalicchio" target="_blank">Giuseppe Casalicchio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernd Bischl" target="_blank">Bernd Bischl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Przemyslaw Biecek" target="_blank">Przemyslaw Biecek</a>
            </p>
            <p id="summary-LiUfN9h0Lx@OpenReview" class="summary">We discover a theoretical connection between explanation estimation and distribution compression that significantly improves the approximation of feature attributions, importance, and effects. While the exact computation of various machine learning explanations requires numerous model inferences and becomes impractical, the computational cost of approximation increases with an ever-increasing size of data and model parameters. We show that the standard i.i.d. sampling used in a broad spectrum of algorithms for post-hoc explanation leads to an approximation error worthy of improvement. To this end, we introduce Compress Then Explain (CTE), a new paradigm of sample-efficient explainability. It relies on distribution compression through kernel thinning to obtain a data sample that best approximates its marginal distribution. CTE significantly improves the accuracy and stability of explanation estimation with negligible computational overhead. It often achieves an on-par explanation approximation error 2-3x faster by using fewer samples, i.e. requiring 2-3x fewer model evaluations. CTE is a simple, yet powerful, plug-in for any explanation method that now relies on i.i.d. sampling.</p>
            <p id="subjects-LiUfN9h0Lx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-LiUfN9h0Lx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LiUfN9h0Lx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LiUfN9h0Lx@OpenReview" onclick="foldPdfKimi('LiUfN9h0Lx@OpenReview', this)" class="hr hr-fold">
        </div><div id="LbgIZpSUCe@OpenReview" class="panel paper" keywords="multiregion,impulse,dynamics,areas,nonlinear,neural,neuroscientifically,channels,regions,population">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LbgIZpSUCe" target="_blank" title="154/373"><span class="index notranslate">#154</span></a>
                <a id="title-LbgIZpSUCe@OpenReview" class="title-link" href="/venue/LbgIZpSUCe@OpenReview" target="_blank">Nonlinear multiregion neural dynamics with parametric impulse response communication channels</a>
                <a id="pdf-LbgIZpSUCe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LbgIZpSUCe@OpenReview', this)" data="https://openreview.net/pdf?id=LbgIZpSUCe">[PDF<sup id="pdf-stars-LbgIZpSUCe@OpenReview">1</sup>]</a>
                <a id="copy-LbgIZpSUCe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LbgIZpSUCe@OpenReview')">[Copy]</a>
                <a id="kimi-LbgIZpSUCe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LbgIZpSUCe@OpenReview', this)">[Kimi<sup id="kimi-stars-LbgIZpSUCe@OpenReview">2</sup>]</a>
                <a id="rel-LbgIZpSUCe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LbgIZpSUCe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LbgIZpSUCe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Dowling" target="_blank">Matthew Dowling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cristina Savin" target="_blank">Cristina Savin</a>
            </p>
            <p id="summary-LbgIZpSUCe@OpenReview" class="summary">Cognition arises from the coordinated interaction of brain regions with distinct computational roles. Despite improvements in our ability to extract the dynamics underlying circuit computation from population activity recorded in individual areas, understanding how multiple areas jointly support distributed computation remains a challenge. As part of this effort, we propose a multi-region neural dynamics model composed of two building blocks: _i)_ within-region (potentially driven) nonlinear dynamics and _ii)_ communication channels between regions, parameterized through their impulse response. Together, these choices make it possible to learn nonlinear neural population dynamics and understand the flow of information between regions by drawing from the rich literature of linear systems theory. We develop a state noise inversion free variational filtering and learning algorithm for our model and show, through neuroscientifically inspired numerical experiments, how the proposed model can reveal interpretable characterizations of the local computations within and the flow of information between neural populations. We further validate the efficacy of our approach using simultaneous population recordings from areas V1 and V2.</p>
            <p id="subjects-LbgIZpSUCe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-LbgIZpSUCe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LbgIZpSUCe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LbgIZpSUCe@OpenReview" onclick="foldPdfKimi('LbgIZpSUCe@OpenReview', this)" class="hr hr-fold">
        </div><div id="LXftdR11io@OpenReview" class="panel paper" keywords="potec,policy,opl,stage,regression,action,cluster,contextual,spaces,via">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LXftdR11io" target="_blank" title="155/373"><span class="index notranslate">#155</span></a>
                <a id="title-LXftdR11io@OpenReview" class="title-link" href="/venue/LXftdR11io@OpenReview" target="_blank">POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition</a>
                <a id="pdf-LXftdR11io@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LXftdR11io@OpenReview', this)" data="https://openreview.net/pdf?id=LXftdR11io">[PDF<sup id="pdf-stars-LXftdR11io@OpenReview">2</sup>]</a>
                <a id="copy-LXftdR11io@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LXftdR11io@OpenReview')">[Copy]</a>
                <a id="kimi-LXftdR11io@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LXftdR11io@OpenReview', this)">[Kimi<sup id="kimi-stars-LXftdR11io@OpenReview">2</sup>]</a>
                <a id="rel-LXftdR11io@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LXftdR11io@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LXftdR11io@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuta Saito" target="_blank">Yuta Saito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jihan Yao" target="_blank">Jihan Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thorsten Joachims" target="_blank">Thorsten Joachims</a>
            </p>
            <p id="summary-LXftdR11io@OpenReview" class="summary">We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the relative expected reward differences of the actions within each cluster, ensures that our policy-gradient estimator is unbiased and the second-stage policy is optimal. We also show that POTEC provides a strict generalization of policy- and regression-based approaches and their associated assumptions. Comprehensive experiments demonstrate that POTEC provides substantial improvements in OPL effectiveness particularly in large and structured action spaces.</p>
            <p id="subjects-LXftdR11io@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-LXftdR11io@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LXftdR11io@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LXftdR11io@OpenReview" onclick="foldPdfKimi('LXftdR11io@OpenReview', this)" class="hr hr-fold">
        </div><div id="LSp4KBhAom@OpenReview" class="panel paper" keywords="lora3d,view,rank,calibration,foundation,confidence,self,geometric,low,dust3r">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LSp4KBhAom" target="_blank" title="156/373"><span class="index notranslate">#156</span></a>
                <a id="title-LSp4KBhAom@OpenReview" class="title-link" href="/venue/LSp4KBhAom@OpenReview" target="_blank">LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models</a>
                <a id="pdf-LSp4KBhAom@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LSp4KBhAom@OpenReview', this)" data="https://openreview.net/pdf?id=LSp4KBhAom">[PDF<sup id="pdf-stars-LSp4KBhAom@OpenReview">6</sup>]</a>
                <a id="copy-LSp4KBhAom@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LSp4KBhAom@OpenReview')">[Copy]</a>
                <a id="kimi-LSp4KBhAom@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LSp4KBhAom@OpenReview', this)">[Kimi<sup id="kimi-stars-LSp4KBhAom@OpenReview">4</sup>]</a>
                <a id="rel-LSp4KBhAom@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LSp4KBhAom@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LSp4KBhAom@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqi Lu" target="_blank">Ziqi Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heng Yang" target="_blank">Heng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danfei Xu" target="_blank">Danfei Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyi Li" target="_blank">Boyi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boris Ivanovic" target="_blank">Boris Ivanovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Pavone" target="_blank">Marco Pavone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Wang" target="_blank">Yue Wang</a>
            </p>
            <p id="summary-LSp4KBhAom@OpenReview" class="summary">Emerging 3D geometric foundation models, such as DUSt3R, offer a promising approach for in-the-wild 3D vision tasks.However, due to the high-dimensional nature of the problem space and scarcity of high-quality 3D data,these pre-trained models still struggle to generalize to many challenging circumstances,such as limited view overlap or low lighting.To address this, we propose LoRA3D, an efficient self-calibration pipeline to *specialize* the pre-trained models to target scenes using their own multi-view predictions.Taking sparse RGB images as input, we leverage robust optimization techniques to refine multi-view predictions and align them into a global coordinate frame.In particular, we incorporate prediction confidence into the geometric optimization process, automatically re-weighting the confidence to better reflect point estimation accuracy. We use the calibrated confidence to generate high-quality pseudo labels for the calibrating views and fine-tune the models using low-rank adaptation (LoRA) on the pseudo-labeled data.Our method does not require any external priors or manual labels. It completes the self-calibration process on a **single standard GPU within just 5 minutes**.Each low-rank adapter requires only **18MB** of storage. We evaluated our method on **more than 160 scenes** from the Replica, TUM and Waymo Open datasets,achieving up to **88\% performance improvement** on 3D reconstruction, multi-view pose estimation and novel-view rendering.</p>
            <p id="subjects-LSp4KBhAom@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-LSp4KBhAom@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LSp4KBhAom@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LSp4KBhAom@OpenReview" onclick="foldPdfKimi('LSp4KBhAom@OpenReview', this)" class="hr hr-fold">
        </div><div id="szRmEM8Kx5@OpenReview" class="panel paper" keywords="embedding,compression,contrastive,embeddings,temperature,training,dimensionality,effective,intrinsic,size">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=szRmEM8Kx5" target="_blank" title="157/373"><span class="index notranslate">#157</span></a>
                <a id="title-szRmEM8Kx5@OpenReview" class="title-link" href="/venue/szRmEM8Kx5@OpenReview" target="_blank">Effective post-training embedding compression via temperature control in contrastive training</a>
                <a id="pdf-szRmEM8Kx5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('szRmEM8Kx5@OpenReview', this)" data="https://openreview.net/pdf?id=szRmEM8Kx5">[PDF<sup id="pdf-stars-szRmEM8Kx5@OpenReview">6</sup>]</a>
                <a id="copy-szRmEM8Kx5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('szRmEM8Kx5@OpenReview')">[Copy]</a>
                <a id="kimi-szRmEM8Kx5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('szRmEM8Kx5@OpenReview', this)">[Kimi<sup id="kimi-stars-szRmEM8Kx5@OpenReview">8</sup>]</a>
                <a id="rel-szRmEM8Kx5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('szRmEM8Kx5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-szRmEM8Kx5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=georgiana dinu" target="_blank">georgiana dinu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Corey Barrett" target="_blank">Corey Barrett</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Xiang" target="_blank">Yi Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miguel Romero Calvo" target="_blank">Miguel Romero Calvo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Currey" target="_blank">Anna Currey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Niu" target="_blank">Xing Niu</a>
            </p>
            <p id="summary-szRmEM8Kx5@OpenReview" class="summary">Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates the role of the temperature parameter in contrastive training for text embeddings. We shed light on the impact this parameter has on the intrinsic dimensionality of the embedding spaces obtained, and show that lower intrinsic dimensionality is further correlated with effective compression of embeddings. We still observe a trade-off between absolute performance and effective compression and we propose temperature aggregation methods which reduce embedding size by an order of magnitude with minimal impact on quality.</p>
            <p id="subjects-szRmEM8Kx5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-szRmEM8Kx5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-szRmEM8Kx5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-szRmEM8Kx5@OpenReview" onclick="foldPdfKimi('szRmEM8Kx5@OpenReview', this)" class="hr hr-fold">
        </div><div id="LBl7Hez0fF@OpenReview" class="panel paper" keywords="lvlms,hallucinations,vision,vti,hallucination,steering,language,decoders,intervention,latent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LBl7Hez0fF" target="_blank" title="158/373"><span class="index notranslate">#158</span></a>
                <a id="title-LBl7Hez0fF@OpenReview" class="title-link" href="/venue/LBl7Hez0fF@OpenReview" target="_blank">Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering</a>
                <a id="pdf-LBl7Hez0fF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LBl7Hez0fF@OpenReview', this)" data="https://openreview.net/pdf?id=LBl7Hez0fF">[PDF<sup id="pdf-stars-LBl7Hez0fF@OpenReview">13</sup>]</a>
                <a id="copy-LBl7Hez0fF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LBl7Hez0fF@OpenReview')">[Copy]</a>
                <a id="kimi-LBl7Hez0fF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LBl7Hez0fF@OpenReview', this)">[Kimi<sup id="kimi-stars-LBl7Hez0fF@OpenReview">10</sup>]</a>
                <a id="rel-LBl7Hez0fF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LBl7Hez0fF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LBl7Hez0fF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Liu" target="_blank">Sheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haotian Ye" target="_blank">Haotian Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Y Zou" target="_blank">James Y Zou</a>
            </p>
            <p id="summary-LBl7Hez0fF@OpenReview" class="summary">Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from LLMs. We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional training cost. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs.</p>
            <p id="subjects-LBl7Hez0fF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-LBl7Hez0fF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LBl7Hez0fF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LBl7Hez0fF@OpenReview" onclick="foldPdfKimi('LBl7Hez0fF@OpenReview', this)" class="hr hr-fold">
        </div><div id="KijslFbfOL@OpenReview" class="panel paper" keywords="similarity,imvc,view,clustering,incomplete,siihpc,prototype,hybrid,imputation,samples">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KijslFbfOL" target="_blank" title="159/373"><span class="index notranslate">#159</span></a>
                <a id="title-KijslFbfOL@OpenReview" class="title-link" href="/venue/KijslFbfOL@OpenReview" target="_blank">Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction</a>
                <a id="pdf-KijslFbfOL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KijslFbfOL@OpenReview', this)" data="https://openreview.net/pdf?id=KijslFbfOL">[PDF<sup id="pdf-stars-KijslFbfOL@OpenReview">6</sup>]</a>
                <a id="copy-KijslFbfOL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KijslFbfOL@OpenReview')">[Copy]</a>
                <a id="kimi-KijslFbfOL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KijslFbfOL@OpenReview', this)">[Kimi<sup id="kimi-stars-KijslFbfOL@OpenReview">3</sup>]</a>
                <a id="rel-KijslFbfOL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KijslFbfOL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KijslFbfOL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shengju Yu" target="_blank">Shengju Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhibin Dong" target="_blank">Zhibin Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Wang" target="_blank">Siwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pei Zhang" target="_blank">Pei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhang" target="_blank">Yi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinwang Liu" target="_blank">Xinwang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Guan" target="_blank">Thomas Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiejun Li" target="_blank">Tiejun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiu-ming Cheung" target="_blank">Yiu-ming Cheung</a>
            </p>
            <p id="summary-KijslFbfOL@OpenReview" class="summary">Most of incomplete multi-view clustering (IMVC) methods typically choose to ignore the missing samples and only utilize observed unpaired samples to construct bipartite similarity. Moreover, they employ a single quantity of prototypes to extract the information of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-123-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;all&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-756" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.2em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-757"><span class="texatom" id="MathJax-Span-758"><span class="mrow" id="MathJax-Span-759"><span class="mtext" id="MathJax-Span-760" style="font-family: MathJax_Main-bold;">all</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">all</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-123">\textbf{all}</script> views. To eliminate these drawbacks, we present a simple yet effective IMVC approach, SIIHPC, in this work. It firstly transforms partial bipartition learning into original sample form by virtue of reconstruction concept to split out of observed similarity, and then loosens traditional non-negative constraints via regularizing samples to more freely characterize the similarity. Subsequently, it learns to recover the incomplete parts by utilizing the connection built between the similarity exclusive on respective view and the consensus graph shared for all views. On this foundation, it further introduces a group of hybrid prototype quantities for each individual view to flexibly extract the data features belonging to each view itself. Accordingly, the resulting graphs are with various scales and describe the overall similarity more comprehensively. It is worth mentioning that these all are optimized in one unified learning framework, which makes it possible for them to reciprocally promote. Then, to effectively solve the formulated optimization problem, we design an ingenious auxiliary function that is with theoretically proven monotonic-increasing properties. Finally, the clustering results are obtained by implementing spectral grouping action on the eigenvectors of stacked multi-scale consensus similarity. Numerous experimental results demonstrate that even under diverse missing proportions, the proposed SIIHPC is still able to provide a preferable clustering performance compared to multiple prominent IMVC methods.</p>
            <p id="subjects-KijslFbfOL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-KijslFbfOL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KijslFbfOL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KijslFbfOL@OpenReview" onclick="foldPdfKimi('KijslFbfOL@OpenReview', this)" class="hr hr-fold">
        </div><div id="K2jOacHUlO@OpenReview" class="panel paper" keywords="scr,rcr,external,situated,faithfulness,confidence,contexts,llms,reasoning,llama">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=K2jOacHUlO" target="_blank" title="160/373"><span class="index notranslate">#160</span></a>
                <a id="title-K2jOacHUlO@OpenReview" class="title-link" href="/venue/K2jOacHUlO@OpenReview" target="_blank">Enhancing Large Language Models' Situated Faithfulness to External Contexts</a>
                <a id="pdf-K2jOacHUlO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('K2jOacHUlO@OpenReview', this)" data="https://openreview.net/pdf?id=K2jOacHUlO">[PDF<sup id="pdf-stars-K2jOacHUlO@OpenReview">9</sup>]</a>
                <a id="copy-K2jOacHUlO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('K2jOacHUlO@OpenReview')">[Copy]</a>
                <a id="kimi-K2jOacHUlO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('K2jOacHUlO@OpenReview', this)">[Kimi<sup id="kimi-stars-K2jOacHUlO@OpenReview">10</sup>]</a>
                <a id="rel-K2jOacHUlO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('K2jOacHUlO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-K2jOacHUlO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yukun Huang" target="_blank">Yukun Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanxing Chen" target="_blank">Sanxing Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyi Cai" target="_blank">Hongyi Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bhuwan Dhingra" target="_blank">Bhuwan Dhingra</a>
            </p>
            <p id="summary-K2jOacHUlO@OpenReview" class="summary">Large Language Models (LLMs) are often augmented with external information as contexts, but this external information can sometimes be inaccurate or even intentionally misleading. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2\% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9\% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs.</p>
            <p id="subjects-K2jOacHUlO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-K2jOacHUlO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-K2jOacHUlO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-K2jOacHUlO@OpenReview" onclick="foldPdfKimi('K2jOacHUlO@OpenReview', this)" class="hr hr-fold">
        </div><div id="IqHeDe2lbl@OpenReview" class="panel paper" keywords="visual,ventral,dorsal,stream,alignment,lateral,components,sca,sparse,pathways">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IqHeDe2lbl" target="_blank" title="161/373"><span class="index notranslate">#161</span></a>
                <a id="title-IqHeDe2lbl@OpenReview" class="title-link" href="/venue/IqHeDe2lbl@OpenReview" target="_blank">Sparse components distinguish visual pathways &amp; their alignment to neural networks</a>
                <a id="pdf-IqHeDe2lbl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IqHeDe2lbl@OpenReview', this)" data="https://openreview.net/pdf?id=IqHeDe2lbl">[PDF<sup id="pdf-stars-IqHeDe2lbl@OpenReview">3</sup>]</a>
                <a id="copy-IqHeDe2lbl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IqHeDe2lbl@OpenReview')">[Copy]</a>
                <a id="kimi-IqHeDe2lbl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IqHeDe2lbl@OpenReview', this)">[Kimi<sup id="kimi-stars-IqHeDe2lbl@OpenReview">1</sup>]</a>
                <a id="rel-IqHeDe2lbl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IqHeDe2lbl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IqHeDe2lbl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ammar Marvi" target="_blank">Ammar Marvi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nancy Kanwisher" target="_blank">Nancy Kanwisher</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meenakshi Khosla" target="_blank">Meenakshi Khosla</a>
            </p>
            <p id="summary-IqHeDe2lbl@OpenReview" class="summary">The ventral, dorsal, and lateral streams in high-level human visual cortex are implicated in distinct functional processes. Yet, deep neural networks (DNNs) trained on a single task model the entire visual system surprisingly well, hinting at common computational principles across these pathways. To explore this inconsistency, we applied a novel sparse decomposition approach to identify the dominant components of visual representations within each stream. Consistent with traditional neuroscience research, we find a clear difference in component response profiles across the three visual streams—identifying components selective for faces, places, bodies, text, and food in the ventral stream; social interactions, implied motion, and hand actions in the lateral stream; and some less interpretable components in the dorsal stream. Building on this, we introduce Sparse Component Alignment (SCA), a new method for measuring representational alignment between brains and machines that better captures the latent neural tuning of these two visual systems. We find that standard visual DNNs are more aligned with ventral than either dorsal or lateral representations. SCA reveals these distinctions with greater resolution than conventional population-level geometry, offering a measure of representational alignment that is sensitive to a system’s underlying axes of neural tuning.</p>
            <p id="subjects-IqHeDe2lbl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IqHeDe2lbl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IqHeDe2lbl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IqHeDe2lbl@OpenReview" onclick="foldPdfKimi('IqHeDe2lbl@OpenReview', this)" class="hr hr-fold">
        </div><div id="IUmj2dw5se@OpenReview" class="panel paper" keywords="bias,ceb,evaluation,llms,compositional,bechmark,language,tasks,datasets,across">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IUmj2dw5se" target="_blank" title="162/373"><span class="index notranslate">#162</span></a>
                <a id="title-IUmj2dw5se@OpenReview" class="title-link" href="/venue/IUmj2dw5se@OpenReview" target="_blank">CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models</a>
                <a id="pdf-IUmj2dw5se@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IUmj2dw5se@OpenReview', this)" data="https://openreview.net/pdf?id=IUmj2dw5se">[PDF<sup id="pdf-stars-IUmj2dw5se@OpenReview">4</sup>]</a>
                <a id="copy-IUmj2dw5se@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IUmj2dw5se@OpenReview')">[Copy]</a>
                <a id="kimi-IUmj2dw5se@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IUmj2dw5se@OpenReview', this)">[Kimi<sup id="kimi-stars-IUmj2dw5se@OpenReview">1</sup>]</a>
                <a id="rel-IUmj2dw5se@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IUmj2dw5se@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IUmj2dw5se@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Song Wang" target="_blank">Song Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Wang" target="_blank">Peng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Zhou" target="_blank">Tong Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yushun Dong" target="_blank">Yushun Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Tan" target="_blank">Zhen Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jundong Li" target="_blank">Jundong Li</a>
            </p>
            <p id="summary-IUmj2dw5se@OpenReview" class="summary">As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods.</p>
            <p id="subjects-IUmj2dw5se@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IUmj2dw5se@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IUmj2dw5se@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IUmj2dw5se@OpenReview" onclick="foldPdfKimi('IUmj2dw5se@OpenReview', this)" class="hr hr-fold">
        </div><div id="IF0Q9KY3p2@OpenReview" class="panel paper" keywords="mirror,flow,univariate,bias,scaled,implicit,networks,lazy,shallow,penalized">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IF0Q9KY3p2" target="_blank" title="163/373"><span class="index notranslate">#163</span></a>
                <a id="title-IF0Q9KY3p2@OpenReview" class="title-link" href="/venue/IF0Q9KY3p2@OpenReview" target="_blank">Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression</a>
                <a id="pdf-IF0Q9KY3p2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IF0Q9KY3p2@OpenReview', this)" data="https://openreview.net/pdf?id=IF0Q9KY3p2">[PDF<sup id="pdf-stars-IF0Q9KY3p2@OpenReview">5</sup>]</a>
                <a id="copy-IF0Q9KY3p2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IF0Q9KY3p2@OpenReview')">[Copy]</a>
                <a id="kimi-IF0Q9KY3p2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IF0Q9KY3p2@OpenReview', this)">[Kimi<sup id="kimi-stars-IF0Q9KY3p2@OpenReview">2</sup>]</a>
                <a id="rel-IF0Q9KY3p2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IF0Q9KY3p2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IF0Q9KY3p2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuang Liang" target="_blank">Shuang Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guido Montufar" target="_blank">Guido Montufar</a>
            </p>
            <p id="summary-IF0Q9KY3p2@OpenReview" class="summary">We examine the implicit bias of mirror flow in least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity. For univariate ReLU networks, we characterize this bias through a variational problem in function space. Our analysis includes prior results for ordinary gradient flow as a special case and lifts limitations which required either an intractable adjustment of the training data or networks with skip connections. We further introduce \emph{scaled potentials} and show that for these, mirror flow still exhibits lazy training but is not in the kernel regime. For univariate networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm. A takeaway is that whereas the parameter initialization determines how strongly the curvature of the learned function is penalized at different locations of the input space, the scaled potential determines how the different magnitudes of the curvature are penalized.</p>
            <p id="subjects-IF0Q9KY3p2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IF0Q9KY3p2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IF0Q9KY3p2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IF0Q9KY3p2@OpenReview" onclick="foldPdfKimi('IF0Q9KY3p2@OpenReview', this)" class="hr hr-fold">
        </div><div id="IC5RJvRoMp@OpenReview" class="panel paper" keywords="streamline,layers,llm,pruning,pruned,ruckbreasoning,streamlining,layer,redundant,compress">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IC5RJvRoMp" target="_blank" title="164/373"><span class="index notranslate">#164</span></a>
                <a id="title-IC5RJvRoMp@OpenReview" class="title-link" href="/venue/IC5RJvRoMp@OpenReview" target="_blank">Streamlining Redundant Layers to Compress Large Language Models</a>
                <a id="pdf-IC5RJvRoMp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IC5RJvRoMp@OpenReview', this)" data="https://openreview.net/pdf?id=IC5RJvRoMp">[PDF<sup id="pdf-stars-IC5RJvRoMp@OpenReview">7</sup>]</a>
                <a id="copy-IC5RJvRoMp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IC5RJvRoMp@OpenReview')">[Copy]</a>
                <a id="kimi-IC5RJvRoMp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IC5RJvRoMp@OpenReview', this)">[Kimi<sup id="kimi-stars-IC5RJvRoMp@OpenReview">6</sup>]</a>
                <a id="rel-IC5RJvRoMp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IC5RJvRoMp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IC5RJvRoMp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaodong Chen" target="_blank">Xiaodong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Hu" target="_blank">Yuxuan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Zhang" target="_blank">Jing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanling Wang" target="_blank">Yanling Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cuiping Li" target="_blank">Cuiping Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong Chen" target="_blank">Hong Chen</a>
            </p>
            <p id="summary-IC5RJvRoMp@OpenReview" class="summary">This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. LLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss. Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression. Experiments show that LLM-Streamline outperforms both previous and concurrent state-of-the-art pruning methods in terms of both performance and training efficiency. Our code is available at \href{https://github.com/RUCKBReasoning/LLM-Streamline}{this repository}.</p>
            <p id="subjects-IC5RJvRoMp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IC5RJvRoMp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IC5RJvRoMp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IC5RJvRoMp@OpenReview" onclick="foldPdfKimi('IC5RJvRoMp@OpenReview', this)" class="hr hr-fold">
        </div><div id="Hz4BYVY8YM@OpenReview" class="panel paper" keywords="svbench,lvlms,streaming,video,understanding,dialogues,temporal,turn,benchmarks,chains">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Hz4BYVY8YM" target="_blank" title="165/373"><span class="index notranslate">#165</span></a>
                <a id="title-Hz4BYVY8YM@OpenReview" class="title-link" href="/venue/Hz4BYVY8YM@OpenReview" target="_blank">SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding</a>
                <a id="pdf-Hz4BYVY8YM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Hz4BYVY8YM@OpenReview', this)" data="https://openreview.net/pdf?id=Hz4BYVY8YM">[PDF<sup id="pdf-stars-Hz4BYVY8YM@OpenReview">6</sup>]</a>
                <a id="copy-Hz4BYVY8YM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Hz4BYVY8YM@OpenReview')">[Copy]</a>
                <a id="kimi-Hz4BYVY8YM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Hz4BYVY8YM@OpenReview', this)">[Kimi<sup id="kimi-stars-Hz4BYVY8YM@OpenReview">3</sup>]</a>
                <a id="rel-Hz4BYVY8YM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Hz4BYVY8YM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Hz4BYVY8YM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu Yang" target="_blank">Zhenyu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhang Hu" target="_blank">Yuhang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zemin Du" target="_blank">Zemin Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dizhan Xue" target="_blank">Dizhan Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengsheng Qian" target="_blank">Shengsheng Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahong Wu" target="_blank">Jiahong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Yang" target="_blank">Fan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiming Dong" target="_blank">Weiming Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changsheng Xu" target="_blank">Changsheng Xu</a>
            </p>
            <p id="summary-Hz4BYVY8YM@OpenReview" class="summary">Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains. Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding. We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs. Our benchmark and model can be accessed at https://anonymous.4open.science/r/SVBench-356F.</p>
            <p id="subjects-Hz4BYVY8YM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Hz4BYVY8YM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hz4BYVY8YM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hz4BYVY8YM@OpenReview" onclick="foldPdfKimi('Hz4BYVY8YM@OpenReview', this)" class="hr hr-fold">
        </div><div id="HE5JmwniHm@OpenReview" class="panel paper" keywords="mkc,dleft,clustering,kernel,tensor,late,min,max,optimizaiton,fusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HE5JmwniHm" target="_blank" title="166/373"><span class="index notranslate">#166</span></a>
                <a id="title-HE5JmwniHm@OpenReview" class="title-link" href="/venue/HE5JmwniHm@OpenReview" target="_blank">DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimizaiton</a>
                <a id="pdf-HE5JmwniHm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HE5JmwniHm@OpenReview', this)" data="https://openreview.net/pdf?id=HE5JmwniHm">[PDF<sup id="pdf-stars-HE5JmwniHm@OpenReview">3</sup>]</a>
                <a id="copy-HE5JmwniHm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HE5JmwniHm@OpenReview')">[Copy]</a>
                <a id="kimi-HE5JmwniHm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HE5JmwniHm@OpenReview', this)">[Kimi<sup id="kimi-stars-HE5JmwniHm@OpenReview">2</sup>]</a>
                <a id="rel-HE5JmwniHm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HE5JmwniHm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HE5JmwniHm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhang" target="_blank">Yi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Wang" target="_blank">Siwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiyuan Liu" target="_blank">Jiyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengju Yu" target="_blank">Shengju Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhibin Dong" target="_blank">Zhibin Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suyuan Liu" target="_blank">Suyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinwang Liu" target="_blank">Xinwang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=En Zhu" target="_blank">En Zhu</a>
            </p>
            <p id="summary-HE5JmwniHm@OpenReview" class="summary">Recent advancements in multiple kernel clustering (MKC) have highlighted the effectiveness of late fusion strategies, particularly in enhancing computational efficiency to near-linear complexity while achieving promising clustering performance. However, existing methods encounter three significant limitations: (1) reliance on fixed base partition matrices that do not adaptively optimize during the clustering process, thereby constraining their performance to the inherent representational capabilities of these matrices; (2) a focus on adjusting kernel weights to explore inter-view consistency and complementarity, which often neglects the intrinsic high-order correlations among views, thereby limiting the extraction of comprehensive multiple kernel information; (3) a lack of adaptive mechanisms to accommodate varying distributions within the data, which limits robustness and generalization. To address these challenges, this paper proposes a novel algorithm termed Dynamic Late Fusion Multiple Kernel Clustering with Robust {Tensor Learning via min-max optimization (DLEFT-MKC), which effectively overcomes the representational bottleneck of base partition matrices and facilitates the learning of meaningful high-order cross-view information. Specifically, it is the first to incorporate a min-max optimization paradigm into tensor-based MKC, enhancing algorithm robustness and generalization. Additionally, it dynamically reconstructs decision layers to enhance representation capabilities and subsequently stacks the reconstructed representations for tensor learning that promotes the capture of high-order associations and cluster structures across views, ultimately yielding consensus clustering partitions. To solve the resultant optimization problem, we innovatively design a strategy that combines reduced gradient descent with the alternating direction method of multipliers, ensuring convergence to local optima while maintaining high computational efficiency. Extensive experimental results across various benchmark datasets validate the superior effectiveness and efficiency of the proposed DLEFT-MKC.</p>
            <p id="subjects-HE5JmwniHm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-HE5JmwniHm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HE5JmwniHm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HE5JmwniHm@OpenReview" onclick="foldPdfKimi('HE5JmwniHm@OpenReview', this)" class="hr hr-fold">
        </div><div id="Gj5JTAwdoy@OpenReview" class="panel paper" keywords="distillation,ttm,presto,steps,music,step,accelerating,435ms,distilling,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Gj5JTAwdoy" target="_blank" title="167/373"><span class="index notranslate">#167</span></a>
                <a id="title-Gj5JTAwdoy@OpenReview" class="title-link" href="/venue/Gj5JTAwdoy@OpenReview" target="_blank">Presto! Distilling Steps and Layers for Accelerating Music Generation</a>
                <a id="pdf-Gj5JTAwdoy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Gj5JTAwdoy@OpenReview', this)" data="https://openreview.net/pdf?id=Gj5JTAwdoy">[PDF<sup id="pdf-stars-Gj5JTAwdoy@OpenReview">3</sup>]</a>
                <a id="copy-Gj5JTAwdoy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Gj5JTAwdoy@OpenReview')">[Copy]</a>
                <a id="kimi-Gj5JTAwdoy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Gj5JTAwdoy@OpenReview', this)">[Kimi<sup id="kimi-stars-Gj5JTAwdoy@OpenReview">4</sup>]</a>
                <a id="rel-Gj5JTAwdoy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Gj5JTAwdoy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Gj5JTAwdoy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zachary Novack" target="_blank">Zachary Novack</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ge Zhu" target="_blank">Ge Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonah Casebeer" target="_blank">Jonah Casebeer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julian McAuley" target="_blank">Julian McAuley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taylor Berg-Kirkpatrick" target="_blank">Taylor Berg-Kirkpatrick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas J. Bryan" target="_blank">Nicholas J. Bryan</a>
            </p>
            <p id="summary-Gj5JTAwdoy@OpenReview" class="summary">Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model) — the fastest TTM to our knowledge.</p>
            <p id="subjects-Gj5JTAwdoy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Gj5JTAwdoy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gj5JTAwdoy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gj5JTAwdoy@OpenReview" onclick="foldPdfKimi('Gj5JTAwdoy@OpenReview', this)" class="hr hr-fold">
        </div><div id="G6dMvRuhFr@OpenReview" class="panel paper" keywords="video,tasks,embodiment,actions,exploration,visual,toenable,libero,ithor,action">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=G6dMvRuhFr" target="_blank" title="168/373"><span class="index notranslate">#168</span></a>
                <a id="title-G6dMvRuhFr@OpenReview" class="title-link" href="/venue/G6dMvRuhFr@OpenReview" target="_blank">Grounding Video Models to Actions through Goal Conditioned Exploration</a>
                <a id="pdf-G6dMvRuhFr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('G6dMvRuhFr@OpenReview', this)" data="https://openreview.net/pdf?id=G6dMvRuhFr">[PDF<sup id="pdf-stars-G6dMvRuhFr@OpenReview">4</sup>]</a>
                <a id="copy-G6dMvRuhFr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('G6dMvRuhFr@OpenReview')">[Copy]</a>
                <a id="kimi-G6dMvRuhFr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('G6dMvRuhFr@OpenReview', this)">[Kimi<sup id="kimi-stars-G6dMvRuhFr@OpenReview">4</sup>]</a>
                <a id="rel-G6dMvRuhFr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('G6dMvRuhFr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-G6dMvRuhFr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhao Luo" target="_blank">Yunhao Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilun Du" target="_blank">Yilun Du</a>
            </p>
            <p id="summary-G6dMvRuhFr@OpenReview" class="summary">Large video models, pretrained on massive quantities of amount of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.We propose a framework that uses trajectory level action generation in combination with video guidance toenable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks.We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.</p>
            <p id="subjects-G6dMvRuhFr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-G6dMvRuhFr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-G6dMvRuhFr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-G6dMvRuhFr@OpenReview" onclick="foldPdfKimi('G6dMvRuhFr@OpenReview', this)" class="hr hr-fold">
        </div><div id="FhBT596F1X@OpenReview" class="panel paper" keywords="equivariant,local,functionals,3bpa,density,functional,md17,maes,electron,exchange">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FhBT596F1X" target="_blank" title="169/373"><span class="index notranslate">#169</span></a>
                <a id="title-FhBT596F1X@OpenReview" class="title-link" href="/venue/FhBT596F1X@OpenReview" target="_blank">Learning Equivariant Non-Local Electron Density Functionals</a>
                <a id="pdf-FhBT596F1X@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FhBT596F1X@OpenReview', this)" data="https://openreview.net/pdf?id=FhBT596F1X">[PDF<sup id="pdf-stars-FhBT596F1X@OpenReview">3</sup>]</a>
                <a id="copy-FhBT596F1X@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FhBT596F1X@OpenReview')">[Copy]</a>
                <a id="kimi-FhBT596F1X@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FhBT596F1X@OpenReview', this)">[Kimi<sup id="kimi-stars-FhBT596F1X@OpenReview">3</sup>]</a>
                <a id="rel-FhBT596F1X@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FhBT596F1X@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FhBT596F1X@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Gao" target="_blank">Nicholas Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eike Eberhard" target="_blank">Eike Eberhard</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Günnemann" target="_blank">Stephan Günnemann</a>
            </p>
            <p id="summary-FhBT596F1X@OpenReview" class="summary">The accuracy of density functional theory hinges on the approximation of non-local contributions to the exchange-correlation (XC) functional. To date, machine-learned and human-designed approximations suffer from insufficient accuracy, limited scalability, or dependence on costly reference data. To address these issues, we introduce Equivariant Graph Exchange Correlation (EG-XC), a novel non-local XC functional based on equivariant graph neural networks (GNNs). Where previous works relied on semi-local functionals or fixed-size descriptors of the density, we compress the electron density into an SO(3)-equivariant nuclei-centered point cloud representation for efficient non-local atomic-range interactions. By applying an equivariant GNN on this point cloud, we capture molecular-range interactions in a scalable and accurate manner. To train EG-XC, we differentiate through a self-consistent field solver requiring only energy targets. In our empirical evaluation, we find EG-XC to accurately reconstruct `gold-standard' CCSD(T) energies on MD17. On out-of-distribution conformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%. Remarkably, EG-XC excels in data efficiency and molecular size extrapolation on QM9, matching force fields trained on 5 times more and larger molecules. On identical training sets, EG-XC yields on average 51% lower MAEs.</p>
            <p id="subjects-FhBT596F1X@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FhBT596F1X@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FhBT596F1X@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FhBT596F1X@OpenReview" onclick="foldPdfKimi('FhBT596F1X@OpenReview', this)" class="hr hr-fold">
        </div><div id="ExrEw8cVlU@OpenReview" class="panel paper" keywords="3dgs,attack,poison,splat,computation,splatting,allocable,cost,vulnerability,overlooked">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ExrEw8cVlU" target="_blank" title="170/373"><span class="index notranslate">#170</span></a>
                <a id="title-ExrEw8cVlU@OpenReview" class="title-link" href="/venue/ExrEw8cVlU@OpenReview" target="_blank">Poison-splat: Computation Cost Attack on 3D Gaussian Splatting</a>
                <a id="pdf-ExrEw8cVlU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ExrEw8cVlU@OpenReview', this)" data="https://openreview.net/pdf?id=ExrEw8cVlU">[PDF<sup id="pdf-stars-ExrEw8cVlU@OpenReview">4</sup>]</a>
                <a id="copy-ExrEw8cVlU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ExrEw8cVlU@OpenReview')">[Copy]</a>
                <a id="kimi-ExrEw8cVlU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ExrEw8cVlU@OpenReview', this)">[Kimi<sup id="kimi-stars-ExrEw8cVlU@OpenReview">3</sup>]</a>
                <a id="rel-ExrEw8cVlU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ExrEw8cVlU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ExrEw8cVlU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Lu" target="_blank">Jiahao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Zhang" target="_blank">Yifan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiuhong Shen" target="_blank">Qiuhong Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinchao Wang" target="_blank">Xinchao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuicheng YAN" target="_blank">Shuicheng YAN</a>
            </p>
            <p id="summary-ExrEw8cVlU@OpenReview" class="summary">3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems.</p>
            <p id="subjects-ExrEw8cVlU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ExrEw8cVlU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ExrEw8cVlU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ExrEw8cVlU@OpenReview" onclick="foldPdfKimi('ExrEw8cVlU@OpenReview', this)" class="hr hr-fold">
        </div><div id="EbWf36quzd@OpenReview" class="panel paper" keywords="lumina,t2x,dit,videos,flag,t2i,generative,nextline,nextframe,flow">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EbWf36quzd" target="_blank" title="171/373"><span class="index notranslate">#171</span></a>
                <a id="title-EbWf36quzd@OpenReview" class="title-link" href="/venue/EbWf36quzd@OpenReview" target="_blank">Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation</a>
                <a id="pdf-EbWf36quzd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EbWf36quzd@OpenReview', this)" data="https://openreview.net/pdf?id=EbWf36quzd">[PDF<sup id="pdf-stars-EbWf36quzd@OpenReview">3</sup>]</a>
                <a id="copy-EbWf36quzd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EbWf36quzd@OpenReview')">[Copy]</a>
                <a id="kimi-EbWf36quzd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EbWf36quzd@OpenReview', this)">[Kimi<sup id="kimi-stars-EbWf36quzd@OpenReview">4</sup>]</a>
                <a id="rel-EbWf36quzd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EbWf36quzd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EbWf36quzd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gao Peng" target="_blank">Gao Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Zhuo" target="_blank">Le Zhuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongyang Liu" target="_blank">Dongyang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=DU" target="_blank">DU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Luo" target="_blank">Xu Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longtian Qiu" target="_blank">Longtian Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhang Zhang" target="_blank">Yuhang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rongjie Huang" target="_blank">Rongjie Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijie Geng" target="_blank">Shijie Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renrui Zhang" target="_blank">Renrui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junlin Xie" target="_blank">Junlin Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Shao" target="_blank">Wenqi Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengkai Jiang" target="_blank">Zhengkai Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianshuo Yang" target="_blank">Tianshuo Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weicai Ye" target="_blank">Weicai Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong He" target="_blank">Tong He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=HE" target="_blank">HE</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjun He" target="_blank">Junjun He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Qiao" target="_blank">Yu Qiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongsheng Li" target="_blank">Hongsheng Li</a>
            </p>
            <p id="summary-EbWf36quzd@OpenReview" class="summary">Sora unveils the potential of scaling Diffusion Transformer (DiT) for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this paper, we introduce the Lumina-T2X family -- a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a simple and scalable generative framework that can be adapted to various modalities, e.g., transforming noise into images, videos, multi-view 3D objects, or audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as |[nextline]| and |[nextframe]| tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. Advanced techniques like RoPE, KQ-Norm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT (PixArt-alpha), indicating that increasing the number of parameters significantly accelerates convergence of generative models without compromising visual quality. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. All code and checkpoints of Lumina-T2X are released to further foster creativity, transparency, and diversity in the generative AI community.</p>
            <p id="subjects-EbWf36quzd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-EbWf36quzd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EbWf36quzd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EbWf36quzd@OpenReview" onclick="foldPdfKimi('EbWf36quzd@OpenReview', this)" class="hr hr-fold">
        </div><div id="EEgYUccwsV@OpenReview" class="panel paper" keywords="gui,tutorials,agent,web,agents,vlm,replay,agenttrek,digital,ourwork">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EEgYUccwsV" target="_blank" title="172/373"><span class="index notranslate">#172</span></a>
                <a id="title-EEgYUccwsV@OpenReview" class="title-link" href="/venue/EEgYUccwsV@OpenReview" target="_blank">AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials</a>
                <a id="pdf-EEgYUccwsV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EEgYUccwsV@OpenReview', this)" data="https://openreview.net/pdf?id=EEgYUccwsV">[PDF<sup id="pdf-stars-EEgYUccwsV@OpenReview">6</sup>]</a>
                <a id="copy-EEgYUccwsV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EEgYUccwsV@OpenReview')">[Copy]</a>
                <a id="kimi-EEgYUccwsV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EEgYUccwsV@OpenReview', this)">[Kimi<sup id="kimi-stars-EEgYUccwsV@OpenReview">10</sup>]</a>
                <a id="rel-EEgYUccwsV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EEgYUccwsV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EEgYUccwsV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiheng Xu" target="_blank">Yiheng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dunjie Lu" target="_blank">Dunjie Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhennan Shen" target="_blank">Zhennan Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junli Wang" target="_blank">Junli Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zekun Wang" target="_blank">Zekun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Mao" target="_blank">Yuchen Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caiming Xiong" target="_blank">Caiming Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Yu" target="_blank">Tao Yu</a>
            </p>
            <p id="summary-EEgYUccwsV@OpenReview" class="summary">Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose \ourwork, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.</p>
            <p id="subjects-EEgYUccwsV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-EEgYUccwsV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EEgYUccwsV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EEgYUccwsV@OpenReview" onclick="foldPdfKimi('EEgYUccwsV@OpenReview', this)" class="hr hr-fold">
        </div><div id="E48QvQppIN@OpenReview" class="panel paper" keywords="antibodies,sequences,antibody,informed,evolving,optimizes,clonebo,clonal,bayesian,mutations">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=E48QvQppIN" target="_blank" title="173/373"><span class="index notranslate">#173</span></a>
                <a id="title-E48QvQppIN@OpenReview" class="title-link" href="/venue/E48QvQppIN@OpenReview" target="_blank">Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences</a>
                <a id="pdf-E48QvQppIN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('E48QvQppIN@OpenReview', this)" data="https://openreview.net/pdf?id=E48QvQppIN">[PDF<sup id="pdf-stars-E48QvQppIN@OpenReview">3</sup>]</a>
                <a id="copy-E48QvQppIN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('E48QvQppIN@OpenReview')">[Copy]</a>
                <a id="kimi-E48QvQppIN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('E48QvQppIN@OpenReview', this)">[Kimi<sup id="kimi-stars-E48QvQppIN@OpenReview">3</sup>]</a>
                <a id="rel-E48QvQppIN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('E48QvQppIN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-E48QvQppIN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Amin" target="_blank">Alan Amin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nate Gruver" target="_blank">Nate Gruver</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilun Kuang" target="_blank">Yilun Kuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yucen Li" target="_blank">Yucen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hunter Elliott" target="_blank">Hunter Elliott</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Calvin McCarter" target="_blank">Calvin McCarter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aniruddh Raghu" target="_blank">Aniruddh Raghu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peyton Greenside" target="_blank">Peyton Greenside</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Gordon Wilson" target="_blank">Andrew Gordon Wilson</a>
            </p>
            <p id="summary-E48QvQppIN@OpenReview" class="summary">To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. Here we introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies in our bodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as a *clonal family*. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody in our bodies. We guide our designs to fit previous measurements using a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realistic *in silico* experiments and designs stronger and more stable binders in *in vitro* wet lab experiments.</p>
            <p id="subjects-E48QvQppIN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-E48QvQppIN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-E48QvQppIN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-E48QvQppIN@OpenReview" onclick="foldPdfKimi('E48QvQppIN@OpenReview', this)" class="hr hr-fold">
        </div><div id="E1EHO0imOb@OpenReview" class="panel paper" keywords="fp8,swiglu,trillion,training,gaudi2,anonymous1252022,instabilities,amplification,bf16,megatron">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=E1EHO0imOb" target="_blank" title="174/373"><span class="index notranslate">#174</span></a>
                <a id="title-E1EHO0imOb@OpenReview" class="title-link" href="/venue/E1EHO0imOb@OpenReview" target="_blank">Scaling FP8 training to trillion-token LLMs</a>
                <a id="pdf-E1EHO0imOb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('E1EHO0imOb@OpenReview', this)" data="https://openreview.net/pdf?id=E1EHO0imOb">[PDF<sup id="pdf-stars-E1EHO0imOb@OpenReview">5</sup>]</a>
                <a id="copy-E1EHO0imOb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('E1EHO0imOb@OpenReview')">[Copy]</a>
                <a id="kimi-E1EHO0imOb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('E1EHO0imOb@OpenReview', this)">[Kimi<sup id="kimi-stars-E1EHO0imOb@OpenReview">4</sup>]</a>
                <a id="rel-E1EHO0imOb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('E1EHO0imOb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-E1EHO0imOb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Maxim Fishman" target="_blank">Maxim Fishman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Chmiel" target="_blank">Brian Chmiel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ron Banner" target="_blank">Ron Banner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Soudry" target="_blank">Daniel Soudry</a>
            </p>
            <p id="summary-E1EHO0imOb@OpenReview" class="summary">We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens --- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-124-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-761" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-762"><span class="mo" id="MathJax-Span-763" style="font-family: MathJax_Main;">∼</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo></math></span></span><script type="math/tex" id="MathJax-Element-124">\sim</script> 34 % throughput improvement. A reference implementation is supplied in https://github.com/Anonymous1252022/Megatron-DeepSpeed</p>
            <p id="subjects-E1EHO0imOb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-E1EHO0imOb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-E1EHO0imOb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-E1EHO0imOb@OpenReview" onclick="foldPdfKimi('E1EHO0imOb@OpenReview', this)" class="hr hr-fold">
        </div><div id="Dzh0hQPpuf@OpenReview" class="panel paper" keywords="teacher,student,privileged,imitation,observability,task,navigation,latters,imitated,behaviors">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Dzh0hQPpuf" target="_blank" title="175/373"><span class="index notranslate">#175</span></a>
                <a id="title-Dzh0hQPpuf@OpenReview" class="title-link" href="/venue/Dzh0hQPpuf@OpenReview" target="_blank">Student-Informed Teacher Training</a>
                <a id="pdf-Dzh0hQPpuf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Dzh0hQPpuf@OpenReview', this)" data="https://openreview.net/pdf?id=Dzh0hQPpuf">[PDF<sup id="pdf-stars-Dzh0hQPpuf@OpenReview">3</sup>]</a>
                <a id="copy-Dzh0hQPpuf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Dzh0hQPpuf@OpenReview')">[Copy]</a>
                <a id="kimi-Dzh0hQPpuf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Dzh0hQPpuf@OpenReview', this)">[Kimi<sup id="kimi-stars-Dzh0hQPpuf@OpenReview">3</sup>]</a>
                <a id="rel-Dzh0hQPpuf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Dzh0hQPpuf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Dzh0hQPpuf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nico Messikommer" target="_blank">Nico Messikommer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxu Xing" target="_blank">Jiaxu Xing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elie Aljalbout" target="_blank">Elie Aljalbout</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Davide Scaramuzza" target="_blank">Davide Scaramuzza</a>
            </p>
            <p id="summary-Dzh0hQPpuf@OpenReview" class="summary">Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks.</p>
            <p id="subjects-Dzh0hQPpuf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Dzh0hQPpuf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dzh0hQPpuf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dzh0hQPpuf@OpenReview" onclick="foldPdfKimi('Dzh0hQPpuf@OpenReview', this)" class="hr hr-fold">
        </div><div id="DRiLWb8bJg@OpenReview" class="panel paper" keywords="deformables,simulation,sapo,rigid,bodies,multiphysics,differentiable,rewarped,tasks,actor">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DRiLWb8bJg" target="_blank" title="176/373"><span class="index notranslate">#176</span></a>
                <a id="title-DRiLWb8bJg@OpenReview" class="title-link" href="/venue/DRiLWb8bJg@OpenReview" target="_blank">Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation</a>
                <a id="pdf-DRiLWb8bJg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DRiLWb8bJg@OpenReview', this)" data="https://openreview.net/pdf?id=DRiLWb8bJg">[PDF<sup id="pdf-stars-DRiLWb8bJg@OpenReview">2</sup>]</a>
                <a id="copy-DRiLWb8bJg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DRiLWb8bJg@OpenReview')">[Copy]</a>
                <a id="kimi-DRiLWb8bJg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DRiLWb8bJg@OpenReview', this)">[Kimi<sup id="kimi-stars-DRiLWb8bJg@OpenReview">1</sup>]</a>
                <a id="rel-DRiLWb8bJg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DRiLWb8bJg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DRiLWb8bJg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Eliot Xing" target="_blank">Eliot Xing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vernon Luk" target="_blank">Vernon Luk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jean Oh" target="_blank">Jean Oh</a>
            </p>
            <p id="summary-DRiLWb8bJg@OpenReview" class="summary">Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics. Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements. To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables. We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy. Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies. We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables.</p>
            <p id="subjects-DRiLWb8bJg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-DRiLWb8bJg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DRiLWb8bJg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DRiLWb8bJg@OpenReview" onclick="foldPdfKimi('DRiLWb8bJg@OpenReview', this)" class="hr hr-fold">
        </div><div id="DC8bsa9bzY@OpenReview" class="panel paper" keywords="probability,sampling,rare,extrapolation,estimation,estimate,worst,activation,language,argmax">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DC8bsa9bzY" target="_blank" title="177/373"><span class="index notranslate">#177</span></a>
                <a id="title-DC8bsa9bzY@OpenReview" class="title-link" href="/venue/DC8bsa9bzY@OpenReview" target="_blank">Estimating the Probabilities of Rare Outputs in Language Models</a>
                <a id="pdf-DC8bsa9bzY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DC8bsa9bzY@OpenReview', this)" data="https://openreview.net/pdf?id=DC8bsa9bzY">[PDF<sup id="pdf-stars-DC8bsa9bzY@OpenReview">4</sup>]</a>
                <a id="copy-DC8bsa9bzY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DC8bsa9bzY@OpenReview')">[Copy]</a>
                <a id="kimi-DC8bsa9bzY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DC8bsa9bzY@OpenReview', this)">[Kimi<sup id="kimi-stars-DC8bsa9bzY@OpenReview">4</sup>]</a>
                <a id="rel-DC8bsa9bzY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DC8bsa9bzY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DC8bsa9bzY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Wu" target="_blank">Gabriel Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Hilton" target="_blank">Jacob Hilton</a>
            </p>
            <p id="summary-DC8bsa9bzY@OpenReview" class="summary">We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance.</p>
            <p id="subjects-DC8bsa9bzY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-DC8bsa9bzY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DC8bsa9bzY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DC8bsa9bzY@OpenReview" onclick="foldPdfKimi('DC8bsa9bzY@OpenReview', this)" class="hr hr-fold">
        </div><div id="Cnwz9jONi5@OpenReview" class="panel paper" keywords="accuracy,rms,policy,barking,reward,measuring,performance,regressional,rethinking,goodhart">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Cnwz9jONi5" target="_blank" title="178/373"><span class="index notranslate">#178</span></a>
                <a id="title-Cnwz9jONi5@OpenReview" class="title-link" href="/venue/Cnwz9jONi5@OpenReview" target="_blank">Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?</a>
                <a id="pdf-Cnwz9jONi5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Cnwz9jONi5@OpenReview', this)" data="https://openreview.net/pdf?id=Cnwz9jONi5">[PDF<sup id="pdf-stars-Cnwz9jONi5@OpenReview">4</sup>]</a>
                <a id="copy-Cnwz9jONi5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Cnwz9jONi5@OpenReview')">[Copy]</a>
                <a id="kimi-Cnwz9jONi5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Cnwz9jONi5@OpenReview', this)">[Kimi<sup id="kimi-stars-Cnwz9jONi5@OpenReview">3</sup>]</a>
                <a id="rel-Cnwz9jONi5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Cnwz9jONi5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Cnwz9jONi5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=xueru wen" target="_blank">xueru wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Lou" target="_blank">Jie Lou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaojie Lu" target="_blank">Yaojie Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyu Lin" target="_blank">Hongyu Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=XingYu" target="_blank">XingYu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Lu" target="_blank">Xinyu Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben He" target="_blank">Ben He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianpei Han" target="_blank">Xianpei Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Debing Zhang" target="_blank">Debing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Sun" target="_blank">Le Sun</a>
            </p>
            <p id="summary-Cnwz9jONi5@OpenReview" class="summary">Reward Models (RMs) are crucial for aligning language models with human preferences. Currently, the evaluation of RMs depends on measuring accuracy against a validation set of manually annotated preference data.Although this method is straightforward and widely adopted, the relationship between RM accuracy and downstream policy performance remains under-explored.In this work, we conduct experiments in a synthetic setting to investigate how differences in RM measured by accuracy translate into gaps in optimized policy performance.Our findings reveal that while there is a weak positive correlation between accuracy and downstream performance, policies optimized towards RMs with similar accuracy can exhibit quite different performance.Moreover, we discover that the way of measuring accuracy significantly impacts its ability to predict the final policy performance. Through the lens of the Regressional Goodhart effect, we recognize that accuracy, when used for measuring RM quality, can fail to fully capture the potential RM overoptimization.This underscores the inadequacy of relying solely on accuracy to reflect their impact on policy optimization.</p>
            <p id="subjects-Cnwz9jONi5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Cnwz9jONi5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cnwz9jONi5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cnwz9jONi5@OpenReview" onclick="foldPdfKimi('Cnwz9jONi5@OpenReview', this)" class="hr hr-fold">
        </div><div id="BmG88rONaU@OpenReview" class="panel paper" keywords="query,shift,tcr,retrieval,modal,cross,adaptation,queries,modality,test">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BmG88rONaU" target="_blank" title="179/373"><span class="index notranslate">#179</span></a>
                <a id="title-BmG88rONaU@OpenReview" class="title-link" href="/venue/BmG88rONaU@OpenReview" target="_blank">Test-time Adaptation for Cross-modal Retrieval with Query Shift</a>
                <a id="pdf-BmG88rONaU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BmG88rONaU@OpenReview', this)" data="https://openreview.net/pdf?id=BmG88rONaU">[PDF<sup id="pdf-stars-BmG88rONaU@OpenReview">3</sup>]</a>
                <a id="copy-BmG88rONaU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BmG88rONaU@OpenReview')">[Copy]</a>
                <a id="kimi-BmG88rONaU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BmG88rONaU@OpenReview', this)">[Kimi<sup id="kimi-stars-BmG88rONaU@OpenReview">7</sup>]</a>
                <a id="rel-BmG88rONaU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BmG88rONaU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BmG88rONaU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haobin Li" target="_blank">Haobin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Hu" target="_blank">Peng Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianjun Zhang" target="_blank">Qianjun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Peng" target="_blank">Xi Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=XitingLiu" target="_blank">XitingLiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mouxing Yang" target="_blank">Mouxing Yang</a>
            </p>
            <p id="summary-BmG88rONaU@OpenReview" class="summary">The success of most existing cross-modal retrieval methods heavily relies on the assumption that the given queries follow the same distribution of the source domain. However, such an assumption is easily violated in real-world scenarios due to the complexity and diversity of queries, thus leading to the query shift problem.Specifically, query shift refers to the online query stream originating from the domain that follows a different distribution with the source one.In this paper, we observe that query shift would not only diminish the uniformity (namely, within-modality scatter) of the query modality but also amplify the gap between query and gallery modalities. Based on the observations, we propose a novel method dubbed Test-time adaptation for Cross-modal Retrieval (TCR). In brief, TCR employs a novel module to refine the query predictions (namely, retrieval results of the query) and a joint objective to prevent query shift from disturbing the common space, thus achieving online adaptation for the cross-modal retrieval models with query shift.Expensive experiments demonstrate the effectiveness of the proposed TCR against query shift. The code will be released upon acceptance.</p>
            <p id="subjects-BmG88rONaU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-BmG88rONaU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BmG88rONaU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BmG88rONaU@OpenReview" onclick="foldPdfKimi('BmG88rONaU@OpenReview', this)" class="hr hr-fold">
        </div><div id="BgxsmpVoOX@OpenReview" class="panel paper" keywords="rare,concepts,diffusion,r2f,guidance,t2i,frequent,compositional,llm,rarebench">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BgxsmpVoOX" target="_blank" title="180/373"><span class="index notranslate">#180</span></a>
                <a id="title-BgxsmpVoOX@OpenReview" class="title-link" href="/venue/BgxsmpVoOX@OpenReview" target="_blank">Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance</a>
                <a id="pdf-BgxsmpVoOX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BgxsmpVoOX@OpenReview', this)" data="https://openreview.net/pdf?id=BgxsmpVoOX">[PDF<sup id="pdf-stars-BgxsmpVoOX@OpenReview">1</sup>]</a>
                <a id="copy-BgxsmpVoOX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BgxsmpVoOX@OpenReview')">[Copy]</a>
                <a id="kimi-BgxsmpVoOX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BgxsmpVoOX@OpenReview', this)">[Kimi<sup id="kimi-stars-BgxsmpVoOX@OpenReview">5</sup>]</a>
                <a id="rel-BgxsmpVoOX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BgxsmpVoOX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BgxsmpVoOX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongmin Park" target="_blank">Dongmin Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebin Kim" target="_blank">Sebin Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taehong Moon" target="_blank">Taehong Moon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minkyu Kim" target="_blank">Minkyu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kangwook Lee" target="_blank">Kangwook Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaewoong Cho" target="_blank">Jaewoong Cho</a>
            </p>
            <p id="summary-BgxsmpVoOX@OpenReview" class="summary">State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-tofrequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across anypre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment.</p>
            <p id="subjects-BgxsmpVoOX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-BgxsmpVoOX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BgxsmpVoOX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BgxsmpVoOX@OpenReview" onclick="foldPdfKimi('BgxsmpVoOX@OpenReview', this)" class="hr hr-fold">
        </div><div id="BL4WBIfyrz@OpenReview" class="panel paper" keywords="limac,control,app,vlms,lightweight,mobile,tuned,fine,source,qwen2">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BL4WBIfyrz" target="_blank" title="181/373"><span class="index notranslate">#181</span></a>
                <a id="title-BL4WBIfyrz@OpenReview" class="title-link" href="/venue/BL4WBIfyrz@OpenReview" target="_blank">Lightweight Neural App Control</a>
                <a id="pdf-BL4WBIfyrz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BL4WBIfyrz@OpenReview', this)" data="https://openreview.net/pdf?id=BL4WBIfyrz">[PDF<sup id="pdf-stars-BL4WBIfyrz@OpenReview">2</sup>]</a>
                <a id="copy-BL4WBIfyrz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BL4WBIfyrz@OpenReview')">[Copy]</a>
                <a id="kimi-BL4WBIfyrz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BL4WBIfyrz@OpenReview', this)">[Kimi<sup id="kimi-stars-BL4WBIfyrz@OpenReview">2</sup>]</a>
                <a id="rel-BL4WBIfyrz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BL4WBIfyrz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BL4WBIfyrz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Filippos Christianos" target="_blank">Filippos Christianos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgios Papoudakis" target="_blank">Georgios Papoudakis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Coste" target="_blank">Thomas Coste</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianye HAO" target="_blank">Jianye HAO</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Wang" target="_blank">Jun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Shao" target="_blank">Kun Shao</a>
            </p>
            <p id="summary-BL4WBIfyrz@OpenReview" class="summary">This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.</p>
            <p id="subjects-BL4WBIfyrz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-BL4WBIfyrz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BL4WBIfyrz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BL4WBIfyrz@OpenReview" onclick="foldPdfKimi('BL4WBIfyrz@OpenReview', this)" class="hr hr-fold">
        </div><div id="AUCYptvAf3@OpenReview" class="panel paper" keywords="robot,diffusion,multi,planning,environments,models,generating,collision,single,mmd">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=AUCYptvAf3" target="_blank" title="182/373"><span class="index notranslate">#182</span></a>
                <a id="title-AUCYptvAf3@OpenReview" class="title-link" href="/venue/AUCYptvAf3@OpenReview" target="_blank">Multi-Robot Motion Planning with Diffusion Models</a>
                <a id="pdf-AUCYptvAf3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('AUCYptvAf3@OpenReview', this)" data="https://openreview.net/pdf?id=AUCYptvAf3">[PDF<sup id="pdf-stars-AUCYptvAf3@OpenReview">3</sup>]</a>
                <a id="copy-AUCYptvAf3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('AUCYptvAf3@OpenReview')">[Copy]</a>
                <a id="kimi-AUCYptvAf3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('AUCYptvAf3@OpenReview', this)">[Kimi<sup id="kimi-stars-AUCYptvAf3@OpenReview">2</sup>]</a>
                <a id="rel-AUCYptvAf3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('AUCYptvAf3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-AUCYptvAf3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yorai Shaoul" target="_blank">Yorai Shaoul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Itamar Mishani" target="_blank">Itamar Mishani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shivam Vats" target="_blank">Shivam Vats</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaoyang Li" target="_blank">Jiaoyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maxim Likhachev" target="_blank">Maxim Likhachev</a>
            </p>
            <p id="summary-AUCYptvAf3@OpenReview" class="summary">Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning multi-robot diffusion models. In this paper, we propose a method for generating collision-free multi-robot trajectories that conform to underlying data distributions while using only single-robot data.Our algorithm, Multi-robot Multi-model planning Diffusion (MMD), does so by combining learned diffusion models with classical search-based techniques---generating data-driven motions under collision constraints. Scaling further, we show how to compose multiple diffusion models to plan in large environments where a single diffusion model fails to generalize well. We demonstrate the effectiveness of our approach in planning for dozens of robots in a variety of simulated scenarios motivated by logistics environments. View video demonstrations in our supplementary material, and our code at: https://github.com/&lt;removed_for_review&gt;.</p>
            <p id="subjects-AUCYptvAf3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-AUCYptvAf3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-AUCYptvAf3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-AUCYptvAf3@OpenReview" onclick="foldPdfKimi('AUCYptvAf3@OpenReview', this)" class="hr hr-fold">
        </div><div id="APojAzJQiq@OpenReview" class="panel paper" keywords="config,conflict,loss,pinns,informed,physics,terms,update,tum,pinn">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=APojAzJQiq" target="_blank" title="183/373"><span class="index notranslate">#183</span></a>
                <a id="title-APojAzJQiq@OpenReview" class="title-link" href="/venue/APojAzJQiq@OpenReview" target="_blank">ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks</a>
                <a id="pdf-APojAzJQiq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('APojAzJQiq@OpenReview', this)" data="https://openreview.net/pdf?id=APojAzJQiq">[PDF<sup id="pdf-stars-APojAzJQiq@OpenReview">6</sup>]</a>
                <a id="copy-APojAzJQiq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('APojAzJQiq@OpenReview')">[Copy]</a>
                <a id="kimi-APojAzJQiq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('APojAzJQiq@OpenReview', this)">[Kimi<sup id="kimi-stars-APojAzJQiq@OpenReview">1</sup>]</a>
                <a id="rel-APojAzJQiq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('APojAzJQiq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-APojAzJQiq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Liu" target="_blank">Qiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengyu Chu" target="_blank">Mengyu Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nils Thuerey" target="_blank">Nils Thuerey</a>
            </p>
            <p id="summary-APojAzJQiq@OpenReview" class="summary">The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are well-established as highly difficult tasks. To improve learning the challenging multi-objective task posed by PINNs, we propose the ConFIG method, which provides conflict-free updates by ensuring a positive dot product between the final update and each loss-specific gradient. It also maintains consistent optimization rates for all loss terms and dynamically adjusts gradient magnitudes based on conflict levels. We additionally leverage momentum to accelerate optimizations by alternating the back-propagation of different loss terms. We provide a mathematical proof showing the convergence of the ConFIG method, and it is evaluated across a range of challenging PINN scenarios. ConFIG consistently shows superior performance and runtime compared to baseline methods. We also test the proposed method in a classic multi-task benchmark, where the ConFIG method likewise exhibits a highly promising performance. Source code is available at https://tum-pbs.github.io/ConFIG</p>
            <p id="subjects-APojAzJQiq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-APojAzJQiq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-APojAzJQiq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-APojAzJQiq@OpenReview" onclick="foldPdfKimi('APojAzJQiq@OpenReview', this)" class="hr hr-fold">
        </div><div id="AEFVa6VMu1@OpenReview" class="panel paper" keywords="approximation,algorithms,predictions,steiner,hardness,problems,optimization,guarantees,weight,tree">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=AEFVa6VMu1" target="_blank" title="184/373"><span class="index notranslate">#184</span></a>
                <a id="title-AEFVa6VMu1@OpenReview" class="title-link" href="/venue/AEFVa6VMu1@OpenReview" target="_blank">Approximation algorithms for combinatorial optimization with predictions</a>
                <a id="pdf-AEFVa6VMu1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('AEFVa6VMu1@OpenReview', this)" data="https://openreview.net/pdf?id=AEFVa6VMu1">[PDF<sup id="pdf-stars-AEFVa6VMu1@OpenReview">2</sup>]</a>
                <a id="copy-AEFVa6VMu1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('AEFVa6VMu1@OpenReview')">[Copy]</a>
                <a id="kimi-AEFVa6VMu1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('AEFVa6VMu1@OpenReview', this)">[Kimi<sup id="kimi-stars-AEFVa6VMu1@OpenReview">1</sup>]</a>
                <a id="rel-AEFVa6VMu1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('AEFVa6VMu1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-AEFVa6VMu1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Antonios Antoniadis" target="_blank">Antonios Antoniadis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marek Elias" target="_blank">Marek Elias</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Polak" target="_blank">Adam Polak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moritz Venzin" target="_blank">Moritz Venzin</a>
            </p>
            <p id="summary-AEFVa6VMu1@OpenReview" class="summary">We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a generic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, Minimum Weight Perfect Matching, Knapsack, and Maximum Clique. Our algorithms produce an optimal solution when provided with perfect predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; we demonstrate this on the Steiner Tree problem. We conclude with an empirical evaluation of our approach.</p>
            <p id="subjects-AEFVa6VMu1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-AEFVa6VMu1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-AEFVa6VMu1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-AEFVa6VMu1@OpenReview" onclick="foldPdfKimi('AEFVa6VMu1@OpenReview', this)" class="hr hr-fold">
        </div><div id="A6Y7AqlzLW@OpenReview" class="panel paper" keywords="prms,policy,base,pavs,orms,progress,provers,step,verifiers,process">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=A6Y7AqlzLW" target="_blank" title="185/373"><span class="index notranslate">#185</span></a>
                <a id="title-A6Y7AqlzLW@OpenReview" class="title-link" href="/venue/A6Y7AqlzLW@OpenReview" target="_blank">Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</a>
                <a id="pdf-A6Y7AqlzLW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('A6Y7AqlzLW@OpenReview', this)" data="https://openreview.net/pdf?id=A6Y7AqlzLW">[PDF<sup id="pdf-stars-A6Y7AqlzLW@OpenReview">8</sup>]</a>
                <a id="copy-A6Y7AqlzLW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('A6Y7AqlzLW@OpenReview')">[Copy]</a>
                <a id="kimi-A6Y7AqlzLW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('A6Y7AqlzLW@OpenReview', this)">[Kimi<sup id="kimi-stars-A6Y7AqlzLW@OpenReview">8</sup>]</a>
                <a id="rel-A6Y7AqlzLW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('A6Y7AqlzLW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-A6Y7AqlzLW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Amrith Setlur" target="_blank">Amrith Setlur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chirag Nagpal" target="_blank">Chirag Nagpal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Fisch" target="_blank">Adam Fisch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyang Geng" target="_blank">Xinyang Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Eisenstein" target="_blank">Jacob Eisenstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Agarwal" target="_blank">Rishabh Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alekh Agarwal" target="_blank">Alekh Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Berant" target="_blank">Jonathan Berant</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aviral Kumar" target="_blank">Aviral Kumar</a>
            </p>
            <p id="summary-A6Y7AqlzLW@OpenReview" class="summary">A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve a *base* policy via test-time search and reinforcement learning (RL), we ask: ``How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measure *progress*: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under a *prover* policy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot. Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by training **process advantage verifiers (PAVs)** to measure progress under such provers and show that compared to ORM, they are &gt;8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enable **one of the first results** showing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs.</p>
            <p id="subjects-A6Y7AqlzLW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-A6Y7AqlzLW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-A6Y7AqlzLW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-A6Y7AqlzLW@OpenReview" onclick="foldPdfKimi('A6Y7AqlzLW@OpenReview', this)" class="hr hr-fold">
        </div><div id="9YNyiCJE3k@OpenReview" class="panel paper" keywords="osdas,osda,chemistry,novo,directing,zeolites,evaluator,agent,organic,llms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9YNyiCJE3k" target="_blank" title="186/373"><span class="index notranslate">#186</span></a>
                <a id="title-9YNyiCJE3k@OpenReview" class="title-link" href="/venue/9YNyiCJE3k@OpenReview" target="_blank">OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents</a>
                <a id="pdf-9YNyiCJE3k@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9YNyiCJE3k@OpenReview', this)" data="https://openreview.net/pdf?id=9YNyiCJE3k">[PDF<sup id="pdf-stars-9YNyiCJE3k@OpenReview">5</sup>]</a>
                <a id="copy-9YNyiCJE3k@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9YNyiCJE3k@OpenReview')">[Copy]</a>
                <a id="kimi-9YNyiCJE3k@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9YNyiCJE3k@OpenReview', this)">[Kimi<sup id="kimi-stars-9YNyiCJE3k@OpenReview">12</sup>]</a>
                <a id="rel-9YNyiCJE3k@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9YNyiCJE3k@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9YNyiCJE3k@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaolin Hu" target="_blank">Zhaolin Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixiao Zhou" target="_blank">Yixiao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongan Wang" target="_blank">Zhongan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Li" target="_blank">Xin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weimin Yang" target="_blank">Weimin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hehe Fan" target="_blank">Hehe Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>
            </p>
            <p id="summary-9YNyiCJE3k@OpenReview" class="summary">Zeolites are crystalline porous materials that have been widely utilized in petrochemical industries as well as sustainable chemistry areas. Synthesis of zeolites often requires small molecules termed Organic Structure Directing Agents (OSDAs), which are critical in forming the porous structure. Molecule generation models can aid the design of OSDAs, but they are limited by single functionality and lack of interactivity. Meanwhile, large language models (LLMs) such as GPT-4, as general-purpose artificial intelligence systems, excel in instruction comprehension, logical reasoning, and interactive communication. However, LLMs lack in-depth chemistry knowledge and first-principle computation capabilities, resulting in uncontrollable outcomes even after fine-tuning. In this paper, we propose OSDA Agent, an interactive OSDA design framework that leverages LLMs as the brain, coupled with computational chemistry tools. The OSDA Agent consists of three main components: the Actor, responsible for generating potential OSDA structures; the Evaluator, which assesses and scores the generated OSDAs using computational chemistry tools; and the Self-reflector, which produces reflective summaries based on the Evaluator's feedback to refine the Actor's subsequent outputs. Experiments on representative zeolite frameworks show the generation-evaluation-reflection-refinement workflow can perform de novo design of OSDAs with superior generation quality than the pure LLM model, generating candidates consistent with experimentally validated OSDAs and optimizing known OSDAs. The code and model will be publicly available.</p>
            <p id="subjects-9YNyiCJE3k@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9YNyiCJE3k@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9YNyiCJE3k@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9YNyiCJE3k@OpenReview" onclick="foldPdfKimi('9YNyiCJE3k@OpenReview', this)" class="hr hr-fold">
        </div><div id="9UGfOJBuL8@OpenReview" class="panel paper" keywords="disease,longitudinal,neurodegenerative,ordinal,progression,generation,data,cohort,conditional,regression">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9UGfOJBuL8" target="_blank" title="187/373"><span class="index notranslate">#187</span></a>
                <a id="title-9UGfOJBuL8@OpenReview" class="title-link" href="/venue/9UGfOJBuL8@OpenReview" target="_blank">Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies</a>
                <a id="pdf-9UGfOJBuL8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9UGfOJBuL8@OpenReview', this)" data="https://openreview.net/pdf?id=9UGfOJBuL8">[PDF<sup id="pdf-stars-9UGfOJBuL8@OpenReview">5</sup>]</a>
                <a id="copy-9UGfOJBuL8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9UGfOJBuL8@OpenReview')">[Copy]</a>
                <a id="kimi-9UGfOJBuL8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9UGfOJBuL8@OpenReview', this)">[Kimi<sup id="kimi-stars-9UGfOJBuL8@OpenReview">2</sup>]</a>
                <a id="rel-9UGfOJBuL8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9UGfOJBuL8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9UGfOJBuL8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hyuna Cho" target="_blank">Hyuna Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziquan Wei" target="_blank">Ziquan Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungjoo Lee" target="_blank">Seungjoo Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tingting Dan" target="_blank">Tingting Dan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guorong Wu" target="_blank">Guorong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Won Hwa Kim" target="_blank">Won Hwa Kim</a>
            </p>
            <p id="summary-9UGfOJBuL8@OpenReview" class="summary">Modeling the progression of neurodegenerative diseases such as Alzheimer’s disease (AD) is crucial for early detection and prevention given their irreversible nature. However, the scarcity of longitudinal data and complex disease dynamics make the analysis highly challenging. Moreover, longitudinal samples often contain irregular and large intervals between subject visits, which underscore the necessity for advanced data generation techniques that can accurately simulate disease progression over time. In this regime, we propose a novel conditional generative model for synthesizing longitudinal sequences and present its application to neurodegenerative disease data generation conditioned on multiple time-dependent ordinal factors, such as age and disease severity. Our method sequentially generates continuous data by bridging gaps between sparse data points with a diffusion model, ensuring a realistic representation of disease progression. The synthetic data are curated to integrate both cohort-level and individual-specific characteristics, where the cohort-level representations are modeled with an ordinal regression to capture longitudinally monotonic behavior. Extensive experiments on four AD biomarkers validate the superiority of our method over nine baseline approaches, highlighting its potential to be applied to a variety of longitudinal data generation.</p>
            <p id="subjects-9UGfOJBuL8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9UGfOJBuL8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9UGfOJBuL8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9UGfOJBuL8@OpenReview" onclick="foldPdfKimi('9UGfOJBuL8@OpenReview', this)" class="hr hr-fold">
        </div><div id="9NfHbWKqMF@OpenReview" class="panel paper" keywords="3dgs,views,splatformer,ood,splatting,rendering,test,gaussian,transformer,splats">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9NfHbWKqMF" target="_blank" title="188/373"><span class="index notranslate">#188</span></a>
                <a id="title-9NfHbWKqMF@OpenReview" class="title-link" href="/venue/9NfHbWKqMF@OpenReview" target="_blank">SplatFormer: Point Transformer for Robust 3D Gaussian Splatting</a>
                <a id="pdf-9NfHbWKqMF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9NfHbWKqMF@OpenReview', this)" data="https://openreview.net/pdf?id=9NfHbWKqMF">[PDF<sup id="pdf-stars-9NfHbWKqMF@OpenReview">5</sup>]</a>
                <a id="copy-9NfHbWKqMF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9NfHbWKqMF@OpenReview')">[Copy]</a>
                <a id="kimi-9NfHbWKqMF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9NfHbWKqMF@OpenReview', this)">[Kimi<sup id="kimi-stars-9NfHbWKqMF@OpenReview">1</sup>]</a>
                <a id="rel-9NfHbWKqMF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9NfHbWKqMF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9NfHbWKqMF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yutong Chen" target="_blank">Yutong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marko Mihajlovic" target="_blank">Marko Mihajlovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Prokudin" target="_blank">Sergey Prokudin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiyi Chen" target="_blank">Xiyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Wang" target="_blank">Yiming Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Tang" target="_blank">Siyu Tang</a>
            </p>
            <p id="summary-9NfHbWKqMF@OpenReview" class="summary">3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks. Code and data will be made public.</p>
            <p id="subjects-9NfHbWKqMF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9NfHbWKqMF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9NfHbWKqMF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9NfHbWKqMF@OpenReview" onclick="foldPdfKimi('9NfHbWKqMF@OpenReview', this)" class="hr hr-fold">
        </div><div id="8xxEBAtD7y@OpenReview" class="panel paper" keywords="trained,internals,stander,group,chughtai,networks,operation,explanations,accuracy,towards">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8xxEBAtD7y" target="_blank" title="189/373"><span class="index notranslate">#189</span></a>
                <a id="title-8xxEBAtD7y@OpenReview" class="title-link" href="/venue/8xxEBAtD7y@OpenReview" target="_blank">Towards a Unified and Verified Understanding of Group-Operation Networks</a>
                <a id="pdf-8xxEBAtD7y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8xxEBAtD7y@OpenReview', this)" data="https://openreview.net/pdf?id=8xxEBAtD7y">[PDF<sup id="pdf-stars-8xxEBAtD7y@OpenReview">1</sup>]</a>
                <a id="copy-8xxEBAtD7y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8xxEBAtD7y@OpenReview')">[Copy]</a>
                <a id="kimi-8xxEBAtD7y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8xxEBAtD7y@OpenReview', this)">[Kimi<sup id="kimi-stars-8xxEBAtD7y@OpenReview">1</sup>]</a>
                <a id="rel-8xxEBAtD7y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8xxEBAtD7y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8xxEBAtD7y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wilson Wu" target="_blank">Wilson Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Louis Jaburi" target="_blank">Louis Jaburi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=jacob drori" target="_blank">jacob drori</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Gross" target="_blank">Jason Gross</a>
            </p>
            <p id="summary-8xxEBAtD7y@OpenReview" class="summary">A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models in a step towards unifying the explanations of previous works (Chughtai et al., 2023; Stander et al., 2024). Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of the extent to which we faithfully and concisely explain model internals. In the main text, we focus on the symmetric group S5. For models trained on this group, our explanation yields a guarantee of model accuracy that runs 3x faster than brute force and gives a &gt;=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works.</p>
            <p id="subjects-8xxEBAtD7y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-8xxEBAtD7y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8xxEBAtD7y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8xxEBAtD7y@OpenReview" onclick="foldPdfKimi('8xxEBAtD7y@OpenReview', this)" class="hr hr-fold">
        </div><div id="8bjspmAMBk@OpenReview" class="panel paper" keywords="dynamic,graphs,generative,graph,metrics,measures,metric,models,quality,topology">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8bjspmAMBk" target="_blank" title="190/373"><span class="index notranslate">#190</span></a>
                <a id="title-8bjspmAMBk@OpenReview" class="title-link" href="/venue/8bjspmAMBk@OpenReview" target="_blank">Quality Measures for Dynamic Graph Generative Models</a>
                <a id="pdf-8bjspmAMBk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8bjspmAMBk@OpenReview', this)" data="https://openreview.net/pdf?id=8bjspmAMBk">[PDF<sup id="pdf-stars-8bjspmAMBk@OpenReview">2</sup>]</a>
                <a id="copy-8bjspmAMBk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8bjspmAMBk@OpenReview')">[Copy]</a>
                <a id="kimi-8bjspmAMBk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8bjspmAMBk@OpenReview', this)">[Kimi<sup id="kimi-stars-8bjspmAMBk@OpenReview">2</sup>]</a>
                <a id="rel-8bjspmAMBk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8bjspmAMBk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8bjspmAMBk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ryien Hosseini" target="_blank">Ryien Hosseini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Filippo Simini" target="_blank">Filippo Simini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Venkatram Vishwanath" target="_blank">Venkatram Vishwanath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rebecca Willett" target="_blank">Rebecca Willett</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Henry Hoffmann" target="_blank">Henry Hoffmann</a>
            </p>
            <p id="summary-8bjspmAMBk@OpenReview" class="summary">Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and language domains, evaluating generative models for dynamic graphs is challenging due to the difficulty of visualizing their output, making quantitative metrics essential. In this work, we develop a new quality metric specifically for evaluating generative models of dynamic graphs. Current metrics for dynamic graphs typically involve discretizing the continuous-evolution of graphs into static snapshots and then applying conventional graph similarity measures. This approach has several limitations: (a) it models temporally related events as i.i.d. samples, failing to capture the non-uniform evolution of dynamic graphs; (b) it lacks a unified measure that is sensitive to both features and topology; (c) it fails to provide a scalar metric, requiring multiple metrics without clear superiority; and (d) it requires explicitly instantiating each static snapshot, leading to impractical runtime demands that hinder evaluation at scale. We propose a novel metric based on the Johnson-Lindenstrauss lemma, applying random projections directly to dynamic graph data. This results in an expressive, scalar, and application-agnostic measure of dynamic graph similarity that overcomes the limitations of traditional methods. We also provide a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of our approach compared to existing methods.</p>
            <p id="subjects-8bjspmAMBk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-8bjspmAMBk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8bjspmAMBk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8bjspmAMBk@OpenReview" onclick="foldPdfKimi('8bjspmAMBk@OpenReview', this)" class="hr hr-fold">
        </div><div id="7XgKAabsPp@OpenReview" class="panel paper" keywords="moe,experts,continual,tasks,forgetting,catastrophic,gating,rounds,mixture,expert">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7XgKAabsPp" target="_blank" title="191/373"><span class="index notranslate">#191</span></a>
                <a id="title-7XgKAabsPp@OpenReview" class="title-link" href="/venue/7XgKAabsPp@OpenReview" target="_blank">Theory on Mixture-of-Experts in Continual Learning</a>
                <a id="pdf-7XgKAabsPp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7XgKAabsPp@OpenReview', this)" data="https://openreview.net/pdf?id=7XgKAabsPp">[PDF<sup id="pdf-stars-7XgKAabsPp@OpenReview">11</sup>]</a>
                <a id="copy-7XgKAabsPp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7XgKAabsPp@OpenReview')">[Copy]</a>
                <a id="kimi-7XgKAabsPp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7XgKAabsPp@OpenReview', this)">[Kimi<sup id="kimi-stars-7XgKAabsPp@OpenReview">9</sup>]</a>
                <a id="rel-7XgKAabsPp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7XgKAabsPp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7XgKAabsPp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongbo Li" target="_blank">Hongbo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sen Lin" target="_blank">Sen Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingjie Duan" target="_blank">Lingjie Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingbin Liang" target="_blank">Yingbin Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ness Shroff" target="_blank">Ness Shroff</a>
            </p>
            <p id="summary-7XgKAabsPp@OpenReview" class="summary">Continual learning (CL) has garnered significant attention because of its ability to adapt to new tasks that arrive over time. Catastrophic forgetting (of old tasks) has been identified as a major issue in CL, as the model adapts to new tasks. The Mixture-of-Experts (MoE) model has recently been shown to effectively mitigate catastrophic forgetting in CL, by employing a gating network to sparsify and distribute diverse tasks among multiple experts. However, there is a lack of theoretical analysis of MoE and its impact on the learning performance in CL. This paper provides the first theoretical results to characterize the impact of MoE in CL via the lens of overparameterized linear regression tasks. We establish the benefit of MoE over a single expert by proving that the MoE model can diversify its experts to specialize in different tasks, while its router learns to select the right expert for each task and balance the loads across all experts. Our study further suggests an intriguing fact that the MoE in CL needs to terminate the update of the gating network after sufficient training rounds to attain system convergence, which is not needed in the existing MoE studies that do not consider the continual task arrival. Furthermore, we provide explicit expressions for the expected forgetting and overall generalization error to characterize the benefit of MoE in the learning performance in CL. Interestingly, adding more experts requires additional rounds before convergence, which may not enhance the learning performance. Finally, we conduct experiments on both synthetic and real datasets to extend these insights from linear models to deep neural networks (DNNs), which also shed light on the practical algorithm design for MoE in CL.</p>
            <p id="subjects-7XgKAabsPp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-7XgKAabsPp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7XgKAabsPp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7XgKAabsPp@OpenReview" onclick="foldPdfKimi('7XgKAabsPp@OpenReview', this)" class="hr hr-fold">
        </div><div id="7IzeL0kflu@OpenReview" class="panel paper" keywords="replay,buffer,pqn,policy,ppo,craftax,learning,smax,reestablishes,target">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7IzeL0kflu" target="_blank" title="192/373"><span class="index notranslate">#192</span></a>
                <a id="title-7IzeL0kflu@OpenReview" class="title-link" href="/venue/7IzeL0kflu@OpenReview" target="_blank">Simplifying Deep Temporal Difference Learning</a>
                <a id="pdf-7IzeL0kflu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7IzeL0kflu@OpenReview', this)" data="https://openreview.net/pdf?id=7IzeL0kflu">[PDF<sup id="pdf-stars-7IzeL0kflu@OpenReview">6</sup>]</a>
                <a id="copy-7IzeL0kflu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7IzeL0kflu@OpenReview')">[Copy]</a>
                <a id="kimi-7IzeL0kflu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7IzeL0kflu@OpenReview', this)">[Kimi<sup id="kimi-stars-7IzeL0kflu@OpenReview">3</sup>]</a>
                <a id="rel-7IzeL0kflu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7IzeL0kflu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7IzeL0kflu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Gallici" target="_blank">Matteo Gallici</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mattie Fellows" target="_blank">Mattie Fellows</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Ellis" target="_blank">Benjamin Ellis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bartomeu Pou" target="_blank">Bartomeu Pou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivan Masmitja" target="_blank">Ivan Masmitja</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Foerster" target="_blank">Jakob Foerster</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mario Martin" target="_blank">Mario Martin</a>
            </p>
            <p id="summary-7IzeL0kflu@OpenReview" class="summary"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-125-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-764" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-765"><span class="mi" id="MathJax-Span-766" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-125">Q</script>-learning played a foundational role in the field reinforcement learning (RL).However, TD algorithms with off-policy data, such as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-126-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-767" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-768"><span class="mi" id="MathJax-Span-769" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-126">Q</script>-learning, or nonlinear function approximation like deep neural networks require several additional tricks to stabilise training, primarily a large replay buffer and target networks. Unfortunately, the delayed updating of frozen network parameters in the target network harms the sample efficiency and, similarly, the large replay buffer introduces memory and implementation overheads. In this paper, we investigate whether it is possible to accelerate and simplify off-policy TD training while maintaining its stability. Our key theoretical result demonstrates for the first time that regularisation techniques such as LayerNorm can yield provably convergent TD algorithms without the need for a target network or replay buffer, even with off-policy data. Empirically, we find that online, parallelised sampling enabled by vectorised environments stabilises training without the need for a large replay buffer. Motivated by these findings, we propose PQN, our simplified deep online <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-127-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-770" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-771"><span class="mi" id="MathJax-Span-772" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-127">Q</script>-Learning algorithm.Surprisingly, this simple algorithm is competitive with more complex methods like: Rainbow in Atari, PPO-RNN in Craftax, QMix in Smax, and can be up to 50x faster than traditional DQN without sacrificing sample efficiency. In an era where PPO has become the go-to RL algorithm, PQN reestablishes off-policy <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-128-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-773" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-774"><span class="mi" id="MathJax-Span-775" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-128">Q</script>-learning as a viable alternative.</p>
            <p id="subjects-7IzeL0kflu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-7IzeL0kflu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7IzeL0kflu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7IzeL0kflu@OpenReview" onclick="foldPdfKimi('7IzeL0kflu@OpenReview', this)" class="hr hr-fold">
        </div><div id="7ANDviElAo@OpenReview" class="panel paper" keywords="mog,sparsification,graph,experts,mixture,gnn,node,graphs,sparsity,pruning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7ANDviElAo" target="_blank" title="193/373"><span class="index notranslate">#193</span></a>
                <a id="title-7ANDviElAo@OpenReview" class="title-link" href="/venue/7ANDviElAo@OpenReview" target="_blank">Graph Sparsification via Mixture of Graphs</a>
                <a id="pdf-7ANDviElAo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7ANDviElAo@OpenReview', this)" data="https://openreview.net/pdf?id=7ANDviElAo">[PDF<sup id="pdf-stars-7ANDviElAo@OpenReview">6</sup>]</a>
                <a id="copy-7ANDviElAo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7ANDviElAo@OpenReview')">[Copy]</a>
                <a id="kimi-7ANDviElAo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7ANDviElAo@OpenReview', this)">[Kimi<sup id="kimi-stars-7ANDviElAo@OpenReview">4</sup>]</a>
                <a id="rel-7ANDviElAo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7ANDviElAo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7ANDviElAo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guibin Zhang" target="_blank">Guibin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangguo SUN" target="_blank">Xiangguo SUN</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Yue" target="_blank">Yanwei Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chonghe Jiang" target="_blank">Chonghe Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Wang" target="_blank">Kun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianlong Chen" target="_blank">Tianlong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shirui Pan" target="_blank">Shirui Pan</a>
            </p>
            <p id="summary-7ANDviElAo@OpenReview" class="summary">Graph Neural Networks (GNNs) have demonstrated superior performance across various graph learning tasks but face significant computational challenges when applied to large-scale graphs. One effective approach to mitigate these challenges is graph sparsification, which involves removing non-essential edges to reduce computational overhead. However, previous graph sparsification methods often rely on a single global sparsity setting and uniform pruning criteria, failing to provide customized sparsification schemes for each node's complex local context.In this paper, we introduce Mixture-of-Graphs (MoG), leveraging the concept of Mixture-of-Experts (MoE), to dynamically select tailored pruning solutions for each node. Specifically, MoG incorporates multiple sparsifier experts, each characterized by unique sparsity levels and pruning criteria, and selects the appropriate experts for each node. Subsequently, MoG performs a mixture of the sparse graphs produced by different experts on the Grassmann manifold to derive an optimal sparse graph. One notable property of MoG is its entirely local nature, as it depends on the specific circumstances of each individual node. Extensive experiments on four large-scale OGB datasets and two superpixel datasets, equipped with five GNN backbones, demonstrate that MoG (I) identifies subgraphs at higher sparsity levels (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-129-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;8.67&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-776" style="width: 100%; display: inline-block; min-width: 2.138em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1001.77em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-777"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1001.77em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-778" style="font-family: MathJax_Main;">8.67</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-779" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>8.67</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-129">8.67\\%\sim 50.85\\%</script>), with performance equal to or better than the dense graph, (II) achieves <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-130-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.47&lt;/mn&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2.62&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-780" style="width: 6.669em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1005.37em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-781"><span class="mn" id="MathJax-Span-782" style="font-family: MathJax_Main;">1.47</span><span class="mo" id="MathJax-Span-783" style="font-family: MathJax_Main; padding-left: 0.211em;">−</span><span class="mn" id="MathJax-Span-784" style="font-family: MathJax_Main; padding-left: 0.211em;">2.62</span><span class="mo" id="MathJax-Span-785" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.47</mn><mo>−</mo><mn>2.62</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-130">1.47-2.62\times</script> speedup in GNN inference with negligible performance drop, and (III) boosts ``top-student'' GNN performance (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-131-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.02&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-786" style="width: 100%; display: inline-block; min-width: 2.138em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1001.72em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-787"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1001.72em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-788" style="font-family: MathJax_Main;">1.02</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-789" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.02</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-131">1.02\\%\uparrow</script> on RevGNN+\textsc{ogbn-proteins} and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-132-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.74&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-790" style="width: 100%; display: inline-block; min-width: 2.138em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1001.77em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-791"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1001.77em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-792" style="font-family: MathJax_Main;">1.74</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-793" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.74</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-132">1.74\\%\\uparrow</script> on DeeperGCN+\textsc{ogbg-ppa}).</p>
            <p id="subjects-7ANDviElAo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-7ANDviElAo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7ANDviElAo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7ANDviElAo@OpenReview" onclick="foldPdfKimi('7ANDviElAo@OpenReview', this)" class="hr hr-fold">
        </div><div id="78Nn4QJTEN@OpenReview" class="panel paper" keywords="sink,attention,lms,sinks,emerges,softmax,optimization,emerge,scores,normalization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=78Nn4QJTEN" target="_blank" title="194/373"><span class="index notranslate">#194</span></a>
                <a id="title-78Nn4QJTEN@OpenReview" class="title-link" href="/venue/78Nn4QJTEN@OpenReview" target="_blank">When Attention Sink Emerges in Language Models: An Empirical View</a>
                <a id="pdf-78Nn4QJTEN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('78Nn4QJTEN@OpenReview', this)" data="https://openreview.net/pdf?id=78Nn4QJTEN">[PDF<sup id="pdf-stars-78Nn4QJTEN@OpenReview">9</sup>]</a>
                <a id="copy-78Nn4QJTEN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('78Nn4QJTEN@OpenReview')">[Copy]</a>
                <a id="kimi-78Nn4QJTEN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('78Nn4QJTEN@OpenReview', this)">[Kimi<sup id="kimi-stars-78Nn4QJTEN@OpenReview">6</sup>]</a>
                <a id="rel-78Nn4QJTEN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('78Nn4QJTEN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-78Nn4QJTEN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangming Gu" target="_blank">Xiangming Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Pang" target="_blank">Tianyu Pang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Du" target="_blank">Chao Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Liu" target="_blank">Qian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengzhuo Zhang" target="_blank">Fengzhuo Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cunxiao Du" target="_blank">Cunxiao Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Wang" target="_blank">Ye Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Lin" target="_blank">Min Lin</a>
            </p>
            <p id="summary-78Nn4QJTEN@OpenReview" class="summary">Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as **attention sink**. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how *optimization*, *data distribution*, *loss function*, and *model architecture* in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, *storing extra attention scores*, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.</p>
            <p id="subjects-78Nn4QJTEN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-78Nn4QJTEN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-78Nn4QJTEN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-78Nn4QJTEN@OpenReview" onclick="foldPdfKimi('78Nn4QJTEN@OpenReview', this)" class="hr hr-fold">
        </div><div id="69Fp4dcmJN@OpenReview" class="panel paper" keywords="bandmf,factorization,banded,noise,training,iterations,scale,differentially,matrix,mechanism">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=69Fp4dcmJN" target="_blank" title="195/373"><span class="index notranslate">#195</span></a>
                <a id="title-69Fp4dcmJN@OpenReview" class="title-link" href="/venue/69Fp4dcmJN@OpenReview" target="_blank">Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML</a>
                <a id="pdf-69Fp4dcmJN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('69Fp4dcmJN@OpenReview', this)" data="https://openreview.net/pdf?id=69Fp4dcmJN">[PDF<sup id="pdf-stars-69Fp4dcmJN@OpenReview">4</sup>]</a>
                <a id="copy-69Fp4dcmJN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('69Fp4dcmJN@OpenReview')">[Copy]</a>
                <a id="kimi-69Fp4dcmJN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('69Fp4dcmJN@OpenReview', this)">[Kimi<sup id="kimi-stars-69Fp4dcmJN@OpenReview">2</sup>]</a>
                <a id="rel-69Fp4dcmJN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('69Fp4dcmJN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-69Fp4dcmJN@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan McKenna" target="_blank">Ryan McKenna</a>
            </p>
            <p id="summary-69Fp4dcmJN@OpenReview" class="summary">Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF , which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may be more than <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-133-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-794" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-795"><span class="msubsup" id="MathJax-Span-796"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-797" style="font-family: MathJax_Main;">10</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.992em;"><span class="mn" id="MathJax-Span-798" style="font-size: 70.7%; font-family: MathJax_Main;">4</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mn>4</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-133">10^4</script> and the number of model parameters may exceed <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-134-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mn&gt;7&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-799" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-800"><span class="msubsup" id="MathJax-Span-801"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-802" style="font-family: MathJax_Main;">10</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.992em;"><span class="mn" id="MathJax-Span-803" style="font-size: 70.7%; font-family: MathJax_Main;">7</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mn>7</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-134">10^7</script>. In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to effectively handle settings with over <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-135-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-804" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-805"><span class="msubsup" id="MathJax-Span-806"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-807" style="font-family: MathJax_Main;">10</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.992em;"><span class="mn" id="MathJax-Span-808" style="font-size: 70.7%; font-family: MathJax_Main;">6</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mn>6</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-135">10^6</script> training iterations and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-136-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-809" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-810"><span class="msubsup" id="MathJax-Span-811"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-812" style="font-family: MathJax_Main;">10</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.992em;"><span class="mn" id="MathJax-Span-813" style="font-size: 70.7%; font-family: MathJax_Main;">9</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mn>9</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-136">10^9</script> model parameters, with no utility degradation at smaller scales.</p>
            <p id="subjects-69Fp4dcmJN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-69Fp4dcmJN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-69Fp4dcmJN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-69Fp4dcmJN@OpenReview" onclick="foldPdfKimi('69Fp4dcmJN@OpenReview', this)" class="hr hr-fold">
        </div><div id="5FXKgOxmb2@OpenReview" class="panel paper" keywords="scaffolds,motif,molecules,magnet,motifs,substructures,expressivity,assignments,compounds,limitation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5FXKgOxmb2" target="_blank" title="196/373"><span class="index notranslate">#196</span></a>
                <a id="title-5FXKgOxmb2@OpenReview" class="title-link" href="/venue/5FXKgOxmb2@OpenReview" target="_blank">MAGNet: Motif-Agnostic Generation of Molecules from Scaffolds</a>
                <a id="pdf-5FXKgOxmb2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5FXKgOxmb2@OpenReview', this)" data="https://openreview.net/pdf?id=5FXKgOxmb2">[PDF<sup id="pdf-stars-5FXKgOxmb2@OpenReview">5</sup>]</a>
                <a id="copy-5FXKgOxmb2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5FXKgOxmb2@OpenReview')">[Copy]</a>
                <a id="kimi-5FXKgOxmb2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5FXKgOxmb2@OpenReview', this)">[Kimi<sup id="kimi-stars-5FXKgOxmb2@OpenReview">1</sup>]</a>
                <a id="rel-5FXKgOxmb2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5FXKgOxmb2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5FXKgOxmb2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Leon Hetzel" target="_blank">Leon Hetzel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johanna Sommer" target="_blank">Johanna Sommer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bastian Rieck" target="_blank">Bastian Rieck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Theis" target="_blank">Fabian Theis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Günnemann" target="_blank">Stephan Günnemann</a>
            </p>
            <p id="summary-5FXKgOxmb2@OpenReview" class="summary">Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions.Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods fail to represent substructures beyond their known motif set, posing a fundamental limitation for discovering novel compounds.To address this limitation and enhance structural expressivity, we propose to separate structure from features by abstracting motifs to scaffolds and, subsequently, allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that considers the entire molecular context and facilitates learning adequate assignments of atoms and bonds to scaffolds. Complementary to this, we propose MAGNet, the first model to freely learn motifs. Importantly, we demonstrate that MAGNet's improved expressivity leads to molecules with more structural diversity and, at the same time, diverse atom and bond assignments.</p>
            <p id="subjects-5FXKgOxmb2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5FXKgOxmb2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5FXKgOxmb2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5FXKgOxmb2@OpenReview" onclick="foldPdfKimi('5FXKgOxmb2@OpenReview', this)" class="hr hr-fold">
        </div><div id="5BjQOUXq7i@OpenReview" class="panel paper" keywords="regmix,mixture,mixtures,regression,performance,model,train,data,doremi,tokens">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5BjQOUXq7i" target="_blank" title="197/373"><span class="index notranslate">#197</span></a>
                <a id="title-5BjQOUXq7i@OpenReview" class="title-link" href="/venue/5BjQOUXq7i@OpenReview" target="_blank">RegMix: Data Mixture as Regression for Language Model Pre-training</a>
                <a id="pdf-5BjQOUXq7i@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5BjQOUXq7i@OpenReview', this)" data="https://openreview.net/pdf?id=5BjQOUXq7i">[PDF<sup id="pdf-stars-5BjQOUXq7i@OpenReview">5</sup>]</a>
                <a id="copy-5BjQOUXq7i@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5BjQOUXq7i@OpenReview')">[Copy]</a>
                <a id="kimi-5BjQOUXq7i@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5BjQOUXq7i@OpenReview', this)">[Kimi<sup id="kimi-stars-5BjQOUXq7i@OpenReview">6</sup>]</a>
                <a id="rel-5BjQOUXq7i@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5BjQOUXq7i@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5BjQOUXq7i@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Liu" target="_blank">Qian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaosen Zheng" target="_blank">Xiaosen Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Muennighoff" target="_blank">Niklas Muennighoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangtao Zeng" target="_blank">Guangtao Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longxu Dou" target="_blank">Longxu Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Pang" target="_blank">Tianyu Pang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Jiang" target="_blank">Jing Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Lin" target="_blank">Min Lin</a>
            </p>
            <p id="summary-5BjQOUXq7i@OpenReview" class="summary">The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method outperforms both human selection and DoReMi in terms of both validation loss and downstream performance. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed.</p>
            <p id="subjects-5BjQOUXq7i@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5BjQOUXq7i@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5BjQOUXq7i@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5BjQOUXq7i@OpenReview" onclick="foldPdfKimi('5BjQOUXq7i@OpenReview', this)" class="hr hr-fold">
        </div><div id="5BSlakturs@OpenReview" class="panel paper" keywords="compositional,pixart,reliable,seeds,image,text,prompts,composition,inconsistencies,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5BSlakturs" target="_blank" title="198/373"><span class="index notranslate">#198</span></a>
                <a id="title-5BSlakturs@OpenReview" class="title-link" href="/venue/5BSlakturs@OpenReview" target="_blank">Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds</a>
                <a id="pdf-5BSlakturs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5BSlakturs@OpenReview', this)" data="https://openreview.net/pdf?id=5BSlakturs">[PDF<sup id="pdf-stars-5BSlakturs@OpenReview">5</sup>]</a>
                <a id="copy-5BSlakturs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5BSlakturs@OpenReview')">[Copy]</a>
                <a id="kimi-5BSlakturs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5BSlakturs@OpenReview', this)">[Kimi<sup id="kimi-stars-5BSlakturs@OpenReview">6</sup>]</a>
                <a id="rel-5BSlakturs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5BSlakturs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5BSlakturs@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuangqi Li" target="_blank">Shuangqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hieu Le" target="_blank">Hieu Le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Xu" target="_blank">Jingyi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mathieu Salzmann" target="_blank">Mathieu Salzmann</a>
            </p>
            <p id="summary-5BSlakturs@OpenReview" class="summary">Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as "two dogs" or "a penguin on the right of a bowl". Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model's compositional ability, we propose a method for mining these reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. By fine-tuning text-to-image models on these generated images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3\% and 19.5\% for Stable Diffusion and PixArt-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-137-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-814" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-815"><span class="mi" id="MathJax-Span-816" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-137">\alpha</script>, respectively. Spatial composition sees even larger gains, with 60.7\% for Stable Diffusion and 21.1\% for PixArt-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-138-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-817" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-818"><span class="mi" id="MathJax-Span-819" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-138">\alpha</script>.</p>
            <p id="subjects-5BSlakturs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5BSlakturs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5BSlakturs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5BSlakturs@OpenReview" onclick="foldPdfKimi('5BSlakturs@OpenReview', this)" class="hr hr-fold">
        </div><div id="4gaySj8kvX@OpenReview" class="panel paper" keywords="jaxgcrl,gcrl,2316,readme,self,4open,reinforcement,supervised,anonymous,conditioned">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4gaySj8kvX" target="_blank" title="199/373"><span class="index notranslate">#199</span></a>
                <a id="title-4gaySj8kvX@OpenReview" class="title-link" href="/venue/4gaySj8kvX@OpenReview" target="_blank">Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research</a>
                <a id="pdf-4gaySj8kvX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4gaySj8kvX@OpenReview', this)" data="https://openreview.net/pdf?id=4gaySj8kvX">[PDF<sup id="pdf-stars-4gaySj8kvX@OpenReview">6</sup>]</a>
                <a id="copy-4gaySj8kvX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4gaySj8kvX@OpenReview')">[Copy]</a>
                <a id="kimi-4gaySj8kvX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4gaySj8kvX@OpenReview', this)">[Kimi<sup id="kimi-stars-4gaySj8kvX@OpenReview">4</sup>]</a>
                <a id="rel-4gaySj8kvX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4gaySj8kvX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4gaySj8kvX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michał Bortkiewicz" target="_blank">Michał Bortkiewicz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Władysław Pałucki" target="_blank">Władysław Pałucki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vivek Myers" target="_blank">Vivek Myers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tadeusz Dziarmaga" target="_blank">Tadeusz Dziarmaga</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomasz Arczewski" target="_blank">Tomasz Arczewski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Łukasz Kuciński" target="_blank">Łukasz Kuciński</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Eysenbach" target="_blank">Benjamin Eysenbach</a>
            </p>
            <p id="summary-4gaySj8kvX@OpenReview" class="summary">Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover *new* behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (`JaxGCRL`) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-139-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-820" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-821"><span class="mn" id="MathJax-Span-822" style="font-family: MathJax_Main;">22</span><span class="mo" id="MathJax-Span-823" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>22</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-139">22\times</script>. Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Code: [https://anonymous.4open.science/r/JaxGCRL-2316/README.md](https://anonymous.4open.science/r/JaxGCRL-2316/README.md)</p>
            <p id="subjects-4gaySj8kvX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4gaySj8kvX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4gaySj8kvX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4gaySj8kvX@OpenReview" onclick="foldPdfKimi('4gaySj8kvX@OpenReview', this)" class="hr hr-fold">
        </div><div id="4NTrco82W0@OpenReview" class="panel paper" keywords="regression,linex,losses,training,flow,exploitation,gflownets,loss,backward,generative">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4NTrco82W0" target="_blank" title="200/373"><span class="index notranslate">#200</span></a>
                <a id="title-4NTrco82W0@OpenReview" class="title-link" href="/venue/4NTrco82W0@OpenReview" target="_blank">Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks</a>
                <a id="pdf-4NTrco82W0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4NTrco82W0@OpenReview', this)" data="https://openreview.net/pdf?id=4NTrco82W0">[PDF<sup id="pdf-stars-4NTrco82W0@OpenReview">3</sup>]</a>
                <a id="copy-4NTrco82W0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4NTrco82W0@OpenReview')">[Copy]</a>
                <a id="kimi-4NTrco82W0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4NTrco82W0@OpenReview', this)">[Kimi<sup id="kimi-stars-4NTrco82W0@OpenReview">3</sup>]</a>
                <a id="rel-4NTrco82W0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4NTrco82W0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4NTrco82W0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Hu" target="_blank">Rui Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Zhang" target="_blank">Yifan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoran Li" target="_blank">Zhuoran Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longbo Huang" target="_blank">Longbo Huang</a>
            </p>
            <p id="summary-4NTrco82W0@OpenReview" class="summary">Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the forward flow to the backward flow on sampled training objects. Prior work focused on the choice of training objects, parameterizations, sampling and resampling strategies, and backward policies, aiming to enhance credit assignment, exploration, or exploitation of the training process. However, the choice of regression loss, which can highly influence the exploration and exploitation behavior of the under-training policy, has been overlooked. Due to the lack of theoretical understanding for choosing an appropriate regression loss, most existing algorithms train the flow network by minimizing the squared error of the forward and backward flows in log-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct regression losses correspond to specific divergence measures, enabling us to design and analyze regression losses according to the desired properties of the corresponding divergence measures. Specifically, we examine two key properties: zero-forcing and zero-avoiding, where the former promotes exploitation and higher rewards, and the latter encourages exploration and enhances diversity. Based on our theoretical framework, we propose three novel regression losses, namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our proposed losses are compatible with most existing training algorithms, and significantly improve the performances of the algorithms concerning convergence speed, sample diversity, and robustness.</p>
            <p id="subjects-4NTrco82W0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4NTrco82W0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4NTrco82W0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4NTrco82W0@OpenReview" onclick="foldPdfKimi('4NTrco82W0@OpenReview', this)" class="hr hr-fold">
        </div><div id="4HRRcqE9SU@OpenReview" class="panel paper" keywords="deflection,sdf,normal,reconstruction,rendering,geometric,priors,accuracy,indoor,surfaces">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4HRRcqE9SU" target="_blank" title="201/373"><span class="index notranslate">#201</span></a>
                <a id="title-4HRRcqE9SU@OpenReview" class="title-link" href="/venue/4HRRcqE9SU@OpenReview" target="_blank">ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction</a>
                <a id="pdf-4HRRcqE9SU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4HRRcqE9SU@OpenReview', this)" data="https://openreview.net/pdf?id=4HRRcqE9SU">[PDF<sup id="pdf-stars-4HRRcqE9SU@OpenReview">3</sup>]</a>
                <a id="copy-4HRRcqE9SU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4HRRcqE9SU@OpenReview')">[Copy]</a>
                <a id="kimi-4HRRcqE9SU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4HRRcqE9SU@OpenReview', this)">[Kimi<sup id="kimi-stars-4HRRcqE9SU@OpenReview">2</sup>]</a>
                <a id="rel-4HRRcqE9SU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4HRRcqE9SU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4HRRcqE9SU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyu Tang" target="_blank">Ziyu Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weicai Ye" target="_blank">Weicai Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Wang" target="_blank">Yifan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Huang" target="_blank">Di Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hujun Bao" target="_blank">Hujun Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong He" target="_blank">Tong He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guofeng Zhang" target="_blank">Guofeng Zhang</a>
            </p>
            <p id="summary-4HRRcqE9SU@OpenReview" class="summary">Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous methods typically employ geometric priors, which are often constrained by the performance of the prior models. In this paper, we propose ND-SDF, which learns a Normal Deflection field to represent the angular deviation between the scene normal and the prior normal. Unlike previous methods that uniformly apply geometric priors on all samples, introducing significant bias in accuracy, our proposed normal deflection field dynamically learns and adapts the utilization of samples based on their specific characteristics, thereby improving both the accuracy and effectiveness of the model. Our method not only obtains smooth weakly textured regions such as walls and floors but also preserves the geometric details of complex structures. In addition, we introduce a novel ray sampling strategy based on the deflection angle to facilitate the unbiased rendering process, which significantly improves the quality and accuracy of intricate surfaces, especially on thin structures. Consistent improvements on various challenging datasets demonstrate the superiority of our method.</p>
            <p id="subjects-4HRRcqE9SU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4HRRcqE9SU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4HRRcqE9SU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4HRRcqE9SU@OpenReview" onclick="foldPdfKimi('4HRRcqE9SU@OpenReview', this)" class="hr hr-fold">
        </div><div id="44cMlQSreK@OpenReview" class="panel paper" keywords="quantization,inr,coding,rate,variable,neuroquant,ptq,video,int2,quantizing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=44cMlQSreK" target="_blank" title="202/373"><span class="index notranslate">#202</span></a>
                <a id="title-44cMlQSreK@OpenReview" class="title-link" href="/venue/44cMlQSreK@OpenReview" target="_blank">On Quantizing Neural Representation for Variable-Rate Video Coding</a>
                <a id="pdf-44cMlQSreK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('44cMlQSreK@OpenReview', this)" data="https://openreview.net/pdf?id=44cMlQSreK">[PDF<sup id="pdf-stars-44cMlQSreK@OpenReview">4</sup>]</a>
                <a id="copy-44cMlQSreK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('44cMlQSreK@OpenReview')">[Copy]</a>
                <a id="kimi-44cMlQSreK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('44cMlQSreK@OpenReview', this)">[Kimi<sup id="kimi-stars-44cMlQSreK@OpenReview">2</sup>]</a>
                <a id="rel-44cMlQSreK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('44cMlQSreK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-44cMlQSreK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junqi Shi" target="_blank">Junqi Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhujia Chen" target="_blank">Zhujia Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanfei Li" target="_blank">Hanfei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Zhao" target="_blank">Qi Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Lu" target="_blank">Ming Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Chen" target="_blank">Tong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhan Ma" target="_blank">Zhan Ma</a>
            </p>
            <p id="summary-44cMlQSreK@OpenReview" class="summary">This work introduces NeuroQuant, a novel post-training quantization (PTQ) approach tailored to non-generalized Implicit Neural Representations for variable-rate Video Coding (INR-VC). Unlike existing methods that require extensive weight retraining for each target bitrate, we hypothesize that variable-rate coding can be achieved by adjusting quantization parameters (QPs) of pre-trained weights. Our study reveals that traditional quantization methods, which assume inter-layer independence, are ineffective for non-generalized INR-VC models due to significant dependencies across layers. To address this, we redefine variable-rate INR-VC as a mixed-precision quantization problem and establish a theoretical framework for sensitivity criteria aimed at simplified, fine-grained rate control. Additionally, we propose network-wise calibration and channel-wise quantization strategies to minimize quantization-induced errors, arriving at a unified formula for representation-oriented PTQ calibration. Our experimental evaluations demonstrate that NeuroQuant significantly outperforms existing techniques in varying bitwidth quantization and compression efficiency, accelerating encoding significantly and enabling quantization down to INT2 with minimal reconstruction loss. This work achieves variable-rate INR-VC through weight quantization for the first time and lays a theoretical foundation for future research in rate-distortion optimization, advancing the field of video coding technology.</p>
            <p id="subjects-44cMlQSreK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-44cMlQSreK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-44cMlQSreK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-44cMlQSreK@OpenReview" onclick="foldPdfKimi('44cMlQSreK@OpenReview', this)" class="hr hr-fold">
        </div><div id="3ddi7Uss2A@OpenReview" class="panel paper" keywords="hessian,transformer,mlps,architectural,inarguably,layer,cnns,attention,transformers,dependencies">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3ddi7Uss2A" target="_blank" title="203/373"><span class="index notranslate">#203</span></a>
                <a id="title-3ddi7Uss2A@OpenReview" class="title-link" href="/venue/3ddi7Uss2A@OpenReview" target="_blank">What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis</a>
                <a id="pdf-3ddi7Uss2A@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3ddi7Uss2A@OpenReview', this)" data="https://openreview.net/pdf?id=3ddi7Uss2A">[PDF<sup id="pdf-stars-3ddi7Uss2A@OpenReview">5</sup>]</a>
                <a id="copy-3ddi7Uss2A@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3ddi7Uss2A@OpenReview')">[Copy]</a>
                <a id="kimi-3ddi7Uss2A@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3ddi7Uss2A@OpenReview', this)">[Kimi<sup id="kimi-stars-3ddi7Uss2A@OpenReview">5</sup>]</a>
                <a id="rel-3ddi7Uss2A@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3ddi7Uss2A@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3ddi7Uss2A@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weronika Ormaniec" target="_blank">Weronika Ormaniec</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Dangel" target="_blank">Felix Dangel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sidak Pal Singh" target="_blank">Sidak Pal Singh</a>
            </p>
            <p id="summary-3ddi7Uss2A@OpenReview" class="summary">The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning - to the extent that Transformers are often accompanied by adaptive optimizers, layer normalization, learning rate warmup, and more, in comparison to MLPs/CNNs. The root causes behind these outward manifestations, and the precise mechanisms that govern them, remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures - grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer's Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attention moment dependencies; and (c) while doing so further highlight the important structural differences to the Hessian of classical networks. Our results suggest that various common architectural and optimization choices in Transformers can be traced back to their highly non-linear dependencies on the data and weight matrices, which vary heterogeneously across parameters. Ultimately, our findings provide a deeper understanding of the Transformer’s unique optimization landscape and the challenges it poses.</p>
            <p id="subjects-3ddi7Uss2A@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3ddi7Uss2A@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3ddi7Uss2A@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3ddi7Uss2A@OpenReview" onclick="foldPdfKimi('3ddi7Uss2A@OpenReview', this)" class="hr hr-fold">
        </div><div id="3Oli4u6q3p@OpenReview" class="panel paper" keywords="relitlrm,relightable,illuminations,appearance,lighting,feed,reconstruction,view,lrm,baking">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3Oli4u6q3p" target="_blank" title="204/373"><span class="index notranslate">#204</span></a>
                <a id="title-3Oli4u6q3p@OpenReview" class="title-link" href="/venue/3Oli4u6q3p@OpenReview" target="_blank">RelitLRM: Generative Relightable Radiance for Large Reconstruction Models</a>
                <a id="pdf-3Oli4u6q3p@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3Oli4u6q3p@OpenReview', this)" data="https://openreview.net/pdf?id=3Oli4u6q3p">[PDF<sup id="pdf-stars-3Oli4u6q3p@OpenReview">4</sup>]</a>
                <a id="copy-3Oli4u6q3p@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3Oli4u6q3p@OpenReview')">[Copy]</a>
                <a id="kimi-3Oli4u6q3p@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3Oli4u6q3p@OpenReview', this)">[Kimi<sup id="kimi-stars-3Oli4u6q3p@OpenReview">3</sup>]</a>
                <a id="rel-3Oli4u6q3p@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3Oli4u6q3p@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3Oli4u6q3p@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyuan Zhang" target="_blank">Tianyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengfei Kuang" target="_blank">Zhengfei Kuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haian Jin" target="_blank">Haian Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zexiang Xu" target="_blank">Zexiang Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sai Bi" target="_blank">Sai Bi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tan" target="_blank">Hao Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=HE Zhang" target="_blank">HE Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiwei Hu" target="_blank">Yiwei Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Milos Hasan" target="_blank">Milos Hasan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=William Freeman" target="_blank">William Freeman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fujun Luan" target="_blank">Fujun Luan</a>
            </p>
            <p id="summary-3Oli4u6q3p@OpenReview" class="summary">We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relitlrm.github.io/.</p>
            <p id="subjects-3Oli4u6q3p@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3Oli4u6q3p@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3Oli4u6q3p@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3Oli4u6q3p@OpenReview" onclick="foldPdfKimi('3Oli4u6q3p@OpenReview', this)" class="hr hr-fold">
        </div><div id="2kGKsyhtvh@OpenReview" class="panel paper" keywords="hyperparameter,hyperparameters,optimization,privacy,schedule,hyperparamter,differential,tuning,computationally,automatic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2kGKsyhtvh" target="_blank" title="205/373"><span class="index notranslate">#205</span></a>
                <a id="title-2kGKsyhtvh@OpenReview" class="title-link" href="/venue/2kGKsyhtvh@OpenReview" target="_blank">Towards hyperparameter-free optimization with differential privacy</a>
                <a id="pdf-2kGKsyhtvh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2kGKsyhtvh@OpenReview', this)" data="https://openreview.net/pdf?id=2kGKsyhtvh">[PDF<sup id="pdf-stars-2kGKsyhtvh@OpenReview">4</sup>]</a>
                <a id="copy-2kGKsyhtvh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2kGKsyhtvh@OpenReview')">[Copy]</a>
                <a id="kimi-2kGKsyhtvh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2kGKsyhtvh@OpenReview', this)">[Kimi<sup id="kimi-stars-2kGKsyhtvh@OpenReview">3</sup>]</a>
                <a id="rel-2kGKsyhtvh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2kGKsyhtvh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2kGKsyhtvh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruixuan Liu" target="_blank">Ruixuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqi Bu" target="_blank">Zhiqi Bu</a>
            </p>
            <p id="summary-2kGKsyhtvh@OpenReview" class="summary">Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparamter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performance on various language and vision tasks.</p>
            <p id="subjects-2kGKsyhtvh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2kGKsyhtvh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2kGKsyhtvh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2kGKsyhtvh@OpenReview" onclick="foldPdfKimi('2kGKsyhtvh@OpenReview', this)" class="hr hr-fold">
        </div><div id="2hcfoCHKoB@OpenReview" class="panel paper" keywords="verilog,deeprtl,understanding,generation,language,code,unified,natural,tasks,descriptions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2hcfoCHKoB" target="_blank" title="206/373"><span class="index notranslate">#206</span></a>
                <a id="title-2hcfoCHKoB@OpenReview" class="title-link" href="/venue/2hcfoCHKoB@OpenReview" target="_blank">DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model</a>
                <a id="pdf-2hcfoCHKoB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2hcfoCHKoB@OpenReview', this)" data="https://openreview.net/pdf?id=2hcfoCHKoB">[PDF<sup id="pdf-stars-2hcfoCHKoB@OpenReview">3</sup>]</a>
                <a id="copy-2hcfoCHKoB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2hcfoCHKoB@OpenReview')">[Copy]</a>
                <a id="kimi-2hcfoCHKoB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2hcfoCHKoB@OpenReview', this)">[Kimi<sup id="kimi-stars-2hcfoCHKoB@OpenReview">3</sup>]</a>
                <a id="rel-2hcfoCHKoB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2hcfoCHKoB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2hcfoCHKoB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Liu" target="_blank">Yi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changran Xu" target="_blank">Changran Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhao Zhou" target="_blank">Yunhao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeju Li" target="_blank">Zeju Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Xu" target="_blank">Qiang Xu</a>
            </p>
            <p id="summary-2hcfoCHKoB@OpenReview" class="summary">Recent advancements in large language models (LLMs) have demonstrated significant potential in automating the generation of hardware description language (HDL) code from high-level natural language instructions. While fine-tuning has improved these models' performance in hardware design tasks, prior efforts have largely focused on Verilog code generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, which hampers the generation of high-quality, synthesizable designs. To overcome these limitations, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks.</p>
            <p id="subjects-2hcfoCHKoB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2hcfoCHKoB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2hcfoCHKoB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2hcfoCHKoB@OpenReview" onclick="foldPdfKimi('2hcfoCHKoB@OpenReview', this)" class="hr hr-fold">
        </div><div id="2c7pfOqu9k@OpenReview" class="panel paper" keywords="cache,deft,attention,tree,prefixes,decoding,shared,gpu,flash,calculation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2c7pfOqu9k" target="_blank" title="207/373"><span class="index notranslate">#207</span></a>
                <a id="title-2c7pfOqu9k@OpenReview" class="title-link" href="/venue/2c7pfOqu9k@OpenReview" target="_blank">DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference</a>
                <a id="pdf-2c7pfOqu9k@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2c7pfOqu9k@OpenReview', this)" data="https://openreview.net/pdf?id=2c7pfOqu9k">[PDF<sup id="pdf-stars-2c7pfOqu9k@OpenReview">6</sup>]</a>
                <a id="copy-2c7pfOqu9k@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2c7pfOqu9k@OpenReview')">[Copy]</a>
                <a id="kimi-2c7pfOqu9k@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2c7pfOqu9k@OpenReview', this)">[Kimi<sup id="kimi-stars-2c7pfOqu9k@OpenReview">8</sup>]</a>
                <a id="rel-2c7pfOqu9k@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2c7pfOqu9k@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2c7pfOqu9k@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwei Yao" target="_blank">Jinwei Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiqi Chen" target="_blank">Kaiqi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kexun Zhang" target="_blank">Kexun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxuan You" target="_blank">Jiaxuan You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Binhang Yuan" target="_blank">Binhang Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeke Wang" target="_blank">Zeke Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Lin" target="_blank">Tao Lin</a>
            </p>
            <p id="summary-2c7pfOqu9k@OpenReview" class="summary">Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation.This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through **KV-Guided Grouping**, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose **Flattened Tree KV Splitting**, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-140-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-824" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-825"><span class="mi" id="MathJax-Span-826" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-140">\%</script> KV cache IO and nearly 100<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-141-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-827" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-828"><span class="mi" id="MathJax-Span-829" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-141">\%</script> IO for partial results during attention calculation, DeFT achieves up to 2.52/3.82<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-142-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-830" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-831"><span class="mo" id="MathJax-Span-832" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-142">\times</script> speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms.</p>
            <p id="subjects-2c7pfOqu9k@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2c7pfOqu9k@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2c7pfOqu9k@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2c7pfOqu9k@OpenReview" onclick="foldPdfKimi('2c7pfOqu9k@OpenReview', this)" class="hr hr-fold">
        </div><div id="28abpUEICJ@OpenReview" class="panel paper" keywords="creimbo,brain,session,recordings,neural,interactions,sub,circuits,ensembles,observations">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=28abpUEICJ" target="_blank" title="208/373"><span class="index notranslate">#208</span></a>
                <a id="title-28abpUEICJ@OpenReview" class="title-link" href="/venue/28abpUEICJ@OpenReview" target="_blank">CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations</a>
                <a id="pdf-28abpUEICJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('28abpUEICJ@OpenReview', this)" data="https://openreview.net/pdf?id=28abpUEICJ">[PDF<sup id="pdf-stars-28abpUEICJ@OpenReview">3</sup>]</a>
                <a id="copy-28abpUEICJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('28abpUEICJ@OpenReview')">[Copy]</a>
                <a id="kimi-28abpUEICJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('28abpUEICJ@OpenReview', this)">[Kimi<sup id="kimi-stars-28abpUEICJ@OpenReview">1</sup>]</a>
                <a id="rel-28abpUEICJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('28abpUEICJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-28abpUEICJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Noga Mudrik" target="_blank">Noga Mudrik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Ly" target="_blank">Ryan Ly</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Ruebel" target="_blank">Oliver Ruebel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Charles" target="_blank">Adam Charles</a>
            </p>
            <p id="summary-28abpUEICJ@OpenReview" class="summary">Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods, however, often fail to fully harness the richness of such data, as they provide either uninterpretable representations (e.g., via deep networks) or oversimplify models (e.g., by assuming stationary dynamics or analyzing each session independently). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. Specifically, we assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles---functional groups of neurons---such that the time-varying decomposition of these sub-circuits defines how the ensembles' interactions evolve over time non-stationarily and non-linearly.We discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model, **CREIMBO** (Cross-Regional Ensemble Interactions in Multi-view Brain Observations). CREIMBO identifies the hidden composition of per-session neural ensembles through novel graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics in human high-density electrode recordings, including cross-subject neural mechanisms as well as inter- vs. intra-region dynamical motifs. Furthermore, using mouse whole-brain recordings, we show CREIMBO's ability to discover dynamical interactions that capture task and behavioral variables and meaningfully align with the biological importance of the brain areas they represent.</p>
            <p id="subjects-28abpUEICJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-28abpUEICJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-28abpUEICJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-28abpUEICJ@OpenReview" onclick="foldPdfKimi('28abpUEICJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="1qP3lsatCR@OpenReview" class="panel paper" keywords="moe,placement,training,netmoe,experts,gpus,communication,sample,tokens,expert">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1qP3lsatCR" target="_blank" title="209/373"><span class="index notranslate">#209</span></a>
                <a id="title-1qP3lsatCR@OpenReview" class="title-link" href="/venue/1qP3lsatCR@OpenReview" target="_blank">NetMoE: Accelerating MoE Training through Dynamic Sample Placement</a>
                <a id="pdf-1qP3lsatCR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1qP3lsatCR@OpenReview', this)" data="https://openreview.net/pdf?id=1qP3lsatCR">[PDF<sup id="pdf-stars-1qP3lsatCR@OpenReview">7</sup>]</a>
                <a id="copy-1qP3lsatCR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1qP3lsatCR@OpenReview')">[Copy]</a>
                <a id="kimi-1qP3lsatCR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1qP3lsatCR@OpenReview', this)">[Kimi<sup id="kimi-stars-1qP3lsatCR@OpenReview">6</sup>]</a>
                <a id="rel-1qP3lsatCR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1qP3lsatCR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1qP3lsatCR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyi Liu" target="_blank">Xinyi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujie Wang" target="_blank">Yujie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangcheng Fu" target="_blank">Fangcheng Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xupeng Miao" target="_blank">Xupeng Miao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shenhan Zhu" target="_blank">Shenhan Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaonan Nie" target="_blank">Xiaonan Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin CUI" target="_blank">Bin CUI</a>
            </p>
            <p id="summary-1qP3lsatCR@OpenReview" class="summary">Mixture of Experts (MoE) is a widely used technique to expand model sizes for better model quality while maintaining the computation cost constant. In a nutshell, an MoE model consists of multiple experts in each model layer and routes the training tokens to only a fixed number of experts rather than all. In distributed training, as experts are distributed among different GPUs, All-to-All communication is necessary to exchange the training tokens among the GPUs after each time of expert routing. Due to the frequent and voluminous data exchanges, All-to-All communication has become a notable challenge to training efficiency.In this paper, we manage to accelerate All-to-All communication in MoE models from the training sample perspective, which is unexplored so far. In particular, we put forward the observation that tokens in the same training sample have certain levels of locality in expert routing. Motivated by this, we develop \name, which takes such locality into account and dynamically rearranges the placement of training samples to minimize All-to-All communication costs. Specifically, we model the All-to-All communication given the sample placement and formulate an integer programming problem to deduce the optimal placement in polynomial time. Experiments with 32 GPUs show that NetMoE achieves a maximum efficiency improvement of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-143-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.67&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-833" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.4em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-834"><span class="mn" id="MathJax-Span-835" style="font-family: MathJax_Main;">1.67</span><span class="mo" id="MathJax-Span-836" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.67</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-143">1.67 \times</script> compared with state-of-the-art MoE training frameworks.</p>
            <p id="subjects-1qP3lsatCR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-1qP3lsatCR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1qP3lsatCR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1qP3lsatCR@OpenReview" onclick="foldPdfKimi('1qP3lsatCR@OpenReview', this)" class="hr hr-fold">
        </div><div id="1jcnvghayD@OpenReview" class="panel paper" keywords="gps,bayesian,bnns,performance,vblls,vbll,optimization,conditioning,variational,last">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1jcnvghayD" target="_blank" title="210/373"><span class="index notranslate">#210</span></a>
                <a id="title-1jcnvghayD@OpenReview" class="title-link" href="/venue/1jcnvghayD@OpenReview" target="_blank">Bayesian Optimization via Continual Variational Last Layer Training</a>
                <a id="pdf-1jcnvghayD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1jcnvghayD@OpenReview', this)" data="https://openreview.net/pdf?id=1jcnvghayD">[PDF<sup id="pdf-stars-1jcnvghayD@OpenReview">3</sup>]</a>
                <a id="copy-1jcnvghayD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1jcnvghayD@OpenReview')">[Copy]</a>
                <a id="kimi-1jcnvghayD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1jcnvghayD@OpenReview', this)">[Kimi<sup id="kimi-stars-1jcnvghayD@OpenReview">2</sup>]</a>
                <a id="rel-1jcnvghayD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1jcnvghayD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1jcnvghayD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Brunzema" target="_blank">Paul Brunzema</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mikkel Jordahn" target="_blank">Mikkel Jordahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Willes" target="_blank">John Willes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Trimpe" target="_blank">Sebastian Trimpe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jasper Snoek" target="_blank">Jasper Snoek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Harrison" target="_blank">James Harrison</a>
            </p>
            <p id="summary-1jcnvghayD@OpenReview" class="summary">Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks (BNNs) are a promising direction for higher capacity surrogate models, they have so far seen limited use due to poor performance on some problem types. In this paper, we propose an approach which shows competitive performance on many problem types, including some that BNNs typically struggle with. We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs. We exploit this connection to develop an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks.</p>
            <p id="subjects-1jcnvghayD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-1jcnvghayD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1jcnvghayD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1jcnvghayD@OpenReview" onclick="foldPdfKimi('1jcnvghayD@OpenReview', this)" class="hr hr-fold">
        </div><div id="11xgiMEI5o@OpenReview" class="panel paper" keywords="omnire,urban,dynamic,scenes,scene,omni,reconstruction,vehicles,simulation,human">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=11xgiMEI5o" target="_blank" title="211/373"><span class="index notranslate">#211</span></a>
                <a id="title-11xgiMEI5o@OpenReview" class="title-link" href="/venue/11xgiMEI5o@OpenReview" target="_blank">OmniRe: Omni Urban Scene Reconstruction</a>
                <a id="pdf-11xgiMEI5o@OpenReview" class="title-pdf notranslate" onclick="togglePdf('11xgiMEI5o@OpenReview', this)" data="https://openreview.net/pdf?id=11xgiMEI5o">[PDF<sup id="pdf-stars-11xgiMEI5o@OpenReview">3</sup>]</a>
                <a id="copy-11xgiMEI5o@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('11xgiMEI5o@OpenReview')">[Copy]</a>
                <a id="kimi-11xgiMEI5o@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('11xgiMEI5o@OpenReview', this)">[Kimi<sup id="kimi-stars-11xgiMEI5o@OpenReview">4</sup>]</a>
                <a id="rel-11xgiMEI5o@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('11xgiMEI5o@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-11xgiMEI5o@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyu Chen" target="_blank">Ziyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Yang" target="_blank">Jiawei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahui Huang" target="_blank">Jiahui Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Riccardo de Lutio" target="_blank">Riccardo de Lutio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Janick Martinez Esturo" target="_blank">Janick Martinez Esturo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boris Ivanovic" target="_blank">Boris Ivanovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Or Litany" target="_blank">Or Litany</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zan Gojcic" target="_blank">Zan Gojcic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanja Fidler" target="_blank">Sanja Fidler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Pavone" target="_blank">Marco Pavone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Song" target="_blank">Li Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Wang" target="_blank">Yue Wang</a>
            </p>
            <p id="summary-11xgiMEI5o@OpenReview" class="summary">We introduce OmniRe, a comprehensive system for efficiently creating high-fidelity digital twins of dynamic real-world scenes from on-device logs. Recent methods using neural fields or Gaussian Splatting primarily focus on vehicles, hindering a holistic framework for all dynamic foregrounds demanded by downstream applications, e.g., the simulation of human behavior. OmniRe extends beyond vehicle modeling to enable accurate, full-length reconstruction of diverse dynamic objects in urban scenes. Our approach builds scene graphs on 3DGS and constructs multiple Gaussian representations in canonical spaces that model various dynamic actors, including vehicles, pedestrians, cyclists, and others. OmniRe allows holistically reconstructing any dynamic object in the scene, enabling advanced simulations (~60 Hz) that include human-participated scenarios, such as pedestrian behavior simulation and human-vehicle interaction. This comprehensive simulation capability is unmatched by existing methods. Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We further extend our results to 5 additional popular driving datasets to demonstrate its generalizability on common urban scenes. We will make the code and data publicly available.</p>
            <p id="subjects-11xgiMEI5o@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-11xgiMEI5o@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-11xgiMEI5o@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-11xgiMEI5o@OpenReview" onclick="foldPdfKimi('11xgiMEI5o@OpenReview', this)" class="hr hr-fold">
        </div><div id="10JOlFIPjt@OpenReview" class="panel paper" keywords="brain,cell,multimodal,contrastive,region,classification,electrophysiological,neurons,type,recorded">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=10JOlFIPjt" target="_blank" title="212/373"><span class="index notranslate">#212</span></a>
                <a id="title-10JOlFIPjt@OpenReview" class="title-link" href="/venue/10JOlFIPjt@OpenReview" target="_blank">In vivo cell-type and brain region classification via multimodal contrastive learning</a>
                <a id="pdf-10JOlFIPjt@OpenReview" class="title-pdf notranslate" onclick="togglePdf('10JOlFIPjt@OpenReview', this)" data="https://openreview.net/pdf?id=10JOlFIPjt">[PDF<sup id="pdf-stars-10JOlFIPjt@OpenReview">5</sup>]</a>
                <a id="copy-10JOlFIPjt@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('10JOlFIPjt@OpenReview')">[Copy]</a>
                <a id="kimi-10JOlFIPjt@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('10JOlFIPjt@OpenReview', this)">[Kimi<sup id="kimi-stars-10JOlFIPjt@OpenReview">6</sup>]</a>
                <a id="rel-10JOlFIPjt@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('10JOlFIPjt@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-10JOlFIPjt@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Han Yu" target="_blank">Han Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanrui Lyu" target="_blank">Hanrui Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=YiXun Xu" target="_blank">YiXun Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charlie Windolf" target="_blank">Charlie Windolf</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Lee" target="_blank">Eric Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Yang" target="_blank">Fan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Shelton" target="_blank">Andrew Shelton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Olivier Winter" target="_blank">Olivier Winter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=International Brain Laboratory" target="_blank">International Brain Laboratory</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eva Dyer" target="_blank">Eva Dyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chandramouli Chandrasekaran" target="_blank">Chandramouli Chandrasekaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Steinmetz" target="_blank">Nicholas Steinmetz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liam Paninski" target="_blank">Liam Paninski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cole Hurwitz" target="_blank">Cole Hurwitz</a>
            </p>
            <p id="summary-10JOlFIPjt@OpenReview" class="summary">Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for an opto-tagged visual cortex dataset and for brain region classification of the public International Brain Laboratory brain-wide map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings.</p>
            <p id="subjects-10JOlFIPjt@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-10JOlFIPjt@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-10JOlFIPjt@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-10JOlFIPjt@OpenReview" onclick="foldPdfKimi('10JOlFIPjt@OpenReview', this)" class="hr hr-fold">
        </div><div id="0h6v4SpLCY@OpenReview" class="panel paper" keywords="robust,wasserstein,guarantees,distributionally,generalization,models,regularizations,cases,universal,curse">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0h6v4SpLCY" target="_blank" title="213/373"><span class="index notranslate">#213</span></a>
                <a id="title-0h6v4SpLCY@OpenReview" class="title-link" href="/venue/0h6v4SpLCY@OpenReview" target="_blank">Universal generalization guarantees for Wasserstein distributionally robust models</a>
                <a id="pdf-0h6v4SpLCY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0h6v4SpLCY@OpenReview', this)" data="https://openreview.net/pdf?id=0h6v4SpLCY">[PDF<sup id="pdf-stars-0h6v4SpLCY@OpenReview">4</sup>]</a>
                <a id="copy-0h6v4SpLCY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0h6v4SpLCY@OpenReview')">[Copy]</a>
                <a id="kimi-0h6v4SpLCY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0h6v4SpLCY@OpenReview', this)">[Kimi<sup id="kimi-stars-0h6v4SpLCY@OpenReview">3</sup>]</a>
                <a id="rel-0h6v4SpLCY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0h6v4SpLCY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0h6v4SpLCY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tam Le" target="_blank">Tam Le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jerome Malick" target="_blank">Jerome Malick</a>
            </p>
            <p id="summary-0h6v4SpLCY@OpenReview" class="summary">Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations.</p>
            <p id="subjects-0h6v4SpLCY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-0h6v4SpLCY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0h6v4SpLCY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0h6v4SpLCY@OpenReview" onclick="foldPdfKimi('0h6v4SpLCY@OpenReview', this)" class="hr hr-fold">
        </div><div id="0fhzSFsGUT@OpenReview" class="panel paper" keywords="petra,reversible,architectures,resnet,parallelizing,backpropagation,stashing,end,autograd,training">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0fhzSFsGUT" target="_blank" title="214/373"><span class="index notranslate">#214</span></a>
                <a id="title-0fhzSFsGUT@OpenReview" class="title-link" href="/venue/0fhzSFsGUT@OpenReview" target="_blank">PETRA: Parallel End-to-end Training with Reversible Architectures</a>
                <a id="pdf-0fhzSFsGUT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0fhzSFsGUT@OpenReview', this)" data="https://openreview.net/pdf?id=0fhzSFsGUT">[PDF<sup id="pdf-stars-0fhzSFsGUT@OpenReview">2</sup>]</a>
                <a id="copy-0fhzSFsGUT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0fhzSFsGUT@OpenReview')">[Copy]</a>
                <a id="kimi-0fhzSFsGUT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0fhzSFsGUT@OpenReview', this)">[Kimi<sup id="kimi-stars-0fhzSFsGUT@OpenReview">3</sup>]</a>
                <a id="rel-0fhzSFsGUT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0fhzSFsGUT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0fhzSFsGUT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Stephane Rivaud" target="_blank">Stephane Rivaud</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Louis Fournier" target="_blank">Louis Fournier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Pumir" target="_blank">Thomas Pumir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eugene Belilovsky" target="_blank">Eugene Belilovsky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Eickenberg" target="_blank">Michael Eickenberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edouard Oyallon" target="_blank">Edouard Oyallon</a>
            </p>
            <p id="summary-0fhzSFsGUT@OpenReview" class="summary">Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling. In this work, we show how reversible architectures can solve challenges in parallelizing deep model training. We introduce PETRA, a novel alternative to backpropagation for parallelizing gradient computations. PETRA facilitates effective model parallelism by enabling stages (i.e., a set of layers) to compute independently on different devices, while only needing to communicate activations and gradients between each other. By decoupling the forward and backward passes and keeping a single updated version of the parameters, the need for weight stashing is also removed. We develop a custom autograd-like training framework for PETRA, and we demonstrate its effectiveness on standard computer vision benchmarks, achieving competitive accuracies comparable to backpropagation using ResNet-18, ResNet-34, and ResNet-50 models.</p>
            <p id="subjects-0fhzSFsGUT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-0fhzSFsGUT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0fhzSFsGUT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0fhzSFsGUT@OpenReview" onclick="foldPdfKimi('0fhzSFsGUT@OpenReview', this)" class="hr hr-fold">
        </div><div id="0fJfVOSUra@OpenReview" class="panel paper" keywords="kernels,thunderkittens,adorable,operations,attention,abstractions,tiles,gpu,16x16,cublas">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0fJfVOSUra" target="_blank" title="215/373"><span class="index notranslate">#215</span></a>
                <a id="title-0fJfVOSUra@OpenReview" class="title-link" href="/venue/0fJfVOSUra@OpenReview" target="_blank">ThunderKittens: Simple, Fast, and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-144-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;Adorable&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-837" style="width: 4.689em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.891em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.321em, 1003.89em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-838"><span class="texatom" id="MathJax-Span-839"><span class="mrow" id="MathJax-Span-840"><span class="mtext" id="MathJax-Span-841" style="font-family: MathJax_Main-italic;">Adorable</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.04em; border-left: 0px solid; width: 0px; height: 0.96em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">Adorable</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-144">\textit{Adorable}</script> Kernels</a>
                <a id="pdf-0fJfVOSUra@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0fJfVOSUra@OpenReview', this)" data="https://openreview.net/pdf?id=0fJfVOSUra">[PDF<sup id="pdf-stars-0fJfVOSUra@OpenReview">1</sup>]</a>
                <a id="copy-0fJfVOSUra@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0fJfVOSUra@OpenReview')">[Copy]</a>
                <a id="kimi-0fJfVOSUra@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0fJfVOSUra@OpenReview', this)">[Kimi<sup id="kimi-stars-0fJfVOSUra@OpenReview">1</sup>]</a>
                <a id="rel-0fJfVOSUra@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0fJfVOSUra@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0fJfVOSUra@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Spector" target="_blank">Benjamin Spector</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simran Arora" target="_blank">Simran Arora</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaryan Singhal" target="_blank">Aaryan Singhal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arjun Parthasarathy" target="_blank">Arjun Parthasarathy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Fu" target="_blank">Dan Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Re" target="_blank">Christopher Re</a>
            </p>
            <p id="summary-0fJfVOSUra@OpenReview" class="summary">The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention.The diverse hardware capabilities of GPUs might suggest that we need a wide variety of techniques to achieve high performance. However, our work explores whether a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use and maintain. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like parallel compute operations over tiles, (2) at the thread-block level, we provide a template for overlapping asynchronous operations across parallel warps, and (3) at the grid-level, TK can help hide the block launch and tear-down, and memory costs. We show the value of TK by providing kernels that match or outperform prior kernels for a range of AI operations. We match CuBLAS and FlashAttention-3 on GEMM and attention inference, and outperforms the strongest baselines by <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-145-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;40&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-842" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.96em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-843"><span class="mn" id="MathJax-Span-844" style="font-family: MathJax_Main;">10</span><span class="mo" id="MathJax-Span-845" style="font-family: MathJax_Main; padding-left: 0.211em;">−</span><span class="mn" id="MathJax-Span-846" style="font-family: MathJax_Main; padding-left: 0.211em;">40</span><span class="mi" id="MathJax-Span-847" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>10</mn><mo>−</mo><mn>40</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-145">10-40\%</script> on attention backwards, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-146-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;8&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-848" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-849"><span class="mn" id="MathJax-Span-850" style="font-family: MathJax_Main;">8</span><span class="mo" id="MathJax-Span-851" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>8</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-146">8\times</script> on state space models, and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-147-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;14&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-852" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-853"><span class="mn" id="MathJax-Span-854" style="font-family: MathJax_Main;">14</span><span class="mo" id="MathJax-Span-855" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>14</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-147">14\times</script> on linear attention.</p>
            <p id="subjects-0fJfVOSUra@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-0fJfVOSUra@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0fJfVOSUra@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0fJfVOSUra@OpenReview" onclick="foldPdfKimi('0fJfVOSUra@OpenReview', this)" class="hr hr-fold">
        </div><div id="0bmGL4q7vJ@OpenReview" class="panel paper" keywords="usage,agent,traj,modal,vlm,tool,verifier,tuning,multi,minicpm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0bmGL4q7vJ" target="_blank" title="216/373"><span class="index notranslate">#216</span></a>
                <a id="title-0bmGL4q7vJ@OpenReview" class="title-link" href="/venue/0bmGL4q7vJ@OpenReview" target="_blank">Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage</a>
                <a id="pdf-0bmGL4q7vJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0bmGL4q7vJ@OpenReview', this)" data="https://openreview.net/pdf?id=0bmGL4q7vJ">[PDF<sup id="pdf-stars-0bmGL4q7vJ@OpenReview">10</sup>]</a>
                <a id="copy-0bmGL4q7vJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0bmGL4q7vJ@OpenReview')">[Copy]</a>
                <a id="kimi-0bmGL4q7vJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0bmGL4q7vJ@OpenReview', this)">[Kimi<sup id="kimi-stars-0bmGL4q7vJ@OpenReview">12</sup>]</a>
                <a id="rel-0bmGL4q7vJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0bmGL4q7vJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0bmGL4q7vJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhi Gao" target="_blank">Zhi Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bofei Zhang" target="_blank">Bofei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengxiang Li" target="_blank">Pengxiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojian Ma" target="_blank">Xiaojian Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Yuan" target="_blank">Tao Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Fan" target="_blank">Yue Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuwei Wu" target="_blank">Yuwei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunde Jia" target="_blank">Yunde Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song-Chun Zhu" target="_blank">Song-Chun Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Li" target="_blank">Qing Li</a>
            </p>
            <p id="summary-0bmGL4q7vJ@OpenReview" class="summary">The advancement of large language models (LLMs) prompts the development of multi-modal agents, providing a feasible way to solve practical tasks by using tools. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o model to separately generate queries, files, and trajectories, followed by a query-file verifier and trajectory verifier. Based on the data synthesis pipeline, we collect the MM-traj dataset with 20k tasks using 10 tools. Then, we build the T3-agent that uses MiniCPM-V as the controller Trajectory Tuning for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-agent has achieved remarkable improvements and outperforms GPT-4 driven agents by 10%, showing the effectiveness of the proposed data synthesis pipeline that leads to better reasoning capabilities in tool usage.</p>
            <p id="subjects-0bmGL4q7vJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-0bmGL4q7vJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0bmGL4q7vJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0bmGL4q7vJ@OpenReview" onclick="foldPdfKimi('0bmGL4q7vJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="00SnKBGTsz@OpenReview" class="panel paper" keywords="dataenvgym,student,agents,environments,generation,feedback,data,teacher,creating,plan">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=00SnKBGTsz" target="_blank" title="217/373"><span class="index notranslate">#217</span></a>
                <a id="title-00SnKBGTsz@OpenReview" class="title-link" href="/venue/00SnKBGTsz@OpenReview" target="_blank">DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback</a>
                <a id="pdf-00SnKBGTsz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('00SnKBGTsz@OpenReview', this)" data="https://openreview.net/pdf?id=00SnKBGTsz">[PDF<sup id="pdf-stars-00SnKBGTsz@OpenReview">2</sup>]</a>
                <a id="copy-00SnKBGTsz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('00SnKBGTsz@OpenReview')">[Copy]</a>
                <a id="kimi-00SnKBGTsz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('00SnKBGTsz@OpenReview', this)">[Kimi<sup id="kimi-stars-00SnKBGTsz@OpenReview">4</sup>]</a>
                <a id="rel-00SnKBGTsz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('00SnKBGTsz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-00SnKBGTsz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zaid Khan" target="_blank">Zaid Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elias Stengel-Eskin" target="_blank">Elias Stengel-Eskin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaemin Cho" target="_blank">Jaemin Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohit Bansal" target="_blank">Mohit Bansal</a>
            </p>
            <p id="summary-00SnKBGTsz@OpenReview" class="summary">The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents – or teachers – is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent’s end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DataEnvGym includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in three diverse tasks covering both text and images (mathematics, programming, and visual question answering) and test multiple student models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms. We will publicly release our code and leaderboard.</p>
            <p id="subjects-00SnKBGTsz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-00SnKBGTsz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-00SnKBGTsz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-00SnKBGTsz@OpenReview" onclick="foldPdfKimi('00SnKBGTsz@OpenReview', this)" class="hr hr-fold">
        </div><div id="sahQq2sH5x@OpenReview" class="panel paper" keywords="pcns,benchmarks,coding,community,galvanizing,predictive,scalability,library,tasks,architectures">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sahQq2sH5x" target="_blank" title="218/373"><span class="index notranslate">#218</span></a>
                <a id="title-sahQq2sH5x@OpenReview" class="title-link" href="/venue/sahQq2sH5x@OpenReview" target="_blank">Benchmarking Predictive Coding Networks -- Made Simple</a>
                <a id="pdf-sahQq2sH5x@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sahQq2sH5x@OpenReview', this)" data="https://openreview.net/pdf?id=sahQq2sH5x">[PDF<sup id="pdf-stars-sahQq2sH5x@OpenReview">3</sup>]</a>
                <a id="copy-sahQq2sH5x@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sahQq2sH5x@OpenReview')">[Copy]</a>
                <a id="kimi-sahQq2sH5x@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sahQq2sH5x@OpenReview', this)">[Kimi<sup id="kimi-stars-sahQq2sH5x@OpenReview">1</sup>]</a>
                <a id="rel-sahQq2sH5x@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sahQq2sH5x@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sahQq2sH5x@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Pinchetti" target="_blank">Luca Pinchetti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Qi" target="_blank">Chang Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oleh Lokshyn" target="_blank">Oleh Lokshyn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cornelius Emde" target="_blank">Cornelius Emde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amine M&amp;#x27;Charrak" target="_blank">Amine M&amp;#x27;Charrak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mufeng Tang" target="_blank">Mufeng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Frieder" target="_blank">Simon Frieder</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bayar Menzat" target="_blank">Bayar Menzat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaspard Oliviers" target="_blank">Gaspard Oliviers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rafal Bogacz" target="_blank">Rafal Bogacz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Lukasiewicz" target="_blank">Thomas Lukasiewicz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommaso Salvatori" target="_blank">Tommaso Salvatori</a>
            </p>
            <p id="summary-sahQq2sH5x@OpenReview" class="summary">In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library, and a comprehensive set of benchmarks, would address all of these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All of this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii) reach new state-of-the-art results in all of the tasks and dataset provided; (iii) clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we will release the code, tests, and benchmarks.</p>
            <p id="subjects-sahQq2sH5x@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-sahQq2sH5x@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sahQq2sH5x@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sahQq2sH5x@OpenReview" onclick="foldPdfKimi('sahQq2sH5x@OpenReview', this)" class="hr hr-fold">
        </div><div id="rpwGUtTeA5@OpenReview" class="panel paper" keywords="unicbe,cbe,uniformity,evaluation,preference,comparing,unified,objective,scalability,sampling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rpwGUtTeA5" target="_blank" title="219/373"><span class="index notranslate">#219</span></a>
                <a id="title-rpwGUtTeA5@OpenReview" class="title-link" href="/venue/rpwGUtTeA5@OpenReview" target="_blank">UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization</a>
                <a id="pdf-rpwGUtTeA5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rpwGUtTeA5@OpenReview', this)" data="https://openreview.net/pdf?id=rpwGUtTeA5">[PDF<sup id="pdf-stars-rpwGUtTeA5@OpenReview">3</sup>]</a>
                <a id="copy-rpwGUtTeA5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rpwGUtTeA5@OpenReview')">[Copy]</a>
                <a id="kimi-rpwGUtTeA5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rpwGUtTeA5@OpenReview', this)">[Kimi<sup id="kimi-stars-rpwGUtTeA5@OpenReview">2</sup>]</a>
                <a id="rel-rpwGUtTeA5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rpwGUtTeA5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rpwGUtTeA5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peiwen Yuan" target="_blank">Peiwen Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaoxiong Feng" target="_blank">Shaoxiong Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiwei Li" target="_blank">Yiwei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinglin Wang" target="_blank">Xinglin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueqi Zhang" target="_blank">Yueqi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Shi" target="_blank">Jiayi Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuyi Tan" target="_blank">Chuyi Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyuan Pan" target="_blank">Boyuan Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Hu" target="_blank">Yao Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kan Li" target="_blank">Kan Li</a>
            </p>
            <p id="summary-rpwGUtTeA5@OpenReview" class="summary">Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty.Following the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE.On the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability.</p>
            <p id="subjects-rpwGUtTeA5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-rpwGUtTeA5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rpwGUtTeA5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rpwGUtTeA5@OpenReview" onclick="foldPdfKimi('rpwGUtTeA5@OpenReview', this)" class="hr hr-fold">
        </div><div id="qpXctF2aLZ@OpenReview" class="panel paper" keywords="sympol,tree,policy,policies,symbolic,interpretable,decision,based,reinforcement,interpretability">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qpXctF2aLZ" target="_blank" title="220/373"><span class="index notranslate">#220</span></a>
                <a id="title-qpXctF2aLZ@OpenReview" class="title-link" href="/venue/qpXctF2aLZ@OpenReview" target="_blank">Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization</a>
                <a id="pdf-qpXctF2aLZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qpXctF2aLZ@OpenReview', this)" data="https://openreview.net/pdf?id=qpXctF2aLZ">[PDF<sup id="pdf-stars-qpXctF2aLZ@OpenReview">4</sup>]</a>
                <a id="copy-qpXctF2aLZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qpXctF2aLZ@OpenReview')">[Copy]</a>
                <a id="kimi-qpXctF2aLZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qpXctF2aLZ@OpenReview', this)">[Kimi<sup id="kimi-stars-qpXctF2aLZ@OpenReview">5</sup>]</a>
                <a id="rel-qpXctF2aLZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qpXctF2aLZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qpXctF2aLZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sascha Marton" target="_blank">Sascha Marton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Grams" target="_blank">Tim Grams</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Vogt" target="_blank">Florian Vogt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Lüdtke" target="_blank">Stefan Lüdtke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Bartelt" target="_blank">Christian Bartelt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heiner Stuckenschmidt" target="_blank">Heiner Stuckenschmidt</a>
            </p>
            <p id="summary-qpXctF2aLZ@OpenReview" class="summary">Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging.In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability.We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. Unlike existing methods, it enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees. Our implementation is available under: https://github.com/s-marton/sympol</p>
            <p id="subjects-qpXctF2aLZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qpXctF2aLZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qpXctF2aLZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qpXctF2aLZ@OpenReview" onclick="foldPdfKimi('qpXctF2aLZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="pPQPQ7Yd58@OpenReview" class="panel paper" keywords="control,clustering,visual,oriented,vision,representation,encoder,pose,pushing,action">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pPQPQ7Yd58" target="_blank" title="221/373"><span class="index notranslate">#221</span></a>
                <a id="title-pPQPQ7Yd58@OpenReview" class="title-link" href="/venue/pPQPQ7Yd58@OpenReview" target="_blank">Control-oriented Clustering of Visual Latent Representation</a>
                <a id="pdf-pPQPQ7Yd58@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pPQPQ7Yd58@OpenReview', this)" data="https://openreview.net/pdf?id=pPQPQ7Yd58">[PDF<sup id="pdf-stars-pPQPQ7Yd58@OpenReview">5</sup>]</a>
                <a id="copy-pPQPQ7Yd58@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pPQPQ7Yd58@OpenReview')">[Copy]</a>
                <a id="kimi-pPQPQ7Yd58@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pPQPQ7Yd58@OpenReview', this)">[Kimi<sup id="kimi-stars-pPQPQ7Yd58@OpenReview">4</sup>]</a>
                <a id="rel-pPQPQ7Yd58@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pPQPQ7Yd58@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pPQPQ7Yd58@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Han Qi" target="_blank">Han Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haocheng Yin" target="_blank">Haocheng Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heng Yang" target="_blank">Heng Yang</a>
            </p>
            <p id="summary-pPQPQ7Yd58@OpenReview" class="summary">We initiate a study of the geometry of the visual representation space ---the information channel from the vision encoder to the action decoder--- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of *neural collapse* (NC) in image classification, we empirically demonstrate the prevalent emergence of a similar *law of clustering* in the visual representation space. Specifically, - In discrete image-based control (e.g., Lunar Lander), the visual representations cluster according to the natural discrete action labels;- In continuous image-based control (e.g., Planar Pushing and Block Stacking), the clustering emerges according to ``control-oriented'' classes that are based on (a) the relative pose between the object and the target in the input or (b) the relative pose of the object induced by expert actions in the output. Each of the classes corresponds to one relative pose orthant (REPO).Beyond empirical observation, we show such a law of clustering can be leveraged as an algorithmic tool to improve test-time performance when training a policy with limited expert demonstrations. Particularly, we pretrain the vision encoder using NC as a regularization to encourage control-oriented clustering of the visual features. Surprisingly, such an NC-pretrained vision encoder, when finetuned end-to-end with the action decoder, boosts the test-time performance by 10% to 35%. Real-world vision-based planar pushing experiments confirmed the surprising advantage of control-oriented visual representation pretraining.</p>
            <p id="subjects-pPQPQ7Yd58@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pPQPQ7Yd58@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pPQPQ7Yd58@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pPQPQ7Yd58@OpenReview" onclick="foldPdfKimi('pPQPQ7Yd58@OpenReview', this)" class="hr hr-fold">
        </div><div id="cznqgb4DNv@OpenReview" class="panel paper" keywords="dspodfl,decentralized,dfl,texttt,federated,clients,sporadic,gradient,algorithmic,settings">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cznqgb4DNv" target="_blank" title="222/373"><span class="index notranslate">#222</span></a>
                <a id="title-cznqgb4DNv@OpenReview" class="title-link" href="/venue/cznqgb4DNv@OpenReview" target="_blank">Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees</a>
                <a id="pdf-cznqgb4DNv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cznqgb4DNv@OpenReview', this)" data="https://openreview.net/pdf?id=cznqgb4DNv">[PDF<sup id="pdf-stars-cznqgb4DNv@OpenReview">3</sup>]</a>
                <a id="copy-cznqgb4DNv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cznqgb4DNv@OpenReview')">[Copy]</a>
                <a id="kimi-cznqgb4DNv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cznqgb4DNv@OpenReview', this)">[Kimi<sup id="kimi-stars-cznqgb4DNv@OpenReview">3</sup>]</a>
                <a id="rel-cznqgb4DNv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cznqgb4DNv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cznqgb4DNv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shahryar Zehtabi" target="_blank">Shahryar Zehtabi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong-Jun Han" target="_blank">Dong-Jun Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohit Parasnis" target="_blank">Rohit Parasnis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seyyedali Hosseinalipour" target="_blank">Seyyedali Hosseinalipour</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Brinton" target="_blank">Christopher Brinton</a>
            </p>
            <p id="summary-cznqgb4DNv@OpenReview" class="summary">Decentralized federated learning (DFL) captures FL settings where both (i) model updates and (ii) model aggregations are exclusively carried out by the clients without a central server. Existing DFL works have mostly focused on settings where clients conduct a fixed number of local updates between local model exchanges, overlooking heterogeneity and dynamics in communication and computation capabilities. In this work, we propose Decentralized Sporadic Federated Learning (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-148-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;DSpodFL&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-856" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-857"><span class="texatom" id="MathJax-Span-858"><span class="mrow" id="MathJax-Span-859"><span class="mtext" id="MathJax-Span-860" style="font-family: MathJax_Typewriter;">DSpodFL</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">DSpodFL</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-148">\texttt{DSpodFL}</script>), a DFL methodology built on a generalized notion of *sporadicity* in both local gradient and aggregation processes. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-149-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;DSpodFL&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-861" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-862"><span class="texatom" id="MathJax-Span-863"><span class="mrow" id="MathJax-Span-864"><span class="mtext" id="MathJax-Span-865" style="font-family: MathJax_Typewriter;">DSpodFL</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">DSpodFL</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-149">\texttt{DSpodFL}</script> subsumes many existing decentralized optimization methods under a unified algorithmic framework by modeling the per-iteration (i) occurrence of gradient descent at each client and (ii) exchange of models between client pairs as arbitrary indicator random variables, thus capturing *heterogeneous and time-varying* computation/communication scenarios. We analytically characterize the convergence behavior of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-150-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;DSpodFL&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-866" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-867"><span class="texatom" id="MathJax-Span-868"><span class="mrow" id="MathJax-Span-869"><span class="mtext" id="MathJax-Span-870" style="font-family: MathJax_Typewriter;">DSpodFL</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">DSpodFL</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-150">\texttt{DSpodFL}</script> for both convex and non-convex models and for both constant and diminishing learning rates, under mild assumptions on the communication graph connectivity, data heterogeneity across clients, and gradient noises. We show how our bounds recover existing results from decentralized gradient descent as special cases. Experiments demonstrate that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-151-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;DSpodFL&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-871" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-872"><span class="texatom" id="MathJax-Span-873"><span class="mrow" id="MathJax-Span-874"><span class="mtext" id="MathJax-Span-875" style="font-family: MathJax_Typewriter;">DSpodFL</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">DSpodFL</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-151">\texttt{DSpodFL}</script> consistently achieves improved training speeds compared with baselines under various system settings.</p>
            <p id="subjects-cznqgb4DNv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cznqgb4DNv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cznqgb4DNv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cznqgb4DNv@OpenReview" onclick="foldPdfKimi('cznqgb4DNv@OpenReview', this)" class="hr hr-fold">
        </div><div id="UVnD9Ze6mF@OpenReview" class="panel paper" keywords="2024,air,bench,safety,policies,categories,risks,regulations,specified,fms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UVnD9Ze6mF" target="_blank" title="223/373"><span class="index notranslate">#223</span></a>
                <a id="title-UVnD9Ze6mF@OpenReview" class="title-link" href="/venue/UVnD9Ze6mF@OpenReview" target="_blank">AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories</a>
                <a id="pdf-UVnD9Ze6mF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UVnD9Ze6mF@OpenReview', this)" data="https://openreview.net/pdf?id=UVnD9Ze6mF">[PDF<sup id="pdf-stars-UVnD9Ze6mF@OpenReview">4</sup>]</a>
                <a id="copy-UVnD9Ze6mF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UVnD9Ze6mF@OpenReview')">[Copy]</a>
                <a id="kimi-UVnD9Ze6mF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UVnD9Ze6mF@OpenReview', this)">[Kimi<sup id="kimi-stars-UVnD9Ze6mF@OpenReview">3</sup>]</a>
                <a id="rel-UVnD9Ze6mF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UVnD9Ze6mF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UVnD9Ze6mF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zeng" target="_blank">Yi Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Yang" target="_blank">Yu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andy Zhou" target="_blank">Andy Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeffrey Tan" target="_blank">Jeffrey Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuheng Tu" target="_blank">Yuheng Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Mai" target="_blank">Yifan Mai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Klyman" target="_blank">Kevin Klyman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minzhou Pan" target="_blank">Minzhou Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoxi Jia" target="_blank">Ruoxi Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawn Song" target="_blank">Dawn Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Percy Liang" target="_blank">Percy Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Li" target="_blank">Bo Li</a>
            </p>
            <p id="summary-UVnD9Ze6mF@OpenReview" class="summary">Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-BENCH 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in the AI Risks taxonomy, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-BENCH 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-BENCH 2024 uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-BENCH 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.</p>
            <p id="subjects-UVnD9Ze6mF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-UVnD9Ze6mF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UVnD9Ze6mF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UVnD9Ze6mF@OpenReview" onclick="foldPdfKimi('UVnD9Ze6mF@OpenReview', this)" class="hr hr-fold">
        </div><div id="9GsgCUJtic@OpenReview" class="panel paper" keywords="gflownets,gnns,flow,sampling,distributions,correctness,gflownet,distribution,learn,hamper">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9GsgCUJtic" target="_blank" title="224/373"><span class="index notranslate">#224</span></a>
                <a id="title-9GsgCUJtic@OpenReview" class="title-link" href="/venue/9GsgCUJtic@OpenReview" target="_blank">When do GFlowNets learn the right distribution?</a>
                <a id="pdf-9GsgCUJtic@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9GsgCUJtic@OpenReview', this)" data="https://openreview.net/pdf?id=9GsgCUJtic">[PDF<sup id="pdf-stars-9GsgCUJtic@OpenReview">3</sup>]</a>
                <a id="copy-9GsgCUJtic@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9GsgCUJtic@OpenReview')">[Copy]</a>
                <a id="kimi-9GsgCUJtic@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9GsgCUJtic@OpenReview', this)">[Kimi<sup id="kimi-stars-9GsgCUJtic@OpenReview">3</sup>]</a>
                <a id="rel-9GsgCUJtic@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9GsgCUJtic@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9GsgCUJtic@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tiago Silva" target="_blank">Tiago Silva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rodrigo Alves" target="_blank">Rodrigo Alves</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eliezer de Souza da Silva" target="_blank">Eliezer de Souza da Silva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amauri Souza" target="_blank">Amauri Souza</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vikas Garg" target="_blank">Vikas Garg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Kaski" target="_blank">Samuel Kaski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Diego Mesquita" target="_blank">Diego Mesquita</a>
            </p>
            <p id="summary-9GsgCUJtic@OpenReview" class="summary">Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets learn to sample from the target distribution remains underexplored. To tackle this issue, we first assess the extent to which a violation of the detailed balance of the underlying flow network might hamper the correctness of GFlowNet's sampling distribution. In particular, we demonstrate that the impact of an imbalanced edge on the model's accuracy is influenced by the total amount of flow passing through it and, as a consequence, is unevenly distributed across the network. We also argue that, depending on the parameterization, imbalance may be inevitable. In this regard, we consider the problem of sampling from distributions over graphs with GFlowNets parameterized by graph neural networks (GNNs) and show that the representation limits of GNNs delineate which distributions these GFlowNets can approximate. Lastly, we address these limitations by proposing a theoretically sound and computationally tractable metric for assessing GFlowNets, experimentally showing it is a better proxy for correctness than popular evaluation protocols.</p>
            <p id="subjects-9GsgCUJtic@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9GsgCUJtic@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9GsgCUJtic@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9GsgCUJtic@OpenReview" onclick="foldPdfKimi('9GsgCUJtic@OpenReview', this)" class="hr hr-fold">
        </div><div id="8oFvUBvF1u@OpenReview" class="panel paper" keywords="densematcher,manipulation,correspondence,demo,correspondences,meshes,category,robotic,object,relatable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8oFvUBvF1u" target="_blank" title="225/373"><span class="index notranslate">#225</span></a>
                <a id="title-8oFvUBvF1u@OpenReview" class="title-link" href="/venue/8oFvUBvF1u@OpenReview" target="_blank">DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from One Demo</a>
                <a id="pdf-8oFvUBvF1u@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8oFvUBvF1u@OpenReview', this)" data="https://openreview.net/pdf?id=8oFvUBvF1u">[PDF<sup id="pdf-stars-8oFvUBvF1u@OpenReview">2</sup>]</a>
                <a id="copy-8oFvUBvF1u@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8oFvUBvF1u@OpenReview')">[Copy]</a>
                <a id="kimi-8oFvUBvF1u@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8oFvUBvF1u@OpenReview', this)">[Kimi<sup id="kimi-stars-8oFvUBvF1u@OpenReview">2</sup>]</a>
                <a id="rel-8oFvUBvF1u@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8oFvUBvF1u@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8oFvUBvF1u@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junzhe Zhu" target="_blank">Junzhe Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanchen Ju" target="_blank">Yuanchen Ju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyi Zhang" target="_blank">Junyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhan Wang" target="_blank">Muhan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhecheng Yuan" target="_blank">Zhecheng Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaizhe Hu" target="_blank">Kaizhe Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huazhe Xu" target="_blank">Huazhe Xu</a>
            </p>
            <p id="summary-8oFvUBvF1u@OpenReview" class="summary">Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. In our experiments, we show that DenseMatcher significantly outperforms prior 3D matching baselines by 43.5%. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found at http://densematcher.github.io.</p>
            <p id="subjects-8oFvUBvF1u@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-8oFvUBvF1u@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8oFvUBvF1u@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8oFvUBvF1u@OpenReview" onclick="foldPdfKimi('8oFvUBvF1u@OpenReview', this)" class="hr hr-fold">
        </div><div id="3PDklqqqfN@OpenReview" class="panel paper" keywords="document,retrieval,field,structured,mfar,lexical,multi,adaptive,dense,header">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3PDklqqqfN" target="_blank" title="226/373"><span class="index notranslate">#226</span></a>
                <a id="title-3PDklqqqfN@OpenReview" class="title-link" href="/venue/3PDklqqqfN@OpenReview" target="_blank">Multi-Field Adaptive Retrieval</a>
                <a id="pdf-3PDklqqqfN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3PDklqqqfN@OpenReview', this)" data="https://openreview.net/pdf?id=3PDklqqqfN">[PDF<sup id="pdf-stars-3PDklqqqfN@OpenReview">3</sup>]</a>
                <a id="copy-3PDklqqqfN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3PDklqqqfN@OpenReview')">[Copy]</a>
                <a id="kimi-3PDklqqqfN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3PDklqqqfN@OpenReview', this)">[Kimi<sup id="kimi-stars-3PDklqqqfN@OpenReview">10</sup>]</a>
                <a id="rel-3PDklqqqfN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3PDklqqqfN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3PDklqqqfN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Millicent Li" target="_blank">Millicent Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tongfei Chen" target="_blank">Tongfei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Van Durme" target="_blank">Ben Van Durme</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patrick Xia" target="_blank">Patrick Xia</a>
            </p>
            <p id="summary-3PDklqqqfN@OpenReview" class="summary">Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are _unstructured_: free-form text without explicit internal structure in each document. However, documents can have a structured form, consisting of fields such as an article title, message body, or HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (mFAR), a flexible framework that accommodates any number of and any type of document indices on _structured_ data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighing of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data.</p>
            <p id="subjects-3PDklqqqfN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3PDklqqqfN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3PDklqqqfN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3PDklqqqfN@OpenReview" onclick="foldPdfKimi('3PDklqqqfN@OpenReview', this)" class="hr hr-fold">
        </div><div id="37EXtKCOkn@OpenReview" class="panel paper" keywords="spatiotemporal,observations,neural,point,dynamical,dynamics,oceanic,timings,modeling,crowdsourced">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=37EXtKCOkn" target="_blank" title="227/373"><span class="index notranslate">#227</span></a>
                <a id="title-37EXtKCOkn@OpenReview" class="title-link" href="/venue/37EXtKCOkn@OpenReview" target="_blank">Learning Spatiotemporal Dynamical Systems from Point Process Observations</a>
                <a id="pdf-37EXtKCOkn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('37EXtKCOkn@OpenReview', this)" data="https://openreview.net/pdf?id=37EXtKCOkn">[PDF<sup id="pdf-stars-37EXtKCOkn@OpenReview">3</sup>]</a>
                <a id="copy-37EXtKCOkn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('37EXtKCOkn@OpenReview')">[Copy]</a>
                <a id="kimi-37EXtKCOkn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('37EXtKCOkn@OpenReview', this)">[Kimi<sup id="kimi-stars-37EXtKCOkn@OpenReview">1</sup>]</a>
                <a id="rel-37EXtKCOkn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('37EXtKCOkn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-37EXtKCOkn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Valerii Iakovlev" target="_blank">Valerii Iakovlev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harri Lähdesmäki" target="_blank">Harri Lähdesmäki</a>
            </p>
            <p id="summary-37EXtKCOkn@OpenReview" class="summary">Spatiotemporal dynamics models are fundamental for various domains, from heat propagation in materials to oceanic and atmospheric flows. However, currently available neural network-based spatiotemporal modeling approaches fall short when faced with data that is collected randomly over time and space, as is often the case with sensor networks in real-world applications like crowdsourced earthquake detection or pollution monitoring. In response, we developed a new method that can effectively learn spatiotemporal dynamics from such point process observations. Our model integrates techniques from neural differential equations, neural point processes, implicit neural representations and amortized variational inference to model both the dynamics of the system and the probabilistic locations and timings of observations. It outperforms existing methods on challenging spatiotemporal datasets by offering substantial improvements in predictive accuracy and computational efficiency, making it a useful tool for modeling and understanding complex dynamical systems observed under realistic, unconstrained conditions.</p>
            <p id="subjects-37EXtKCOkn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-37EXtKCOkn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-37EXtKCOkn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-37EXtKCOkn@OpenReview" onclick="foldPdfKimi('37EXtKCOkn@OpenReview', this)" class="hr hr-fold">
        </div><div id="SOWZ59UyNc@OpenReview" class="panel paper" keywords="lean,thoughts,proving,star,tactics,proof,informal,interleave,language,model">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SOWZ59UyNc" target="_blank" title="228/373"><span class="index notranslate">#228</span></a>
                <a id="title-SOWZ59UyNc@OpenReview" class="title-link" href="/venue/SOWZ59UyNc@OpenReview" target="_blank">Lean-STaR: Learning to Interleave Thinking and Proving</a>
                <a id="pdf-SOWZ59UyNc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SOWZ59UyNc@OpenReview', this)" data="https://openreview.net/pdf?id=SOWZ59UyNc">[PDF<sup id="pdf-stars-SOWZ59UyNc@OpenReview">5</sup>]</a>
                <a id="copy-SOWZ59UyNc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SOWZ59UyNc@OpenReview')">[Copy]</a>
                <a id="kimi-SOWZ59UyNc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SOWZ59UyNc@OpenReview', this)">[Kimi<sup id="kimi-stars-SOWZ59UyNc@OpenReview">5</sup>]</a>
                <a id="rel-SOWZ59UyNc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SOWZ59UyNc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SOWZ59UyNc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haohan Lin" target="_blank">Haohan Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqing Sun" target="_blank">Zhiqing Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sean Welleck" target="_blank">Sean Welleck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Yang" target="_blank">Yiming Yang</a>
            </p>
            <p id="summary-SOWZ59UyNc@OpenReview" class="summary">Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems. For instance, humans think through steps of a proof, but this thought process is not visible in the resulting code. We present Lean-STaR, a framework for training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics to generate synthetic thoughts for training the language model. At inference time, the trained model directly generates the thoughts prior to the prediction of the tactics in each proof step. Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver. Lean-STaR significantly outperform base models (43.4% → 46.3%, Pass@64). We also analyze the impact of the augmented thoughts on various aspects of the theorem proving process, providing insights into their effectiveness.</p>
            <p id="subjects-SOWZ59UyNc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SOWZ59UyNc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SOWZ59UyNc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SOWZ59UyNc@OpenReview" onclick="foldPdfKimi('SOWZ59UyNc@OpenReview', this)" class="hr hr-fold">
        </div><div id="kuutidLf6R@OpenReview" class="panel paper" keywords="attribution,diffusion,das,score,sample,training,predicted,models,contribution,loss">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kuutidLf6R" target="_blank" title="229/373"><span class="index notranslate">#229</span></a>
                <a id="title-kuutidLf6R@OpenReview" class="title-link" href="/venue/kuutidLf6R@OpenReview" target="_blank">Diffusion Attribution Score: Which Training Sample Determines Your Generation?</a>
                <a id="pdf-kuutidLf6R@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kuutidLf6R@OpenReview', this)" data="https://openreview.net/pdf?id=kuutidLf6R">[PDF<sup id="pdf-stars-kuutidLf6R@OpenReview">6</sup>]</a>
                <a id="copy-kuutidLf6R@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kuutidLf6R@OpenReview')">[Copy]</a>
                <a id="kimi-kuutidLf6R@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kuutidLf6R@OpenReview', this)">[Kimi<sup id="kimi-stars-kuutidLf6R@OpenReview">4</sup>]</a>
                <a id="rel-kuutidLf6R@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kuutidLf6R@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kuutidLf6R@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinxu Lin" target="_blank">Jinxu Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linwei Tao" target="_blank">Linwei Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minjing Dong" target="_blank">Minjing Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Xu" target="_blank">Chang Xu</a>
            </p>
            <p id="summary-kuutidLf6R@OpenReview" class="summary">As diffusion models advance, the scientific community is actively developing methods to curb the misuse of generative models, which aims to prevent the reproduction of copyrighted, explicitly violent, or personally sensitive information in generated images.One strategy is to identify the contribution of training samples in generative models by evaluating their influence to the generated images, a task known as data attribution.Existing data attribution approaches on diffusion models suggest representing the contribution of a specific training sample by evaluating the change in the diffusion loss when the sample is included versus excluded from the training process.However, we argue that the direct usage of diffusion loss cannot represent such a contribution accurately due to the diffusion loss calculation. Specifically, these approaches measure the divergence between predicted and ground truth distributions, which leads to an indirect comparison between the predicted distributions and cannot represent the variances between model behaviors.To address these issues, we aim to measure the direct comparison between predicted distributions with an attribution score to analyse the training sample importance, which is achieved by Diffusion Attribution Score (DAS).Underpinned by rigorous theoretical analysis, we elucidate the effectiveness of DAS.Additionally, we explore strategies to accelerate DAS calculations, facilitating its application to large-scale diffusion models.Our extensive experiments across various datasets and diffusion models demonstrate that DAS significantly surpasses previous benchmarks in terms of the linear data-modelling score, establishing new state-of-the-art performance.Code is available at https://anonymous.4open.science/r/Diffusion-Attribution-Score-411F.</p>
            <p id="subjects-kuutidLf6R@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-kuutidLf6R@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kuutidLf6R@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kuutidLf6R@OpenReview" onclick="foldPdfKimi('kuutidLf6R@OpenReview', this)" class="hr hr-fold">
        </div><div id="RaR3ETzyKp@OpenReview" class="panel paper" keywords="dansm,path,rectified,inter,distance,preferable,noises,flow,spaces,lengthening">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=RaR3ETzyKp" target="_blank" title="230/373"><span class="index notranslate">#230</span></a>
                <a id="title-RaR3ETzyKp@OpenReview" class="title-link" href="/venue/RaR3ETzyKp@OpenReview" target="_blank">Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance</a>
                <a id="pdf-RaR3ETzyKp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('RaR3ETzyKp@OpenReview', this)" data="https://openreview.net/pdf?id=RaR3ETzyKp">[PDF<sup id="pdf-stars-RaR3ETzyKp@OpenReview">7</sup>]</a>
                <a id="copy-RaR3ETzyKp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('RaR3ETzyKp@OpenReview')">[Copy]</a>
                <a id="kimi-RaR3ETzyKp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('RaR3ETzyKp@OpenReview', this)">[Kimi<sup id="kimi-stars-RaR3ETzyKp@OpenReview">7</sup>]</a>
                <a id="rel-RaR3ETzyKp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('RaR3ETzyKp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-RaR3ETzyKp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=shifeng Xu" target="_blank">shifeng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=yanzhu liu" target="_blank">yanzhu liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adams Kong" target="_blank">Adams Kong</a>
            </p>
            <p id="summary-RaR3ETzyKp@OpenReview" class="summary">Recent research pinpoints that different diffusion methods and architectures trained on the same dataset produce similar results for the same input noise. This property suggests that they have some preferable noises for a given sample. By visualizing the noise-sample pairs of rectified flow models and stable diffusion models in two-dimensional spaces, we observe that the preferable paths, connecting preferable noises to the corresponding samples, are much well organized with significant fewer crossings comparing with the random paths, connecting random noises to training samples. In high-dimensional space, paths rarely intersect. The path crossings in two-dimensional spaces indicate the shorter inter-path distance in the corresponding high-dimensional spaces. Inspired by this observation, we propose the Distance-Aware Noise-Sample Matching (DANSM) method to lengthen the inter-path distance for speeding up the model training. DANSM is derived from rectified flow models, which allow using a closed-form formula to calculate the inter-path distance. To further simplify the optimization, we derive the relationship between inter-path distance and path length, and use the latter in the optimization surrogate. DANSM is evaluated on both image and latent spaces by rectified flow models and diffusion models. The experimental results show that DANSM can significantly improve the training speed by 30\% <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-152-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-876" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-877"><span class="mo" id="MathJax-Span-878" style="font-family: MathJax_Main;">∼</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo></math></span></span><script type="math/tex" id="MathJax-Element-152">\sim</script> 40\%without sacrificing the generation quality.</p>
            <p id="subjects-RaR3ETzyKp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-RaR3ETzyKp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-RaR3ETzyKp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-RaR3ETzyKp@OpenReview" onclick="foldPdfKimi('RaR3ETzyKp@OpenReview', this)" class="hr hr-fold">
        </div><div id="msEr27EejF@OpenReview" class="panel paper" keywords="hacking,reward,proxy,rlhf,definition,rewards,true,policy,proxies,policies">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=msEr27EejF" target="_blank" title="231/373"><span class="index notranslate">#231</span></a>
                <a id="title-msEr27EejF@OpenReview" class="title-link" href="/venue/msEr27EejF@OpenReview" target="_blank">Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking</a>
                <a id="pdf-msEr27EejF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('msEr27EejF@OpenReview', this)" data="https://openreview.net/pdf?id=msEr27EejF">[PDF<sup id="pdf-stars-msEr27EejF@OpenReview">3</sup>]</a>
                <a id="copy-msEr27EejF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('msEr27EejF@OpenReview')">[Copy]</a>
                <a id="kimi-msEr27EejF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('msEr27EejF@OpenReview', this)">[Kimi<sup id="kimi-stars-msEr27EejF@OpenReview">4</sup>]</a>
                <a id="rel-msEr27EejF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('msEr27EejF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-msEr27EejF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cassidy Laidlaw" target="_blank">Cassidy Laidlaw</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shivam Singhal" target="_blank">Shivam Singhal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anca Dragan" target="_blank">Anca Dragan</a>
            </p>
            <p id="summary-msEr27EejF@OpenReview" class="summary">Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using flawed proxy rewards that seem to capture the true objective. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a good proxy and the resulting policy performs poorly with respect to the unspecified true reward. Principled solutions to reward hacking have been impeded by the lack of a good definition for the problem. We introduce a definition of reward hacking based on correlation between proxy and true rewards for states and actions seen by a "base policy" that breaks down under optimization. We show that this definition captures reward hacking behavior across several realistic settings, including in reinforcement learning from human feedback (RLHF). We then show theoretically that regularization to the base policy can effectively prevent reward hacking. Our theory suggests regularizing <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-153-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-879" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.04em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-880"><span class="msubsup" id="MathJax-Span-881"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-882" style="font-family: MathJax_Math-italic;">χ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-883" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>χ</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-153">\chi^2</script> divergence between the policies' occupancy measures, rather than the current practice in RLHF of using a KL penalty between action distributions. We intuitively show why this type of regularization is better, and demonstrate that it outperforms alternatives at mitigating reward hacking in practice across four realistic settings, including RLHF.</p>
            <p id="subjects-msEr27EejF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-msEr27EejF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-msEr27EejF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-msEr27EejF@OpenReview" onclick="foldPdfKimi('msEr27EejF@OpenReview', this)" class="hr hr-fold">
        </div><div id="l2zFn6TIQi@OpenReview" class="panel paper" keywords="t2is,activations,act,steering,transporting,control,induce,activation,language,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=l2zFn6TIQi" target="_blank" title="232/373"><span class="index notranslate">#232</span></a>
                <a id="title-l2zFn6TIQi@OpenReview" class="title-link" href="/venue/l2zFn6TIQi@OpenReview" target="_blank">Controlling Language and Diffusion Models by Transporting Activations</a>
                <a id="pdf-l2zFn6TIQi@OpenReview" class="title-pdf notranslate" onclick="togglePdf('l2zFn6TIQi@OpenReview', this)" data="https://openreview.net/pdf?id=l2zFn6TIQi">[PDF<sup id="pdf-stars-l2zFn6TIQi@OpenReview">2</sup>]</a>
                <a id="copy-l2zFn6TIQi@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('l2zFn6TIQi@OpenReview')">[Copy]</a>
                <a id="kimi-l2zFn6TIQi@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('l2zFn6TIQi@OpenReview', this)">[Kimi<sup id="kimi-stars-l2zFn6TIQi@OpenReview">6</sup>]</a>
                <a id="rel-l2zFn6TIQi@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('l2zFn6TIQi@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-l2zFn6TIQi@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pau Rodriguez" target="_blank">Pau Rodriguez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arno Blaas" target="_blank">Arno Blaas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michal Klein" target="_blank">Michal Klein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Zappella" target="_blank">Luca Zappella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Apostoloff" target="_blank">Nicholas Apostoloff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=marco cuturi" target="_blank">marco cuturi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xavier Suau" target="_blank">Xavier Suau</a>
            </p>
            <p id="summary-l2zFn6TIQi@OpenReview" class="summary">The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output.In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.</p>
            <p id="subjects-l2zFn6TIQi@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-l2zFn6TIQi@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-l2zFn6TIQi@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-l2zFn6TIQi@OpenReview" onclick="foldPdfKimi('l2zFn6TIQi@OpenReview', this)" class="hr hr-fold">
        </div><div id="ws5phQki00@OpenReview" class="panel paper" keywords="stance,discussions,political,synthetic,online,detection,data,deployment,improve,llm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ws5phQki00" target="_blank" title="233/373"><span class="index notranslate">#233</span></a>
                <a id="title-ws5phQki00@OpenReview" class="title-link" href="/venue/ws5phQki00@OpenReview" target="_blank">The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions</a>
                <a id="pdf-ws5phQki00@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ws5phQki00@OpenReview', this)" data="https://openreview.net/pdf?id=ws5phQki00">[PDF<sup id="pdf-stars-ws5phQki00@OpenReview">2</sup>]</a>
                <a id="copy-ws5phQki00@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ws5phQki00@OpenReview')">[Copy]</a>
                <a id="kimi-ws5phQki00@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ws5phQki00@OpenReview', this)">[Kimi<sup id="kimi-stars-ws5phQki00@OpenReview">3</sup>]</a>
                <a id="rel-ws5phQki00@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ws5phQki00@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ws5phQki00@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Sylvius Wagner" target="_blank">Stefan Sylvius Wagner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maike Behrendt" target="_blank">Maike Behrendt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Ziegele" target="_blank">Marc Ziegele</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Harmeling" target="_blank">Stefan Harmeling</a>
            </p>
            <p id="summary-ws5phQki00@OpenReview" class="summary">Stance detection holds great potential to improve online political discussions through its deployment in discussion platforms for purposes such as content moderation, topic summarisation or to facilitate more balanced discussions. Typically, transformer-based models are employed directly for stance detection, requiring vast amounts of data. However, the wide variety of debate topics in online political discussions makes data collection particularly challenging. LLMs have revived stance detection, but their online deployment in online political discussions faces challenges like inconsistent outputs, biases, and vulnerability to adversarial attacks. We show how LLM-generated synthetic data can improve stance detection for online political discussions by using reliable traditional stance detection models for online deployment, while leveraging the text generation capabilities of LLMs for synthetic data generation in a secure offline environment. To achieve this, (i) we generate synthetic data for specific debate questions by prompting a Mistral-7B model and show that fine-tuning with the generated synthetic data can substantially improve the performance of stance detection, while remaining interpretable and aligned with real world data. (ii) Using the synthetic data as a reference, we can improve performance even further by identifying the most informative samples in an unlabelled dataset, i.e., those samples which the stance detection model is most uncertain about and can benefit from the most. By fine-tuning with both synthetic data and the most informative samples, we surpass the performance of the baseline model that is fine-tuned on all true labels, while labelling considerably less data.</p>
            <p id="subjects-ws5phQki00@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ws5phQki00@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ws5phQki00@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ws5phQki00@OpenReview" onclick="foldPdfKimi('ws5phQki00@OpenReview', this)" class="hr hr-fold">
        </div><div id="Njx1NjHIx4@OpenReview" class="panel paper" keywords="crh,representations,neural,pah,alignment,networks,formation,modern,canonical,hypothesis">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Njx1NjHIx4" target="_blank" title="234/373"><span class="index notranslate">#234</span></a>
                <a id="title-Njx1NjHIx4@OpenReview" class="title-link" href="/venue/Njx1NjHIx4@OpenReview" target="_blank">Formation of Representations in Neural Networks</a>
                <a id="pdf-Njx1NjHIx4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Njx1NjHIx4@OpenReview', this)" data="https://openreview.net/pdf?id=Njx1NjHIx4">[PDF<sup id="pdf-stars-Njx1NjHIx4@OpenReview">8</sup>]</a>
                <a id="copy-Njx1NjHIx4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Njx1NjHIx4@OpenReview')">[Copy]</a>
                <a id="kimi-Njx1NjHIx4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Njx1NjHIx4@OpenReview', this)">[Kimi<sup id="kimi-stars-Njx1NjHIx4@OpenReview">4</sup>]</a>
                <a id="rel-Njx1NjHIx4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Njx1NjHIx4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Njx1NjHIx4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liu Ziyin" target="_blank">Liu Ziyin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Isaac Chuang" target="_blank">Isaac Chuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomer Galanti" target="_blank">Tomer Galanti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomaso Poggio" target="_blank">Tomaso Poggio</a>
            </p>
            <p id="summary-Njx1NjHIx4@OpenReview" class="summary">Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory demonstrating that the balance between gradient noise and regularization is crucial for the emergence the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework.</p>
            <p id="subjects-Njx1NjHIx4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Njx1NjHIx4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Njx1NjHIx4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Njx1NjHIx4@OpenReview" onclick="foldPdfKimi('Njx1NjHIx4@OpenReview', this)" class="hr hr-fold">
        </div><div id="U834XHJuqk@OpenReview" class="panel paper" keywords="sequence,domain,monotone,nonlinear,variational,symbolic,representations,recovery,electrocardiograms,sequences">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=U834XHJuqk" target="_blank" title="235/373"><span class="index notranslate">#235</span></a>
                <a id="title-U834XHJuqk@OpenReview" class="title-link" href="/venue/U834XHJuqk@OpenReview" target="_blank">Nonlinear Sequence Embedding by Monotone Variational Inequality</a>
                <a id="pdf-U834XHJuqk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('U834XHJuqk@OpenReview', this)" data="https://openreview.net/pdf?id=U834XHJuqk">[PDF<sup id="pdf-stars-U834XHJuqk@OpenReview">2</sup>]</a>
                <a id="copy-U834XHJuqk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('U834XHJuqk@OpenReview')">[Copy]</a>
                <a id="kimi-U834XHJuqk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('U834XHJuqk@OpenReview', this)">[Kimi<sup id="kimi-stars-U834XHJuqk@OpenReview">1</sup>]</a>
                <a id="rel-U834XHJuqk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('U834XHJuqk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-U834XHJuqk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Zhou" target="_blank">Jonathan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Xie" target="_blank">Yao Xie</a>
            </p>
            <p id="summary-U834XHJuqk@OpenReview" class="summary">In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and time-series data without supervision which has provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method assumes that the observed sequences arise from a common domain, with each sequence following its own autoregressive model, and these models are related through low-rank regularization. We cast the problem as a convex matrix parameter recovery problem using monotone variational inequalities (VIs) and encode the common domain assumption via low-rank constraint across the learned representations, which can learn a subspace approximately spanning the entire domain as well as faithful representations for the dynamics of each individual sequence incorporating the domain information in totality. We show the competitive performance of our method on real-world time-series data with baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering.</p>
            <p id="subjects-U834XHJuqk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-U834XHJuqk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-U834XHJuqk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-U834XHJuqk@OpenReview" onclick="foldPdfKimi('U834XHJuqk@OpenReview', this)" class="hr hr-fold">
        </div><div id="hwSmPOAmhk@OpenReview" class="panel paper" keywords="factual,recall,associative,mlp,memories,transformers,store,facts,count,task">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hwSmPOAmhk" target="_blank" title="236/373"><span class="index notranslate">#236</span></a>
                <a id="title-hwSmPOAmhk@OpenReview" class="title-link" href="/venue/hwSmPOAmhk@OpenReview" target="_blank">Understanding Factual Recall in Transformers via Associative Memories</a>
                <a id="pdf-hwSmPOAmhk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hwSmPOAmhk@OpenReview', this)" data="https://openreview.net/pdf?id=hwSmPOAmhk">[PDF<sup id="pdf-stars-hwSmPOAmhk@OpenReview">4</sup>]</a>
                <a id="copy-hwSmPOAmhk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hwSmPOAmhk@OpenReview')">[Copy]</a>
                <a id="kimi-hwSmPOAmhk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hwSmPOAmhk@OpenReview', this)">[Kimi<sup id="kimi-stars-hwSmPOAmhk@OpenReview">5</sup>]</a>
                <a id="rel-hwSmPOAmhk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hwSmPOAmhk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hwSmPOAmhk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Eshaan Nichani" target="_blank">Eshaan Nichani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Lee" target="_blank">Jason Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alberto Bietti" target="_blank">Alberto Bietti</a>
            </p>
            <p id="summary-hwSmPOAmhk@OpenReview" class="summary">Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100\% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior.</p>
            <p id="subjects-hwSmPOAmhk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hwSmPOAmhk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hwSmPOAmhk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hwSmPOAmhk@OpenReview" onclick="foldPdfKimi('hwSmPOAmhk@OpenReview', this)" class="hr hr-fold">
        </div><div id="vWR3KuiQur@OpenReview" class="panel paper" keywords="outliers,svdquant,activations,rank,branch,bit,quantization,low,weights,models">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vWR3KuiQur" target="_blank" title="237/373"><span class="index notranslate">#237</span></a>
                <a id="title-vWR3KuiQur@OpenReview" class="title-link" href="/venue/vWR3KuiQur@OpenReview" target="_blank">SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models</a>
                <a id="pdf-vWR3KuiQur@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vWR3KuiQur@OpenReview', this)" data="https://openreview.net/pdf?id=vWR3KuiQur">[PDF<sup id="pdf-stars-vWR3KuiQur@OpenReview">6</sup>]</a>
                <a id="copy-vWR3KuiQur@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vWR3KuiQur@OpenReview')">[Copy]</a>
                <a id="kimi-vWR3KuiQur@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vWR3KuiQur@OpenReview', this)">[Kimi<sup id="kimi-stars-vWR3KuiQur@OpenReview">4</sup>]</a>
                <a id="rel-vWR3KuiQur@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vWR3KuiQur@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vWR3KuiQur@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Muyang Li" target="_blank">Muyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujun Lin" target="_blank">Yujun Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhekai Zhang" target="_blank">Zhekai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianle Cai" target="_blank">Tianle Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junxian Guo" target="_blank">Junxian Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiuyu Li" target="_blank">Xiuyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enze Xie" target="_blank">Enze Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenlin Meng" target="_blank">Chenlin Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun-Yan Zhu" target="_blank">Jun-Yan Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Han" target="_blank">Song Han</a>
            </p>
            <p id="summary-vWR3KuiQur@OpenReview" class="summary">Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine LoRunner that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-154-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A3;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-884" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-885"><span class="mi" id="MathJax-Span-886" style="font-family: MathJax_Main;">Σ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Σ</mi></math></span></span><script type="math/tex" id="MathJax-Element-154">\Sigma</script>, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.6×, achieving 3.5× speedup over the 4-bit weight-only quantized baseline on a 16GB RTX-4090 GPU, paving the way for more interactive applications on PCs. We will release the code and models upon publication.</p>
            <p id="subjects-vWR3KuiQur@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-vWR3KuiQur@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vWR3KuiQur@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vWR3KuiQur@OpenReview" onclick="foldPdfKimi('vWR3KuiQur@OpenReview', this)" class="hr hr-fold">
        </div><div id="uvHmnahyp1@OpenReview" class="panel paper" keywords="synflownet,gflownet,molecules,synthesis,buyable,mdp,backward,synthesizability,constraints,generative">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uvHmnahyp1" target="_blank" title="238/373"><span class="index notranslate">#238</span></a>
                <a id="title-uvHmnahyp1@OpenReview" class="title-link" href="/venue/uvHmnahyp1@OpenReview" target="_blank">SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints</a>
                <a id="pdf-uvHmnahyp1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uvHmnahyp1@OpenReview', this)" data="https://openreview.net/pdf?id=uvHmnahyp1">[PDF<sup id="pdf-stars-uvHmnahyp1@OpenReview">4</sup>]</a>
                <a id="copy-uvHmnahyp1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uvHmnahyp1@OpenReview')">[Copy]</a>
                <a id="kimi-uvHmnahyp1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uvHmnahyp1@OpenReview', this)">[Kimi<sup id="kimi-stars-uvHmnahyp1@OpenReview">3</sup>]</a>
                <a id="rel-uvHmnahyp1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uvHmnahyp1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uvHmnahyp1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Miruna Cretu" target="_blank">Miruna Cretu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Harris" target="_blank">Charles Harris</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ilia Igashov" target="_blank">Ilia Igashov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arne Schneuing" target="_blank">Arne Schneuing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marwin Segler" target="_blank">Marwin Segler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bruno Correia" target="_blank">Bruno Correia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julien Roy" target="_blank">Julien Roy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emmanuel Bengio" target="_blank">Emmanuel Bengio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pietro Lio" target="_blank">Pietro Lio</a>
            </p>
            <p id="summary-uvHmnahyp1@OpenReview" class="summary">Generative models see increasing use in computer-aided drug design. However, while performing well at capturing distributions of molecular motifs, they often produce synthetically inaccessible molecules. To address this, we introduce SynFlowNet, a GFlowNet model whose action space uses chemical reactions and buyable reactants to sequentially build new molecules. By incorporating forward synthesis as an explicit constraint of the generative mechanism, we aim at bridging the gap between in silico molecular generation and real world synthesis capabilities. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool to assess the synthesizability of our compounds, and motivate the choice of GFlowNets through considerable improvement in sample diversity compared to baselines. Additionally, we identify challenges with reaction encodings that can complicate traversal of the MDP in the backward direction. To address this, we introduce various strategies for learning the GFlowNet backward policy and thus demonstrate how additional constraints can be integrated into the GFlowNet MDP framework. This approach enables our model to successfully identify synthesis pathways for previously unseen molecules.</p>
            <p id="subjects-uvHmnahyp1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-uvHmnahyp1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uvHmnahyp1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uvHmnahyp1@OpenReview" onclick="foldPdfKimi('uvHmnahyp1@OpenReview', this)" class="hr hr-fold">
        </div><div id="uuriavczkL@OpenReview" class="panel paper" keywords="counterfactual,realizability,interventional,pearl,causal,motivating,believed,bareinboim,forney,observational">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uuriavczkL" target="_blank" title="239/373"><span class="index notranslate">#239</span></a>
                <a id="title-uuriavczkL@OpenReview" class="title-link" href="/venue/uuriavczkL@OpenReview" target="_blank">Counterfactual Realizability</a>
                <a id="pdf-uuriavczkL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uuriavczkL@OpenReview', this)" data="https://openreview.net/pdf?id=uuriavczkL">[PDF<sup id="pdf-stars-uuriavczkL@OpenReview">4</sup>]</a>
                <a id="copy-uuriavczkL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uuriavczkL@OpenReview')">[Copy]</a>
                <a id="kimi-uuriavczkL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uuriavczkL@OpenReview', this)">[Kimi<sup id="kimi-stars-uuriavczkL@OpenReview">5</sup>]</a>
                <a id="rel-uuriavczkL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uuriavczkL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uuriavczkL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Arvind Raghavan" target="_blank">Arvind Raghavan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elias Bareinboim" target="_blank">Elias Bareinboim</a>
            </p>
            <p id="summary-uuriavczkL@OpenReview" class="summary">It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the Pearl Causal Hierarchy. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of *realizability*, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both.</p>
            <p id="subjects-uuriavczkL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-uuriavczkL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uuriavczkL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uuriavczkL@OpenReview" onclick="foldPdfKimi('uuriavczkL@OpenReview', this)" class="hr hr-fold">
        </div><div id="uSz2K30RRd@OpenReview" class="panel paper" keywords="similarity,contrastive,infonce,weighted,multimodal,onepoint,clip,point,optimal,representation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uSz2K30RRd" target="_blank" title="240/373"><span class="index notranslate">#240</span></a>
                <a id="title-uSz2K30RRd@OpenReview" class="title-link" href="/venue/uSz2K30RRd@OpenReview" target="_blank">Weighted Point Cloud Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric</a>
                <a id="pdf-uSz2K30RRd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uSz2K30RRd@OpenReview', this)" data="https://openreview.net/pdf?id=uSz2K30RRd">[PDF<sup id="pdf-stars-uSz2K30RRd@OpenReview">4</sup>]</a>
                <a id="copy-uSz2K30RRd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uSz2K30RRd@OpenReview')">[Copy]</a>
                <a id="kimi-uSz2K30RRd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uSz2K30RRd@OpenReview', this)">[Kimi<sup id="kimi-stars-uSz2K30RRd@OpenReview">6</sup>]</a>
                <a id="rel-uSz2K30RRd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uSz2K30RRd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uSz2K30RRd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Toshimitsu Uesaka" target="_blank">Toshimitsu Uesaka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taiji Suzuki" target="_blank">Taiji Suzuki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhta Takida" target="_blank">Yuhta Takida</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chieh-Hsin Lai" target="_blank">Chieh-Hsin Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naoki Murata" target="_blank">Naoki Murata</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuki Mitsufuji" target="_blank">Yuki Mitsufuji</a>
            </p>
            <p id="summary-uSz2K30RRd@OpenReview" class="summary">In typical multimodal contrastive learning, such as CLIP, encoders produce onepoint in the latent representation space for each input. However, one-point representation has difficulty in capturing the relationship and the similarity structure of a huge amount of instances in the real world. For richer classes of the similarity, we propose the use of weighted point clouds, namely, sets of pairs of weight and vector, as representations of instances. In this work, we theoretically show the benefit of our proposed method through a new understanding of the contrastive loss of CLIP, which we call symmetric InfoNCE. We clarify that the optimal similaritythat minimizes symmetric InfoNCE is the pointwise mutual information, and show an upper bound of excess risk on downstream classification tasks of representations that achieve the optimal similarity. In addition, we show that our proposed similarity based on weighted point clouds consistently achieves the optimal similarity. To verify the effectiveness of our proposed method, we demonstrate pretraining of text-image representation models and classification tasks on common benchmarks.</p>
            <p id="subjects-uSz2K30RRd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-uSz2K30RRd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uSz2K30RRd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uSz2K30RRd@OpenReview" onclick="foldPdfKimi('uSz2K30RRd@OpenReview', this)" class="hr hr-fold">
        </div><div id="r3DF5sOo5B@OpenReview" class="panel paper" keywords="cot,prompting,transformers,descent,thought,transformer,step,implement,chain,learn">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=r3DF5sOo5B" target="_blank" title="241/373"><span class="index notranslate">#241</span></a>
                <a id="title-r3DF5sOo5B@OpenReview" class="title-link" href="/venue/r3DF5sOo5B@OpenReview" target="_blank">Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought</a>
                <a id="pdf-r3DF5sOo5B@OpenReview" class="title-pdf notranslate" onclick="togglePdf('r3DF5sOo5B@OpenReview', this)" data="https://openreview.net/pdf?id=r3DF5sOo5B">[PDF<sup id="pdf-stars-r3DF5sOo5B@OpenReview">8</sup>]</a>
                <a id="copy-r3DF5sOo5B@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('r3DF5sOo5B@OpenReview')">[Copy]</a>
                <a id="kimi-r3DF5sOo5B@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('r3DF5sOo5B@OpenReview', this)">[Kimi<sup id="kimi-stars-r3DF5sOo5B@OpenReview">6</sup>]</a>
                <a id="rel-r3DF5sOo5B@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('r3DF5sOo5B@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-r3DF5sOo5B@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianhao Huang" target="_blank">Jianhao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Wang" target="_blank">Zixuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Lee" target="_blank">Jason Lee</a>
            </p>
            <p id="summary-r3DF5sOo5B@OpenReview" class="summary">Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms underlying CoT training remain largely unexplored. In this paper, we study the training dynamics of transformers over a CoT objective on a in-context weight prediction task for linear regression. We prove that while a one-layer linear transformer without CoT can only implement a single step of gradient descent (GD) and fails to recover the ground-truth weight vector, a transformer with CoT prompting can learn to perform multi-step GD autoregressively, achieving near-exact recovery. Furthermore, we show that the trained transformer effectively generalizes on the unseen data. Empirically, we demonstrate that CoT prompting yields substantial performance improvements.</p>
            <p id="subjects-r3DF5sOo5B@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-r3DF5sOo5B@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-r3DF5sOo5B@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-r3DF5sOo5B@OpenReview" onclick="foldPdfKimi('r3DF5sOo5B@OpenReview', this)" class="hr hr-fold">
        </div><div id="pZISppZSTv@OpenReview" class="panel paper" keywords="closd,diffusion,motion,controller,dip,control,planner,loop,strengths,text">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pZISppZSTv" target="_blank" title="242/373"><span class="index notranslate">#242</span></a>
                <a id="title-pZISppZSTv@OpenReview" class="title-link" href="/venue/pZISppZSTv@OpenReview" target="_blank">CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control</a>
                <a id="pdf-pZISppZSTv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pZISppZSTv@OpenReview', this)" data="https://openreview.net/pdf?id=pZISppZSTv">[PDF<sup id="pdf-stars-pZISppZSTv@OpenReview">5</sup>]</a>
                <a id="copy-pZISppZSTv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pZISppZSTv@OpenReview')">[Copy]</a>
                <a id="kimi-pZISppZSTv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pZISppZSTv@OpenReview', this)">[Kimi<sup id="kimi-stars-pZISppZSTv@OpenReview">2</sup>]</a>
                <a id="rel-pZISppZSTv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pZISppZSTv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pZISppZSTv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guy Tevet" target="_blank">Guy Tevet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sigal Raab" target="_blank">Sigal Raab</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Setareh Cohan" target="_blank">Setareh Cohan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniele Reda" target="_blank">Daniele Reda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyi Luo" target="_blank">Zhengyi Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xue Bin Peng" target="_blank">Xue Bin Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Bermano" target="_blank">Amit Bermano</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michiel van de Panne" target="_blank">Michiel van de Panne</a>
            </p>
            <p id="summary-pZISppZSTv@OpenReview" class="summary">Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically plausible motion and direct interaction with the environment. In this work, we present a method that combines their respective strengths. CLoSD is a text-driven RL physics-based controller, guided by diffusion generation for various tasks. Our key insight is that motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. To this end, CLoSD maintains a closed-loop interaction between two modules — a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up.</p>
            <p id="subjects-pZISppZSTv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pZISppZSTv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pZISppZSTv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pZISppZSTv@OpenReview" onclick="foldPdfKimi('pZISppZSTv@OpenReview', this)" class="hr hr-fold">
        </div><div id="pHOH8FVrTp@OpenReview" class="panel paper" keywords="smalltalk,mixture,asynchronous,language,inference,talk,need,eachmodel,directs,dense">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pHOH8FVrTp" target="_blank" title="243/373"><span class="index notranslate">#243</span></a>
                <a id="title-pHOH8FVrTp@OpenReview" class="title-link" href="/venue/pHOH8FVrTp@OpenReview" target="_blank">No Need to Talk: Asynchronous Mixture of Language Models</a>
                <a id="pdf-pHOH8FVrTp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pHOH8FVrTp@OpenReview', this)" data="https://openreview.net/pdf?id=pHOH8FVrTp">[PDF<sup id="pdf-stars-pHOH8FVrTp@OpenReview">2</sup>]</a>
                <a id="copy-pHOH8FVrTp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pHOH8FVrTp@OpenReview')">[Copy]</a>
                <a id="kimi-pHOH8FVrTp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pHOH8FVrTp@OpenReview', this)">[Kimi<sup id="kimi-stars-pHOH8FVrTp@OpenReview">5</sup>]</a>
                <a id="rel-pHOH8FVrTp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pHOH8FVrTp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pHOH8FVrTp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anastasiia Filippova" target="_blank">Anastasiia Filippova</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angelos Katharopoulos" target="_blank">Angelos Katharopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Grangier" target="_blank">David Grangier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronan Collobert" target="_blank">Ronan Collobert</a>
            </p>
            <p id="summary-pHOH8FVrTp@OpenReview" class="summary">We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Eachmodel of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks.</p>
            <p id="subjects-pHOH8FVrTp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pHOH8FVrTp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pHOH8FVrTp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pHOH8FVrTp@OpenReview" onclick="foldPdfKimi('pHOH8FVrTp@OpenReview', this)" class="hr hr-fold">
        </div><div id="lvw3UgeVxS@OpenReview" class="panel paper" keywords="grnade,rna,rosetta,design,backbones,sequences,geometric,inverse,backbone,ribozyme">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=lvw3UgeVxS" target="_blank" title="244/373"><span class="index notranslate">#244</span></a>
                <a id="title-lvw3UgeVxS@OpenReview" class="title-link" href="/venue/lvw3UgeVxS@OpenReview" target="_blank">gRNAde: Geometric Deep Learning for 3D RNA inverse design</a>
                <a id="pdf-lvw3UgeVxS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('lvw3UgeVxS@OpenReview', this)" data="https://openreview.net/pdf?id=lvw3UgeVxS">[PDF<sup id="pdf-stars-lvw3UgeVxS@OpenReview">3</sup>]</a>
                <a id="copy-lvw3UgeVxS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('lvw3UgeVxS@OpenReview')">[Copy]</a>
                <a id="kimi-lvw3UgeVxS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('lvw3UgeVxS@OpenReview', this)">[Kimi<sup id="kimi-stars-lvw3UgeVxS@OpenReview">2</sup>]</a>
                <a id="rel-lvw3UgeVxS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('lvw3UgeVxS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-lvw3UgeVxS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chaitanya Joshi" target="_blank">Chaitanya Joshi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arian Jamasb" target="_blank">Arian Jamasb</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ramon Viñas" target="_blank">Ramon Viñas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Harris" target="_blank">Charles Harris</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Mathis" target="_blank">Simon Mathis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Morehead" target="_blank">Alex Morehead</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Anand" target="_blank">Rishabh Anand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pietro Lio" target="_blank">Pietro Lio</a>
            </p>
            <p id="summary-lvw3UgeVxS@OpenReview" class="summary">Computational RNA design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired secondary structure without considering 3D conformational diversity. We introduce gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design sequences that explicitly account for structure and dynamics. gRNAde uses a multi-state Graph Neural Network and autoregressive decoding to generates candidate RNA sequences conditioned on one or more 3D backbone structures where the identities of the bases are unknown. On a single-state fixed backbone re-design benchmark of 14 RNA structures from the PDB identified by Das et al. (2010), gRNAde obtains higher native sequence recovery rates (56% on average) compared to Rosetta (45% on average), taking under a second to produce designs compared to the reported hours for Rosetta. We further demonstrate the utility of gRNAde on a new benchmark of multi-state design for structurally flexible RNAs, as well as zero-shot ranking of mutational fitness landscapes in a retrospective analysis of a recent ribozyme. Experimental wet lab validation on 10 different structured RNA backbones finds that gRNAde has an impressive success rate of 50%, a significant advance over 35% for Rosetta. Open source code and tutorials are available at: https://anonymous.4open.science/r/geometric-rna-design</p>
            <p id="subjects-lvw3UgeVxS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-lvw3UgeVxS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-lvw3UgeVxS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-lvw3UgeVxS@OpenReview" onclick="foldPdfKimi('lvw3UgeVxS@OpenReview', this)" class="hr hr-fold">
        </div><div id="lXRDQsiP2v@OpenReview" class="panel paper" keywords="attention,transformer,tssa,texttt,mcr,token,architecture,tokens,statistics,variational">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=lXRDQsiP2v" target="_blank" title="245/373"><span class="index notranslate">#245</span></a>
                <a id="title-lXRDQsiP2v@OpenReview" class="title-link" href="/venue/lXRDQsiP2v@OpenReview" target="_blank">Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction</a>
                <a id="pdf-lXRDQsiP2v@OpenReview" class="title-pdf notranslate" onclick="togglePdf('lXRDQsiP2v@OpenReview', this)" data="https://openreview.net/pdf?id=lXRDQsiP2v">[PDF<sup id="pdf-stars-lXRDQsiP2v@OpenReview">7</sup>]</a>
                <a id="copy-lXRDQsiP2v@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('lXRDQsiP2v@OpenReview')">[Copy]</a>
                <a id="kimi-lXRDQsiP2v@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('lXRDQsiP2v@OpenReview', this)">[Kimi<sup id="kimi-stars-lXRDQsiP2v@OpenReview">3</sup>]</a>
                <a id="rel-lXRDQsiP2v@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('lXRDQsiP2v@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-lXRDQsiP2v@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyang Wu" target="_blank">Ziyang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianjiao Ding" target="_blank">Tianjiao Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifu Lu" target="_blank">Yifu Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Druv Pai" target="_blank">Druv Pai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyuan Zhang" target="_blank">Jingyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weida Wang" target="_blank">Weida Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaodong Yu" target="_blank">Yaodong Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Ma" target="_blank">Yi Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Haeffele" target="_blank">Benjamin Haeffele</a>
            </p>
            <p id="summary-lXRDQsiP2v@OpenReview" class="summary">The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by "white-box" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-155-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-887" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-888"><span class="msubsup" id="MathJax-Span-889"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-890"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="mn" id="MathJax-Span-891" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-155">^2</script>). Specifically, we derive a novel variational form of the MCR<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-156-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-892" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-893"><span class="msubsup" id="MathJax-Span-894"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-895"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="mn" id="MathJax-Span-896" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-156">^2</script> objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-157-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;TSSA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-897" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.03em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-898"><span class="texatom" id="MathJax-Span-899"><span class="mrow" id="MathJax-Span-900"><span class="mtext" id="MathJax-Span-901" style="font-family: MathJax_Typewriter;">TSSA</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">TSSA</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-157">\texttt{TSSA}</script>). <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-158-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;TSSA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-902" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.03em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-903"><span class="texatom" id="MathJax-Span-904"><span class="mrow" id="MathJax-Span-905"><span class="mtext" id="MathJax-Span-906" style="font-family: MathJax_Typewriter;">TSSA</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">TSSA</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-158">\texttt{TSSA}</script> has <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-159-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;linear computational and memory complexity&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-907" style="width: 23.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.378em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1019.43em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-908"><span class="texatom" id="MathJax-Span-909"><span class="mrow" id="MathJax-Span-910"><span class="mtext" id="MathJax-Span-911" style="font-family: MathJax_Main-italic;">linear computational and memory complexity</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">linear computational and memory complexity</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-159">\textit{linear computational and memory complexity}</script> and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-160-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;TSSA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-912" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.03em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-913"><span class="texatom" id="MathJax-Span-914"><span class="mrow" id="MathJax-Span-915"><span class="mtext" id="MathJax-Span-916" style="font-family: MathJax_Typewriter;">TSSA</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">TSSA</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-160">\texttt{TSSA}</script> for standard self-attention, which we refer to as the Token Statistics Transformer (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-161-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;ToST&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-917" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.03em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-918"><span class="texatom" id="MathJax-Span-919"><span class="mrow" id="MathJax-Span-920"><span class="mtext" id="MathJax-Span-921" style="font-family: MathJax_Typewriter;">ToST</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">ToST</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-161">\texttt{ToST}</script>), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures.</p>
            <p id="subjects-lXRDQsiP2v@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-lXRDQsiP2v@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-lXRDQsiP2v@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-lXRDQsiP2v@OpenReview" onclick="foldPdfKimi('lXRDQsiP2v@OpenReview', this)" class="hr hr-fold">
        </div><div id="lJpqxFgWCM@OpenReview" class="panel paper" keywords="monst3r,dust3r,scenes,geometry,timestep,estimating,dynamic,depth,pointmap,motion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=lJpqxFgWCM" target="_blank" title="246/373"><span class="index notranslate">#246</span></a>
                <a id="title-lJpqxFgWCM@OpenReview" class="title-link" href="/venue/lJpqxFgWCM@OpenReview" target="_blank">MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion</a>
                <a id="pdf-lJpqxFgWCM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('lJpqxFgWCM@OpenReview', this)" data="https://openreview.net/pdf?id=lJpqxFgWCM">[PDF<sup id="pdf-stars-lJpqxFgWCM@OpenReview">3</sup>]</a>
                <a id="copy-lJpqxFgWCM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('lJpqxFgWCM@OpenReview')">[Copy]</a>
                <a id="kimi-lJpqxFgWCM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('lJpqxFgWCM@OpenReview', this)">[Kimi<sup id="kimi-stars-lJpqxFgWCM@OpenReview">2</sup>]</a>
                <a id="rel-lJpqxFgWCM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('lJpqxFgWCM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-lJpqxFgWCM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junyi Zhang" target="_blank">Junyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Herrmann" target="_blank">Charles Herrmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junhwa Hur" target="_blank">Junhwa Hur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Varun Jampani" target="_blank">Varun Jampani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=trevor darrell" target="_blank">trevor darrell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Forrester Cole" target="_blank">Forrester Cole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deqing Sun" target="_blank">Deqing Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Hsuan Yang" target="_blank">Ming-Hsuan Yang</a>
            </p>
            <p id="summary-lJpqxFgWCM@OpenReview" class="summary">Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DuSt3R (MonST3R), a novel geometry-first approachthat directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUSt3R’s representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction. Interactive 4D results are available at: https://monst3r-paper.github.io/</p>
            <p id="subjects-lJpqxFgWCM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-lJpqxFgWCM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-lJpqxFgWCM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-lJpqxFgWCM@OpenReview" onclick="foldPdfKimi('lJpqxFgWCM@OpenReview', this)" class="hr hr-fold">
        </div><div id="k03mB41vyM@OpenReview" class="panel paper" keywords="causal,exchangeable,iem,identifiable,representation,identifiability,structure,mechanisms,learning,finetti">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=k03mB41vyM" target="_blank" title="247/373"><span class="index notranslate">#247</span></a>
                <a id="title-k03mB41vyM@OpenReview" class="title-link" href="/venue/k03mB41vyM@OpenReview" target="_blank">Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning</a>
                <a id="pdf-k03mB41vyM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('k03mB41vyM@OpenReview', this)" data="https://openreview.net/pdf?id=k03mB41vyM">[PDF<sup id="pdf-stars-k03mB41vyM@OpenReview">4</sup>]</a>
                <a id="copy-k03mB41vyM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('k03mB41vyM@OpenReview')">[Copy]</a>
                <a id="kimi-k03mB41vyM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('k03mB41vyM@OpenReview', this)">[Kimi<sup id="kimi-stars-k03mB41vyM@OpenReview">1</sup>]</a>
                <a id="rel-k03mB41vyM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('k03mB41vyM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-k03mB41vyM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Patrik Reizinger" target="_blank">Patrik Reizinger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Guo" target="_blank">Siyuan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ferenc Huszar" target="_blank">Ferenc Huszar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernhard Schoelkopf" target="_blank">Bernhard Schoelkopf</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wieland Brendel" target="_blank">Wieland Brendel</a>
            </p>
            <p id="summary-k03mB41vyM@OpenReview" class="summary">Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields developed rather independently.We observe that several structure and representation identifiability methods, particularly those that require multiple environments, rely on exchangeable non--i.i.d. (independent and identically distributed) data.To formalize this connection, we propose the Identifiable Exchangeable Mechanisms (IEM) framework to unify key representation and causal structure learning methods. IEM provides a unified probabilistic graphical model encompassing causal discovery, Independent Component Analysis, and Causal Representation Learning.With the help of the IEM model, we generalize the Causal de Finetti theorem of Guo et al., 2022 by relaxing the necessary conditions for causal structure identification in exchangeable data.We term these conditions cause and mechanism variability, and show how they imply a duality condition in identifiable representation learning, leading to new identifiability results.</p>
            <p id="subjects-k03mB41vyM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-k03mB41vyM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-k03mB41vyM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-k03mB41vyM@OpenReview" onclick="foldPdfKimi('k03mB41vyM@OpenReview', this)" class="hr hr-fold">
        </div><div id="hXm0Wu2U9K@OpenReview" class="panel paper" keywords="overoptimization,chi,alignment,offline,regularization,mythos,preference,provably,dpo,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hXm0Wu2U9K" target="_blank" title="248/373"><span class="index notranslate">#248</span></a>
                <a id="title-hXm0Wu2U9K@OpenReview" class="title-link" href="/venue/hXm0Wu2U9K@OpenReview" target="_blank">Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization</a>
                <a id="pdf-hXm0Wu2U9K@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hXm0Wu2U9K@OpenReview', this)" data="https://openreview.net/pdf?id=hXm0Wu2U9K">[PDF<sup id="pdf-stars-hXm0Wu2U9K@OpenReview">3</sup>]</a>
                <a id="copy-hXm0Wu2U9K@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hXm0Wu2U9K@OpenReview')">[Copy]</a>
                <a id="kimi-hXm0Wu2U9K@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hXm0Wu2U9K@OpenReview', this)">[Kimi<sup id="kimi-stars-hXm0Wu2U9K@OpenReview">1</sup>]</a>
                <a id="rel-hXm0Wu2U9K@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hXm0Wu2U9K@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hXm0Wu2U9K@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Audrey Huang" target="_blank">Audrey Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Zhan" target="_blank">Wenhao Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tengyang Xie" target="_blank">Tengyang Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Lee" target="_blank">Jason Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen Sun" target="_blank">Wen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akshay Krishnamurthy" target="_blank">Akshay Krishnamurthy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dylan Foster" target="_blank">Dylan Foster</a>
            </p>
            <p id="summary-hXm0Wu2U9K@OpenReview" class="summary">Language model alignment methods, such as reinforcement learning from human feedback (RLHF), haveled to impressive advances in language model capabilities. However, existing techniques are limited by a widely observed phenomenon known as *overoptimization*, where the quality of the language model degrades over the course of the alignment process. Overoptimization occurs when a language model overfits to inaccuracies in an (either explicit or implicit) offline reward model, and drifts away from preferred responses covered by the data. To discourage such distribution shift, offline alignment methods typically employ KL-regularization, but this, as we show, is too weak to prevent degradation in performance. Then, can we design an efficient algorithm that is provably robust to overoptimization?In this paper, we advance theoretical understanding of sample-efficient offline alignment and introduce a new algorithm called <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-162-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-922" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.04em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-923"><span class="msubsup" id="MathJax-Span-924"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-925" style="font-family: MathJax_Math-italic;">χ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-926" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>χ</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-162">\chi^2</script>-Preference Optimization (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-163-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-927" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-928"><span class="mi" id="MathJax-Span-929" style="font-family: MathJax_Math-italic;">χ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>χ</mi></math></span></span><script type="math/tex" id="MathJax-Element-163">\chi</script>PO). <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-164-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-930" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-931"><span class="mi" id="MathJax-Span-932" style="font-family: MathJax_Math-italic;">χ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>χ</mi></math></span></span><script type="math/tex" id="MathJax-Element-164">\chi</script>PO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al. 2023), that modifies only the logarithmic link function in the DPO objective. Despite this minimal change, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-165-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-933" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-934"><span class="mi" id="MathJax-Span-935" style="font-family: MathJax_Math-italic;">χ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>χ</mi></math></span></span><script type="math/tex" id="MathJax-Element-165">\chi</script>PO implicitly implements the principle of *pessimism in the face of uncertainty* via regularization with the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-166-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-936" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.04em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-937"><span class="msubsup" id="MathJax-Span-938"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-939" style="font-family: MathJax_Math-italic;">χ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-940" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>χ</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-166">\chi^2</script>-divergence---which quantifies uncertainty more effectively than KL-regularization---and provably alleviates overoptimization, achieving sample-complexity guarantees based on *single-policy concentrability*---the gold standard in offline reinforcement learning. This guarantee makes <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-167-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-941" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-942"><span class="mi" id="MathJax-Span-943" style="font-family: MathJax_Math-italic;">χ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>χ</mi></math></span></span><script type="math/tex" id="MathJax-Element-167">\chi</script>PO the first simple, yet general-purpose offline alignment algorithm that is provably robust to overoptimization.</p>
            <p id="subjects-hXm0Wu2U9K@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hXm0Wu2U9K@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hXm0Wu2U9K@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hXm0Wu2U9K@OpenReview" onclick="foldPdfKimi('hXm0Wu2U9K@OpenReview', this)" class="hr hr-fold">
        </div><div id="gTwRMU3lJ5@OpenReview" class="panel paper" keywords="lora,rank,tuning,fine,pro,low,full,gradients,gradient,narrowing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gTwRMU3lJ5" target="_blank" title="249/373"><span class="index notranslate">#249</span></a>
                <a id="title-gTwRMU3lJ5@OpenReview" class="title-link" href="/venue/gTwRMU3lJ5@OpenReview" target="_blank">LoRA-Pro: Are Low-Rank Adapters Properly Optimized?</a>
                <a id="pdf-gTwRMU3lJ5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gTwRMU3lJ5@OpenReview', this)" data="https://openreview.net/pdf?id=gTwRMU3lJ5">[PDF<sup id="pdf-stars-gTwRMU3lJ5@OpenReview">7</sup>]</a>
                <a id="copy-gTwRMU3lJ5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gTwRMU3lJ5@OpenReview')">[Copy]</a>
                <a id="kimi-gTwRMU3lJ5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gTwRMU3lJ5@OpenReview', this)">[Kimi<sup id="kimi-stars-gTwRMU3lJ5@OpenReview">10</sup>]</a>
                <a id="rel-gTwRMU3lJ5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gTwRMU3lJ5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gTwRMU3lJ5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengbo Wang" target="_blank">Zhengbo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Liang" target="_blank">Jian Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran He" target="_blank">Ran He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zilei Wang" target="_blank">Zilei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tieniu Tan" target="_blank">Tieniu Tan</a>
            </p>
            <p id="summary-gTwRMU3lJ5@OpenReview" class="summary">Low-rank adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient fine-tuning of foundation models.Despite its computational efficiency, LoRA still yields inferior performance compared to full fine-tuning.In this paper, we first uncover a fundamental connection between the optimization processes of LoRA and full fine-tuning: using LoRA for optimization is mathematically equivalent to full fine-tuning using a low-rank gradient for parameter updates.And this low-rank gradient can be expressed in terms of the gradients of the two low-rank matrices in LoRA.Leveraging this insight, we introduce LoRA-Pro, a method that enhances LoRA's performance by strategically adjusting the gradients of these low-rank matrices.This adjustment allows the low-rank gradient to more accurately approximate the full fine-tuning gradient, thereby narrowing the performance gap between LoRA and full fine-tuning.Furthermore, we theoretically derive the optimal solutions for adjusting the gradients of the low-rank matrices, applying them during fine-tuning in LoRA-Pro.We conduct extensive experiments across natural language understanding, dialogue generation, mathematical reasoning, code generation, and image classification tasks, demonstrating that LoRA-Pro substantially improves LoRA's performance, effectively narrowing the gap with full fine-tuning.Code is available in the supplementary.</p>
            <p id="subjects-gTwRMU3lJ5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gTwRMU3lJ5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gTwRMU3lJ5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gTwRMU3lJ5@OpenReview" onclick="foldPdfKimi('gTwRMU3lJ5@OpenReview', this)" class="hr hr-fold">
        </div><div id="gI0kPklUKS@OpenReview" class="panel paper" keywords="mlps,bilinear,interpretability,mlp,mechanistic,weights,weight,understanding,interpretable,glu">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gI0kPklUKS" target="_blank" title="250/373"><span class="index notranslate">#250</span></a>
                <a id="title-gI0kPklUKS@OpenReview" class="title-link" href="/venue/gI0kPklUKS@OpenReview" target="_blank">Bilinear MLPs enable weight-based mechanistic interpretability</a>
                <a id="pdf-gI0kPklUKS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gI0kPklUKS@OpenReview', this)" data="https://openreview.net/pdf?id=gI0kPklUKS">[PDF<sup id="pdf-stars-gI0kPklUKS@OpenReview">4</sup>]</a>
                <a id="copy-gI0kPklUKS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gI0kPklUKS@OpenReview')">[Copy]</a>
                <a id="kimi-gI0kPklUKS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gI0kPklUKS@OpenReview', this)">[Kimi<sup id="kimi-stars-gI0kPklUKS@OpenReview">2</sup>]</a>
                <a id="rel-gI0kPklUKS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gI0kPklUKS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gI0kPklUKS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Pearce" target="_blank">Michael Pearce</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Dooms" target="_blank">Thomas Dooms</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alice Rigg" target="_blank">Alice Rigg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jose Oramas" target="_blank">Jose Oramas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lee Sharkey" target="_blank">Lee Sharkey</a>
            </p>
            <p id="summary-gI0kPklUKS@OpenReview" class="summary">A mechanistic understanding of how MLPs do computation in deep neural networks remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that nevertheless achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecomposition reveals interpretable low-rank structure across toy tasks, image classification, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models.</p>
            <p id="subjects-gI0kPklUKS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gI0kPklUKS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gI0kPklUKS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gI0kPklUKS@OpenReview" onclick="foldPdfKimi('gI0kPklUKS@OpenReview', this)" class="hr hr-fold">
        </div><div id="fZK6AQXlUU@OpenReview" class="panel paper" keywords="disparate,sets,prediction,equalized,coverage,impact,disquietingly,conformal,fairness,uncertainty">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fZK6AQXlUU" target="_blank" title="251/373"><span class="index notranslate">#251</span></a>
                <a id="title-fZK6AQXlUU@OpenReview" class="title-link" href="/venue/fZK6AQXlUU@OpenReview" target="_blank">Conformal Prediction Sets Can Cause Disparate Impact</a>
                <a id="pdf-fZK6AQXlUU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fZK6AQXlUU@OpenReview', this)" data="https://openreview.net/pdf?id=fZK6AQXlUU">[PDF<sup id="pdf-stars-fZK6AQXlUU@OpenReview">4</sup>]</a>
                <a id="copy-fZK6AQXlUU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fZK6AQXlUU@OpenReview')">[Copy]</a>
                <a id="kimi-fZK6AQXlUU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fZK6AQXlUU@OpenReview', this)">[Kimi<sup id="kimi-stars-fZK6AQXlUU@OpenReview">3</sup>]</a>
                <a id="rel-fZK6AQXlUU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fZK6AQXlUU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fZK6AQXlUU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jesse Cresswell" target="_blank">Jesse Cresswell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bhargava Kumar" target="_blank">Bhargava Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Sui" target="_blank">Yi Sui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mouloud Belbahri" target="_blank">Mouloud Belbahri</a>
            </p>
            <p id="summary-fZK6AQXlUU@OpenReview" class="summary">Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact.</p>
            <p id="subjects-fZK6AQXlUU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-fZK6AQXlUU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fZK6AQXlUU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fZK6AQXlUU@OpenReview" onclick="foldPdfKimi('fZK6AQXlUU@OpenReview', this)" class="hr hr-fold">
        </div><div id="f7KxfUrRSb@OpenReview" class="panel paper" keywords="weak,alignment,wspo,strong,win,preference,model,stealing,weaker,qwen2">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=f7KxfUrRSb" target="_blank" title="252/373"><span class="index notranslate">#252</span></a>
                <a id="title-f7KxfUrRSb@OpenReview" class="title-link" href="/venue/f7KxfUrRSb@OpenReview" target="_blank">Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model</a>
                <a id="pdf-f7KxfUrRSb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('f7KxfUrRSb@OpenReview', this)" data="https://openreview.net/pdf?id=f7KxfUrRSb">[PDF<sup id="pdf-stars-f7KxfUrRSb@OpenReview">8</sup>]</a>
                <a id="copy-f7KxfUrRSb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('f7KxfUrRSb@OpenReview')">[Copy]</a>
                <a id="kimi-f7KxfUrRSb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('f7KxfUrRSb@OpenReview', this)">[Kimi<sup id="kimi-stars-f7KxfUrRSb@OpenReview">6</sup>]</a>
                <a id="rel-f7KxfUrRSb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('f7KxfUrRSb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-f7KxfUrRSb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhong Zhu" target="_blank">Wenhong Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiwei He" target="_blank">Zhiwei He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofeng Wang" target="_blank">Xiaofeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Liu" target="_blank">Pengfei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Wang" target="_blank">Rui Wang</a>
            </p>
            <p id="summary-f7KxfUrRSb@OpenReview" class="summary">Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible.</p>
            <p id="subjects-f7KxfUrRSb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-f7KxfUrRSb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-f7KxfUrRSb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-f7KxfUrRSb@OpenReview" onclick="foldPdfKimi('f7KxfUrRSb@OpenReview', this)" class="hr hr-fold">
        </div><div id="dOAkHmsjRX@OpenReview" class="panel paper" keywords="budget,memory,continual,freezing,budgets,total,storage,replay,budgeted,online">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dOAkHmsjRX" target="_blank" title="253/373"><span class="index notranslate">#253</span></a>
                <a id="title-dOAkHmsjRX@OpenReview" class="title-link" href="/venue/dOAkHmsjRX@OpenReview" target="_blank">Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling</a>
                <a id="pdf-dOAkHmsjRX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dOAkHmsjRX@OpenReview', this)" data="https://openreview.net/pdf?id=dOAkHmsjRX">[PDF<sup id="pdf-stars-dOAkHmsjRX@OpenReview">1</sup>]</a>
                <a id="copy-dOAkHmsjRX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dOAkHmsjRX@OpenReview')">[Copy]</a>
                <a id="kimi-dOAkHmsjRX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dOAkHmsjRX@OpenReview', this)">[Kimi<sup id="kimi-stars-dOAkHmsjRX@OpenReview">7</sup>]</a>
                <a id="rel-dOAkHmsjRX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dOAkHmsjRX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dOAkHmsjRX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Minhyuk Seo" target="_blank">Minhyuk Seo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunseo Koh" target="_blank">Hyunseo Koh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonghyun Choi" target="_blank">Jonghyun Choi</a>
            </p>
            <p id="summary-dOAkHmsjRX@OpenReview" class="summary">The majority of online continual learning (CL) advocates single-epoch training and imposes restrictions on the size of replay memory.However, single-epoch training would incur a different amount of computations per CL algorithm, and the additional storage cost to store logit or model in addition to replay memory is largely ignored in calculating the storage budget.Arguing different computational and storage budgets hinder fair comparison among CL algorithms in practice, we propose to use floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare and develop CL algorithms in the same "total resource budget".To improve a CL method in a limited total budget, we propose adaptive layer freezing that does not update the layers for less informative batches to reduce computational costs with a negligible loss of accuracy.In addition, we propose a memory retrieval method that allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations.Empirical validations on the CIFAR-10/100, CLEAR-10/100, and ImageNet-1K datasets demonstrate that the proposed approach outperforms the state-of-the-art methods within the same total budget.</p>
            <p id="subjects-dOAkHmsjRX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dOAkHmsjRX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dOAkHmsjRX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dOAkHmsjRX@OpenReview" onclick="foldPdfKimi('dOAkHmsjRX@OpenReview', this)" class="hr hr-fold">
        </div><div id="dEypApI1MZ@OpenReview" class="panel paper" keywords="tasks,scaling,hard,feature,easy,rkhs,ntk,kernel,learning,laws">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dEypApI1MZ" target="_blank" title="254/373"><span class="index notranslate">#254</span></a>
                <a id="title-dEypApI1MZ@OpenReview" class="title-link" href="/venue/dEypApI1MZ@OpenReview" target="_blank">How Feature Learning Can Improve Neural Scaling Laws</a>
                <a id="pdf-dEypApI1MZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dEypApI1MZ@OpenReview', this)" data="https://openreview.net/pdf?id=dEypApI1MZ">[PDF<sup id="pdf-stars-dEypApI1MZ@OpenReview">4</sup>]</a>
                <a id="copy-dEypApI1MZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dEypApI1MZ@OpenReview')">[Copy]</a>
                <a id="kimi-dEypApI1MZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dEypApI1MZ@OpenReview', this)">[Kimi<sup id="kimi-stars-dEypApI1MZ@OpenReview">3</sup>]</a>
                <a id="rel-dEypApI1MZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dEypApI1MZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dEypApI1MZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Blake Bordelon" target="_blank">Blake Bordelon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Atanasov" target="_blank">Alexander Atanasov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cengiz Pehlevan" target="_blank">Cengiz Pehlevan</a>
            </p>
            <p id="summary-dEypApI1MZ@OpenReview" class="summary">We develop a simple solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model predicts the performance scaling predictions with model size, training time and total amount of available data. From the scaling analysis we identify three relevant regimes: hard tasks, easy tasks, and super easy tasks. For easy and super-easy target functions, which are in the Hilbert space (RKHS) of the initial infinite-width neural tangent kernel (NTK), there is no change in the scaling exponents between feature learning models and models in the kernel regime. For hard tasks, which we define as tasks outside of the RKHS of the initial NTK, we show analytically and empirically that feature learning can improve the scaling with training time and compute, approximately doubling the exponent for very hard tasks. This leads to a new compute optimal scaling law for hard tasks in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks.</p>
            <p id="subjects-dEypApI1MZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dEypApI1MZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dEypApI1MZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dEypApI1MZ@OpenReview" onclick="foldPdfKimi('dEypApI1MZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="csbf1p8xUq@OpenReview" class="panel paper" keywords="translation,languages,alma,multilingual,aya,regimen,english,plug,rejection,llms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=csbf1p8xUq" target="_blank" title="255/373"><span class="index notranslate">#255</span></a>
                <a id="title-csbf1p8xUq@OpenReview" class="title-link" href="/venue/csbf1p8xUq@OpenReview" target="_blank">X-ALMA: Plug &amp; Play Modules and Adaptive Rejection for Quality Translation at Scale</a>
                <a id="pdf-csbf1p8xUq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('csbf1p8xUq@OpenReview', this)" data="https://openreview.net/pdf?id=csbf1p8xUq">[PDF<sup id="pdf-stars-csbf1p8xUq@OpenReview">2</sup>]</a>
                <a id="copy-csbf1p8xUq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('csbf1p8xUq@OpenReview')">[Copy]</a>
                <a id="kimi-csbf1p8xUq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('csbf1p8xUq@OpenReview', this)">[Kimi<sup id="kimi-stars-csbf1p8xUq@OpenReview">1</sup>]</a>
                <a id="rel-csbf1p8xUq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('csbf1p8xUq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-csbf1p8xUq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoran Xu" target="_blank">Haoran Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kenton Murray" target="_blank">Kenton Murray</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Koehn" target="_blank">Philipp Koehn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hieu Hoang" target="_blank">Hieu Hoang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akiko Eriguchi" target="_blank">Akiko Eriguchi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huda Khayrallah" target="_blank">Huda Khayrallah</a>
            </p>
            <p id="summary-csbf1p8xUq@OpenReview" class="summary">Large language models (LLMs) have achieved remarkable success across various NLP tasks, yet their focus has predominantly been on English due to English-centric pre-training and limited multilingual data. While some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality response for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages like English and Chinese. In this paper, we prioritize quality over scaling number of languages, with a focus on multilingual machine translation task, and introduce **X-ALMA**, a model designed with a commitment to ensuring top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. At the final stage of training regimen, our proposed Adaptive Rejection Preference Optimization (**ARPO**) surpasses existing preference optimization methods in translation tasks.</p>
            <p id="subjects-csbf1p8xUq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-csbf1p8xUq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-csbf1p8xUq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-csbf1p8xUq@OpenReview" onclick="foldPdfKimi('csbf1p8xUq@OpenReview', this)" class="hr hr-fold">
        </div><div id="cmXWYolrlo@OpenReview" class="panel paper" keywords="gih,geometry,average,architecture,dub,input,space,resnet,neural,evolution">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cmXWYolrlo" target="_blank" title="256/373"><span class="index notranslate">#256</span></a>
                <a id="title-cmXWYolrlo@OpenReview" class="title-link" href="/venue/cmXWYolrlo@OpenReview" target="_blank">Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture</a>
                <a id="pdf-cmXWYolrlo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cmXWYolrlo@OpenReview', this)" data="https://openreview.net/pdf?id=cmXWYolrlo">[PDF<sup id="pdf-stars-cmXWYolrlo@OpenReview">2</sup>]</a>
                <a id="copy-cmXWYolrlo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cmXWYolrlo@OpenReview')">[Copy]</a>
                <a id="kimi-cmXWYolrlo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cmXWYolrlo@OpenReview', this)">[Kimi<sup id="kimi-stars-cmXWYolrlo@OpenReview">2</sup>]</a>
                <a id="rel-cmXWYolrlo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cmXWYolrlo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cmXWYolrlo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sajad Movahedi" target="_blank">Sajad Movahedi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antonio Orvieto" target="_blank">Antonio Orvieto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seyed-Mohsen Moosavi-Dezfooli" target="_blank">Seyed-Mohsen Moosavi-Dezfooli</a>
            </p>
            <p id="summary-cmXWYolrlo@OpenReview" class="summary">In this paper, we propose the *geometric invariance hypothesis (GIH)*, which argues that when training a neural network, the input space curvature remains invariant under transformation in certain directions determined by its architecture. Starting with a simple non-linear binary classification problem residing on a plane in a high dimensional space, we observe that while an MLP can generalize on this problem regardless of the orientation of the plane, this is not the case for a ResNet. Motivated by this example, we define two maps that provide a compact *architecture-dependent* summary of the input space geometry of a neural network and its evolution during training, which we dub the **average geometry** and **average geometry evolution**, respectively. By investigating average geometry evolution at initialization, we discover that the geometry of a neural network evolves according to the projection of data covariance onto average geometry. As a result, in cases where the average geometry is low-rank (such as in a ResNet), the geometry only changes in a subset of the input space. This causes an architecture-dependent invariance property in input space curvature, which we dub GIH. Finally, we present extensive experimental results to observe the consequences of GIH and how it relates to generalization in neural networks.</p>
            <p id="subjects-cmXWYolrlo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cmXWYolrlo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cmXWYolrlo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cmXWYolrlo@OpenReview" onclick="foldPdfKimi('cmXWYolrlo@OpenReview', this)" class="hr hr-fold">
        </div><div id="cRR0oDFEBC@OpenReview" class="panel paper" keywords="autoif,instruction,following,llms,sft,dpo,feedback,execution,language,instructions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cRR0oDFEBC" target="_blank" title="257/373"><span class="index notranslate">#257</span></a>
                <a id="title-cRR0oDFEBC@OpenReview" class="title-link" href="/venue/cRR0oDFEBC@OpenReview" target="_blank">Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models</a>
                <a id="pdf-cRR0oDFEBC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cRR0oDFEBC@OpenReview', this)" data="https://openreview.net/pdf?id=cRR0oDFEBC">[PDF<sup id="pdf-stars-cRR0oDFEBC@OpenReview">3</sup>]</a>
                <a id="copy-cRR0oDFEBC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cRR0oDFEBC@OpenReview')">[Copy]</a>
                <a id="kimi-cRR0oDFEBC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cRR0oDFEBC@OpenReview', this)">[Kimi<sup id="kimi-stars-cRR0oDFEBC@OpenReview">8</sup>]</a>
                <a id="rel-cRR0oDFEBC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cRR0oDFEBC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cRR0oDFEBC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guanting Dong" target="_blank">Guanting Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keming Lu" target="_blank">Keming Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengpeng Li" target="_blank">Chengpeng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tingyu Xia" target="_blank">Tingyu Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Yu" target="_blank">Bowen Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Zhou" target="_blank">Chang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingren Zhou" target="_blank">Jingren Zhou</a>
            </p>
            <p id="summary-cRR0oDFEBC@OpenReview" class="summary">One core capability of large language models~(LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to verify the correctness of the instruction responses, and unit test samples to cross-validate the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the advanced open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Using two widely-used and three challenging general instruction-following benchmarks, we demonstrate that AutoIF significantly improves LLM performance across a wide range of natural instruction constraints. Notably, AutoIF is the first to surpass 90\% accuracy in IFEval’s loose instruction accuracy, without compromising general, math and coding capabilities. Further analysis of quality, scaling, combination, and data efficiency highlights AutoIF's strong generalization and alignment potential.</p>
            <p id="subjects-cRR0oDFEBC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cRR0oDFEBC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cRR0oDFEBC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cRR0oDFEBC@OpenReview" onclick="foldPdfKimi('cRR0oDFEBC@OpenReview', this)" class="hr hr-fold">
        </div><div id="aD2uwhLbnA@OpenReview" class="panel paper" keywords="sam,flatter,sharpness,late,training,minima,selects,minimization,sgd,aware">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aD2uwhLbnA" target="_blank" title="258/373"><span class="index notranslate">#258</span></a>
                <a id="title-aD2uwhLbnA@OpenReview" class="title-link" href="/venue/aD2uwhLbnA@OpenReview" target="_blank">Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training</a>
                <a id="pdf-aD2uwhLbnA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aD2uwhLbnA@OpenReview', this)" data="https://openreview.net/pdf?id=aD2uwhLbnA">[PDF<sup id="pdf-stars-aD2uwhLbnA@OpenReview">4</sup>]</a>
                <a id="copy-aD2uwhLbnA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aD2uwhLbnA@OpenReview')">[Copy]</a>
                <a id="kimi-aD2uwhLbnA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aD2uwhLbnA@OpenReview', this)">[Kimi<sup id="kimi-stars-aD2uwhLbnA@OpenReview">2</sup>]</a>
                <a id="rel-aD2uwhLbnA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aD2uwhLbnA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aD2uwhLbnA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanpeng Zhou" target="_blank">Zhanpeng Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingze Wang" target="_blank">Mingze Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Mao" target="_blank">Yuchen Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingrui Li" target="_blank">Bingrui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junchi Yan" target="_blank">Junchi Yan</a>
            </p>
            <p id="summary-aD2uwhLbnA@OpenReview" class="summary">Sharpness-Aware Minimization (SAM) has substantially improved the generalization of neural networks under various settings.Despite the success, its effectiveness remains poorly understood.In this work, we discover an intriguing phenomenon in the training dynamics of SAM, shedding lights on understanding its implicit bias towards flatter minima over Stochastic Gradient Descent (SGD).Specifically, we find that *SAM efficiently selects flatter minima late in training*.Remarkably, even a few epochs of SAM applied at the end of training yield nearly the same generalization and solution sharpness as full SAM training.Subsequently, we delve deeper into the underlying mechanism behind this phenomenon.Theoretically, we identify two phases in the learning dynamics after applying SAM late in training: i) SAM first escapes the minimum found by SGD exponentially fast; and ii) then rapidly converges to a flatter minimum within the same valley.Furthermore, we empirically investigate the role of SAM during the early training phase.We conjecture that the optimization method chosen in the late phase is more crucial in shaping the final solution's properties.Based on this viewpoint, we extend our findings from SAM to Adversarial Training.We provide source code in supplementary materials and will release checkpoints in future.</p>
            <p id="subjects-aD2uwhLbnA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-aD2uwhLbnA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aD2uwhLbnA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aD2uwhLbnA@OpenReview" onclick="foldPdfKimi('aD2uwhLbnA@OpenReview', this)" class="hr hr-fold">
        </div><div id="Yk87CwhBDx@OpenReview" class="panel paper" keywords="symbolic,programs,graphics,llms,procedurally,ability,llm,sit,reasoning,skills">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Yk87CwhBDx" target="_blank" title="259/373"><span class="index notranslate">#259</span></a>
                <a id="title-Yk87CwhBDx@OpenReview" class="title-link" href="/venue/Yk87CwhBDx@OpenReview" target="_blank">Can Large Language Models Understand Symbolic Graphics Programs?</a>
                <a id="pdf-Yk87CwhBDx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Yk87CwhBDx@OpenReview', this)" data="https://openreview.net/pdf?id=Yk87CwhBDx">[PDF<sup id="pdf-stars-Yk87CwhBDx@OpenReview">9</sup>]</a>
                <a id="copy-Yk87CwhBDx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Yk87CwhBDx@OpenReview')">[Copy]</a>
                <a id="kimi-Yk87CwhBDx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Yk87CwhBDx@OpenReview', this)">[Kimi<sup id="kimi-stars-Yk87CwhBDx@OpenReview">10</sup>]</a>
                <a id="rel-Yk87CwhBDx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Yk87CwhBDx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Yk87CwhBDx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeju Qiu" target="_blank">Zeju Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiyang Liu" target="_blank">Weiyang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiwen Feng" target="_blank">Haiwen Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Liu" target="_blank">Zhen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Xiao" target="_blank">Tim Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katherine Collins" target="_blank">Katherine Collins</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua B Tenenbaum" target="_blank">Joshua B Tenenbaum</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adrian Weller" target="_blank">Adrian Weller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael J Black" target="_blank">Michael J Black</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernhard Schoelkopf" target="_blank">Bernhard Schoelkopf</a>
            </p>
            <p id="summary-Yk87CwhBDx@OpenReview" class="summary">Against the backdrop of enthusiasm for large language models (LLMs), there is an urgent need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training.Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLM's ability to answer different-grained semantic-level questions of the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to "imagine" and reason how the corresponding graphics content would look with only the symbolic description of the local curvatures and strokes. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLM's understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks.</p>
            <p id="subjects-Yk87CwhBDx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Yk87CwhBDx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yk87CwhBDx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yk87CwhBDx@OpenReview" onclick="foldPdfKimi('Yk87CwhBDx@OpenReview', this)" class="hr hr-fold">
        </div><div id="YK9G4Htdew@OpenReview" class="panel paper" keywords="world,transformer,twister,contrastive,based,coding,predictive,dreamerv3,dreamer,model">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YK9G4Htdew" target="_blank" title="260/373"><span class="index notranslate">#260</span></a>
                <a id="title-YK9G4Htdew@OpenReview" class="title-link" href="/venue/YK9G4Htdew@OpenReview" target="_blank">Learning Transformer-based World Models with Contrastive Predictive Coding</a>
                <a id="pdf-YK9G4Htdew@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YK9G4Htdew@OpenReview', this)" data="https://openreview.net/pdf?id=YK9G4Htdew">[PDF<sup id="pdf-stars-YK9G4Htdew@OpenReview">5</sup>]</a>
                <a id="copy-YK9G4Htdew@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YK9G4Htdew@OpenReview')">[Copy]</a>
                <a id="kimi-YK9G4Htdew@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YK9G4Htdew@OpenReview', this)">[Kimi<sup id="kimi-stars-YK9G4Htdew@OpenReview">4</sup>]</a>
                <a id="rel-YK9G4Htdew@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YK9G4Htdew@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YK9G4Htdew@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Maxime Burchi" target="_blank">Maxime Burchi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Radu Timofte" target="_blank">Radu Timofte</a>
            </p>
            <p id="summary-YK9G4Htdew@OpenReview" class="summary">The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search.</p>
            <p id="subjects-YK9G4Htdew@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-YK9G4Htdew@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YK9G4Htdew@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YK9G4Htdew@OpenReview" onclick="foldPdfKimi('YK9G4Htdew@OpenReview', this)" class="hr hr-fold">
        </div><div id="XgH1wfHSX8@OpenReview" class="panel paper" keywords="icl,context,merely,nature,behavior,algorithms,bigram,superseding,inputted,motivated">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XgH1wfHSX8" target="_blank" title="261/373"><span class="index notranslate">#261</span></a>
                <a id="title-XgH1wfHSX8@OpenReview" class="title-link" href="/venue/XgH1wfHSX8@OpenReview" target="_blank">Algorithmic Phases of In-Context Learning</a>
                <a id="pdf-XgH1wfHSX8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XgH1wfHSX8@OpenReview', this)" data="https://openreview.net/pdf?id=XgH1wfHSX8">[PDF<sup id="pdf-stars-XgH1wfHSX8@OpenReview">2</sup>]</a>
                <a id="copy-XgH1wfHSX8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XgH1wfHSX8@OpenReview')">[Copy]</a>
                <a id="kimi-XgH1wfHSX8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XgH1wfHSX8@OpenReview', this)">[Kimi<sup id="kimi-stars-XgH1wfHSX8@OpenReview">7</sup>]</a>
                <a id="rel-XgH1wfHSX8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XgH1wfHSX8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XgH1wfHSX8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Core Francisco Park" target="_blank">Core Francisco Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ekdeep Singh Lubana" target="_blank">Ekdeep Singh Lubana</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hidenori Tanaka" target="_blank">Hidenori Tanaka</a>
            </p>
            <p id="summary-XgH1wfHSX8@OpenReview" class="summary">In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model’s behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competitive dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.</p>
            <p id="subjects-XgH1wfHSX8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-XgH1wfHSX8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XgH1wfHSX8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XgH1wfHSX8@OpenReview" onclick="foldPdfKimi('XgH1wfHSX8@OpenReview', this)" class="hr hr-fold">
        </div><div id="XNA3Mnnbvb@OpenReview" class="panel paper" keywords="motion,dart,text,conditioned,motions,descriptions,primitive,utoregressive,driven,control">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XNA3Mnnbvb" target="_blank" title="262/373"><span class="index notranslate">#262</span></a>
                <a id="title-XNA3Mnnbvb@OpenReview" class="title-link" href="/venue/XNA3Mnnbvb@OpenReview" target="_blank">DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</a>
                <a id="pdf-XNA3Mnnbvb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XNA3Mnnbvb@OpenReview', this)" data="https://openreview.net/pdf?id=XNA3Mnnbvb">[PDF<sup id="pdf-stars-XNA3Mnnbvb@OpenReview">6</sup>]</a>
                <a id="copy-XNA3Mnnbvb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XNA3Mnnbvb@OpenReview')">[Copy]</a>
                <a id="kimi-XNA3Mnnbvb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XNA3Mnnbvb@OpenReview', this)">[Kimi<sup id="kimi-stars-XNA3Mnnbvb@OpenReview">4</sup>]</a>
                <a id="rel-XNA3Mnnbvb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XNA3Mnnbvb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XNA3Mnnbvb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaifeng Zhao" target="_blank">Kaifeng Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gen Li" target="_blank">Gen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Tang" target="_blank">Siyu Tang</a>
            </p>
            <p id="summary-XNA3Mnnbvb@OpenReview" class="summary">Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose **DART**, a **D**iffusion-based **A**utoregressive motion primitive model for **R**eal-time **T**ext-driven motion control. Our model, DART, effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model’s versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability.</p>
            <p id="subjects-XNA3Mnnbvb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-XNA3Mnnbvb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XNA3Mnnbvb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XNA3Mnnbvb@OpenReview" onclick="foldPdfKimi('XNA3Mnnbvb@OpenReview', this)" class="hr hr-fold">
        </div><div id="VaowElpVzd@OpenReview" class="panel paper" keywords="gesture,concurrent,gestures,speech,interactive,ges,tim,person,coherent,mathbf">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VaowElpVzd" target="_blank" title="263/373"><span class="index notranslate">#263</span></a>
                <a id="title-VaowElpVzd@OpenReview" class="title-link" href="/venue/VaowElpVzd@OpenReview" target="_blank">Co<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-168-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn mathvariant=&quot;bold&quot;&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-944" style="width: 0.592em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.217em, 1000.49em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-945"><span class="msubsup" id="MathJax-Span-946"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px;"><span style="position: absolute; clip: rect(3.891em, 1000em, 4.099em, -999.998em); top: -3.991em; left: 0em;"><span class="mi" id="MathJax-Span-947"></span><span style="display: inline-block; width: 0px; height: 3.995em;"></span></span><span style="position: absolute; top: -2.498em; left: 0em;"><span class="texatom" id="MathJax-Span-948"><span class="mrow" id="MathJax-Span-949"><span class="texatom" id="MathJax-Span-950"><span class="mrow" id="MathJax-Span-951"><span class="mn" id="MathJax-Span-952" style="font-size: 70.7%; font-family: MathJax_Main-bold;">3</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.04em; border-left: 0px solid; width: 0px; height: 1.085em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mn mathvariant="bold">3</mn></mrow></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-168">^{\mathbf{3}}</script>Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</a>
                <a id="pdf-VaowElpVzd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VaowElpVzd@OpenReview', this)" data="https://openreview.net/pdf?id=VaowElpVzd">[PDF<sup id="pdf-stars-VaowElpVzd@OpenReview">2</sup>]</a>
                <a id="copy-VaowElpVzd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VaowElpVzd@OpenReview')">[Copy]</a>
                <a id="kimi-VaowElpVzd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VaowElpVzd@OpenReview', this)">[Kimi<sup id="kimi-stars-VaowElpVzd@OpenReview">1</sup>]</a>
                <a id="rel-VaowElpVzd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VaowElpVzd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VaowElpVzd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingqun Qi" target="_blank">Xingqun Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yatian Wang" target="_blank">Yatian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengyuan Zhang" target="_blank">Hengyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Pan" target="_blank">Jiahao Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Xue" target="_blank">Wei Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shanghang Zhang" target="_blank">Shanghang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhan Luo" target="_blank">Wenhan Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qifeng Liu" target="_blank">Qifeng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yike Guo" target="_blank">Yike Guo</a>
            </p>
            <p id="summary-VaowElpVzd@OpenReview" class="summary">Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by people self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-169-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;GES-Inter&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-953" style="width: 6.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.211em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.16em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-954"><span class="texatom" id="MathJax-Span-955"><span class="mrow" id="MathJax-Span-956"><span class="mtext" id="MathJax-Span-957" style="font-family: MathJax_Main-bold;">GES-Inter</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">GES-Inter</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-169">\textbf{GES-Inter}</script>. Moreover, we propose Co<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-170-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn mathvariant=&quot;bold&quot;&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-958" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-959"><span class="msubsup" id="MathJax-Span-960"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-961"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="texatom" id="MathJax-Span-962"><span class="mrow" id="MathJax-Span-963"><span class="texatom" id="MathJax-Span-964"><span class="mrow" id="MathJax-Span-965"><span class="mn" id="MathJax-Span-966" style="font-size: 70.7%; font-family: MathJax_Main-bold;">3</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mn mathvariant="bold">3</mn></mrow></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-170">^{\mathbf{3}}</script>Gesture, a novel framework that enables concurrent coherent co-speech gesture synthesis including two-person interactive movements. Our framework is built upon two cooperative generation branches conditioned on decomposed speaker audio. Specifically, to enhance the coordination of human postures w.r.t corresponding speaker audios while interacting with the conversational partner, we present a Temporal-Interaction Module (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-171-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;TIM&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-967" style="width: 2.815em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.29em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-968"><span class="texatom" id="MathJax-Span-969"><span class="mrow" id="MathJax-Span-970"><span class="mtext" id="MathJax-Span-971" style="font-family: MathJax_Main-bold;">TIM</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">TIM</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-171">\textbf{TIM}</script>). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset.</p>
            <p id="subjects-VaowElpVzd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-VaowElpVzd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VaowElpVzd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VaowElpVzd@OpenReview" onclick="foldPdfKimi('VaowElpVzd@OpenReview', this)" class="hr hr-fold">
        </div><div id="VGQugiuCQs@OpenReview" class="panel paper" keywords="sliding,window,fair,streaming,varepsilon,clustering,insertion,chierichetti,fairness,model">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VGQugiuCQs" target="_blank" title="264/373"><span class="index notranslate">#264</span></a>
                <a id="title-VGQugiuCQs@OpenReview" class="title-link" href="/venue/VGQugiuCQs@OpenReview" target="_blank">Fair Clustering in the Sliding Window Model</a>
                <a id="pdf-VGQugiuCQs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VGQugiuCQs@OpenReview', this)" data="https://openreview.net/pdf?id=VGQugiuCQs">[PDF<sup id="pdf-stars-VGQugiuCQs@OpenReview">3</sup>]</a>
                <a id="copy-VGQugiuCQs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VGQugiuCQs@OpenReview')">[Copy]</a>
                <a id="kimi-VGQugiuCQs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VGQugiuCQs@OpenReview', this)">[Kimi<sup id="kimi-stars-VGQugiuCQs@OpenReview">2</sup>]</a>
                <a id="rel-VGQugiuCQs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VGQugiuCQs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VGQugiuCQs@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent Cohen-Addad" target="_blank">Vincent Cohen-Addad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaofeng Jiang" target="_blank">Shaofeng Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiaoyuan Yang" target="_blank">Qiaoyuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yubo Zhang" target="_blank">Yubo Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samson Zhou" target="_blank">Samson Zhou</a>
            </p>
            <p id="summary-VGQugiuCQs@OpenReview" class="summary">We study streaming algorithms for proportionally fair clustering (a notion originally suggested by Chierichetti et al. (2017) in the sliding window model. We show that although there exist efficient streaming algorithms exist in the insertion-only model, surprisingly no algorithm can achieve finite ratio without violating the fairness constraint in sliding window. Hence, the problem of fair clustering is a rare separation between the insertion-only streaming model and the sliding window model. On the other hand, we show that if the fairness constraint by a multiplicative <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-172-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-972" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-973"><span class="mi" id="MathJax-Span-974" style="font-family: MathJax_Math-italic;">ε</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi></math></span></span><script type="math/tex" id="MathJax-Element-172">\varepsilon</script> factor, there exists a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-173-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-975" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.87em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-976"><span class="mo" id="MathJax-Span-977" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-978" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-979" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-980" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">ε</span><span class="mo" id="MathJax-Span-981" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>ε</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-173">(1 + \varepsilon)</script>-approximate sliding window algorithm that uses <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-174-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mtext&gt;poly&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-982" style="width: 8.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1006.67em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-983"><span class="mtext" id="MathJax-Span-984" style="font-family: MathJax_Main;">poly</span><span class="mo" id="MathJax-Span-985" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-986" style="font-family: MathJax_Math-italic;">k</span><span class="msubsup" id="MathJax-Span-987"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-988" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-989"><span class="mrow" id="MathJax-Span-990"><span class="mo" id="MathJax-Span-991" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-992" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mi" id="MathJax-Span-993" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-994"></span><span class="mi" id="MathJax-Span-995" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">n</span><span class="mo" id="MathJax-Span-996" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>poly</mtext><mo stretchy="false">(</mo><mi>k</mi><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-174">\text{poly}(k\varepsilon^{-1}\log n)</script> space. This achieves essentially the best parameters (up to degree in the polynomial) provided the aforementioned lower bound. We also implement a number of empirical evaluations on real datasets to complement our theoretical results.</p>
            <p id="subjects-VGQugiuCQs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-VGQugiuCQs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VGQugiuCQs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VGQugiuCQs@OpenReview" onclick="foldPdfKimi('VGQugiuCQs@OpenReview', this)" class="hr hr-fold">
        </div><div id="UL8b54P96G@OpenReview" class="panel paper" keywords="vgen,slow,slowfast,fast,learning,video,episodic,loop,action,generation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UL8b54P96G" target="_blank" title="265/373"><span class="index notranslate">#265</span></a>
                <a id="title-UL8b54P96G@OpenReview" class="title-link" href="/venue/UL8b54P96G@OpenReview" target="_blank">SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation</a>
                <a id="pdf-UL8b54P96G@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UL8b54P96G@OpenReview', this)" data="https://openreview.net/pdf?id=UL8b54P96G">[PDF<sup id="pdf-stars-UL8b54P96G@OpenReview">3</sup>]</a>
                <a id="copy-UL8b54P96G@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UL8b54P96G@OpenReview')">[Copy]</a>
                <a id="kimi-UL8b54P96G@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UL8b54P96G@OpenReview', this)">[Kimi<sup id="kimi-stars-UL8b54P96G@OpenReview">3</sup>]</a>
                <a id="rel-UL8b54P96G@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UL8b54P96G@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UL8b54P96G@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yining Hong" target="_blank">Yining Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beide Liu" target="_blank">Beide Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maxine Wu" target="_blank">Maxine Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanhao Zhai" target="_blank">Yuanhao Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai-Wei Chang" target="_blank">Kai-Wei Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linjie Li" target="_blank">Linjie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Lin" target="_blank">Kevin Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chung-Ching Lin" target="_blank">Chung-Ching Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfeng Wang" target="_blank">Jianfeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lijuan Wang" target="_blank">Lijuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingnian Wu" target="_blank">Yingnian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyuan Yang" target="_blank">Zhengyuan Yang</a>
            </p>
            <p id="summary-UL8b54P96G@OpenReview" class="summary">Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well.</p>
            <p id="subjects-UL8b54P96G@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-UL8b54P96G@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UL8b54P96G@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UL8b54P96G@OpenReview" onclick="foldPdfKimi('UL8b54P96G@OpenReview', this)" class="hr hr-fold">
        </div><div id="U67J0QNtzo@OpenReview" class="panel paper" keywords="lic,auxt,compaction,nonlinear,energy,transforms,decorrelation,uneven,wlss,wavelet">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=U67J0QNtzo" target="_blank" title="266/373"><span class="index notranslate">#266</span></a>
                <a id="title-U67J0QNtzo@OpenReview" class="title-link" href="/venue/U67J0QNtzo@OpenReview" target="_blank">On Disentangled Training for Nonlinear Transform in Learned Image Compression</a>
                <a id="pdf-U67J0QNtzo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('U67J0QNtzo@OpenReview', this)" data="https://openreview.net/pdf?id=U67J0QNtzo">[PDF<sup id="pdf-stars-U67J0QNtzo@OpenReview">6</sup>]</a>
                <a id="copy-U67J0QNtzo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('U67J0QNtzo@OpenReview')">[Copy]</a>
                <a id="kimi-U67J0QNtzo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('U67J0QNtzo@OpenReview', this)">[Kimi<sup id="kimi-stars-U67J0QNtzo@OpenReview">1</sup>]</a>
                <a id="rel-U67J0QNtzo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('U67J0QNtzo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-U67J0QNtzo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Han Li" target="_blank">Han Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohui Li" target="_blank">Shaohui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenrui Dai" target="_blank">Wenrui Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maida Cao" target="_blank">Maida Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nuowen Kan" target="_blank">Nuowen Kan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenglin Li" target="_blank">Chenglin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junni Zou" target="_blank">Junni Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongkai Xiong" target="_blank">Hongkai Xiong</a>
            </p>
            <p id="summary-U67J0QNtzo@OpenReview" class="summary">Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow convergence caused by compacting energy in learning nonlinear transforms. In this paper, we first reveal that such energy compaction consists of two components, \emph{i.e.}, feature decorrelation and uneven energy modulation. On such basis, we propose a linear auxiliary transform (AuxT) to disentangle energy compaction in training nonlinear transforms. The proposed AuxT obtains coarse approximation to achieve efficient energy compaction such that distribution fitting with the nonlinear transforms can be simplified to fine details. We then develop wavelet-based linear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and orthogonal linear projection for feature decorrelation and subband-aware scaling for uneven energy modulation. AuxT is lightweight and plug-and-play to be integrated into diverse LIC models to address the slow convergence issue. Experimental results demonstrate that the proposed approach can accelerate training of LIC models by 2 times and simultaneously achieves an average 1\% BD-rate reduction. To our best knowledge, this is one of the first successful attempt that can significantly improve the convergence of LIC with comparable or superior rate-distortion performance.</p>
            <p id="subjects-U67J0QNtzo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-U67J0QNtzo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-U67J0QNtzo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-U67J0QNtzo@OpenReview" onclick="foldPdfKimi('U67J0QNtzo@OpenReview', this)" class="hr hr-fold">
        </div><div id="TYSQYx9vwd@OpenReview" class="panel paper" keywords="uncertainty,graph,lgnsdes,differential,gnodes,lgnsde,stochastic,gnode,neural,equations">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TYSQYx9vwd" target="_blank" title="267/373"><span class="index notranslate">#267</span></a>
                <a id="title-TYSQYx9vwd@OpenReview" class="title-link" href="/venue/TYSQYx9vwd@OpenReview" target="_blank">Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations</a>
                <a id="pdf-TYSQYx9vwd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TYSQYx9vwd@OpenReview', this)" data="https://openreview.net/pdf?id=TYSQYx9vwd">[PDF<sup id="pdf-stars-TYSQYx9vwd@OpenReview">7</sup>]</a>
                <a id="copy-TYSQYx9vwd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TYSQYx9vwd@OpenReview')">[Copy]</a>
                <a id="kimi-TYSQYx9vwd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TYSQYx9vwd@OpenReview', this)">[Kimi<sup id="kimi-stars-TYSQYx9vwd@OpenReview">3</sup>]</a>
                <a id="rel-TYSQYx9vwd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TYSQYx9vwd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TYSQYx9vwd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Bergna" target="_blank">Richard Bergna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergio Calvo Ordoñez" target="_blank">Sergio Calvo Ordoñez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Opolka" target="_blank">Felix Opolka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pietro Lio" target="_blank">Pietro Lio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=José Miguel Hernández Lobato" target="_blank">José Miguel Hernández Lobato</a>
            </p>
            <p id="summary-TYSQYx9vwd@OpenReview" class="summary">We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks demonstrate that our framework is competitive in out-of-distribution detection, robustness to noise perturbations, and active learning, underscoring the ability of LGNSDEs to quantify uncertainty reliably.</p>
            <p id="subjects-TYSQYx9vwd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TYSQYx9vwd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TYSQYx9vwd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TYSQYx9vwd@OpenReview" onclick="foldPdfKimi('TYSQYx9vwd@OpenReview', this)" class="hr hr-fold">
        </div><div id="TJo6aQb7mK@OpenReview" class="panel paper" keywords="trilms,floatlm,floatlms,quantlms,bitwidth,language,llm,bit,pretraining,ternary">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TJo6aQb7mK" target="_blank" title="268/373"><span class="index notranslate">#268</span></a>
                <a id="title-TJo6aQb7mK@OpenReview" class="title-link" href="/venue/TJo6aQb7mK@OpenReview" target="_blank">Surprising Effectiveness of pretraining Ternary Language Model at Scale</a>
                <a id="pdf-TJo6aQb7mK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TJo6aQb7mK@OpenReview', this)" data="https://openreview.net/pdf?id=TJo6aQb7mK">[PDF<sup id="pdf-stars-TJo6aQb7mK@OpenReview">4</sup>]</a>
                <a id="copy-TJo6aQb7mK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TJo6aQb7mK@OpenReview')">[Copy]</a>
                <a id="kimi-TJo6aQb7mK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TJo6aQb7mK@OpenReview', this)">[Kimi<sup id="kimi-stars-TJo6aQb7mK@OpenReview">6</sup>]</a>
                <a id="rel-TJo6aQb7mK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TJo6aQb7mK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TJo6aQb7mK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ayush Kaushal" target="_blank">Ayush Kaushal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tejas Vaidhya" target="_blank">Tejas Vaidhya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arnab Mondal" target="_blank">Arnab Mondal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tejas Pandey" target="_blank">Tejas Pandey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaryan Bhagat" target="_blank">Aaryan Bhagat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Irina Rish" target="_blank">Irina Rish</a>
            </p>
            <p id="summary-TJo6aQb7mK@OpenReview" class="summary">Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs.</p>
            <p id="subjects-TJo6aQb7mK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TJo6aQb7mK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TJo6aQb7mK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TJo6aQb7mK@OpenReview" onclick="foldPdfKimi('TJo6aQb7mK@OpenReview', this)" class="hr hr-fold">
        </div><div id="THqWPzL00e@OpenReview" class="panel paper" keywords="toponets,brain,topography,topoloss,topographic,performing,language,organized,resnet,nanogpt">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=THqWPzL00e" target="_blank" title="269/373"><span class="index notranslate">#269</span></a>
                <a id="title-THqWPzL00e@OpenReview" class="title-link" href="/venue/THqWPzL00e@OpenReview" target="_blank">TopoNets: High performing vision and language models with brain-like topography</a>
                <a id="pdf-THqWPzL00e@OpenReview" class="title-pdf notranslate" onclick="togglePdf('THqWPzL00e@OpenReview', this)" data="https://openreview.net/pdf?id=THqWPzL00e">[PDF<sup id="pdf-stars-THqWPzL00e@OpenReview">1</sup>]</a>
                <a id="copy-THqWPzL00e@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('THqWPzL00e@OpenReview')">[Copy]</a>
                <a id="kimi-THqWPzL00e@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('THqWPzL00e@OpenReview', this)">[Kimi<sup id="kimi-stars-THqWPzL00e@OpenReview">3</sup>]</a>
                <a id="rel-THqWPzL00e@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('THqWPzL00e@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-THqWPzL00e@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mayukh Deb" target="_blank">Mayukh Deb</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mainak Deb" target="_blank">Mainak Deb</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Apurva Murty" target="_blank">Apurva Murty</a>
            </p>
            <p id="summary-THqWPzL00e@OpenReview" class="summary">Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present *TopoLoss*, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively *TopoNets*. TopoNets are the highest performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain’s visual and language cortices, further bridging the gap between biological and artificial systems. This work establishes a robust and generalizable framework for integrating topography into AI, advancing the development of high performing models that more closely emulate the computational strategies of the human brain.</p>
            <p id="subjects-THqWPzL00e@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-THqWPzL00e@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-THqWPzL00e@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-THqWPzL00e@OpenReview" onclick="foldPdfKimi('THqWPzL00e@OpenReview', this)" class="hr hr-fold">
        </div><div id="SUc1UOWndp@OpenReview" class="panel paper" keywords="heads,rllcs,refined,specialization,attention,differentiation,developmental,learning,multigram,coefficient">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SUc1UOWndp" target="_blank" title="270/373"><span class="index notranslate">#270</span></a>
                <a id="title-SUc1UOWndp@OpenReview" class="title-link" href="/venue/SUc1UOWndp@OpenReview" target="_blank">Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient</a>
                <a id="pdf-SUc1UOWndp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SUc1UOWndp@OpenReview', this)" data="https://openreview.net/pdf?id=SUc1UOWndp">[PDF<sup id="pdf-stars-SUc1UOWndp@OpenReview">4</sup>]</a>
                <a id="copy-SUc1UOWndp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SUc1UOWndp@OpenReview')">[Copy]</a>
                <a id="kimi-SUc1UOWndp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SUc1UOWndp@OpenReview', this)">[Kimi<sup id="kimi-stars-SUc1UOWndp@OpenReview">4</sup>]</a>
                <a id="rel-SUc1UOWndp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SUc1UOWndp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SUc1UOWndp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=George Wang" target="_blank">George Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jesse Hoogland" target="_blank">Jesse Hoogland</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stan van Wingerden" target="_blank">Stan van Wingerden</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zach Furman" target="_blank">Zach Furman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michelle Chen" target="_blank">Michelle Chen</a>
            </p>
            <p id="summary-SUc1UOWndp@OpenReview" class="summary">We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these refined LLCs (rLLCs) to individual components of a two-layer attention-only transformer, we gain novel insights into the progressive differentiation and specialization of attention heads. Our methodology reveals how attention heads differentiate into distinct functional roles over the course of training, analyzes the types of data these heads specialize to process, and discovers a previously unidentified multigram circuit. These findings demonstrate that rLLCs provide a principled, quantitative toolkit for developmental interpretability, which aims to understand models through their evolution across the learning process. This work advances the field of developmental interpretability by providing a mathematically rigorous approach to understanding neural networks through the lens of their learning process. More broadly, this work takes a step towards establishing the correspondence between data distributional structure, geometric properties of the loss landscape, learning dynamics, and emergent computational structures in neural networks.</p>
            <p id="subjects-SUc1UOWndp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SUc1UOWndp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SUc1UOWndp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SUc1UOWndp@OpenReview" onclick="foldPdfKimi('SUc1UOWndp@OpenReview', this)" class="hr hr-fold">
        </div><div id="S4dItvpvAv@OpenReview" class="panel paper" keywords="pareto,front,policies,mdps,deterministic,objective,mdp,vertices,traversing,finding">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=S4dItvpvAv" target="_blank" title="271/373"><span class="index notranslate">#271</span></a>
                <a id="title-S4dItvpvAv@OpenReview" class="title-link" href="/venue/S4dItvpvAv@OpenReview" target="_blank">How to Find the Exact Pareto Front for Multi-Objective MDPs?</a>
                <a id="pdf-S4dItvpvAv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('S4dItvpvAv@OpenReview', this)" data="https://openreview.net/pdf?id=S4dItvpvAv">[PDF<sup id="pdf-stars-S4dItvpvAv@OpenReview">2</sup>]</a>
                <a id="copy-S4dItvpvAv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('S4dItvpvAv@OpenReview')">[Copy]</a>
                <a id="kimi-S4dItvpvAv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('S4dItvpvAv@OpenReview', this)">[Kimi<sup id="kimi-stars-S4dItvpvAv@OpenReview">1</sup>]</a>
                <a id="rel-S4dItvpvAv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('S4dItvpvAv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-S4dItvpvAv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yining Li" target="_blank">Yining Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peizhong Ju" target="_blank">Peizhong Ju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ness Shroff" target="_blank">Ness Shroff</a>
            </p>
            <p id="summary-S4dItvpvAv@OpenReview" class="summary">Multi-Objective Markov Decision Processes (MO-MDPs) are receiving increasing attention, as real-world decision-making problems often involve conflicting objectives that cannot be addressed by a single-objective MDP. The Pareto front identifies the set of policies that cannot be dominated, providing a foundation for finding Pareto optimal solutions that can efficiently adapt to various preferences.However, finding the Pareto front is a highly challenging problem. Most existing methods either (i) rely on traversing the *continuous preference space*, which is impractical and results in approximations that are difficult to evaluate against the true Pareto front, or (ii) focus solely on deterministic Pareto optimal policies, from which there are no known techniques to characterize the full Pareto front. Moreover, finding the structure of the Pareto front itself remains unclear even in the context of dynamic programming, where the MDP is fully known in advance.In this work, we address the challenge of efficiently discovering the Pareto front, involving both deterministic and stochastic Pareto optimal policies.By investigating the geometric structure of the Pareto front in MO-MDPs, we uncover a key property: the Pareto front is on the boundary of a convex polytope whose vertices all correspond to deterministic policies, and neighboring vertices of the Pareto front differ by only one state-action pair of the deterministic policy, almost surely.This insight transforms the global comparison across all policies into a localized search among deterministic policies that differ by only one state-action pair, drastically reducing the complexity of searching for the exact Pareto front. We develop an efficient algorithm that identifies the vertices of the Pareto front by solving a single-objective MDP only once and then traversing the edges of the Pareto front, making it more efficient than existing methods. Furthermore, the entire Pareto front can be found in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-175-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-997" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-998"><span class="mi" id="MathJax-Span-999" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi></math></span></span><script type="math/tex" id="MathJax-Element-175">V</script> iterations, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-176-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1000" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1001"><span class="mi" id="MathJax-Span-1002" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi></math></span></span><script type="math/tex" id="MathJax-Element-176">V</script> represents the number of vertices on the Pareto front.Our empirical studies demonstrate the effectiveness of our theoretical strategy in discovering the Pareto front efficiently.</p>
            <p id="subjects-S4dItvpvAv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-S4dItvpvAv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-S4dItvpvAv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-S4dItvpvAv@OpenReview" onclick="foldPdfKimi('S4dItvpvAv@OpenReview', this)" class="hr hr-fold">
        </div><div id="RInisw1yin@OpenReview" class="panel paper" keywords="srsa,skill,tasks,library,assembly,skills,success,policies,retrieval,task">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=RInisw1yin" target="_blank" title="272/373"><span class="index notranslate">#272</span></a>
                <a id="title-RInisw1yin@OpenReview" class="title-link" href="/venue/RInisw1yin@OpenReview" target="_blank">SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks</a>
                <a id="pdf-RInisw1yin@OpenReview" class="title-pdf notranslate" onclick="togglePdf('RInisw1yin@OpenReview', this)" data="https://openreview.net/pdf?id=RInisw1yin">[PDF<sup id="pdf-stars-RInisw1yin@OpenReview">1</sup>]</a>
                <a id="copy-RInisw1yin@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('RInisw1yin@OpenReview')">[Copy]</a>
                <a id="kimi-RInisw1yin@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('RInisw1yin@OpenReview', this)">[Kimi<sup id="kimi-stars-RInisw1yin@OpenReview">3</sup>]</a>
                <a id="rel-RInisw1yin@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('RInisw1yin@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-RInisw1yin@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yijie Guo" target="_blank">Yijie Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingjie Tang" target="_blank">Bingjie Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Iretiayo Akinola" target="_blank">Iretiayo Akinola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dieter Fox" target="_blank">Dieter Fox</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhishek Gupta" target="_blank">Abhishek Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yashraj Narang" target="_blank">Yashraj Narang</a>
            </p>
            <p id="summary-RInisw1yin@OpenReview" class="summary">Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made in developing such strategies for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential. In this work, we present SRSA (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. Through extensive experiments, we demonstrate that SRSA significantly outperforms the leading baseline, achieving a 22\% relative improvement in success rate, 3.7x higher stability, and 2.4x greater sample efficiency when retrieving and fine-tuning skills on unseen tasks. Moreover, in a continual learning setup, SRSA efficiently learns policies for new tasks and incorporates them into the skill library, enhancing future policy learning. Additionally, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world. Please visit our project webpage at https://srsa2024.github.io/ for videos.</p>
            <p id="subjects-RInisw1yin@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-RInisw1yin@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-RInisw1yin@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-RInisw1yin@OpenReview" onclick="foldPdfKimi('RInisw1yin@OpenReview', this)" class="hr hr-fold">
        </div><div id="R1hIXdST22@OpenReview" class="panel paper" keywords="generalist,benchmarks,hyperparameters,model,purpose,reinforcement,free,towards,linearize,algorithms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=R1hIXdST22" target="_blank" title="273/373"><span class="index notranslate">#273</span></a>
                <a id="title-R1hIXdST22@OpenReview" class="title-link" href="/venue/R1hIXdST22@OpenReview" target="_blank">Towards General-Purpose Model-Free Reinforcement Learning</a>
                <a id="pdf-R1hIXdST22@OpenReview" class="title-pdf notranslate" onclick="togglePdf('R1hIXdST22@OpenReview', this)" data="https://openreview.net/pdf?id=R1hIXdST22">[PDF<sup id="pdf-stars-R1hIXdST22@OpenReview">7</sup>]</a>
                <a id="copy-R1hIXdST22@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('R1hIXdST22@OpenReview')">[Copy]</a>
                <a id="kimi-R1hIXdST22@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('R1hIXdST22@OpenReview', this)">[Kimi<sup id="kimi-stars-R1hIXdST22@OpenReview">3</sup>]</a>
                <a id="rel-R1hIXdST22@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('R1hIXdST22@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-R1hIXdST22@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Scott Fujimoto" target="_blank">Scott Fujimoto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierluca D'Oro" target="_blank">Pierluca D'Oro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amy Zhang" target="_blank">Amy Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuandong Tian" target="_blank">Yuandong Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Rabbat" target="_blank">Michael Rabbat</a>
            </p>
            <p id="summary-R1hIXdST22@OpenReview" class="summary">Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive generalist results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate the resulting algorithm on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and generalist baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.</p>
            <p id="subjects-R1hIXdST22@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-R1hIXdST22@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-R1hIXdST22@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-R1hIXdST22@OpenReview" onclick="foldPdfKimi('R1hIXdST22@OpenReview', this)" class="hr hr-fold">
        </div><div id="QjSOgxJ0hp@OpenReview" class="panel paper" keywords="shuffled,end,privately,privacy,data,downstream,kernel,private,central,curator">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QjSOgxJ0hp" target="_blank" title="274/373"><span class="index notranslate">#274</span></a>
                <a id="title-QjSOgxJ0hp@OpenReview" class="title-link" href="/venue/QjSOgxJ0hp@OpenReview" target="_blank">Learning from End User Data with Shuffled Differential Privacy over Kernel Densities</a>
                <a id="pdf-QjSOgxJ0hp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QjSOgxJ0hp@OpenReview', this)" data="https://openreview.net/pdf?id=QjSOgxJ0hp">[PDF<sup id="pdf-stars-QjSOgxJ0hp@OpenReview">5</sup>]</a>
                <a id="copy-QjSOgxJ0hp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QjSOgxJ0hp@OpenReview')">[Copy]</a>
                <a id="kimi-QjSOgxJ0hp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QjSOgxJ0hp@OpenReview', this)">[Kimi<sup id="kimi-stars-QjSOgxJ0hp@OpenReview">3</sup>]</a>
                <a id="rel-QjSOgxJ0hp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QjSOgxJ0hp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QjSOgxJ0hp@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tal Wagner" target="_blank">Tal Wagner</a>
            </p>
            <p id="summary-QjSOgxJ0hp@OpenReview" class="summary">We study a setting of collecting and learning from private data distributed across end users.In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy. Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. We use it to privately learn a classifier from the end user data, by learning a private density function per class. Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP.</p>
            <p id="subjects-QjSOgxJ0hp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-QjSOgxJ0hp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QjSOgxJ0hp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QjSOgxJ0hp@OpenReview" onclick="foldPdfKimi('QjSOgxJ0hp@OpenReview', this)" class="hr hr-fold">
        </div><div id="Q150eWkQ4I@OpenReview" class="panel paper" keywords="msi,sci,unmixing,subspace,spectral,diffusion,compressive,refinement,imaging,driven">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Q150eWkQ4I" target="_blank" title="275/373"><span class="index notranslate">#275</span></a>
                <a id="title-Q150eWkQ4I@OpenReview" class="title-link" href="/venue/Q150eWkQ4I@OpenReview" target="_blank">Spectral Compressive Imaging via Unmixing-driven Subspace Diffusion Refinement</a>
                <a id="pdf-Q150eWkQ4I@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Q150eWkQ4I@OpenReview', this)" data="https://openreview.net/pdf?id=Q150eWkQ4I">[PDF<sup id="pdf-stars-Q150eWkQ4I@OpenReview">1</sup>]</a>
                <a id="copy-Q150eWkQ4I@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Q150eWkQ4I@OpenReview')">[Copy]</a>
                <a id="kimi-Q150eWkQ4I@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Q150eWkQ4I@OpenReview', this)">[Kimi<sup id="kimi-stars-Q150eWkQ4I@OpenReview">2</sup>]</a>
                <a id="rel-Q150eWkQ4I@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Q150eWkQ4I@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Q150eWkQ4I@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haijin Zeng" target="_blank">Haijin Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benteng Sun" target="_blank">Benteng Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongyong Chen" target="_blank">Yongyong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyong Su" target="_blank">Jingyong Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Xu" target="_blank">Yong Xu</a>
            </p>
            <p id="summary-Q150eWkQ4I@OpenReview" class="summary">Spectral Compressive Imaging (SCI) reconstruction is inherently ill-posed, offering multiple plausible solutions from a single observation. Traditional deterministic methods typically struggle to effectively recover high-frequency details. Although diffusion models offer promising solutions to this challenge, their application is constrained by the limited training data and high computational demands associated with multispectral images (MSIs), complicating direct training. To address these issues, we propose a novel Predict-and-unmixing-driven-Subspace-Refine framework (PSR-SCI). This framework begins with a cost-effective predictor that produces an initial, rough estimate of the MSI. Subsequently, we introduce a unmixing-driven reversible spectral embedding module that decomposes the MSI into subspace images and spectral coefficients. This decomposition facilitates the adaptation of pre-trained RGB diffusion models and focuses refinement processes on high-frequency details, thereby enabling efficient diffusion generation with minimal MSI data. Additionally, we design a high-dimensional guidance mechanism with imaging consistency to enhance the model's efficacy. The refined subspace image is then reconstructed back into an MSI using the reversible embedding, yielding the final MSI with full spectral resolution. Experimental results on the standard KAIST and zero-shot datasets NTIRE, ICVL, and Harvard show that PSR-SCI enhances visual quality and delivers PSNR and SSIM metrics comparable to existing diffusion, transformer, and deep unfolding techniques. This framework provides a robust alternative to traditional deterministic SCI reconstruction methods.</p>
            <p id="subjects-Q150eWkQ4I@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Q150eWkQ4I@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Q150eWkQ4I@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Q150eWkQ4I@OpenReview" onclick="foldPdfKimi('Q150eWkQ4I@OpenReview', this)" class="hr hr-fold">
        </div><div id="PGhiPGBf47@OpenReview" class="panel paper" keywords="dailydilemmas,moral,values,dilemmas,llms,life,daily,quandaries,value,prioritization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PGhiPGBf47" target="_blank" title="276/373"><span class="index notranslate">#276</span></a>
                <a id="title-PGhiPGBf47@OpenReview" class="title-link" href="/venue/PGhiPGBf47@OpenReview" target="_blank">DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life</a>
                <a id="pdf-PGhiPGBf47@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PGhiPGBf47@OpenReview', this)" data="https://openreview.net/pdf?id=PGhiPGBf47">[PDF<sup id="pdf-stars-PGhiPGBf47@OpenReview">2</sup>]</a>
                <a id="copy-PGhiPGBf47@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PGhiPGBf47@OpenReview')">[Copy]</a>
                <a id="kimi-PGhiPGBf47@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PGhiPGBf47@OpenReview', this)">[Kimi<sup id="kimi-stars-PGhiPGBf47@OpenReview">4</sup>]</a>
                <a id="rel-PGhiPGBf47@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PGhiPGBf47@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PGhiPGBf47@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Ying (Kelly) Chiu" target="_blank">Yu Ying (Kelly) Chiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Jiang" target="_blank">Liwei Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yejin Choi" target="_blank">Yejin Choi</a>
            </p>
            <p id="summary-PGhiPGBf47@OpenReview" class="summary">As we increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of the users. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma includes two possible actions and with each action, the affected parties and human values invoked. Based on these dilemmas, we consolidated a set of human values across everyday topics e.g., interpersonal relationships, workplace, and environmental issues. We evaluated LLMs on these dilemmas to determine what action they will take and the values represented by these actions. Then, we analyzed these values through the lens of five popular theories inspired by sociology, psychology and philosophy. These theories are: World Value Survey, Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik Wheel of Emotion. We find that LLMs are most aligned with the self-expression over survival values in terms of World Value Survey, care over loyalty in Moral Foundation Theory. Interestingly, we find large preferences differences in models for some core values such as truthfulness e.g., Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to select it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their released principles reflect their actual value prioritization when facing nuanced moral reasoning in daily-life settings. We find that end users cannot effectively steer such prioritization using system prompts.</p>
            <p id="subjects-PGhiPGBf47@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PGhiPGBf47@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PGhiPGBf47@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PGhiPGBf47@OpenReview" onclick="foldPdfKimi('PGhiPGBf47@OpenReview', this)" class="hr hr-fold">
        </div><div id="OeBY9XqiTz@OpenReview" class="panel paper" keywords="samba,sambamotr,tracklets,dependencies,occlusions,tracklet,interdependencies,synchronized,sequences,tracking">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OeBY9XqiTz" target="_blank" title="277/373"><span class="index notranslate">#277</span></a>
                <a id="title-OeBY9XqiTz@OpenReview" class="title-link" href="/venue/OeBY9XqiTz@OpenReview" target="_blank">Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking</a>
                <a id="pdf-OeBY9XqiTz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OeBY9XqiTz@OpenReview', this)" data="https://openreview.net/pdf?id=OeBY9XqiTz">[PDF<sup id="pdf-stars-OeBY9XqiTz@OpenReview">2</sup>]</a>
                <a id="copy-OeBY9XqiTz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OeBY9XqiTz@OpenReview')">[Copy]</a>
                <a id="kimi-OeBY9XqiTz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OeBY9XqiTz@OpenReview', this)">[Kimi<sup id="kimi-stars-OeBY9XqiTz@OpenReview">3</sup>]</a>
                <a id="rel-OeBY9XqiTz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OeBY9XqiTz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OeBY9XqiTz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mattia Segu" target="_blank">Mattia Segu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luigi Piccinelli" target="_blank">Luigi Piccinelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Li" target="_blank">Siyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yung-Hsu Yang" target="_blank">Yung-Hsu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernt Schiele" target="_blank">Bernt Schiele</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luc Van Gool" target="_blank">Luc Van Gool</a>
            </p>
            <p id="summary-OeBY9XqiTz@OpenReview" class="summary">Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets.</p>
            <p id="subjects-OeBY9XqiTz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-OeBY9XqiTz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OeBY9XqiTz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OeBY9XqiTz@OpenReview" onclick="foldPdfKimi('OeBY9XqiTz@OpenReview', this)" class="hr hr-fold">
        </div><div id="NGKQoaqLpo@OpenReview" class="panel paper" keywords="priming,pollutes,llm,knowledge,texts,alterations,new,learning,undesirable,outlandish">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NGKQoaqLpo" target="_blank" title="278/373"><span class="index notranslate">#278</span></a>
                <a id="title-NGKQoaqLpo@OpenReview" class="title-link" href="/venue/NGKQoaqLpo@OpenReview" target="_blank">How new data pollutes LLM knowledge and how to dilute it</a>
                <a id="pdf-NGKQoaqLpo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NGKQoaqLpo@OpenReview', this)" data="https://openreview.net/pdf?id=NGKQoaqLpo">[PDF<sup id="pdf-stars-NGKQoaqLpo@OpenReview">2</sup>]</a>
                <a id="copy-NGKQoaqLpo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NGKQoaqLpo@OpenReview')">[Copy]</a>
                <a id="kimi-NGKQoaqLpo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NGKQoaqLpo@OpenReview', this)">[Kimi<sup id="kimi-stars-NGKQoaqLpo@OpenReview">4</sup>]</a>
                <a id="rel-NGKQoaqLpo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NGKQoaqLpo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NGKQoaqLpo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Sun" target="_blank">Chen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renat Aksitov" target="_blank">Renat Aksitov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrey Zhmoginov" target="_blank">Andrey Zhmoginov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nolan Miller" target="_blank">Nolan Miller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Vladymyrov" target="_blank">Max Vladymyrov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ulrich Rueckert" target="_blank">Ulrich Rueckert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Been Kim" target="_blank">Been Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Sandler" target="_blank">Mark Sandler</a>
            </p>
            <p id="summary-NGKQoaqLpo@OpenReview" class="summary">Understanding how the learning of new texts alter the existing knowledge in a large language model is of great importance, because it is through these accumulated changes that the LLM was initially pre-trained, and is also through such changes that continual, new learning in LLMs can proceed. As a result, both desirable alterations (i.e. generalization) and undesirable alterations (i.e. hallucination) can occur. Here, we study the learning of new texts, one at a time, and ask: how does it impact the underlying LLM knowledge? We show that learning new texts induce 'priming', an undesirable effect that pollutes existing knowledge where it should not.Centrally, we demonstrate that we can predict how much priming will happen after learning, using token probability before learning. This was empirically robust across models (PALM-2-xs/s, Gemma-2b, Llama-2-7b), of various sizes, and training stages. To show this, we created a new dataset, called "Outlandish" consisting of 1320 different samples with diverse textual characteristics. Finally, we propose two strategies to mitigate the spread of priming: first, a simple text augmentation technique which we call the "stepping-stone'', and second, a novel update pruning technique ("ignore-k"). These decrease priming by a median of 50%-75% and 50%-95% respectively depending on the model architecture, and enhance the specificity of new learning in language models. The dataset and reproducible findings can be found [LINK omitted for double blind review].</p>
            <p id="subjects-NGKQoaqLpo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-NGKQoaqLpo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NGKQoaqLpo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NGKQoaqLpo@OpenReview" onclick="foldPdfKimi('NGKQoaqLpo@OpenReview', this)" class="hr hr-fold">
        </div><div id="N4NhVN30ph@OpenReview" class="panel paper" keywords="erl,policy,top,episodic,action,transformer,reinforcement,entire,trajectories,sequences">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=N4NhVN30ph" target="_blank" title="279/373"><span class="index notranslate">#279</span></a>
                <a id="title-N4NhVN30ph@OpenReview" class="title-link" href="/venue/N4NhVN30ph@OpenReview" target="_blank">TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning</a>
                <a id="pdf-N4NhVN30ph@OpenReview" class="title-pdf notranslate" onclick="togglePdf('N4NhVN30ph@OpenReview', this)" data="https://openreview.net/pdf?id=N4NhVN30ph">[PDF<sup id="pdf-stars-N4NhVN30ph@OpenReview">7</sup>]</a>
                <a id="copy-N4NhVN30ph@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('N4NhVN30ph@OpenReview')">[Copy]</a>
                <a id="kimi-N4NhVN30ph@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('N4NhVN30ph@OpenReview', this)">[Kimi<sup id="kimi-stars-N4NhVN30ph@OpenReview">3</sup>]</a>
                <a id="rel-N4NhVN30ph@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('N4NhVN30ph@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-N4NhVN30ph@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ge Li" target="_blank">Ge Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Tian" target="_blank">Dong Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyi Zhou" target="_blank">Hongyi Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinkai Jiang" target="_blank">Xinkai Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rudolf Lioutikov" target="_blank">Rudolf Lioutikov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gerhard Neumann" target="_blank">Gerhard Neumann</a>
            </p>
            <p id="summary-N4NhVN30ph@OpenReview" class="summary">This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.</p>
            <p id="subjects-N4NhVN30ph@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-N4NhVN30ph@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-N4NhVN30ph@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-N4NhVN30ph@OpenReview" onclick="foldPdfKimi('N4NhVN30ph@OpenReview', this)" class="hr hr-fold">
        </div><div id="MtDd7rWok1@OpenReview" class="panel paper" keywords="prompt,exposure,bias,dms,sampling,anti,prediction,diffusion,rectifies,trajectory">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MtDd7rWok1" target="_blank" title="280/373"><span class="index notranslate">#280</span></a>
                <a id="title-MtDd7rWok1@OpenReview" class="title-link" href="/venue/MtDd7rWok1@OpenReview" target="_blank">Anti-Exposure Bias in Diffusion Models via Prompt Learning</a>
                <a id="pdf-MtDd7rWok1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MtDd7rWok1@OpenReview', this)" data="https://openreview.net/pdf?id=MtDd7rWok1">[PDF<sup id="pdf-stars-MtDd7rWok1@OpenReview">2</sup>]</a>
                <a id="copy-MtDd7rWok1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MtDd7rWok1@OpenReview')">[Copy]</a>
                <a id="kimi-MtDd7rWok1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MtDd7rWok1@OpenReview', this)">[Kimi<sup id="kimi-stars-MtDd7rWok1@OpenReview">2</sup>]</a>
                <a id="rel-MtDd7rWok1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MtDd7rWok1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MtDd7rWok1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junyu Zhang" target="_blank">Junyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daochang Liu" target="_blank">Daochang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eunbyung Park" target="_blank">Eunbyung Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shichao Zhang" target="_blank">Shichao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Xu" target="_blank">Chang Xu</a>
            </p>
            <p id="summary-MtDd7rWok1@OpenReview" class="summary">Diffusion models (DMs) have achieved record-breaking performance in image generation tasks.Nevertheless, in practice, the training-sampling discrepancy, caused by score estimation error and discretization error, limits the modeling ability of DMs, a phenomenon known as exposure bias.To alleviate such exposure bias and further improve the generative performance, we put forward a prompt learning framework built upon a lightweight prompt prediction model.Concretely, our model learns an anti-bias prompt for the generated sample at each sampling step, aiming to compensate for the exposure bias that arises.Following this design philosophy, our framework rectifies the sampling trajectory to match the training trajectory, thereby reducing the divergence between the target data distribution and the modeling distribution.To train the prompt prediction model, we simulate exposure bias by constructing training data and introduce a time-dependent weighting function for optimization.Empirical results on various DMs demonstrate the superiority of our prompt learning framework across three benchmark datasets.Importantly, the optimized prompt prediction model effectively improves image quality with only a 5\% increase in sampling overhead, which remains negligible.</p>
            <p id="subjects-MtDd7rWok1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-MtDd7rWok1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MtDd7rWok1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MtDd7rWok1@OpenReview" onclick="foldPdfKimi('MtDd7rWok1@OpenReview', this)" class="hr hr-fold">
        </div><div id="MagmwodCAB@OpenReview" class="panel paper" keywords="3dis,mig,instance,rendering,depth,generation,controlnet,attribute,decoupled,layouts">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MagmwodCAB" target="_blank" title="281/373"><span class="index notranslate">#281</span></a>
                <a id="title-MagmwodCAB@OpenReview" class="title-link" href="/venue/MagmwodCAB@OpenReview" target="_blank">3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image Generation</a>
                <a id="pdf-MagmwodCAB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MagmwodCAB@OpenReview', this)" data="https://openreview.net/pdf?id=MagmwodCAB">[PDF<sup id="pdf-stars-MagmwodCAB@OpenReview">2</sup>]</a>
                <a id="copy-MagmwodCAB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MagmwodCAB@OpenReview')">[Copy]</a>
                <a id="kimi-MagmwodCAB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MagmwodCAB@OpenReview', this)">[Kimi<sup id="kimi-stars-MagmwodCAB@OpenReview">2</sup>]</a>
                <a id="rel-MagmwodCAB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MagmwodCAB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MagmwodCAB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dewei Zhou" target="_blank">Dewei Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ji Xie" target="_blank">Ji Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongxin Yang" target="_blank">Zongxin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>
            </p>
            <p id="summary-MagmwodCAB@OpenReview" class="summary">The increasing demand for controllable outputs in text-to-image generation has spurred advancements in multi-instance generation (MIG), allowing users to define both instance layouts and attributes. However, unlike image-conditional generation methods such as ControlNet, MIG techniques have not been widely adopted in state-of-the-art models like SD2 and SDXL, primarily due to the challenge of building robust renderers that simultaneously handle instance positioning and attribute rendering. In this paper, we introduce Depth-Driven Decoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG process into two stages: (i) generating a coarse scene depth map for accurate instance positioning and scene composition, and (ii) rendering fine-grained attributes using pre-trained ControlNet on any foundational model, without additional training. Our 3DIS framework integrates a custom adapter into LDM3D for precise depth-based layouts and employs a finetuning-free method for enhanced instance-level attribute rendering. Extensive experiments on COCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly outperforms existing methods in both layout precision and attribute rendering. Notably, 3DIS offers seamless compatibility with diverse foundational models, providing a robust, adaptable solution for advanced multi-instance generation.</p>
            <p id="subjects-MagmwodCAB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-MagmwodCAB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MagmwodCAB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MagmwodCAB@OpenReview" onclick="foldPdfKimi('MagmwodCAB@OpenReview', this)" class="hr hr-fold">
        </div><div id="LqTz13JS2P@OpenReview" class="panel paper" keywords="agent,principal,regret,sreg,swap,generalized,mathrm,persuasion,reg,stackelberg">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LqTz13JS2P" target="_blank" title="282/373"><span class="index notranslate">#282</span></a>
                <a id="title-LqTz13JS2P@OpenReview" class="title-link" href="/venue/LqTz13JS2P@OpenReview" target="_blank">Generalized Principal-Agent Problem with a Learning Agent</a>
                <a id="pdf-LqTz13JS2P@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LqTz13JS2P@OpenReview', this)" data="https://openreview.net/pdf?id=LqTz13JS2P">[PDF<sup id="pdf-stars-LqTz13JS2P@OpenReview">5</sup>]</a>
                <a id="copy-LqTz13JS2P@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LqTz13JS2P@OpenReview')">[Copy]</a>
                <a id="kimi-LqTz13JS2P@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LqTz13JS2P@OpenReview', this)">[Kimi<sup id="kimi-stars-LqTz13JS2P@OpenReview">9</sup>]</a>
                <a id="rel-LqTz13JS2P@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LqTz13JS2P@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LqTz13JS2P@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Lin" target="_blank">Tao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiling Chen" target="_blank">Yiling Chen</a>
            </p>
            <p id="summary-LqTz13JS2P@OpenReview" class="summary">Generalized principal-agent problems, including Stackelberg games, contract design, and Bayesian persuasion, are a class of economic problems where an agent best responds to a principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) if the agent uses contextual no-regret learning algorithms with regret <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-177-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;R&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1003" style="width: 3.857em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.08em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1004"><span class="texatom" id="MathJax-Span-1005"><span class="mrow" id="MathJax-Span-1006"><span class="mi" id="MathJax-Span-1007" style="font-family: MathJax_Main;">R</span><span class="mi" id="MathJax-Span-1008" style="font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-1009" style="font-family: MathJax_Main;">g</span></span></span><span class="mo" id="MathJax-Span-1010" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1011" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-1012" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-177">\mathrm{Reg}(T)</script>, then the principal can guarantee utility at least <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-178-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0398;&lt;/mi&gt;&lt;mstyle scriptlevel=&quot;0&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;&gt;(&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;msqrt&gt;&lt;mstyle displaystyle=&quot;false&quot; scriptlevel=&quot;0&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;R&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/msqrt&gt;&lt;mstyle scriptlevel=&quot;0&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1013" style="width: 9.378em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.836em, 1007.66em, 2.919em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-1014"><span class="msubsup" id="MathJax-Span-1015"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1016" style="font-family: MathJax_Math-italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.836em;"><span class="mo" id="MathJax-Span-1017" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1018" style="font-family: MathJax_Main; padding-left: 0.211em;">−</span><span class="mi" id="MathJax-Span-1019" style="font-family: MathJax_Main; padding-left: 0.211em;">Θ</span><span class="mstyle" id="MathJax-Span-1020" style=""><span class="mrow" id="MathJax-Span-1021"><span class="texatom" id="MathJax-Span-1022" style=""><span class="mrow" id="MathJax-Span-1023"><span class="mo" id="MathJax-Span-1024" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span></span></span></span></span><span class="msqrt" id="MathJax-Span-1025"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px;"><span style="position: absolute; clip: rect(2.763em, 1002.61em, 4.534em, -999.997em); top: -4.008em; left: 0.992em;"><span class="mrow" id="MathJax-Span-1026"><span class="mstyle" id="MathJax-Span-1027"><span class="mrow" id="MathJax-Span-1028"><span class="mfrac" id="MathJax-Span-1029"><span style="display: inline-block; position: relative; width: 2.346em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.461em, 1002.19em, 2.451em, -999.997em); top: -2.706em; left: 50%; margin-left: -1.091em;"><span class="mrow" id="MathJax-Span-1030"><span class="texatom" id="MathJax-Span-1031"><span class="mrow" id="MathJax-Span-1032"><span class="mi" id="MathJax-Span-1033" style="font-size: 70.7%; font-family: MathJax_Main;">R</span><span class="mi" id="MathJax-Span-1034" style="font-size: 70.7%; font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-1035" style="font-size: 70.7%; font-family: MathJax_Main;">g</span></span></span><span class="mo" id="MathJax-Span-1036" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1037" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-1038" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.47em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-1039" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1002.35em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.346em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1002.55em, 3.961em, -999.997em); top: -4.997em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.826em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.419em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.888em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.409em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.711em, 1000.99em, 4.794em, -999.997em); top: -4.112em; left: 0em;"><span style="font-family: MathJax_Size2;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mstyle" id="MathJax-Span-1040" style=""><span class="mrow" id="MathJax-Span-1041"><span class="texatom" id="MathJax-Span-1042" style=""><span class="mrow" id="MathJax-Span-1043"><span class="mo" id="MathJax-Span-1044" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.684em; border-left: 0px solid; width: 0px; height: 2.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>U</mi><mo>∗</mo></msup><mo>−</mo><mi mathvariant="normal">Θ</mi><mstyle scriptlevel="0"><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.2em" minsize="1.2em">(</mo></mrow></mstyle><msqrt><mstyle displaystyle="false" scriptlevel="0"><mfrac><mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></mfrac></mstyle></msqrt><mstyle scriptlevel="0"><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.2em" minsize="1.2em">)</mo></mrow></mstyle></math></span></span><script type="math/tex" id="MathJax-Element-178">U^* - \Theta\big(\sqrt{\tfrac{\mathrm{Reg}(T)}{T}}\big)</script>, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-179-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1045" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1046"><span class="msubsup" id="MathJax-Span-1047"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1048" style="font-family: MathJax_Math-italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.836em;"><span class="mo" id="MathJax-Span-1049" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>U</mi><mo>∗</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-179">U^*</script> is the principal's optimal utility in the classic model with a best-responding agent.(2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-180-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;S&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;R&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1050" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.6em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1051"><span class="texatom" id="MathJax-Span-1052"><span class="mrow" id="MathJax-Span-1053"><span class="mi" id="MathJax-Span-1054" style="font-family: MathJax_Main;">S</span><span class="mi" id="MathJax-Span-1055" style="font-family: MathJax_Main;">R</span><span class="mi" id="MathJax-Span-1056" style="font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-1057" style="font-family: MathJax_Main;">g</span></span></span><span class="mo" id="MathJax-Span-1058" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1059" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-1060" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">S</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-180">\mathrm{SReg}(T)</script>, then the principal cannot obtain utility more than <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-181-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;S&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;R&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1061" style="width: 8.44em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.888em, 1006.93em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1062"><span class="msubsup" id="MathJax-Span-1063"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1064" style="font-family: MathJax_Math-italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.836em;"><span class="mo" id="MathJax-Span-1065" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1066" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-1067" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">O</span><span class="mo" id="MathJax-Span-1068" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-1069"><span style="display: inline-block; position: relative; width: 2.763em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.461em, 1002.61em, 2.451em, -999.997em); top: -2.706em; left: 50%; margin-left: -1.299em;"><span class="texatom" id="MathJax-Span-1070"><span class="mrow" id="MathJax-Span-1071"><span class="mi" id="MathJax-Span-1072" style="font-size: 70.7%; font-family: MathJax_Main;">S</span><span class="mi" id="MathJax-Span-1073" style="font-size: 70.7%; font-family: MathJax_Main;">R</span><span class="mi" id="MathJax-Span-1074" style="font-size: 70.7%; font-family: MathJax_Main;">e</span><span class="mi" id="MathJax-Span-1075" style="font-size: 70.7%; font-family: MathJax_Main;">g</span><span class="mo" id="MathJax-Span-1076" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1077" style="font-size: 70.7%; font-family: MathJax_Main;">T</span><span class="mo" id="MathJax-Span-1078" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.47em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-1079" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1002.76em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.763em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-1080" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>U</mi><mo>∗</mo></msup><mo>+</mo><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">S</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">g</mi><mo stretchy="false">(</mo><mi mathvariant="normal">T</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></mfrac><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-181">U^* + O(\frac{\mathrm{SReg(T)}}{T})</script>. But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-182-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1081" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1082"><span class="msubsup" id="MathJax-Span-1083"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1084" style="font-family: MathJax_Math-italic;">U<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.836em;"><span class="mo" id="MathJax-Span-1085" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>U</mi><mo>∗</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-182">U^*</script>.These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information.</p>
            <p id="subjects-LqTz13JS2P@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-LqTz13JS2P@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LqTz13JS2P@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LqTz13JS2P@OpenReview" onclick="foldPdfKimi('LqTz13JS2P@OpenReview', this)" class="hr hr-fold">
        </div><div id="L14sqcrUC3@OpenReview" class="panel paper" keywords="tabular,tabred,academic,benchmarks,datasets,splits,pitfalls,evaluation,underrepresented,deployment">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=L14sqcrUC3" target="_blank" title="283/373"><span class="index notranslate">#283</span></a>
                <a id="title-L14sqcrUC3@OpenReview" class="title-link" href="/venue/L14sqcrUC3@OpenReview" target="_blank">Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks</a>
                <a id="pdf-L14sqcrUC3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('L14sqcrUC3@OpenReview', this)" data="https://openreview.net/pdf?id=L14sqcrUC3">[PDF<sup id="pdf-stars-L14sqcrUC3@OpenReview">4</sup>]</a>
                <a id="copy-L14sqcrUC3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('L14sqcrUC3@OpenReview')">[Copy]</a>
                <a id="kimi-L14sqcrUC3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('L14sqcrUC3@OpenReview', this)">[Kimi<sup id="kimi-stars-L14sqcrUC3@OpenReview">2</sup>]</a>
                <a id="rel-L14sqcrUC3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('L14sqcrUC3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-L14sqcrUC3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ivan Rubachev" target="_blank">Ivan Rubachev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikolay Kartashev" target="_blank">Nikolay Kartashev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yury Gorishniy" target="_blank">Yury Gorishniy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Artem Babenko" target="_blank">Artem Babenko</a>
            </p>
            <p id="summary-L14sqcrUC3@OpenReview" class="summary">Advances in machine learning research drive progress in real-world applications. To ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.First, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets generally lack timestamp metadata to enable such evaluation.Second, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets.In this work, we aim to understand how recent advances in tabular deep learning, which are evaluated on academic benchmarks, transfer to these underrepresented conditions.To this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. We reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on time-based data splits leads to different methods ranking, compared to evaluation on random splits, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting.</p>
            <p id="subjects-L14sqcrUC3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-L14sqcrUC3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-L14sqcrUC3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-L14sqcrUC3@OpenReview" onclick="foldPdfKimi('L14sqcrUC3@OpenReview', this)" class="hr hr-fold">
        </div><div id="Kpjvm2mB0K@OpenReview" class="panel paper" keywords="mathbf,ell,tilde,varepsilon,streaming,space,kappa,regression,columns,poly">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Kpjvm2mB0K" target="_blank" title="284/373"><span class="index notranslate">#284</span></a>
                <a id="title-Kpjvm2mB0K@OpenReview" class="title-link" href="/venue/Kpjvm2mB0K@OpenReview" target="_blank">Streaming Algorithms For <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-183-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1086" style="width: 1.009em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.835em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.356em, 1000.84em, 2.536em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-1087"><span class="msubsup" id="MathJax-Span-1088"><span style="display: inline-block; position: relative; width: 0.835em; height: 0px;"><span style="position: absolute; clip: rect(1.391em, 1000.38em, 2.328em, -999.998em); top: -2.186em; left: 0em;"><span class="mi" id="MathJax-Span-1089" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.189em;"></span></span><span style="position: absolute; top: -2.012em; left: 0.418em;"><span class="mi" id="MathJax-Span-1090" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.373em; border-left: 0px solid; width: 0px; height: 1.294em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-183">\ell_p</script> Flows and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-184-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1091" style="width: 1.009em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.835em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.356em, 1000.84em, 2.536em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-1092"><span class="msubsup" id="MathJax-Span-1093"><span style="display: inline-block; position: relative; width: 0.835em; height: 0px;"><span style="position: absolute; clip: rect(1.391em, 1000.38em, 2.328em, -999.998em); top: -2.186em; left: 0em;"><span class="mi" id="MathJax-Span-1094" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.189em;"></span></span><span style="position: absolute; top: -2.012em; left: 0.418em;"><span class="mi" id="MathJax-Span-1095" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.373em; border-left: 0px solid; width: 0px; height: 1.294em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-184">\ell_p</script> Regression</a>
                <a id="pdf-Kpjvm2mB0K@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Kpjvm2mB0K@OpenReview', this)" data="https://openreview.net/pdf?id=Kpjvm2mB0K">[PDF<sup id="pdf-stars-Kpjvm2mB0K@OpenReview">3</sup>]</a>
                <a id="copy-Kpjvm2mB0K@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Kpjvm2mB0K@OpenReview')">[Copy]</a>
                <a id="kimi-Kpjvm2mB0K@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Kpjvm2mB0K@OpenReview', this)">[Kimi<sup id="kimi-stars-Kpjvm2mB0K@OpenReview">2</sup>]</a>
                <a id="rel-Kpjvm2mB0K@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Kpjvm2mB0K@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Kpjvm2mB0K@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Chakrabarti" target="_blank">Amit Chakrabarti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeffrey Jiang" target="_blank">Jeffrey Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Woodruff" target="_blank">David Woodruff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taisuke Yasuda" target="_blank">Taisuke Yasuda</a>
            </p>
            <p id="summary-Kpjvm2mB0K@OpenReview" class="summary">We initiate the study of one-pass streaming algorithms for underdetermined <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-185-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1096" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1097"><span class="msubsup" id="MathJax-Span-1098"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1099" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-1100" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-185">\ell_p</script> linear regression problems of the form <span class="MathJax_Preview" style="color: inherit;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax notranslate" id="MathJax-Element-186-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;munder&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;mspace width=&quot;thinmathspace&quot; /&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;2em&quot; /&gt;&lt;mtext&gt;where&amp;#xA0;&lt;/mtext&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mtext&gt;&amp;#xA0;with&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x226A;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mspace width=&quot;thinmathspace&quot; /&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1101" style="width: 23.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1019.17em, 3.128em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1102"><span class="munderover" id="MathJax-Span-1103"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.67em, 2.294em, -999.997em); top: -2.133em; left: 0.159em;"><span class="mo" id="MathJax-Span-1104" style="font-family: MathJax_Main;">min</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1001.98em, 2.398em, -999.997em); top: -1.456em; left: 0em;"><span class="texatom" id="MathJax-Span-1105"><span class="mrow" id="MathJax-Span-1106"><span class="texatom" id="MathJax-Span-1107"><span class="mrow" id="MathJax-Span-1108"><span class="mi" id="MathJax-Span-1109" style="font-size: 70.7%; font-family: MathJax_Main-bold;">A</span></span></span><span class="texatom" id="MathJax-Span-1110"><span class="mrow" id="MathJax-Span-1111"><span class="mi" id="MathJax-Span-1112" style="font-size: 70.7%; font-family: MathJax_Main-bold;">x</span></span></span><span class="mo" id="MathJax-Span-1113" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="texatom" id="MathJax-Span-1114"><span class="mrow" id="MathJax-Span-1115"><span class="mi" id="MathJax-Span-1116" style="font-size: 70.7%; font-family: MathJax_Main-bold;">b</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1117" style="font-family: MathJax_Main;">∥</span><span class="texatom" id="MathJax-Span-1118"><span class="mrow" id="MathJax-Span-1119"><span class="mi" id="MathJax-Span-1120" style="font-family: MathJax_Main-bold;">x</span></span></span><span class="msubsup" id="MathJax-Span-1121"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.37em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-1122" style="font-family: MathJax_Main;">∥</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.523em;"><span class="mi" id="MathJax-Span-1123" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mspace" id="MathJax-Span-1124" style="height: 0em; vertical-align: 0em; width: 0.159em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1125" style="font-family: MathJax_Main;">,</span><span class="mspace" id="MathJax-Span-1126" style="height: 0em; vertical-align: 0em; width: 1.982em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-1127" style="font-family: MathJax_Main; padding-left: 0.159em;">where&nbsp;</span><span class="texatom" id="MathJax-Span-1128"><span class="mrow" id="MathJax-Span-1129"><span class="mi" id="MathJax-Span-1130" style="font-family: MathJax_Main-bold;">A</span></span></span><span class="mo" id="MathJax-Span-1131" style="font-family: MathJax_Main; padding-left: 0.263em;">∈</span><span class="msubsup" id="MathJax-Span-1132" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.607em, -999.997em); top: -2.445em; left: 0em;"><span class="texatom" id="MathJax-Span-1133"><span class="mrow" id="MathJax-Span-1134"><span class="mi" id="MathJax-Span-1135" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1136"><span class="mrow" id="MathJax-Span-1137"><span class="mi" id="MathJax-Span-1138" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1139" style="font-size: 70.7%; font-family: MathJax_Main;">×</span><span class="mi" id="MathJax-Span-1140" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mtext" id="MathJax-Span-1141" style="font-family: MathJax_Main;">&nbsp;with&nbsp;</span><span class="mi" id="MathJax-Span-1142" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1143" style="font-family: MathJax_Main; padding-left: 0.263em;">≪</span><span class="mi" id="MathJax-Span-1144" style="font-family: MathJax_Math-italic; padding-left: 0.263em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mspace" id="MathJax-Span-1145" style="height: 0em; vertical-align: 0em; width: 0.159em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-1146" style="font-family: MathJax_Main;">,</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.997em; border-left: 0px solid; width: 0px; height: 2.128em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">A</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">b</mi></mrow></mrow></munder><mo fence="false" stretchy="false">‖</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><msub><mo fence="false" stretchy="false">‖</mo><mi>p</mi></msub><mspace width="thinmathspace"></mspace><mo>,</mo><mspace width="2em"></mspace><mtext>where&nbsp;</mtext><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">A</mi></mrow><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup><mtext>&nbsp;with&nbsp;</mtext><mi>n</mi><mo>≪</mo><mi>d</mi><mspace width="thinmathspace"></mspace><mo>,</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-186"> \min_{\mathbf A\mathbf x = \mathbf b} \lVert\mathbf x\rVert_p \,, \qquad \text{where } \mathbf A \in \mathbb R^{n \times d} \text{ with } n \ll d \,, </script> which generalizes basis pursuit (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-187-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1147" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1148"><span class="mi" id="MathJax-Span-1149" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1150" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-1151" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-187">p = 1</script>) and least squares solutions to underdetermined linear systems (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-188-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1152" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1153"><span class="mi" id="MathJax-Span-1154" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1155" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-1156" style="font-family: MathJax_Main; padding-left: 0.263em;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-188">p = 2</script>). We study the column-arrival streaming model, in which the columns of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-189-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1157" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1158"><span class="texatom" id="MathJax-Span-1159"><span class="mrow" id="MathJax-Span-1160"><span class="mi" id="MathJax-Span-1161" style="font-family: MathJax_Main-bold;">A</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">A</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-189">\mathbf A</script> are presented one by one in a stream. When <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-190-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1162" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1163"><span class="texatom" id="MathJax-Span-1164"><span class="mrow" id="MathJax-Span-1165"><span class="mi" id="MathJax-Span-1166" style="font-family: MathJax_Main-bold;">A</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">A</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-190">\mathbf A</script> is the incidence matrix of a graph, this corresponds to an edge insertion graph stream, and the regression problem captures <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-191-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1167" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1168"><span class="msubsup" id="MathJax-Span-1169"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1170" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-1171" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-191">\ell_p</script> flows which includes transshipment (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-192-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1172" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1173"><span class="mi" id="MathJax-Span-1174" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1175" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-1176" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-192">p = 1</script>), electrical flows (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-193-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1177" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1178"><span class="mi" id="MathJax-Span-1179" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1180" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-1181" style="font-family: MathJax_Main; padding-left: 0.263em;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-193">p = 2</script>), and max flow (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-194-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1182" style="width: 3.388em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.815em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1002.76em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1183"><span class="mi" id="MathJax-Span-1184" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1185" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mi" id="MathJax-Span-1186" style="font-family: MathJax_Main; padding-left: 0.263em;">∞</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mi mathvariant="normal">∞</mi></math></span></span><script type="math/tex" id="MathJax-Element-194">p = \infty</script>) on undirected graphs as special cases. Our goal is to design algorithms which use space much less than the entire stream, which has a length of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-195-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1187" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1188"><span class="mi" id="MathJax-Span-1189" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-195">d</script>. For the task of estimating the cost of the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-196-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1190" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1191"><span class="msubsup" id="MathJax-Span-1192"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1193" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-1194" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-196">\ell_p</script> regression problem for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-197-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1195" style="width: 5.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.07em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1196"><span class="mi" id="MathJax-Span-1197" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1198" style="font-family: MathJax_Main; padding-left: 0.263em;">∈</span><span class="mo" id="MathJax-Span-1199" style="font-family: MathJax_Main; padding-left: 0.263em;">[</span><span class="mn" id="MathJax-Span-1200" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-1201" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-1202" style="font-family: MathJax_Main; padding-left: 0.159em;">∞</span><span class="mo" id="MathJax-Span-1203" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-197">p\in[2,\infty]</script>, we show a streaming algorithm which constructs a sparse instance supported on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-198-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1204" style="width: 4.326em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.49em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1205"><span class="texatom" id="MathJax-Span-1206"><span class="mrow" id="MathJax-Span-1207"><span class="munderover" id="MathJax-Span-1208"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1209" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.211em;"><span class="mo" id="MathJax-Span-1210" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1211" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-1212"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1213" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1214"><span class="mrow" id="MathJax-Span-1215"><span class="mo" id="MathJax-Span-1216" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-1217" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mi" id="MathJax-Span-1218" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1219" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>2</mn></mrow></msup><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-198">\tilde O(\varepsilon^{-2}n)</script> columns of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-199-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1220" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1221"><span class="texatom" id="MathJax-Span-1222"><span class="mrow" id="MathJax-Span-1223"><span class="mi" id="MathJax-Span-1224" style="font-family: MathJax_Main-bold;">A</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">A</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-199">\mathbf A</script> which approximates the cost up to a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-200-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x00B1;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1225" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.87em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1226"><span class="mo" id="MathJax-Span-1227" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-1228" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-1229" style="font-family: MathJax_Main; padding-left: 0.211em;">±</span><span class="mi" id="MathJax-Span-1230" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">ε</span><span class="mo" id="MathJax-Span-1231" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>1</mn><mo>±</mo><mi>ε</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-200">(1\pm\varepsilon)</script> factor, which corresponds to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-201-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1232" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.91em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1233"><span class="texatom" id="MathJax-Span-1234"><span class="mrow" id="MathJax-Span-1235"><span class="munderover" id="MathJax-Span-1236"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1237" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.211em;"><span class="mo" id="MathJax-Span-1238" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1239" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-1240"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1241" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1242"><span class="mrow" id="MathJax-Span-1243"><span class="mo" id="MathJax-Span-1244" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-1245" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-1246"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1247" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-1248" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1249" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>2</mn></mrow></msup><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-201">\tilde O(\varepsilon^{-2}n^2)</script> bits of space in general and an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-202-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1250" style="width: 4.326em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.49em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1251"><span class="texatom" id="MathJax-Span-1252"><span class="mrow" id="MathJax-Span-1253"><span class="munderover" id="MathJax-Span-1254"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1255" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.211em;"><span class="mo" id="MathJax-Span-1256" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1257" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-1258"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1259" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1260"><span class="mrow" id="MathJax-Span-1261"><span class="mo" id="MathJax-Span-1262" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-1263" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mi" id="MathJax-Span-1264" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1265" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>2</mn></mrow></msup><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-202">\tilde O(\varepsilon^{-2}n)</script> space semi-streaming algorithm for constructing <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-203-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1266" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1267"><span class="msubsup" id="MathJax-Span-1268"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1269" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-1270" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-203">\ell_p</script> flow sparsifiers on graphs. This extends to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-204-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1271" style="width: 4.69em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.909em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.8em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1272"><span class="mi" id="MathJax-Span-1273" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1274" style="font-family: MathJax_Main; padding-left: 0.263em;">∈</span><span class="mo" id="MathJax-Span-1275" style="font-family: MathJax_Main; padding-left: 0.263em;">(</span><span class="mn" id="MathJax-Span-1276" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-1277" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-1278" style="font-family: MathJax_Main; padding-left: 0.159em;">2</span><span class="mo" id="MathJax-Span-1279" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>∈</mo><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-204">p\in(1, 2)</script> with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-205-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1280" style="width: 5.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1004.07em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1281"><span class="texatom" id="MathJax-Span-1282"><span class="mrow" id="MathJax-Span-1283"><span class="munderover" id="MathJax-Span-1284"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1285" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.211em;"><span class="mo" id="MathJax-Span-1286" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1287" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-1288"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1289" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1290"><span class="mrow" id="MathJax-Span-1291"><span class="mn" id="MathJax-Span-1292" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-1293"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1294" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-1295"><span class="mrow" id="MathJax-Span-1296"><span class="mi" id="MathJax-Span-1297" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-1298"><span class="mrow" id="MathJax-Span-1299"><span class="mo" id="MathJax-Span-1300" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-1301" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1302" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn></mrow></msup><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-205">\tilde O(\varepsilon^{2}n^{q/2})</script> columns, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-206-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1303" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1304"><span class="mi" id="MathJax-Span-1305" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>q</mi></math></span></span><script type="math/tex" id="MathJax-Element-206">q</script> is the Hölder conjugate exponent of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-207-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1306" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1307"><span class="mi" id="MathJax-Span-1308" style="font-family: MathJax_Math-italic;">p</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></span></span><script type="math/tex" id="MathJax-Element-207">p</script>. For <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-208-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1309" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1310"><span class="mi" id="MathJax-Span-1311" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1312" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-1313" style="font-family: MathJax_Main; padding-left: 0.263em;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-208">p = 2</script>, we show that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-209-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1314" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.45em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1315"><span class="mi" id="MathJax-Span-1316" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-1317" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-1318"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1319" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-1320" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1321" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-209">\Omega(n^2)</script> bits of space are required in general even for outputting a constant factor solution. For <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-210-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1322" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1323"><span class="mi" id="MathJax-Span-1324" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1325" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-1326" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-210">p = 1</script>, we show that the cost cannot be estimated even to an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-211-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1327" style="width: 3.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.61em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1328"><span class="mi" id="MathJax-Span-1329" style="font-family: MathJax_Math-italic;">o</span><span class="mo" id="MathJax-Span-1330" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-1331"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-1332"><span class="mi" id="MathJax-Span-1333" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1334" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>o</mi><mo stretchy="false">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-211">o(\sqrt n)</script> factor in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-212-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1335" style="width: 3.909em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.13em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1336"><span class="texatom" id="MathJax-Span-1337"><span class="mrow" id="MathJax-Span-1338"><span class="mi" id="MathJax-Span-1339" style="font-family: MathJax_Main;">p</span><span class="mi" id="MathJax-Span-1340" style="font-family: MathJax_Main;">o</span><span class="mi" id="MathJax-Span-1341" style="font-family: MathJax_Main;">l</span><span class="mi" id="MathJax-Span-1342" style="font-family: MathJax_Main;">y</span></span></span><span class="mo" id="MathJax-Span-1343" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1344" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1345" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">p</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">y</mi></mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-212">\mathrm{poly}(n)</script> space. On the other hand, if we are interested in outputting a solution <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-213-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1346" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1347"><span class="texatom" id="MathJax-Span-1348"><span class="mrow" id="MathJax-Span-1349"><span class="mi" id="MathJax-Span-1350" style="font-family: MathJax_Main-bold;">x</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-213">\mathbf x</script>, then we show that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-214-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1351" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.87em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1352"><span class="mo" id="MathJax-Span-1353" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-1354" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-1355" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-1356" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">ε</span><span class="mo" id="MathJax-Span-1357" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>ε</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-214">(1+\varepsilon)</script>-approximations require <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-215-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1358" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.93em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1359"><span class="mi" id="MathJax-Span-1360" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-1361" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1362" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1363" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-215">\Omega(d)</script> space for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-216-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1364" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1365"><span class="mi" id="MathJax-Span-1366" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1367" style="font-family: MathJax_Main; padding-left: 0.263em;">&gt;</span><span class="mn" id="MathJax-Span-1368" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>&gt;</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-216">p > 1</script>, and in general, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-217-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BA;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1369" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1370"><span class="mi" id="MathJax-Span-1371" style="font-family: MathJax_Math-italic;">κ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>κ</mi></math></span></span><script type="math/tex" id="MathJax-Element-217">\kappa</script>-approximations require <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-218-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03BA;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1372" style="width: 4.638em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.75em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1373"><span class="texatom" id="MathJax-Span-1374"><span class="mrow" id="MathJax-Span-1375"><span class="munderover" id="MathJax-Span-1376"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1377" style="font-family: MathJax_Main;">Ω</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.107em;"><span class="mo" id="MathJax-Span-1378" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1379" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1380" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-1381"><span class="mrow" id="MathJax-Span-1382"><span class="mo" id="MathJax-Span-1383" style="font-family: MathJax_Main;">/</span></span></span><span class="msubsup" id="MathJax-Span-1384"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1385" style="font-family: MathJax_Math-italic;">κ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-1386"><span class="mrow" id="MathJax-Span-1387"><span class="mn" id="MathJax-Span-1388" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-1389" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1390" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi mathvariant="normal">Ω</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msup><mi>κ</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn><mi>q</mi></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-218">\tilde\Omega(d/\kappa^{2q})</script> space for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-219-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1391" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1392"><span class="mi" id="MathJax-Span-1393" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1394" style="font-family: MathJax_Main; padding-left: 0.263em;">&gt;</span><span class="mn" id="MathJax-Span-1395" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>&gt;</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-219">p > 1</script>. We complement these lower bounds with the first sublinear space upper bounds for this problem, showing that we can output a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-220-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BA;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1396" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1397"><span class="mi" id="MathJax-Span-1398" style="font-family: MathJax_Math-italic;">κ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>κ</mi></math></span></span><script type="math/tex" id="MathJax-Element-220">\kappa</script>-approximation using space only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-221-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x22C5;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03BA;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1399" style="width: 8.961em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1007.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1400"><span class="texatom" id="MathJax-Span-1401"><span class="mrow" id="MathJax-Span-1402"><span class="mi" id="MathJax-Span-1403" style="font-family: MathJax_Main;">p</span><span class="mi" id="MathJax-Span-1404" style="font-family: MathJax_Main;">o</span><span class="mi" id="MathJax-Span-1405" style="font-family: MathJax_Main;">l</span><span class="mi" id="MathJax-Span-1406" style="font-family: MathJax_Main;">y</span></span></span><span class="mo" id="MathJax-Span-1407" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1408" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1409" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-1410" style="font-family: MathJax_Main; padding-left: 0.211em;">⋅</span><span class="texatom" id="MathJax-Span-1411" style="padding-left: 0.211em;"><span class="mrow" id="MathJax-Span-1412"><span class="munderover" id="MathJax-Span-1413"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1414" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.211em;"><span class="mo" id="MathJax-Span-1415" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1416" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1417" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="texatom" id="MathJax-Span-1418"><span class="mrow" id="MathJax-Span-1419"><span class="mo" id="MathJax-Span-1420" style="font-family: MathJax_Main;">/</span></span></span><span class="msubsup" id="MathJax-Span-1421"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1422" style="font-family: MathJax_Math-italic;">κ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="mi" id="MathJax-Span-1423" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1424" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">p</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">y</mi></mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>⋅</mo><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msup><mi>κ</mi><mi>q</mi></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-221">\mathrm{poly}(n) \cdot \tilde O(d/\kappa^q)</script> for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-222-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1425" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1426"><span class="mi" id="MathJax-Span-1427" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1428" style="font-family: MathJax_Main; padding-left: 0.263em;">&gt;</span><span class="mn" id="MathJax-Span-1429" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>&gt;</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-222">p > 1</script>, as well as a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-223-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1430" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.46em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1431"><span class="msqrt" id="MathJax-Span-1432"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-1433"><span class="mi" id="MathJax-Span-1434" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi></msqrt></math></span></span><script type="math/tex" id="MathJax-Element-223">\sqrt n</script>-approximation using <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-224-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1435" style="width: 6.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.52em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1436"><span class="texatom" id="MathJax-Span-1437"><span class="mrow" id="MathJax-Span-1438"><span class="mi" id="MathJax-Span-1439" style="font-family: MathJax_Main;">p</span><span class="mi" id="MathJax-Span-1440" style="font-family: MathJax_Main;">o</span><span class="mi" id="MathJax-Span-1441" style="font-family: MathJax_Main;">l</span><span class="mi" id="MathJax-Span-1442" style="font-family: MathJax_Main;">y</span></span></span><span class="mo" id="MathJax-Span-1443" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1444" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1445" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-1446" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-1447"></span><span class="mi" id="MathJax-Span-1448" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1449" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">p</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">y</mi></mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>log</mi><mo>⁡</mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-224">\mathrm{poly}(n, \log d)</script> space for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-225-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1450" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1451"><span class="mi" id="MathJax-Span-1452" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-1453" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-1454" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-225">p = 1</script>.</p>
            <p id="subjects-Kpjvm2mB0K@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Kpjvm2mB0K@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kpjvm2mB0K@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kpjvm2mB0K@OpenReview" onclick="foldPdfKimi('Kpjvm2mB0K@OpenReview', this)" class="hr hr-fold">
        </div><div id="IjQ2Jtemzy@OpenReview" class="panel paper" keywords="articulate,awareness,llms,policies,objective,risk,seeking,examples,certain,behaviors">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IjQ2Jtemzy" target="_blank" title="285/373"><span class="index notranslate">#285</span></a>
                <a id="title-IjQ2Jtemzy@OpenReview" class="title-link" href="/venue/IjQ2Jtemzy@OpenReview" target="_blank">Language Models Can Articulate Their Implicit Goals</a>
                <a id="pdf-IjQ2Jtemzy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IjQ2Jtemzy@OpenReview', this)" data="https://openreview.net/pdf?id=IjQ2Jtemzy">[PDF<sup id="pdf-stars-IjQ2Jtemzy@OpenReview">1</sup>]</a>
                <a id="copy-IjQ2Jtemzy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IjQ2Jtemzy@OpenReview')">[Copy]</a>
                <a id="kimi-IjQ2Jtemzy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IjQ2Jtemzy@OpenReview', this)">[Kimi<sup id="kimi-stars-IjQ2Jtemzy@OpenReview">6</sup>]</a>
                <a id="rel-IjQ2Jtemzy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IjQ2Jtemzy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IjQ2Jtemzy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Betley" target="_blank">Jan Betley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuchan Bao" target="_blank">Xuchan Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martín Soto" target="_blank">Martín Soto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Sztyber-Betley" target="_blank">Anna Sztyber-Betley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Chua" target="_blank">James Chua</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Owain Evans" target="_blank">Owain Evans</a>
            </p>
            <p id="summary-IjQ2Jtemzy@OpenReview" class="summary">We study *objective awareness*, which we define as an LLM's capability to articulate its behavioral policies without relying on in-context examples. We finetune LLMs on examples that exhibit particular behaviors, including (a) making risk-seeking / risk-averse economic decisions, and (b) making the user say a certain word. Although these examples never contain explicit descriptions of the policy (e.g. ``I will now take the risk-seeking option''), we find that the finetuned LLMs can explicitly describe their policies through out-of-context reasoning. We demonstrate LLMs' objective awareness across various evaluation tasks, both for multiple-choice and free-form questions. Furthermore, we demonstrate that models can correctly attribute different learned policies to distinct personas. Finally, we explore the connection between objective awareness and the concept of backdoors in AI safety, where certain behaviors are implanted in a model, often through data poisoning, and can be triggered under certain conditions. We find evidence that LLMs can recognize the existence of the backdoor-like behavior that they have acquired through finetuning.</p>
            <p id="subjects-IjQ2Jtemzy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IjQ2Jtemzy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IjQ2Jtemzy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IjQ2Jtemzy@OpenReview" onclick="foldPdfKimi('IjQ2Jtemzy@OpenReview', this)" class="hr hr-fold">
        </div><div id="INyi7qUdjZ@OpenReview" class="panel paper" keywords="icl,memorization,transition,generalization,memorize,task,diversity,context,circuits,explains">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=INyi7qUdjZ" target="_blank" title="286/373"><span class="index notranslate">#286</span></a>
                <a id="title-INyi7qUdjZ@OpenReview" class="title-link" href="/venue/INyi7qUdjZ@OpenReview" target="_blank">Differential learning kinetics govern the transition from memorization to generalization during in-context learning</a>
                <a id="pdf-INyi7qUdjZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('INyi7qUdjZ@OpenReview', this)" data="https://openreview.net/pdf?id=INyi7qUdjZ">[PDF<sup id="pdf-stars-INyi7qUdjZ@OpenReview">3</sup>]</a>
                <a id="copy-INyi7qUdjZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('INyi7qUdjZ@OpenReview')">[Copy]</a>
                <a id="kimi-INyi7qUdjZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('INyi7qUdjZ@OpenReview', this)">[Kimi<sup id="kimi-stars-INyi7qUdjZ@OpenReview">4</sup>]</a>
                <a id="rel-INyi7qUdjZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('INyi7qUdjZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-INyi7qUdjZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Nguyen" target="_blank">Alex Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gautam Reddy Nallamala" target="_blank">Gautam Reddy Nallamala</a>
            </p>
            <p id="summary-INyi7qUdjZ@OpenReview" class="summary">Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative *rates* at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.</p>
            <p id="subjects-INyi7qUdjZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-INyi7qUdjZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-INyi7qUdjZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-INyi7qUdjZ@OpenReview" onclick="foldPdfKimi('INyi7qUdjZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="HqLHY4TzGj@OpenReview" class="panel paper" keywords="winner,box,truth,union,proposal,regressing,proposals,intersections,takes,ground">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HqLHY4TzGj" target="_blank" title="287/373"><span class="index notranslate">#287</span></a>
                <a id="title-HqLHY4TzGj@OpenReview" class="title-link" href="/venue/HqLHY4TzGj@OpenReview" target="_blank">Union-over-Intersections: Object Detection beyond Winner-Takes-All</a>
                <a id="pdf-HqLHY4TzGj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HqLHY4TzGj@OpenReview', this)" data="https://openreview.net/pdf?id=HqLHY4TzGj">[PDF<sup id="pdf-stars-HqLHY4TzGj@OpenReview">1</sup>]</a>
                <a id="copy-HqLHY4TzGj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HqLHY4TzGj@OpenReview')">[Copy]</a>
                <a id="kimi-HqLHY4TzGj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HqLHY4TzGj@OpenReview', this)">[Kimi<sup id="kimi-stars-HqLHY4TzGj@OpenReview">4</sup>]</a>
                <a id="rel-HqLHY4TzGj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HqLHY4TzGj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HqLHY4TzGj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aritra Bhowmik" target="_blank">Aritra Bhowmik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Mettes" target="_blank">Pascal Mettes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin R. Oswald" target="_blank">Martin R. Oswald</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cees G Snoek" target="_blank">Cees G Snoek</a>
            </p>
            <p id="summary-HqLHY4TzGj@OpenReview" class="summary">This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach—regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.</p>
            <p id="subjects-HqLHY4TzGj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-HqLHY4TzGj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HqLHY4TzGj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HqLHY4TzGj@OpenReview" onclick="foldPdfKimi('HqLHY4TzGj@OpenReview', this)" class="hr hr-fold">
        </div><div id="H2Gxil855b@OpenReview" class="panel paper" keywords="gaussians,generation,atlas,latent,diffusion,patch,representation,feed,native,decode">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=H2Gxil855b" target="_blank" title="288/373"><span class="index notranslate">#288</span></a>
                <a id="title-H2Gxil855b@OpenReview" class="title-link" href="/venue/H2Gxil855b@OpenReview" target="_blank">Atlas Gaussians Diffusion for 3D Generation</a>
                <a id="pdf-H2Gxil855b@OpenReview" class="title-pdf notranslate" onclick="togglePdf('H2Gxil855b@OpenReview', this)" data="https://openreview.net/pdf?id=H2Gxil855b">[PDF<sup id="pdf-stars-H2Gxil855b@OpenReview">2</sup>]</a>
                <a id="copy-H2Gxil855b@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('H2Gxil855b@OpenReview')">[Copy]</a>
                <a id="kimi-H2Gxil855b@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('H2Gxil855b@OpenReview', this)">[Kimi<sup id="kimi-stars-H2Gxil855b@OpenReview">2</sup>]</a>
                <a id="rel-H2Gxil855b@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('H2Gxil855b@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-H2Gxil855b@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haitao Yang" target="_blank">Haitao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Dong" target="_blank">Yuan Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwen Jiang" target="_blank">Hanwen Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dejia Xu" target="_blank">Dejia Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgios Pavlakos" target="_blank">Georgios Pavlakos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixing Huang" target="_blank">Qixing Huang</a>
            </p>
            <p id="summary-H2Gxil855b@OpenReview" class="summary">Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas Gaussians, a novel representation for feed-forward native 3D generation. Atlas Gaussians represent a shape as the union of local patches, and each patch can decode 3D Gaussians. We parameterize a patch as a sequence of feature vectors and design a learnable function to decode 3D Gaussians from the feature vectors. In this process, we incorporate UV-based sampling, enabling the generation of a sufficiently large, and theoretically infinite, number of 3D Gaussian points. The large amount of 3D Gaussians enables the generation of high-quality details. Moreover, due to local awareness of the representation, the transformer-based decoding procedure operates on a patch level, ensuring efficiency. We train a variational autoencoder to learn the Atlas Gaussians representation, and then apply a latent diffusion model on its latent space for learning 3D Generation. Experiments show that our approach outperforms the prior arts of feed-forward native 3D generation.</p>
            <p id="subjects-H2Gxil855b@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-H2Gxil855b@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-H2Gxil855b@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-H2Gxil855b@OpenReview" onclick="foldPdfKimi('H2Gxil855b@OpenReview', this)" class="hr hr-fold">
        </div><div id="GjfIZan5jN@OpenReview" class="panel paper" keywords="classifiability,interpretability,pre,trained,representations,representation,interpretable,semantics,interpretations,captured">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GjfIZan5jN" target="_blank" title="289/373"><span class="index notranslate">#289</span></a>
                <a id="title-GjfIZan5jN@OpenReview" class="title-link" href="/venue/GjfIZan5jN@OpenReview" target="_blank">Enhancing Pre-trained Representation Classifiability can Boost its Interpretability</a>
                <a id="pdf-GjfIZan5jN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GjfIZan5jN@OpenReview', this)" data="https://openreview.net/pdf?id=GjfIZan5jN">[PDF<sup id="pdf-stars-GjfIZan5jN@OpenReview">4</sup>]</a>
                <a id="copy-GjfIZan5jN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GjfIZan5jN@OpenReview')">[Copy]</a>
                <a id="kimi-GjfIZan5jN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GjfIZan5jN@OpenReview', this)">[Kimi<sup id="kimi-stars-GjfIZan5jN@OpenReview">5</sup>]</a>
                <a id="rel-GjfIZan5jN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GjfIZan5jN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GjfIZan5jN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shufan Shen" target="_blank">Shufan Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaobo Qi" target="_blank">Zhaobo Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junshu Sun" target="_blank">Junshu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingming Huang" target="_blank">Qingming Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Tian" target="_blank">Qi Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhui Wang" target="_blank">Shuhui Wang</a>
            </p>
            <p id="summary-GjfIZan5jN@OpenReview" class="summary">The visual representation of a pre-trained model prioritizes the classifiability on downstream tasks. However, widespread applications for pre-trained visual models have proposed new requirements for representation interpretability. It remains unclear whether the pre-trained representations can achieve high interpretability and classifiability simultaneously. To answer this question, we quantify the representation interpretability by leveraging its correlation with the ratio of interpretable semantics within representations. Given the pre-trained representations, only the interpretable semantics can be captured by interpretations, whereas the uninterpretable part leads to information loss. Based on this fact, we propose the Inherent Interpretability Score (IIS) that evaluates the information loss, measures the ratio of interpretable semantics, and quantifies the representation interpretability. In the evaluation of the representation interpretability with different classifiability, we surprisingly discover that the interpretability and classifiability are positively correlated, i.e., representations with higher classifiability provide more interpretable semantics that can be captured in the interpretations. This observation further supports two benefits to the pre-trained representations. First, the classifiability of representations can be further improved by fine-tuning with interpretability maximization. Second, with the classifiability improvement for the representations, we obtain predictions based on their interpretations with less accuracy degradation. The discovered positive correlation and corresponding applications show that practitioners can unify the improvements in interpretability and classifiability for pre-trained vision models. Codes are included in the supplement and will be released on GitHub.</p>
            <p id="subjects-GjfIZan5jN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GjfIZan5jN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GjfIZan5jN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GjfIZan5jN@OpenReview" onclick="foldPdfKimi('GjfIZan5jN@OpenReview', this)" class="hr hr-fold">
        </div><div id="Fk3eod9aaD@OpenReview" class="panel paper" keywords="ood,laion,generalization,datasets,forgotten,domain,rendition,imagenet,domains,era">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Fk3eod9aaD" target="_blank" title="290/373"><span class="index notranslate">#290</span></a>
                <a id="title-Fk3eod9aaD@OpenReview" class="title-link" href="/venue/Fk3eod9aaD@OpenReview" target="_blank">In Search of Forgotten Domain Generalization</a>
                <a id="pdf-Fk3eod9aaD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Fk3eod9aaD@OpenReview', this)" data="https://openreview.net/pdf?id=Fk3eod9aaD">[PDF<sup id="pdf-stars-Fk3eod9aaD@OpenReview">3</sup>]</a>
                <a id="copy-Fk3eod9aaD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Fk3eod9aaD@OpenReview')">[Copy]</a>
                <a id="kimi-Fk3eod9aaD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Fk3eod9aaD@OpenReview', this)">[Kimi<sup id="kimi-stars-Fk3eod9aaD@OpenReview">4</sup>]</a>
                <a id="rel-Fk3eod9aaD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Fk3eod9aaD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Fk3eod9aaD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Prasanna Mayilvahanan" target="_blank">Prasanna Mayilvahanan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roland Zimmermann" target="_blank">Roland Zimmermann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thaddäus Wiedemer" target="_blank">Thaddäus Wiedemer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Evgenia Rusak" target="_blank">Evgenia Rusak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Attila Juhos" target="_blank">Attila Juhos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthias Bethge" target="_blank">Matthias Bethge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wieland Brendel" target="_blank">Wieland Brendel</a>
            </p>
            <p id="summary-Fk3eod9aaD@OpenReview" class="summary">Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION---LAION-Natural and LAION-Rendition---that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale---a crucial prerequisite for improving model robustness.</p>
            <p id="subjects-Fk3eod9aaD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Fk3eod9aaD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fk3eod9aaD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fk3eod9aaD@OpenReview" onclick="foldPdfKimi('Fk3eod9aaD@OpenReview', this)" class="hr hr-fold">
        </div><div id="ywFOSIT9ik@OpenReview" class="panel paper" keywords="perturbations,directionally,zeroth,aligned,variance,gradient,daps,dap,revisiting,optimization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ywFOSIT9ik" target="_blank" title="291/373"><span class="index notranslate">#291</span></a>
                <a id="title-ywFOSIT9ik@OpenReview" class="title-link" href="/venue/ywFOSIT9ik@OpenReview" target="_blank">Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations</a>
                <a id="pdf-ywFOSIT9ik@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ywFOSIT9ik@OpenReview', this)" data="https://openreview.net/pdf?id=ywFOSIT9ik">[PDF<sup id="pdf-stars-ywFOSIT9ik@OpenReview">4</sup>]</a>
                <a id="copy-ywFOSIT9ik@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ywFOSIT9ik@OpenReview')">[Copy]</a>
                <a id="kimi-ywFOSIT9ik@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ywFOSIT9ik@OpenReview', this)">[Kimi<sup id="kimi-stars-ywFOSIT9ik@OpenReview">2</sup>]</a>
                <a id="rel-ywFOSIT9ik@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ywFOSIT9ik@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ywFOSIT9ik@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaocong Ma" target="_blank">Shaocong Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heng Huang" target="_blank">Heng Huang</a>
            </p>
            <p id="summary-ywFOSIT9ik@OpenReview" class="summary">In this paper, we explore the two-point zeroth-order gradient estimator and identify the optimal distribution of random perturbations that minimizes the estimator's variance. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that optimal perturbations either maintain a fixed length or align directionally with the true gradient. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-226-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B4;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1455" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1456"><span class="mi" id="MathJax-Span-1457" style="font-family: MathJax_Math-italic;">δ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></span></span><script type="math/tex" id="MathJax-Element-226">\delta</script>-unbiased random perturbations, extending optimal complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions.</p>
            <p id="subjects-ywFOSIT9ik@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ywFOSIT9ik@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ywFOSIT9ik@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ywFOSIT9ik@OpenReview" onclick="foldPdfKimi('ywFOSIT9ik@OpenReview', this)" class="hr hr-fold">
        </div><div id="EPHsIa0Ytg@OpenReview" class="panel paper" keywords="submodular,maximization,monotone,multilinear,knapsack,approximation,constraints,matroid,extension,245">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EPHsIa0Ytg" target="_blank" title="292/373"><span class="index notranslate">#292</span></a>
                <a id="title-EPHsIa0Ytg@OpenReview" class="title-link" href="/venue/EPHsIa0Ytg@OpenReview" target="_blank">Improved Approximation Algorithms for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-227-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1458" style="width: 0.627em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.356em, 1000.49em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-1459"><span class="mi" id="MathJax-Span-1460" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.04em; border-left: 0px solid; width: 0px; height: 0.919em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-227">k</script>-Submodular Maximization via Multilinear Extension</a>
                <a id="pdf-EPHsIa0Ytg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EPHsIa0Ytg@OpenReview', this)" data="https://openreview.net/pdf?id=EPHsIa0Ytg">[PDF<sup id="pdf-stars-EPHsIa0Ytg@OpenReview">2</sup>]</a>
                <a id="copy-EPHsIa0Ytg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EPHsIa0Ytg@OpenReview')">[Copy]</a>
                <a id="kimi-EPHsIa0Ytg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EPHsIa0Ytg@OpenReview', this)">[Kimi<sup id="kimi-stars-EPHsIa0Ytg@OpenReview">1</sup>]</a>
                <a id="rel-EPHsIa0Ytg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EPHsIa0Ytg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EPHsIa0Ytg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huanjian Zhou" target="_blank">Huanjian Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingxiao Huang" target="_blank">Lingxiao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoxiang Wang" target="_blank">Baoxiang Wang</a>
            </p>
            <p id="summary-EPHsIa0Ytg@OpenReview" class="summary">We investigate a generalized form of submodular maximization, referred to as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-228-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1461" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1462"><span class="mi" id="MathJax-Span-1463" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-228">k</script>-submodular maximization, with applications across the domains of social networks and machine learning. In this work, we propose the multilinear extension of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-229-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1464" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1465"><span class="mi" id="MathJax-Span-1466" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-229">k</script>-submodular functions and unified Frank-Wolfe-type frameworks based on that. This continuous framework accommodates 1) monotone or non-monotone functions, and 2) various constraint types including matroid constraints, knapsack constraints, and their combinations. Notably, we attain an asymptotically optimal <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-230-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1467" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.46em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1468"><span class="mn" id="MathJax-Span-1469" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-1470"><span class="mrow" id="MathJax-Span-1471"><span class="mo" id="MathJax-Span-1472" style="font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-1473" style="font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-230">1/2</script>-approximation for monotone <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-231-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1474" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1475"><span class="mi" id="MathJax-Span-1476" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-231">k</script>-submodular maximization problems with knapsack constraints, surpassing previous <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-232-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1477" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.46em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1478"><span class="mn" id="MathJax-Span-1479" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-1480"><span class="mrow" id="MathJax-Span-1481"><span class="mo" id="MathJax-Span-1482" style="font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-1483" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-232">1/3</script>-approximation results, and a factor-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-233-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1484" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.46em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1485"><span class="mn" id="MathJax-Span-1486" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-1487"><span class="mrow" id="MathJax-Span-1488"><span class="mo" id="MathJax-Span-1489" style="font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-1490" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-233">1/3</script> approximation for non-monotone <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-234-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1491" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1492"><span class="mi" id="MathJax-Span-1493" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-234">k</script>-submodular maximization problems with knapsack constraints and matroid constraints which outperforms previous <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-235-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;0.245&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1494" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1495"><span class="mn" id="MathJax-Span-1496" style="font-family: MathJax_Main;">0.245</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.245</mn></math></span></span><script type="math/tex" id="MathJax-Element-235">0.245</script>-approximation results. The foundation for our analysis stems from new insights into specific linear and monotone properties pertaining to the multilinear extension.</p>
            <p id="subjects-EPHsIa0Ytg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-EPHsIa0Ytg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EPHsIa0Ytg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EPHsIa0Ytg@OpenReview" onclick="foldPdfKimi('EPHsIa0Ytg@OpenReview', this)" class="hr hr-fold">
        </div><div id="D1Y2XFgsPI@OpenReview" class="panel paper" keywords="imputation,imputations,investing,missingness,beware,predictive,prediction,matters,diminishing,predictions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=D1Y2XFgsPI" target="_blank" title="293/373"><span class="index notranslate">#293</span></a>
                <a id="title-D1Y2XFgsPI@OpenReview" class="title-link" href="/venue/D1Y2XFgsPI@OpenReview" target="_blank">Imputation for prediction: beware of diminishing returns.</a>
                <a id="pdf-D1Y2XFgsPI@OpenReview" class="title-pdf notranslate" onclick="togglePdf('D1Y2XFgsPI@OpenReview', this)" data="https://openreview.net/pdf?id=D1Y2XFgsPI">[PDF<sup id="pdf-stars-D1Y2XFgsPI@OpenReview">2</sup>]</a>
                <a id="copy-D1Y2XFgsPI@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('D1Y2XFgsPI@OpenReview')">[Copy]</a>
                <a id="kimi-D1Y2XFgsPI@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('D1Y2XFgsPI@OpenReview', this)">[Kimi<sup id="kimi-stars-D1Y2XFgsPI@OpenReview">1</sup>]</a>
                <a id="rel-D1Y2XFgsPI@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('D1Y2XFgsPI@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-D1Y2XFgsPI@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marine Le Morvan" target="_blank">Marine Le Morvan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gael Varoquaux" target="_blank">Gael Varoquaux</a>
            </p>
            <p id="summary-D1Y2XFgsPI@OpenReview" class="summary">Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifying *if* and *when* investing in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 19 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, imputation quality has only a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits.</p>
            <p id="subjects-D1Y2XFgsPI@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-D1Y2XFgsPI@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-D1Y2XFgsPI@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-D1Y2XFgsPI@OpenReview" onclick="foldPdfKimi('D1Y2XFgsPI@OpenReview', this)" class="hr hr-fold">
        </div><div id="CkgKSqZbuC@OpenReview" class="panel paper" keywords="guardrail,guard,reasoning,safety,categories,jailbreaking,llamaguard,moderation,unsafety,logical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CkgKSqZbuC" target="_blank" title="294/373"><span class="index notranslate">#294</span></a>
                <a id="title-CkgKSqZbuC@OpenReview" class="title-link" href="/venue/CkgKSqZbuC@OpenReview" target="_blank"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-236-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1497" style="width: 1.425em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.182em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.217em, 1001.18em, 2.293em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-1498"><span class="msubsup" id="MathJax-Span-1499"><span style="display: inline-block; position: relative; width: 1.182em; height: 0px;"><span style="position: absolute; clip: rect(1.356em, 1000.77em, 2.293em, -999.998em); top: -2.151em; left: 0em;"><span class="mi" id="MathJax-Span-1500" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span><span style="position: absolute; top: -2.498em; left: 0.766em;"><span class="mn" id="MathJax-Span-1501" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.081em; border-left: 0px solid; width: 0px; height: 1.127em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-236">R^2</script>-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning</a>
                <a id="pdf-CkgKSqZbuC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CkgKSqZbuC@OpenReview', this)" data="https://openreview.net/pdf?id=CkgKSqZbuC">[PDF<sup id="pdf-stars-CkgKSqZbuC@OpenReview">5</sup>]</a>
                <a id="copy-CkgKSqZbuC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CkgKSqZbuC@OpenReview')">[Copy]</a>
                <a id="kimi-CkgKSqZbuC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CkgKSqZbuC@OpenReview', this)">[Kimi<sup id="kimi-stars-CkgKSqZbuC@OpenReview">9</sup>]</a>
                <a id="rel-CkgKSqZbuC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CkgKSqZbuC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CkgKSqZbuC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mintong Kang" target="_blank">Mintong Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Li" target="_blank">Bo Li</a>
            </p>
            <p id="summary-CkgKSqZbuC@OpenReview" class="summary">As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety categories (e.g., self-harm, self-harm/instructions) independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreaking attacks, and inflexibility regarding new safety categories.To address these limitations, we propose <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-237-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1502" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1503"><span class="msubsup" id="MathJax-Span-1504"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1505" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-1506" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-237">R^2</script>-Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-238-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1507" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1508"><span class="msubsup" id="MathJax-Span-1509"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1510" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-1511" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-238">R^2</script>-Guard comprises two parts: data-driven guardrail models and reasoning components. The data-driven guardrail models provide unsafety probabilities of moderated content on different safety categories.We then encode safety knowledge among different categories as first-order logical rules and embed them into a probabilistic graphic model (PGM) based reasoning component. The unsafety probabilities of different categories from data-driven guardrail models are sent to the reasoning component for final inference. We employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and optimize PCs to achieve precision-efficiency balance via improved graph structure. We also propose different methods to optimize the weights of knowledge. To further perform stress tests for guardrail models, we employ a pairwise construction method to construct a new safety benchmark TwinSafety, which features principled categories and presents new challenges for moderation. We show that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-239-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1512" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1513"><span class="msubsup" id="MathJax-Span-1514"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1515" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-1516" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-239">R^2</script>-Guard is effective even given unrepresentative categories or challenging jailbreaking prompts. We demonstrate the effectiveness of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-240-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1517" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1518"><span class="msubsup" id="MathJax-Span-1519"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1520" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-1521" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-240">R^2</script>-Guard by comparisons with eight strong guardrail models on six standard moderation datasets, and demonstrate the robustness of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-241-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1522" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1523"><span class="msubsup" id="MathJax-Span-1524"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1525" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-1526" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-241">R^2</script>-Guard against four SOTA jailbreaking attacks. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-242-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1527" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1528"><span class="msubsup" id="MathJax-Span-1529"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1530" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-1531" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-242">R^2</script>-Guard significantly surpasses SOTA method LlamaGuard by 12.6% on standard moderation datasets and by 59.9% against jailbreaking attacks.We further reveal that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-243-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1532" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1533"><span class="msubsup" id="MathJax-Span-1534"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1535" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-1536" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-243">R^2</script>-Guard can effectively adapt to safety category updates by simply editing the PGM reasoning graph.</p>
            <p id="subjects-CkgKSqZbuC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-CkgKSqZbuC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CkgKSqZbuC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CkgKSqZbuC@OpenReview" onclick="foldPdfKimi('CkgKSqZbuC@OpenReview', this)" class="hr hr-fold">
        </div><div id="Acvo2RGSCy@OpenReview" class="panel paper" keywords="dellma,decision,making,language,uncertainty,llms,enhance,environments,reasoning,procedure">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Acvo2RGSCy" target="_blank" title="295/373"><span class="index notranslate">#295</span></a>
                <a id="title-Acvo2RGSCy@OpenReview" class="title-link" href="/venue/Acvo2RGSCy@OpenReview" target="_blank">DeLLMa: Decision Making Under Uncertainty with Large Language Models</a>
                <a id="pdf-Acvo2RGSCy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Acvo2RGSCy@OpenReview', this)" data="https://openreview.net/pdf?id=Acvo2RGSCy">[PDF<sup id="pdf-stars-Acvo2RGSCy@OpenReview">7</sup>]</a>
                <a id="copy-Acvo2RGSCy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Acvo2RGSCy@OpenReview')">[Copy]</a>
                <a id="kimi-Acvo2RGSCy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Acvo2RGSCy@OpenReview', this)">[Kimi<sup id="kimi-stars-Acvo2RGSCy@OpenReview">8</sup>]</a>
                <a id="rel-Acvo2RGSCy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Acvo2RGSCy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Acvo2RGSCy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ollie Liu" target="_blank">Ollie Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deqing Fu" target="_blank">Deqing Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dani Yogatama" target="_blank">Dani Yogatama</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Willie Neiswanger" target="_blank">Willie Neiswanger</a>
            </p>
            <p id="summary-Acvo2RGSCy@OpenReview" class="summary">The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of *decision-making under uncertainty*. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling inference-time reasoning, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.</p>
            <p id="subjects-Acvo2RGSCy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Acvo2RGSCy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Acvo2RGSCy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Acvo2RGSCy@OpenReview" onclick="foldPdfKimi('Acvo2RGSCy@OpenReview', this)" class="hr hr-fold">
        </div><div id="9ehJCZz4aM@OpenReview" class="panel paper" keywords="concept,concepts,manipulation,guided,policies,loop,task,environmental,demonstrations,closed">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9ehJCZz4aM" target="_blank" title="296/373"><span class="index notranslate">#296</span></a>
                <a id="title-9ehJCZz4aM@OpenReview" class="title-link" href="/venue/9ehJCZz4aM@OpenReview" target="_blank">Learning Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations</a>
                <a id="pdf-9ehJCZz4aM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9ehJCZz4aM@OpenReview', this)" data="https://openreview.net/pdf?id=9ehJCZz4aM">[PDF<sup id="pdf-stars-9ehJCZz4aM@OpenReview">5</sup>]</a>
                <a id="copy-9ehJCZz4aM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9ehJCZz4aM@OpenReview')">[Copy]</a>
                <a id="kimi-9ehJCZz4aM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9ehJCZz4aM@OpenReview', this)">[Kimi<sup id="kimi-stars-9ehJCZz4aM@OpenReview">4</sup>]</a>
                <a id="rel-9ehJCZz4aM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9ehJCZz4aM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9ehJCZz4aM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pei Zhou" target="_blank">Pei Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruizhe Liu" target="_blank">Ruizhe Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Luo" target="_blank">Qian Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yibing Song" target="_blank">Yibing Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Wang" target="_blank">Fan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanchao Yang" target="_blank">Yanchao Yang</a>
            </p>
            <p id="summary-9ehJCZz4aM@OpenReview" class="summary">Training embodied agents to perform complex robotic tasks presents significant challenges due to the entangled factors of task compositionality, environmental diversity, and dynamic changes. In this work, we introduce a novel imitation learning framework to train closed-loop concept-guided policies that enhance long-horizon task performance by leveraging discovered manipulation concepts. Unlike methods that rely on predefined skills and human-annotated labels, our approach allows agents to autonomously abstract manipulation concepts from their proprioceptive states, thereby alleviating misalignment due to ambiguities in human semantics and environmental complexity. Our framework comprises two primary components: an *Automatic Concept Discovery* module that identifies meaningful and consistent manipulation concepts, and a *Concept-Aware Policy Learning* module that effectively utilizes these manipulation concepts for adaptive task execution, including a *Concept Selection Transformer* for concept-based guidance and a *Concept-Guided Policy* for action prediction with the selected concepts. Experimental results demonstrate that our approach significantly outperforms baseline methods across a range of tasks and environments, while showcasing emergent consistency in motion patterns associated with the discovered concepts. Our code and models will be public.</p>
            <p id="subjects-9ehJCZz4aM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9ehJCZz4aM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9ehJCZz4aM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9ehJCZz4aM@OpenReview" onclick="foldPdfKimi('9ehJCZz4aM@OpenReview', this)" class="hr hr-fold">
        </div><div id="9bMZ29SPVx@OpenReview" class="panel paper" keywords="selection,clip,generalizable,datasets,samples,powered,noisy,data,training,dataset">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9bMZ29SPVx" target="_blank" title="297/373"><span class="index notranslate">#297</span></a>
                <a id="title-9bMZ29SPVx@OpenReview" class="title-link" href="/venue/9bMZ29SPVx@OpenReview" target="_blank">A CLIP-Powered Framework for Robust and Generalizable Data Selection</a>
                <a id="pdf-9bMZ29SPVx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9bMZ29SPVx@OpenReview', this)" data="https://openreview.net/pdf?id=9bMZ29SPVx">[PDF<sup id="pdf-stars-9bMZ29SPVx@OpenReview">6</sup>]</a>
                <a id="copy-9bMZ29SPVx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9bMZ29SPVx@OpenReview')">[Copy]</a>
                <a id="kimi-9bMZ29SPVx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9bMZ29SPVx@OpenReview', this)">[Kimi<sup id="kimi-stars-9bMZ29SPVx@OpenReview">3</sup>]</a>
                <a id="rel-9bMZ29SPVx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9bMZ29SPVx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9bMZ29SPVx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Suorong Yang" target="_blank">Suorong Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Ye" target="_blank">Peng Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanli Ouyang" target="_blank">Wanli Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongzhan Zhou" target="_blank">Dongzhan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Furao Shen" target="_blank">Furao Shen</a>
            </p>
            <p id="summary-9bMZ29SPVx@OpenReview" class="summary">Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets invariably incurs substantial storage and computational overhead. Meanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact on training efficiency and model performance. Data selection has shown promise in identifying the most representative samples from the entire dataset, which aims to minimize the performance gap with reduced training costs. Existing works typically rely on single-modality information to assign importance scores for individual samples, which may lead to inaccurate assessments, especially when dealing with noisy or corrupted samples. To address this limitation, we propose a novel CLIP-powered data selection framework that leverages multimodal information for more robust and generalizable sample selection. Specifically, our framework consists of three key modules—dataset adaptation, sample scoring, and selection optimization—that together harness extensive pre-trained multimodal knowledge to comprehensively assess sample influence and optimize the selection results through multi-objective optimization. Extensive experiments demonstrate that our approach consistently outperforms existing state-of-the-art baselines on various benchmark datasets. Notably, our method effectively removes noisy or damaged samples from the dataset, enabling it to achieve even higher performance with less data. This indicates that it is not only a way to accelerate training but can also improve overall data quality. The implementation will be made publicly available soon.</p>
            <p id="subjects-9bMZ29SPVx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9bMZ29SPVx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9bMZ29SPVx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9bMZ29SPVx@OpenReview" onclick="foldPdfKimi('9bMZ29SPVx@OpenReview', this)" class="hr hr-fold">
        </div><div id="97rOQDPmk2@OpenReview" class="panel paper" keywords="signgd,adam,transformers,optimizes,optimization,generalization,layer,descent,transformer,sign">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=97rOQDPmk2" target="_blank" title="298/373"><span class="index notranslate">#298</span></a>
                <a id="title-97rOQDPmk2@OpenReview" class="title-link" href="/venue/97rOQDPmk2@OpenReview" target="_blank">On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent</a>
                <a id="pdf-97rOQDPmk2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('97rOQDPmk2@OpenReview', this)" data="https://openreview.net/pdf?id=97rOQDPmk2">[PDF<sup id="pdf-stars-97rOQDPmk2@OpenReview">6</sup>]</a>
                <a id="copy-97rOQDPmk2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('97rOQDPmk2@OpenReview')">[Copy]</a>
                <a id="kimi-97rOQDPmk2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('97rOQDPmk2@OpenReview', this)">[Kimi<sup id="kimi-stars-97rOQDPmk2@OpenReview">3</sup>]</a>
                <a id="rel-97rOQDPmk2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('97rOQDPmk2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-97rOQDPmk2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bingrui Li" target="_blank">Bingrui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Huang" target="_blank">Wei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andi Han" target="_blank">Andi Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanpeng Zhou" target="_blank">Zhanpeng Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taiji Suzuki" target="_blank">Taiji Suzuki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Zhu" target="_blank">Jun Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfei Chen" target="_blank">Jianfei Chen</a>
            </p>
            <p id="summary-97rOQDPmk2@OpenReview" class="summary">The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem.However, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. Fortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam.Despite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind.In this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on a linearly separable noisy dataset.We identify four stages in the training dynamics, each exhibiting intriguing behaviors.Based on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset.We also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting.Additionally, we find that the poor generalization of SignGD is not solely due to data noise,suggesting that both SignGD and Adam requires high-quality data for real-world tasks.Finally, experiments on synthetic and real-world datasets empirically support our theoretical results.</p>
            <p id="subjects-97rOQDPmk2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-97rOQDPmk2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-97rOQDPmk2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-97rOQDPmk2@OpenReview" onclick="foldPdfKimi('97rOQDPmk2@OpenReview', this)" class="hr hr-fold">
        </div><div id="8oCrlOaYcc@OpenReview" class="panel paper" keywords="softmoes,softmoe,tokenize,efficacy,flatten,unlocking,experts,behind,tokenizing,key">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8oCrlOaYcc" target="_blank" title="299/373"><span class="index notranslate">#299</span></a>
                <a id="title-8oCrlOaYcc@OpenReview" class="title-link" href="/venue/8oCrlOaYcc@OpenReview" target="_blank">Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL</a>
                <a id="pdf-8oCrlOaYcc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8oCrlOaYcc@OpenReview', this)" data="https://openreview.net/pdf?id=8oCrlOaYcc">[PDF<sup id="pdf-stars-8oCrlOaYcc@OpenReview">3</sup>]</a>
                <a id="copy-8oCrlOaYcc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8oCrlOaYcc@OpenReview')">[Copy]</a>
                <a id="kimi-8oCrlOaYcc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8oCrlOaYcc@OpenReview', this)">[Kimi<sup id="kimi-stars-8oCrlOaYcc@OpenReview">2</sup>]</a>
                <a id="rel-8oCrlOaYcc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8oCrlOaYcc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8oCrlOaYcc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ghada Sokar" target="_blank">Ghada Sokar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johan S Obando Ceron" target="_blank">Johan S Obando Ceron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Courville" target="_blank">Aaron Courville</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hugo Larochelle" target="_blank">Hugo Larochelle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pablo Samuel Castro" target="_blank">Pablo Samuel Castro</a>
            </p>
            <p id="summary-8oCrlOaYcc@OpenReview" class="summary">The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely unknown. In this work we provide an in-depth analysis identifying the key factors driving this performance gain. We discover the surprising result that tokenizing the encoder output, rather than the use of multiple experts, is what is behind the efficacy of SoftMoEs. Indeed, we demonstrate that even with an appropriately scaled single expert, we are able to maintain the performance gains, largely thanks to tokenization.</p>
            <p id="subjects-8oCrlOaYcc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-8oCrlOaYcc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8oCrlOaYcc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8oCrlOaYcc@OpenReview" onclick="foldPdfKimi('8oCrlOaYcc@OpenReview', this)" class="hr hr-fold">
        </div><div id="8UFG9D8xeU@OpenReview" class="panel paper" keywords="demonstrations,preference,unpreferred,generations,alignment,rankings,implicit,motion,token,preferred">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8UFG9D8xeU" target="_blank" title="300/373"><span class="index notranslate">#300</span></a>
                <a id="title-8UFG9D8xeU@OpenReview" class="title-link" href="/venue/8UFG9D8xeU@OpenReview" target="_blank">Direct Post-Training Preference Alignment of Multi-Agent Motion Generation Model with Implicit Feedback from Demonstrations</a>
                <a id="pdf-8UFG9D8xeU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8UFG9D8xeU@OpenReview', this)" data="https://openreview.net/pdf?id=8UFG9D8xeU">[PDF<sup id="pdf-stars-8UFG9D8xeU@OpenReview">5</sup>]</a>
                <a id="copy-8UFG9D8xeU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8UFG9D8xeU@OpenReview')">[Copy]</a>
                <a id="kimi-8UFG9D8xeU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8UFG9D8xeU@OpenReview', this)">[Kimi<sup id="kimi-stars-8UFG9D8xeU@OpenReview">13</sup>]</a>
                <a id="rel-8UFG9D8xeU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8UFG9D8xeU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8UFG9D8xeU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Tian" target="_blank">Thomas Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kratarth Goel" target="_blank">Kratarth Goel</a>
            </p>
            <p id="summary-8UFG9D8xeU@OpenReview" class="summary">Recent advancements in Large Language Models (LLMs) have transformed motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type motion models benefit from scalability and efficient formulation, there remains a discrepancy between their token-prediction imitation objectives and human preferences. This often results in behaviors that deviate from human-preferred demonstrations, making post-training behavior alignment crucial for generating human-preferred motions. Post-training alignment requires a large number of preference rankings over model generations, which are costly and time-consuming to annotate in multi-agent motion generation settings. Recently, there has been growing interest in using expert demonstrations to scalably build preference data for alignment. However, these methods often adopt a worst-case scenario assumption, treating all generated samples from the reference model as unpreferred and relying on expert demonstrations to directly or indirectly construct preferred generations. This approach overlooks the rich signal provided by preference rankings among the model's own generations. In this work, instead of treating all generated samples as equally unpreferred, we propose a principled approach leveraging the implicit preferences encoded in expert demonstrations to construct preference rankings among the generations produced by the reference model, offering more nuanced guidance at low-cost. We present the first investigation of direct preference alignment for multi-agent motion token-prediction models using implicit preference feedback from demonstrations. We apply our approach to large-scale traffic simulation and demonstrate its effectiveness in improving the realism of generated behaviors involving up to 128 agents, making a 1M token-prediction model comparable to state-of-the-art large models by relying solely on implicit feedback from demonstrations, without requiring additional human annotations or high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future investigations.</p>
            <p id="subjects-8UFG9D8xeU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-8UFG9D8xeU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8UFG9D8xeU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8UFG9D8xeU@OpenReview" onclick="foldPdfKimi('8UFG9D8xeU@OpenReview', this)" class="hr hr-fold">
        </div><div id="7nyJBVCTGQ@OpenReview" class="panel paper" keywords="lora,lift,meta,fine,posterior,sgld,bayesian,tuning,task,tune">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7nyJBVCTGQ" target="_blank" title="301/373"><span class="index notranslate">#301</span></a>
                <a id="title-7nyJBVCTGQ@OpenReview" class="title-link" href="/venue/7nyJBVCTGQ@OpenReview" target="_blank">LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning</a>
                <a id="pdf-7nyJBVCTGQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7nyJBVCTGQ@OpenReview', this)" data="https://openreview.net/pdf?id=7nyJBVCTGQ">[PDF<sup id="pdf-stars-7nyJBVCTGQ@OpenReview">2</sup>]</a>
                <a id="copy-7nyJBVCTGQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7nyJBVCTGQ@OpenReview')">[Copy]</a>
                <a id="kimi-7nyJBVCTGQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7nyJBVCTGQ@OpenReview', this)">[Kimi<sup id="kimi-stars-7nyJBVCTGQ@OpenReview">1</sup>]</a>
                <a id="rel-7nyJBVCTGQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7nyJBVCTGQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7nyJBVCTGQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Minyoung Kim" target="_blank">Minyoung Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Timothy Hospedales" target="_blank">Timothy Hospedales</a>
            </p>
            <p id="summary-7nyJBVCTGQ@OpenReview" class="summary">We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for unseen test tasks to improve the prediction performance further. That is, we propose a method for {\em learning-to-fine-tune} (LiFT). LiFT introduces a novel hierarchical Bayesian model that can be superior to both existing general meta learning algorithms like MAML and recent LoRA zoo mixing approaches such as LoRA-Retriever and model-based clustering. In our Bayesian model, the parameters of the task-specific LoRA modules are regarded as random variables where these task-wise LoRA modules are governed/regularized by higher-level latent random variables, which represents the prior of the LoRA modules that capture the shared information across all training tasks. To make the posterior inference feasible, we propose a novel SGLD-Gibbs sampling algorithm that is computationally efficient. To represent the posterior samples from the SGLD-Gibbs, we propose an online EM algorithm that maintains a Gaussian mixture representation for the posterior in an online manner in the course of iterative posterior sampling. We demonstrate the effectiveness of LiFT on NLP and vision multi-task meta learning benchmarks.</p>
            <p id="subjects-7nyJBVCTGQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-7nyJBVCTGQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7nyJBVCTGQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7nyJBVCTGQ@OpenReview" onclick="foldPdfKimi('7nyJBVCTGQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="71pur4y8gs@OpenReview" class="panel paper" keywords="tabwak,tables,row,watermarking,watermark,detectability,watermarked,tabular,seeds,synthetic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=71pur4y8gs" target="_blank" title="302/373"><span class="index notranslate">#302</span></a>
                <a id="title-71pur4y8gs@OpenReview" class="title-link" href="/venue/71pur4y8gs@OpenReview" target="_blank">TabWak: A Watermark for Tabular Diffusion Models</a>
                <a id="pdf-71pur4y8gs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('71pur4y8gs@OpenReview', this)" data="https://openreview.net/pdf?id=71pur4y8gs">[PDF<sup id="pdf-stars-71pur4y8gs@OpenReview">2</sup>]</a>
                <a id="copy-71pur4y8gs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('71pur4y8gs@OpenReview')">[Copy]</a>
                <a id="kimi-71pur4y8gs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('71pur4y8gs@OpenReview', this)">[Kimi<sup id="kimi-stars-71pur4y8gs@OpenReview">3</sup>]</a>
                <a id="rel-71pur4y8gs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('71pur4y8gs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-71pur4y8gs@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chaoyi Zhu" target="_blank">Chaoyi Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Tang" target="_blank">Jiayi Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeroen Galjaard" target="_blank">Jeroen Galjaard</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pin-Yu Chen" target="_blank">Pin-Yu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Birke" target="_blank">Robert Birke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cornelis Bos" target="_blank">Cornelis Bos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lydia Chen" target="_blank">Lydia Chen</a>
            </p>
            <p id="summary-71pur4y8gs@OpenReview" class="summary">Synthetic data offers alternatives for data augmentation and sharing. Till date, it remains unknown how to use watermarking techniques to trace and audit synthetic tables generated by tabular diffusion models to mitigate potential misuses. In this paper, we design TabWak, the first watermarking method to embed invisible signatures that control the sampling of Gaussian latent codes used to synthesize table rows via the diffusion backbone. TabWak has two key features. Different from existing image watermarking techniques, TabWak uses self-cloning and shuffling to embed the secret key in positional information of random seeds that control the Gaussian latents, allowing to use different seeds at each row for high inter-row diversity and enabling row-wise detectability. To further boost the robustness of watermark detection against post-editing attacks, TabWak uses a valid-bit mechanism that focuses on the tail of the latent code distribution for superior noise resilience. We provide theoretical guarantees on the row diversity and effectiveness of detectability. We evaluate TabWak on five datasets against baselines to show that the quality of watermarked tables remains nearly indistinguishable from non-watermarked tables while achieving high detectability in the presence of strong post-editing attacks, with a 100% true positive rate at a 0.1% false positive rate on synthetic tables with fewer than 300 rows. Our code is available at the following anonymized repository https://anonymous.4open.science/r/TabWak-4E65/.</p>
            <p id="subjects-71pur4y8gs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-71pur4y8gs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-71pur4y8gs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-71pur4y8gs@OpenReview" onclick="foldPdfKimi('71pur4y8gs@OpenReview', this)" class="hr hr-fold">
        </div><div id="60i0ksMAhd@OpenReview" class="panel paper" keywords="blendrl,symbolic,agents,neural,policies,disjointed,harmoniously,reasoning,overcome,merging">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=60i0ksMAhd" target="_blank" title="303/373"><span class="index notranslate">#303</span></a>
                <a id="title-60i0ksMAhd@OpenReview" class="title-link" href="/venue/60i0ksMAhd@OpenReview" target="_blank">BlendRL: A Framework for Merging Symbolic and Neural Policy Learning</a>
                <a id="pdf-60i0ksMAhd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('60i0ksMAhd@OpenReview', this)" data="https://openreview.net/pdf?id=60i0ksMAhd">[PDF<sup id="pdf-stars-60i0ksMAhd@OpenReview">2</sup>]</a>
                <a id="copy-60i0ksMAhd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('60i0ksMAhd@OpenReview')">[Copy]</a>
                <a id="kimi-60i0ksMAhd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('60i0ksMAhd@OpenReview', this)">[Kimi<sup id="kimi-stars-60i0ksMAhd@OpenReview">1</sup>]</a>
                <a id="rel-60i0ksMAhd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('60i0ksMAhd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-60i0ksMAhd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hikaru Shindo" target="_blank">Hikaru Shindo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quentin Delfosse" target="_blank">Quentin Delfosse</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Devendra Singh Dhami" target="_blank">Devendra Singh Dhami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kristian Kersting" target="_blank">Kristian Kersting</a>
            </p>
            <p id="summary-60i0ksMAhd@OpenReview" class="summary">Humans can leverage both symbolic reasoning and intuitive responses. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents’ capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. To overcome this challenge, we introduce *BlendRL*, a neuro-symbolic RL framework that harmoniously integrates both paradigms. We empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations.</p>
            <p id="subjects-60i0ksMAhd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-60i0ksMAhd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-60i0ksMAhd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-60i0ksMAhd@OpenReview" onclick="foldPdfKimi('60i0ksMAhd@OpenReview', this)" class="hr hr-fold">
        </div><div id="5ZEbpBYGwH@OpenReview" class="panel paper" keywords="clustering,coper,view,lda,multi,permutations,correlation,learned,pseudo,labels">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5ZEbpBYGwH" target="_blank" title="304/373"><span class="index notranslate">#304</span></a>
                <a id="title-5ZEbpBYGwH@OpenReview" class="title-link" href="/venue/5ZEbpBYGwH@OpenReview" target="_blank">COPER: Correlation-based Permutations for Multi-View Clustering</a>
                <a id="pdf-5ZEbpBYGwH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5ZEbpBYGwH@OpenReview', this)" data="https://openreview.net/pdf?id=5ZEbpBYGwH">[PDF<sup id="pdf-stars-5ZEbpBYGwH@OpenReview">3</sup>]</a>
                <a id="copy-5ZEbpBYGwH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5ZEbpBYGwH@OpenReview')">[Copy]</a>
                <a id="kimi-5ZEbpBYGwH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5ZEbpBYGwH@OpenReview', this)">[Kimi<sup id="kimi-stars-5ZEbpBYGwH@OpenReview">1</sup>]</a>
                <a id="rel-5ZEbpBYGwH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5ZEbpBYGwH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5ZEbpBYGwH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Eisenberg" target="_blank">Ran Eisenberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Svirsky" target="_blank">Jonathan Svirsky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ofir Lindenbaum" target="_blank">Ofir Lindenbaum</a>
            </p>
            <p id="summary-5ZEbpBYGwH@OpenReview" class="summary">Combining data from different sources can improve data analysis tasks such as clustering. However, most of the current multi-view clustering methods are limited to specific domains or rely on a suboptimal and computationally intensive two-stage process of representation learning and clustering. We propose an end-to-end deep learning-based multi-view clustering framework for general data types (such as images and tables). Our approach involves generating meaningful fused representations using a novel permutation-based canonical correlation objective. We provide a theoretical analysis showing how the learned embeddings approximate those obtained by supervised linear discriminant analysis (LDA). Cluster assignments are learned by identifying consistent pseudo-labels across multiple views. Additionally, we establish a theoretical bound on the error caused by incorrect pseudo-labels in the unsupervised representations compared to LDA. Extensive experiments on ten multi-view clustering benchmark datasets provide empirical evidence for the effectiveness of the proposed model.</p>
            <p id="subjects-5ZEbpBYGwH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5ZEbpBYGwH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5ZEbpBYGwH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5ZEbpBYGwH@OpenReview" onclick="foldPdfKimi('5ZEbpBYGwH@OpenReview', this)" class="hr hr-fold">
        </div><div id="4ub9gpx9xw@OpenReview" class="panel paper" keywords="explanations,faithfulness,llm,concepts,measuring,question,influenced,uncover,model,misrepresent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4ub9gpx9xw" target="_blank" title="305/373"><span class="index notranslate">#305</span></a>
                <a id="title-4ub9gpx9xw@OpenReview" class="title-link" href="/venue/4ub9gpx9xw@OpenReview" target="_blank">Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations</a>
                <a id="pdf-4ub9gpx9xw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4ub9gpx9xw@OpenReview', this)" data="https://openreview.net/pdf?id=4ub9gpx9xw">[PDF<sup id="pdf-stars-4ub9gpx9xw@OpenReview">3</sup>]</a>
                <a id="copy-4ub9gpx9xw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4ub9gpx9xw@OpenReview')">[Copy]</a>
                <a id="kimi-4ub9gpx9xw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4ub9gpx9xw@OpenReview', this)">[Kimi<sup id="kimi-stars-4ub9gpx9xw@OpenReview">9</sup>]</a>
                <a id="rel-4ub9gpx9xw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4ub9gpx9xw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4ub9gpx9xw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Katie Matton" target="_blank">Katie Matton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Ness" target="_blank">Robert Ness</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Guttag" target="_blank">John Guttag</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emre Kiciman" target="_blank">Emre Kiciman</a>
            </p>
            <p id="summary-4ub9gpx9xw@OpenReview" class="summary">Large language models (LLMs) are capable of generating *plausible* explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's "reasoning" process, i.e., they can be *unfaithful*. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level *concepts* in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM's *explanations imply* are influential and the set that *truly* are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLMs provide false claims about which pieces of evidence influenced its decisions.</p>
            <p id="subjects-4ub9gpx9xw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4ub9gpx9xw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4ub9gpx9xw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4ub9gpx9xw@OpenReview" onclick="foldPdfKimi('4ub9gpx9xw@OpenReview', this)" class="hr hr-fold">
        </div><div id="4FVGowGzQb@OpenReview" class="panel paper" keywords="preferred,dis,preference,feedback,optimization,dayan,probabilistic,examples,available,positive">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4FVGowGzQb" target="_blank" title="306/373"><span class="index notranslate">#306</span></a>
                <a id="title-4FVGowGzQb@OpenReview" class="title-link" href="/venue/4FVGowGzQb@OpenReview" target="_blank">Preference Optimization as Probabilistic Inference</a>
                <a id="pdf-4FVGowGzQb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4FVGowGzQb@OpenReview', this)" data="https://openreview.net/pdf?id=4FVGowGzQb">[PDF<sup id="pdf-stars-4FVGowGzQb@OpenReview">4</sup>]</a>
                <a id="copy-4FVGowGzQb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4FVGowGzQb@OpenReview')">[Copy]</a>
                <a id="kimi-4FVGowGzQb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4FVGowGzQb@OpenReview', this)">[Kimi<sup id="kimi-stars-4FVGowGzQb@OpenReview">3</sup>]</a>
                <a id="rel-4FVGowGzQb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4FVGowGzQb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4FVGowGzQb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Abbas Abdolmaleki" target="_blank">Abbas Abdolmaleki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bilal Piot" target="_blank">Bilal Piot</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jost Springenberg" target="_blank">Jost Springenberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bobak Shahriari" target="_blank">Bobak Shahriari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Hertweck" target="_blank">Tim Hertweck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Bloesch" target="_blank">Michael Bloesch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Joshi" target="_blank">Rishabh Joshi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Lampe" target="_blank">Thomas Lampe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junhyuk Oh" target="_blank">Junhyuk Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Heess" target="_blank">Nicolas Heess</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Buchli" target="_blank">Jonas Buchli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Riedmiller" target="_blank">Martin Riedmiller</a>
            </p>
            <p id="summary-4FVGowGzQb@OpenReview" class="summary">Existing preference optimization methods are mainly designed for directly learning from human feedback with the assumption that paired examples (preferred vs. dis-preferred) are available. In contrast, we propose a method that can leverage unpaired preferred or dis-preferred examples by decoupling learning from positive and negative feedback, allowing control over the contribution of each, and works even when only one type of feedback (positive or negative) is available. Our approach builds upon the probabilistic framework introduced in (Dayan &amp; Hinton,1997) , which proposes to use expectation-maximization (EM) to directly optimize the probability of preferred outcomes (as opposed to classic expected reward maximization). To obtain a practical algorithm, we identify and address a key limitation in current EM-based methods: when applied to preference optimization, they solely maximize the likelihood of preferred examples, while neglecting dis-preferred samples. We show how to extend EM algorithms to explicitly incorporate dis-preferred outcomes, leading to a novel, theoretically grounded, preference optimization algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback. We evaluate our approach for training language models based on human feedback as well as training policies for sequential decision-making problems, where learned (value) functions are available.</p>
            <p id="subjects-4FVGowGzQb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4FVGowGzQb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4FVGowGzQb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4FVGowGzQb@OpenReview" onclick="foldPdfKimi('4FVGowGzQb@OpenReview', this)" class="hr hr-fold">
        </div><div id="48WAZhwHHw@OpenReview" class="panel paper" keywords="plansearch,search,livecodebench,pass,plans,language,llm,diverse,natural,gains">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=48WAZhwHHw" target="_blank" title="307/373"><span class="index notranslate">#307</span></a>
                <a id="title-48WAZhwHHw@OpenReview" class="title-link" href="/venue/48WAZhwHHw@OpenReview" target="_blank">Planning in Natural Language Improves LLM Search for Code Generation</a>
                <a id="pdf-48WAZhwHHw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('48WAZhwHHw@OpenReview', this)" data="https://openreview.net/pdf?id=48WAZhwHHw">[PDF<sup id="pdf-stars-48WAZhwHHw@OpenReview">2</sup>]</a>
                <a id="copy-48WAZhwHHw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('48WAZhwHHw@OpenReview')">[Copy]</a>
                <a id="kimi-48WAZhwHHw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('48WAZhwHHw@OpenReview', this)">[Kimi<sup id="kimi-stars-48WAZhwHHw@OpenReview">7</sup>]</a>
                <a id="rel-48WAZhwHHw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('48WAZhwHHw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-48WAZhwHHw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Evan Wang" target="_blank">Evan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Cassano" target="_blank">Federico Cassano</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Catherine Wu" target="_blank">Catherine Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunfeng Bai" target="_blank">Yunfeng Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=William Song" target="_blank">William Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vaskar Nath" target="_blank">Vaskar Nath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwen Han" target="_blank">Ziwen Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sean Hendryx" target="_blank">Sean Hendryx</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Summer Yue" target="_blank">Summer Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hugh Zhang" target="_blank">Hugh Zhang</a>
            </p>
            <p id="summary-48WAZhwHHw@OpenReview" class="summary">While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a pass@200 of 77.0% on LiveCodeBench, outperforming both the best pass-rate achieved without any search (pass@1 = 41.4%) and using standard repeated sampling on top of existing non-search models (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains from search as a function of the diversity over generated ideas.</p>
            <p id="subjects-48WAZhwHHw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-48WAZhwHHw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-48WAZhwHHw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-48WAZhwHHw@OpenReview" onclick="foldPdfKimi('48WAZhwHHw@OpenReview', this)" class="hr hr-fold">
        </div><div id="3fl1SENSYO@OpenReview" class="panel paper" keywords="diffputer,imputation,missing,diffusion,data,step,generative,unleashing,tailored,sampling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3fl1SENSYO" target="_blank" title="308/373"><span class="index notranslate">#308</span></a>
                <a id="title-3fl1SENSYO@OpenReview" class="title-link" href="/venue/3fl1SENSYO@OpenReview" target="_blank">Unleashing the Potential of Diffusion Models for Incomplete Data Imputation</a>
                <a id="pdf-3fl1SENSYO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3fl1SENSYO@OpenReview', this)" data="https://openreview.net/pdf?id=3fl1SENSYO">[PDF<sup id="pdf-stars-3fl1SENSYO@OpenReview">3</sup>]</a>
                <a id="copy-3fl1SENSYO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3fl1SENSYO@OpenReview')">[Copy]</a>
                <a id="kimi-3fl1SENSYO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3fl1SENSYO@OpenReview', this)">[Kimi<sup id="kimi-stars-3fl1SENSYO@OpenReview"></sup>]</a>
                <a id="rel-3fl1SENSYO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3fl1SENSYO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3fl1SENSYO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hengrui Zhang" target="_blank">Hengrui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liancheng Fang" target="_blank">Liancheng Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qitian Wu" target="_blank">Qitian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Yu" target="_blank">Philip Yu</a>
            </p>
            <p id="summary-3fl1SENSYO@OpenReview" class="summary">Generative models play an important role in missing data imputation in that they aim to learn the joint distribution of full data. However, applying advanced deep generative models (such as Diffusion models) to missing data imputation is challenging due to 1) the inherent incompleteness of the training data and 2) the difficulty in performing conditional inference from unconditional generative models. To deal with these challenges, this paper introduces DiffPuter, a tailored diffusion model combined with the Expectation-Maximization (EM) algorithm for missing data imputation. DiffPuter iteratively trains a diffusion model to learn the joint distribution of missing and observed data and performs an accurate conditional sampling to update the missing values using a tailored reversed sampling strategy. Our theoretical analysis shows that DiffPuter's training step corresponds to the maximum likelihood estimation of data density (M-step), and its sampling step represents the Expected A Posteriori estimation of missing values (E-step). Extensive experiments across ten diverse datasets and comparisons with 17 different imputation methods demonstrate DiffPuter's superior performance. Notably, DiffPuter achieves an average improvement of 8.10\% in MAE and 5.64\% in RMSE compared to the most competitive existing method.</p>
            <p id="subjects-3fl1SENSYO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3fl1SENSYO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3fl1SENSYO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3fl1SENSYO@OpenReview" onclick="foldPdfKimi('3fl1SENSYO@OpenReview', this)" class="hr hr-fold">
        </div><div id="2pNLknCTvG@OpenReview" class="panel paper" keywords="uniinf,bobw,tailed,heavy,adversarial,mab,worlds,algorithm,mabs,stochastic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2pNLknCTvG" target="_blank" title="309/373"><span class="index notranslate">#309</span></a>
                <a id="title-2pNLknCTvG@OpenReview" class="title-link" href="/venue/2pNLknCTvG@OpenReview" target="_blank">uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed MABs</a>
                <a id="pdf-2pNLknCTvG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2pNLknCTvG@OpenReview', this)" data="https://openreview.net/pdf?id=2pNLknCTvG">[PDF<sup id="pdf-stars-2pNLknCTvG@OpenReview">2</sup>]</a>
                <a id="copy-2pNLknCTvG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2pNLknCTvG@OpenReview')">[Copy]</a>
                <a id="kimi-2pNLknCTvG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2pNLknCTvG@OpenReview', this)">[Kimi<sup id="kimi-stars-2pNLknCTvG@OpenReview">1</sup>]</a>
                <a id="rel-2pNLknCTvG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2pNLknCTvG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2pNLknCTvG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Chen" target="_blank">Yu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Dai" target="_blank">Yan Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiatai Huang" target="_blank">Jiatai Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longbo Huang" target="_blank">Longbo Huang</a>
            </p>
            <p id="summary-2pNLknCTvG@OpenReview" class="summary">In this paper, we present a novel algorithm, `uniINF`, for the Heavy-Tailed Multi-Armed Bandits (HTMAB) problem, demonstrating robustness and adaptability in both stochastic and adversarial environments. Unlike the stochastic MAB setting where loss distributions are stationary with time, our study extends to the adversarial setup, where losses are generated from heavy-tailed distributions that depend on both arms and time. Our novel algorithm `uniINF` enjoys the so-called Best-of-Both-Worlds (BoBW) property, performing optimally in both stochastic and adversarial environments *without* knowing the exact environment type. Moreover, our algorithm also possesses a Parameter-Free feature, *i.e.*, it operates *without* the need of knowing the heavy-tail parameters <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-244-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1537" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1538"><span class="mo" id="MathJax-Span-1539" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1540" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1541" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-1542" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">α</span><span class="mo" id="MathJax-Span-1543" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>σ</mi><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-244">(\sigma, \alpha)</script> a-priori.To be precise, `uniINF` ensures nearly-optimal regret in both stochastic and adversarial environments, matching the corresponding lower bounds when <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-245-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1544" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1545"><span class="mo" id="MathJax-Span-1546" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1547" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1548" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-1549" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">α</span><span class="mo" id="MathJax-Span-1550" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>σ</mi><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-245">(\sigma, \alpha)</script> is known (up to logarithmic factors). To our knowledge, `uniINF` is the first parameter-free algorithm to achieve the BoBW property for the heavy-tailed MAB problem. Technically, we develop innovative techniques to achieve BoBW guarantees for Parameter-Free HTMABs, including a refined analysis for the dynamics of log-barrier, an auto-balancing learning rate scheduling scheme, an adaptive skipping-clipping loss tuning technique, and a stopping-time analysis for logarithmic regret.</p>
            <p id="subjects-2pNLknCTvG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2pNLknCTvG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2pNLknCTvG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2pNLknCTvG@OpenReview" onclick="foldPdfKimi('2pNLknCTvG@OpenReview', this)" class="hr hr-fold">
        </div><div id="1xzqz73hvL@OpenReview" class="panel paper" keywords="surrogate,w2s,ridgeless,weak,model,distillation,strong,knowledge,generalization,scaling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1xzqz73hvL" target="_blank" title="310/373"><span class="index notranslate">#310</span></a>
                <a id="title-1xzqz73hvL@OpenReview" class="title-link" href="/venue/1xzqz73hvL@OpenReview" target="_blank">High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws</a>
                <a id="pdf-1xzqz73hvL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1xzqz73hvL@OpenReview', this)" data="https://openreview.net/pdf?id=1xzqz73hvL">[PDF<sup id="pdf-stars-1xzqz73hvL@OpenReview">1</sup>]</a>
                <a id="copy-1xzqz73hvL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1xzqz73hvL@OpenReview')">[Copy]</a>
                <a id="kimi-1xzqz73hvL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1xzqz73hvL@OpenReview', this)">[Kimi<sup id="kimi-stars-1xzqz73hvL@OpenReview">3</sup>]</a>
                <a id="rel-1xzqz73hvL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1xzqz73hvL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1xzqz73hvL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammed Ildiz" target="_blank">Muhammed Ildiz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Halil Gozeten" target="_blank">Halil Gozeten</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ege Taga" target="_blank">Ege Taga</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Mondelli" target="_blank">Marco Mondelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samet Oymak" target="_blank">Samet Oymak</a>
            </p>
            <p id="summary-1xzqz73hvL@OpenReview" class="summary">A growing number of machine learning scenarios rely on knowledge distillation where one uses the output of a surrogate model as labels to supervise the training of a target model. In this work, we provide a sharp characterization of this process for ridgeless, high-dimensional regression, under two settings: *(i)* model shift, where the surrogate model is arbitrary, and *(ii)* distribution shift, where the surrogate model is the solution of empirical risk minimization with out-of-distribution data. In both cases, we characterize the precise risk of the target model through non-asymptotic bounds in terms of sample size and data distribution under mild conditions. As a consequence, we identify the form of the optimal surrogate model, which reveals the benefits and limitations of discarding weak features in a data-dependent fashion. In the context of weak-to-strong (W2S) generalization, this has the interpretation that *(i)* W2S training, with the surrogate as the weak model, can provably outperform training with strong labels under the same data budget, but *(ii)* it is unable to improve the data scaling law. We validate our results on numerical experiments both on ridgeless regression and on neural network architectures.</p>
            <p id="subjects-1xzqz73hvL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-1xzqz73hvL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1xzqz73hvL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1xzqz73hvL@OpenReview" onclick="foldPdfKimi('1xzqz73hvL@OpenReview', this)" class="hr hr-fold">
        </div><div id="1Iuw1jcIrf@OpenReview" class="panel paper" keywords="mathematical,reasoning,pretraining,code,mathcoder2,continued,math,steps,expressions,mathcode">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1Iuw1jcIrf" target="_blank" title="311/373"><span class="index notranslate">#311</span></a>
                <a id="title-1Iuw1jcIrf@OpenReview" class="title-link" href="/venue/1Iuw1jcIrf@OpenReview" target="_blank">MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code</a>
                <a id="pdf-1Iuw1jcIrf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1Iuw1jcIrf@OpenReview', this)" data="https://openreview.net/pdf?id=1Iuw1jcIrf">[PDF<sup id="pdf-stars-1Iuw1jcIrf@OpenReview">6</sup>]</a>
                <a id="copy-1Iuw1jcIrf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1Iuw1jcIrf@OpenReview')">[Copy]</a>
                <a id="kimi-1Iuw1jcIrf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1Iuw1jcIrf@OpenReview', this)">[Kimi<sup id="kimi-stars-1Iuw1jcIrf@OpenReview">7</sup>]</a>
                <a id="rel-1Iuw1jcIrf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1Iuw1jcIrf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1Iuw1jcIrf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zimu Lu" target="_blank">Zimu Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aojun Zhou" target="_blank">Aojun Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ke Wang" target="_blank">Ke Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Houxing Ren" target="_blank">Houxing Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weikang Shi" target="_blank">Weikang Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junting Pan" target="_blank">Junting Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingjie Zhan" target="_blank">Mingjie Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongsheng Li" target="_blank">Hongsheng Li</a>
            </p>
            <p id="summary-1Iuw1jcIrf@OpenReview" class="summary">Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline.</p>
            <p id="subjects-1Iuw1jcIrf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-1Iuw1jcIrf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1Iuw1jcIrf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1Iuw1jcIrf@OpenReview" onclick="foldPdfKimi('1Iuw1jcIrf@OpenReview', this)" class="hr hr-fold">
        </div><div id="0yvZm2AjUr@OpenReview" class="panel paper" keywords="greg,lms,propositions,nurse,worksas,propositional,physicist,unfaithfully,laura,probes">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0yvZm2AjUr" target="_blank" title="312/373"><span class="index notranslate">#312</span></a>
                <a id="title-0yvZm2AjUr@OpenReview" class="title-link" href="/venue/0yvZm2AjUr@OpenReview" target="_blank">Monitoring Latent World States in Language Models with Propositional Probes</a>
                <a id="pdf-0yvZm2AjUr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0yvZm2AjUr@OpenReview', this)" data="https://openreview.net/pdf?id=0yvZm2AjUr">[PDF<sup id="pdf-stars-0yvZm2AjUr@OpenReview">2</sup>]</a>
                <a id="copy-0yvZm2AjUr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0yvZm2AjUr@OpenReview')">[Copy]</a>
                <a id="kimi-0yvZm2AjUr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0yvZm2AjUr@OpenReview', this)">[Kimi<sup id="kimi-stars-0yvZm2AjUr@OpenReview">2</sup>]</a>
                <a id="rel-0yvZm2AjUr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0yvZm2AjUr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0yvZm2AjUr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahai Feng" target="_blank">Jiahai Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stuart Russell" target="_blank">Stuart Russell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Steinhardt" target="_blank">Jacob Steinhardt</a>
            </p>
            <p id="summary-0yvZm2AjUr@OpenReview" class="summary">Language models (LMs) are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of LMs could help monitor and correct unfaithful behavior. We hypothesize that LMs faithfully represent their input contexts in a latent world model, and we seek to extract these latent world states as logical propositions. For example, given the input context ``Greg is a nurse. Laura is a physicist.'', we aim to decode the propositions WorksAs(Greg, nurse) and WorksAs(Laura, physicist) from the model's internal activations. To do so we introduce _propositional probes_, which compositionally extract lexical concepts from token activations and bind them into propositions. Key to this is identifying a _binding subspace_ in which bound tokens have high similarity (Greg <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-246-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2194;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1551" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1552"><span class="mo" id="MathJax-Span-1553" style="font-family: MathJax_Main;">↔</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↔</mo></math></span></span><script type="math/tex" id="MathJax-Element-246">\leftrightarrow</script> nurse) but unbound ones do not (Greg <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-247-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x21AE;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1554" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.878em, 1000.94em, 2.555em, -999.997em); top: -2.445em; left: 0em;"><span class="mrow" id="MathJax-Span-1555"><span class="mo" id="MathJax-Span-1556" style="font-family: MathJax_AMS;">↮</span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>↮</mo></math></span></span><script type="math/tex" id="MathJax-Element-247">\not\leftrightarrow</script> physicist). Despite only being trained on linguistically simple English templates, we find that propositional probes generalize to inputs written as short stories and translated to Spanish. Moreover, in three settings where LMs respond unfaithfully to the input context---prompt injections, backdoor attacks, and gender bias--- the decoded propositions remain faithful. This suggests that LMs often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.</p>
            <p id="subjects-0yvZm2AjUr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-0yvZm2AjUr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0yvZm2AjUr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0yvZm2AjUr@OpenReview" onclick="foldPdfKimi('0yvZm2AjUr@OpenReview', this)" class="hr hr-fold">
        </div><div id="yfW1x7uBS5@OpenReview" class="panel paper" keywords="artists,mimicry,protections,adversarial,perturbations,protect,reliably,generative,style,cannot">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=yfW1x7uBS5" target="_blank" title="313/373"><span class="index notranslate">#313</span></a>
                <a id="title-yfW1x7uBS5@OpenReview" class="title-link" href="/venue/yfW1x7uBS5@OpenReview" target="_blank">Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI</a>
                <a id="pdf-yfW1x7uBS5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('yfW1x7uBS5@OpenReview', this)" data="https://openreview.net/pdf?id=yfW1x7uBS5">[PDF<sup id="pdf-stars-yfW1x7uBS5@OpenReview">2</sup>]</a>
                <a id="copy-yfW1x7uBS5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('yfW1x7uBS5@OpenReview')">[Copy]</a>
                <a id="kimi-yfW1x7uBS5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('yfW1x7uBS5@OpenReview', this)">[Kimi<sup id="kimi-stars-yfW1x7uBS5@OpenReview">1</sup>]</a>
                <a id="rel-yfW1x7uBS5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('yfW1x7uBS5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-yfW1x7uBS5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Hönig" target="_blank">Robert Hönig</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Javier Rando" target="_blank">Javier Rando</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Carlini" target="_blank">Nicholas Carlini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Tramer" target="_blank">Florian Tramer</a>
            </p>
            <p id="summary-yfW1x7uBS5@OpenReview" class="summary">Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles.In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security. We find that low-effort and "off-the-shelf" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that **all existing protections can be easily bypassed**, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions.</p>
            <p id="subjects-yfW1x7uBS5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-yfW1x7uBS5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-yfW1x7uBS5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-yfW1x7uBS5@OpenReview" onclick="foldPdfKimi('yfW1x7uBS5@OpenReview', this)" class="hr hr-fold">
        </div><div id="xsELpEPn4A@OpenReview" class="panel paper" keywords="judgelm,judges,judge,llms,fine,bias,ended,pandalm,scalable,answers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xsELpEPn4A" target="_blank" title="314/373"><span class="index notranslate">#314</span></a>
                <a id="title-xsELpEPn4A@OpenReview" class="title-link" href="/venue/xsELpEPn4A@OpenReview" target="_blank">JudgeLM: Fine-tuned Large Language Models are Scalable Judges</a>
                <a id="pdf-xsELpEPn4A@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xsELpEPn4A@OpenReview', this)" data="https://openreview.net/pdf?id=xsELpEPn4A">[PDF<sup id="pdf-stars-xsELpEPn4A@OpenReview">4</sup>]</a>
                <a id="copy-xsELpEPn4A@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xsELpEPn4A@OpenReview')">[Copy]</a>
                <a id="kimi-xsELpEPn4A@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xsELpEPn4A@OpenReview', this)">[Kimi<sup id="kimi-stars-xsELpEPn4A@OpenReview">6</sup>]</a>
                <a id="rel-xsELpEPn4A@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xsELpEPn4A@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xsELpEPn4A@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lianghui Zhu" target="_blank">Lianghui Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinggang Wang" target="_blank">Xinggang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlong Wang" target="_blank">Xinlong Wang</a>
            </p>
            <p id="summary-xsELpEPn4A@OpenReview" class="summary">Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc.</p>
            <p id="subjects-xsELpEPn4A@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xsELpEPn4A@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xsELpEPn4A@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xsELpEPn4A@OpenReview" onclick="foldPdfKimi('xsELpEPn4A@OpenReview', this)" class="hr hr-fold">
        </div><div id="xQBRrtQM8u@OpenReview" class="panel paper" keywords="tuning,reward,fine,soc,memoryless,matching,adjoint,generative,models,flow">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xQBRrtQM8u" target="_blank" title="315/373"><span class="index notranslate">#315</span></a>
                <a id="title-xQBRrtQM8u@OpenReview" class="title-link" href="/venue/xQBRrtQM8u@OpenReview" target="_blank">Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control</a>
                <a id="pdf-xQBRrtQM8u@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xQBRrtQM8u@OpenReview', this)" data="https://openreview.net/pdf?id=xQBRrtQM8u">[PDF<sup id="pdf-stars-xQBRrtQM8u@OpenReview">2</sup>]</a>
                <a id="copy-xQBRrtQM8u@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xQBRrtQM8u@OpenReview')">[Copy]</a>
                <a id="kimi-xQBRrtQM8u@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xQBRrtQM8u@OpenReview', this)">[Kimi<sup id="kimi-stars-xQBRrtQM8u@OpenReview">3</sup>]</a>
                <a id="rel-xQBRrtQM8u@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xQBRrtQM8u@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xQBRrtQM8u@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Carles Domingo i Enrich" target="_blank">Carles Domingo i Enrich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michal Drozdzal" target="_blank">Michal Drozdzal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Karrer" target="_blank">Brian Karrer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ricky T. Q. Chen" target="_blank">Ricky T. Q. Chen</a>
            </p>
            <p id="summary-xQBRrtQM8u@OpenReview" class="summary">Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific *memoryless* noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named *Adjoint Matching* which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.</p>
            <p id="subjects-xQBRrtQM8u@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xQBRrtQM8u@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xQBRrtQM8u@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xQBRrtQM8u@OpenReview" onclick="foldPdfKimi('xQBRrtQM8u@OpenReview', this)" class="hr hr-fold">
        </div><div id="uTqnyF0JNR@OpenReview" class="panel paper" keywords="igl,imbalanced,graph,bench,algorithms,imbalance,comprehensive,learning,disproportionally,embarking">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uTqnyF0JNR" target="_blank" title="316/373"><span class="index notranslate">#316</span></a>
                <a id="title-uTqnyF0JNR@OpenReview" class="title-link" href="/venue/uTqnyF0JNR@OpenReview" target="_blank">IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning</a>
                <a id="pdf-uTqnyF0JNR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uTqnyF0JNR@OpenReview', this)" data="https://openreview.net/pdf?id=uTqnyF0JNR">[PDF<sup id="pdf-stars-uTqnyF0JNR@OpenReview">5</sup>]</a>
                <a id="copy-uTqnyF0JNR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uTqnyF0JNR@OpenReview')">[Copy]</a>
                <a id="kimi-uTqnyF0JNR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uTqnyF0JNR@OpenReview', this)">[Kimi<sup id="kimi-stars-uTqnyF0JNR@OpenReview">4</sup>]</a>
                <a id="rel-uTqnyF0JNR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uTqnyF0JNR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uTqnyF0JNR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawen Qin" target="_blank">Jiawen Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haonan Yuan" target="_blank">Haonan Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingyun Sun" target="_blank">Qingyun Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lyujin Xu" target="_blank">Lyujin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Yuan" target="_blank">Jiaqi Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfeng Huang" target="_blank">Pengfeng Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaonan Wang" target="_blank">Zhaonan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingcheng Fu" target="_blank">Xingcheng Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Peng" target="_blank">Hao Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianxin Li" target="_blank">Jianxin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Yu" target="_blank">Philip Yu</a>
            </p>
            <p id="summary-uTqnyF0JNR@OpenReview" class="summary">Deep graph learning has gained grand popularity over the past years due to its versatility and success in representing graph data across a wide range of domains. However, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. To address this challenge, Imbalanced Graph Learning (IGL) has garnered substantial attention, enabling more balanced data distributions and better task performance. Despite the proliferation of IGL algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to comprehending advancements in this field. To bridge this gap, we introduce **IGL-Bench**, a foundational comprehensive benchmark for imbalanced graph learning, embarking on **17** diverse graph datasets and **24** distinct IGL algorithms with uniform data processing and splitting strategies. Specifically, IGL-Bench systematically investigates state-of-the-art IGL algorithms in terms of **effectiveness**, **robustness**, and **efficiency** on node-level and graph-level tasks, with the scope of class-imbalance and topology-imbalance. Extensive experiments demonstrate the potential benefits of IGL algorithms on various imbalanced conditions, offering insights and opportunities in the IGL field. Further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, which is available at https://anonymous.4open.science/r/IGL-Bench.</p>
            <p id="subjects-uTqnyF0JNR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-uTqnyF0JNR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uTqnyF0JNR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uTqnyF0JNR@OpenReview" onclick="foldPdfKimi('uTqnyF0JNR@OpenReview', this)" class="hr hr-fold">
        </div><div id="vQhn4wrQ6j@OpenReview" class="panel paper" keywords="math,language,lingual,swapping,transfer,merging,instruction,expert,languages,cross">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vQhn4wrQ6j" target="_blank" title="317/373"><span class="index notranslate">#317</span></a>
                <a id="title-vQhn4wrQ6j@OpenReview" class="title-link" href="/venue/vQhn4wrQ6j@OpenReview" target="_blank">Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models</a>
                <a id="pdf-vQhn4wrQ6j@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vQhn4wrQ6j@OpenReview', this)" data="https://openreview.net/pdf?id=vQhn4wrQ6j">[PDF<sup id="pdf-stars-vQhn4wrQ6j@OpenReview">1</sup>]</a>
                <a id="copy-vQhn4wrQ6j@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vQhn4wrQ6j@OpenReview')">[Copy]</a>
                <a id="kimi-vQhn4wrQ6j@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vQhn4wrQ6j@OpenReview', this)">[Kimi<sup id="kimi-stars-vQhn4wrQ6j@OpenReview">3</sup>]</a>
                <a id="rel-vQhn4wrQ6j@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vQhn4wrQ6j@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vQhn4wrQ6j@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lucas Bandarkar" target="_blank">Lucas Bandarkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Muller" target="_blank">Benjamin Muller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pritish Yuvraj" target="_blank">Pritish Yuvraj</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Hou" target="_blank">Rui Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nayan Singhal" target="_blank">Nayan Singhal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongjiang Lv" target="_blank">Hongjiang Lv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bing Liu" target="_blank">Bing Liu</a>
            </p>
            <p id="summary-vQhn4wrQ6j@OpenReview" class="summary">Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate "experts" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.</p>
            <p id="subjects-vQhn4wrQ6j@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-vQhn4wrQ6j@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vQhn4wrQ6j@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vQhn4wrQ6j@OpenReview" onclick="foldPdfKimi('vQhn4wrQ6j@OpenReview', this)" class="hr hr-fold">
        </div><div id="m9wG6ai2Xk@OpenReview" class="panel paper" keywords="mquake,editing,knowledge,messi,remastered,club,hop,questions,erroneous,fix">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=m9wG6ai2Xk" target="_blank" title="318/373"><span class="index notranslate">#318</span></a>
                <a id="title-m9wG6ai2Xk@OpenReview" class="title-link" href="/venue/m9wG6ai2Xk@OpenReview" target="_blank">MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations</a>
                <a id="pdf-m9wG6ai2Xk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('m9wG6ai2Xk@OpenReview', this)" data="https://openreview.net/pdf?id=m9wG6ai2Xk">[PDF<sup id="pdf-stars-m9wG6ai2Xk@OpenReview">3</sup>]</a>
                <a id="copy-m9wG6ai2Xk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('m9wG6ai2Xk@OpenReview')">[Copy]</a>
                <a id="kimi-m9wG6ai2Xk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('m9wG6ai2Xk@OpenReview', this)">[Kimi<sup id="kimi-stars-m9wG6ai2Xk@OpenReview">1</sup>]</a>
                <a id="rel-m9wG6ai2Xk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('m9wG6ai2Xk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-m9wG6ai2Xk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaochen Zhong" target="_blank">Shaochen Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan (Louie) Lu" target="_blank">Yifan (Louie) Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lize Shao" target="_blank">Lize Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bhargav Bhushanam" target="_blank">Bhargav Bhushanam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaocong Du" target="_blank">Xiaocong Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixin Wan" target="_blank">Yixin Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yucheng Shi" target="_blank">Yucheng Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daochen Zha" target="_blank">Daochen Zha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiwei Wang" target="_blank">Yiwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ninghao Liu" target="_blank">Ninghao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaixiong Zhou" target="_blank">Kaixiong Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=shuai xu" target="_blank">shuai xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai-Wei Chang" target="_blank">Kai-Wei Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Louis Feng" target="_blank">Louis Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vipin Chaudhary" target="_blank">Vipin Chaudhary</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xia Hu" target="_blank">Xia Hu</a>
            </p>
            <p id="summary-m9wG6ai2Xk@OpenReview" class="summary">Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, knowledge editing often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rests, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., "what club does Lionel Messi currently play for?").However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (e.g., "who is the offspring of the owner of the club that Messi currently plays for?"). Prior arts have coined this task as multi-hop knowledge editing with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of making knowledge editing datasets at scale.In this work, we reveal that **up to 33% or 76% of MQuAKE's questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights.** Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed \mquake{}-evaluated editing methods on our post-fix dataset, \mquaker{}. It is our observation that many methods try to overfit the original \mquake{} by exploiting some data-specific properties of \mquake{}. We provide a guideline on how to faithfully approach such datasets and show that a simple, minimally invasive approach can bring excellent editing performance without such exploitation. Please refer to the supplemental material for assets.</p>
            <p id="subjects-m9wG6ai2Xk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-m9wG6ai2Xk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-m9wG6ai2Xk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-m9wG6ai2Xk@OpenReview" onclick="foldPdfKimi('m9wG6ai2Xk@OpenReview', this)" class="hr hr-fold">
        </div><div id="mkDam1xIzW@OpenReview" class="panel paper" keywords="pgpca,ppca,manifold,data,dimensionality,coordinate,around,probabilistic,reduction,euclidean">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mkDam1xIzW" target="_blank" title="319/373"><span class="index notranslate">#319</span></a>
                <a id="title-mkDam1xIzW@OpenReview" class="title-link" href="/venue/mkDam1xIzW@OpenReview" target="_blank">Probabilistic Geometric Principal Component Analysis with application to neural data</a>
                <a id="pdf-mkDam1xIzW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mkDam1xIzW@OpenReview', this)" data="https://openreview.net/pdf?id=mkDam1xIzW">[PDF<sup id="pdf-stars-mkDam1xIzW@OpenReview">3</sup>]</a>
                <a id="copy-mkDam1xIzW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mkDam1xIzW@OpenReview')">[Copy]</a>
                <a id="kimi-mkDam1xIzW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mkDam1xIzW@OpenReview', this)">[Kimi<sup id="kimi-stars-mkDam1xIzW@OpenReview">2</sup>]</a>
                <a id="rel-mkDam1xIzW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mkDam1xIzW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mkDam1xIzW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Han-Lin Hsieh" target="_blank">Han-Lin Hsieh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maryam Shanechi" target="_blank">Maryam Shanechi</a>
            </p>
            <p id="summary-mkDam1xIzW@OpenReview" class="summary">Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system around the mean of data. However, in many neuroscience applications, data may be distributed around a nonlinear geometry (i.e., manifold) rather than lying in the Euclidean space around the mean. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) for such datasets as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from these data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data that exhibit noise and are distributed around a nonlinear manifold, especially for neural data.</p>
            <p id="subjects-mkDam1xIzW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mkDam1xIzW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mkDam1xIzW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mkDam1xIzW@OpenReview" onclick="foldPdfKimi('mkDam1xIzW@OpenReview', this)" class="hr hr-fold">
        </div><div id="l6QnSQizmN@OpenReview" class="panel paper" keywords="lcpo,experiences,online,environments,context,policy,stationary,anchoring,reinforcement,traces">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=l6QnSQizmN" target="_blank" title="320/373"><span class="index notranslate">#320</span></a>
                <a id="title-l6QnSQizmN@OpenReview" class="title-link" href="/venue/l6QnSQizmN@OpenReview" target="_blank">Online Reinforcement Learning in Non-Stationary Context-Driven Environments</a>
                <a id="pdf-l6QnSQizmN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('l6QnSQizmN@OpenReview', this)" data="https://openreview.net/pdf?id=l6QnSQizmN">[PDF<sup id="pdf-stars-l6QnSQizmN@OpenReview">3</sup>]</a>
                <a id="copy-l6QnSQizmN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('l6QnSQizmN@OpenReview')">[Copy]</a>
                <a id="kimi-l6QnSQizmN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('l6QnSQizmN@OpenReview', this)">[Kimi<sup id="kimi-stars-l6QnSQizmN@OpenReview">3</sup>]</a>
                <a id="rel-l6QnSQizmN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('l6QnSQizmN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-l6QnSQizmN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pouya Hamadanian" target="_blank">Pouya Hamadanian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arash Nasr-Esfahany" target="_blank">Arash Nasr-Esfahany</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Malte Schwarzkopf" target="_blank">Malte Schwarzkopf</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddhartha Sen" target="_blank">Siddhartha Sen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Alizadeh" target="_blank">Mohammad Alizadeh</a>
            </p>
            <p id="summary-l6QnSQizmN@OpenReview" class="summary">We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to ``catastrophic forgetting'' (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics or use off-policy methods that suffer from instability and poor performance.We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a ``prescient'' agent trained offline across all context traces.</p>
            <p id="subjects-l6QnSQizmN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-l6QnSQizmN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-l6QnSQizmN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-l6QnSQizmN@OpenReview" onclick="foldPdfKimi('l6QnSQizmN@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZV7CLf0RHK@OpenReview" class="panel paper" keywords="lora,textbf,tuning,fine,preft,redundancies,majority,ranks,reserved,arameter">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZV7CLf0RHK" target="_blank" title="321/373"><span class="index notranslate">#321</span></a>
                <a id="title-ZV7CLf0RHK@OpenReview" class="title-link" href="/venue/ZV7CLf0RHK@OpenReview" target="_blank">Fine-tuning with Reserved Majority for Noise Reduction</a>
                <a id="pdf-ZV7CLf0RHK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZV7CLf0RHK@OpenReview', this)" data="https://openreview.net/pdf?id=ZV7CLf0RHK">[PDF<sup id="pdf-stars-ZV7CLf0RHK@OpenReview">2</sup>]</a>
                <a id="copy-ZV7CLf0RHK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZV7CLf0RHK@OpenReview')">[Copy]</a>
                <a id="kimi-ZV7CLf0RHK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZV7CLf0RHK@OpenReview', this)">[Kimi<sup id="kimi-stars-ZV7CLf0RHK@OpenReview">7</sup>]</a>
                <a id="rel-ZV7CLf0RHK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZV7CLf0RHK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZV7CLf0RHK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuyang Jiang" target="_blank">Shuyang Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yusheng Liao" target="_blank">Yusheng Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanfeng Wang" target="_blank">Yanfeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ya Zhang" target="_blank">Ya Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Wang" target="_blank">Yu Wang</a>
            </p>
            <p id="summary-ZV7CLf0RHK@OpenReview" class="summary">Parameter-efficient fine-tuning (PEFT) has revolutionized supervised fine-tuning, where LoRA and its variants gain the most popularity due to their low training costs and zero inference latency.However, LoRA tuning not only injects knowledgeable features but also noisy hallucination during fine-tuning, which hinders the utilization of tunable parameters with the increasing LoRA rank.In this work, we first investigate in-depth the redundancies among LoRA parameters with substantial empirical studies.Aiming to resemble the learning capacity of high ranks from the findings, we set up a new fine-tuning framework, \textbf{P}arameter-\textbf{Re}dundant \textbf{F}ine-\textbf{T}uning (\preft), which follows the vanilla LoRA tuning process but is required to reduce redundancies before merging LoRA parameters back to pre-trained models.Based on this framework, we propose \textbf{No}ise reduction with \textbf{R}eserved \textbf{M}ajority (\norm), which decomposes the LoRA parameters into majority parts and redundant parts with random singular value decomposition.The major components are determined by the proposed \search method, specifically employing subspace similarity to confirm the parameter groups that share the highest similarity with the base weight.By employing \norm, we enhance both the learning capacity and benefits from larger ranks, which consistently outperforms both LoRA and other \preft-based methods on various downstream tasks, such as general instruction tuning, math reasoning and code generation.</p>
            <p id="subjects-ZV7CLf0RHK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ZV7CLf0RHK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZV7CLf0RHK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZV7CLf0RHK@OpenReview" onclick="foldPdfKimi('ZV7CLf0RHK@OpenReview', this)" class="hr hr-fold">
        </div><div id="h8yg0hT96f@OpenReview" class="panel paper" keywords="boed,eig,maximization,posterior,expected,bayesian,diffusions,optimization,design,gain">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=h8yg0hT96f" target="_blank" title="322/373"><span class="index notranslate">#322</span></a>
                <a id="title-h8yg0hT96f@OpenReview" class="title-link" href="/venue/h8yg0hT96f@OpenReview" target="_blank">Bayesian Experimental Design Via Contrastive Diffusions</a>
                <a id="pdf-h8yg0hT96f@OpenReview" class="title-pdf notranslate" onclick="togglePdf('h8yg0hT96f@OpenReview', this)" data="https://openreview.net/pdf?id=h8yg0hT96f">[PDF<sup id="pdf-stars-h8yg0hT96f@OpenReview">2</sup>]</a>
                <a id="copy-h8yg0hT96f@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('h8yg0hT96f@OpenReview')">[Copy]</a>
                <a id="kimi-h8yg0hT96f@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('h8yg0hT96f@OpenReview', this)">[Kimi<sup id="kimi-stars-h8yg0hT96f@OpenReview"></sup>]</a>
                <a id="rel-h8yg0hT96f@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('h8yg0hT96f@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-h8yg0hT96f@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jacopo Iollo" target="_blank">Jacopo Iollo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christophe Heinkelé" target="_blank">Christophe Heinkelé</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre Alliez" target="_blank">Pierre Alliez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florence Forbes" target="_blank">Florence Forbes</a>
            </p>
            <p id="summary-h8yg0hT96f@OpenReview" class="summary">Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments.When based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected *contrast* between prior and posterior distributions.Scaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity.In this work, we introduce an *expected posterior* distribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the expected posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop, without resorting to lower bound approximations of the EIG. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion models. By incorporating generative models into the BOED framework, we expand its scope and its use in scenarios that were previously impractical. Numerical experiments and comparison with state-of-the-art methods show the potential of the approach.</p>
            <p id="subjects-h8yg0hT96f@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-h8yg0hT96f@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-h8yg0hT96f@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-h8yg0hT96f@OpenReview" onclick="foldPdfKimi('h8yg0hT96f@OpenReview', this)" class="hr hr-fold">
        </div><div id="hwnObmOTrV@OpenReview" class="panel paper" keywords="mmfm,matching,flow,points,across,time,world,dynamics,conditions,modeling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hwnObmOTrV" target="_blank" title="323/373"><span class="index notranslate">#323</span></a>
                <a id="title-hwnObmOTrV@OpenReview" class="title-link" href="/venue/hwnObmOTrV@OpenReview" target="_blank">Modeling Complex System Dynamics with Flow Matching Across Time and Conditions</a>
                <a id="pdf-hwnObmOTrV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hwnObmOTrV@OpenReview', this)" data="https://openreview.net/pdf?id=hwnObmOTrV">[PDF<sup id="pdf-stars-hwnObmOTrV@OpenReview">2</sup>]</a>
                <a id="copy-hwnObmOTrV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hwnObmOTrV@OpenReview')">[Copy]</a>
                <a id="kimi-hwnObmOTrV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hwnObmOTrV@OpenReview', this)">[Kimi<sup id="kimi-stars-hwnObmOTrV@OpenReview">1</sup>]</a>
                <a id="rel-hwnObmOTrV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hwnObmOTrV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hwnObmOTrV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Rohbeck" target="_blank">Martin Rohbeck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charlotte Bunne" target="_blank">Charlotte Bunne</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edward De Brouwer" target="_blank">Edward De Brouwer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan-Christian Huetter" target="_blank">Jan-Christian Huetter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anne Biton" target="_blank">Anne Biton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kelvin Chen" target="_blank">Kelvin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aviv Regev" target="_blank">Aviv Regev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Romain Lopez" target="_blank">Romain Lopez</a>
            </p>
            <p id="summary-hwnObmOTrV@OpenReview" class="summary">Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow Matching to tackle this problem, but these approaches remain limited in their ability to effectively combine data from multiple time points and different experimental settings. This integration is essential in real-world scenarios where observations from certain combinations of time points and experimental conditions are missing, either because of experimental costs or sensory failure. To address this challenge, we propose a novel method named Multi-Marginal Flow Matching (MMFM). MMFM first constructs a flow using smooth spline-based interpolation across time points and conditions and regresses it with a neural network using the classifier-free guided Flow Matching framework. This framework allows for the sharing of contextual information about the dynamics across multiple trajectories. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, including a recent single-cell genomics data set with around a hundred chemical perturbations across time points. Our results show that MMFM significantly outperforms existing methods at imputing data at missing time points.</p>
            <p id="subjects-hwnObmOTrV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hwnObmOTrV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hwnObmOTrV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hwnObmOTrV@OpenReview" onclick="foldPdfKimi('hwnObmOTrV@OpenReview', this)" class="hr hr-fold">
        </div><div id="NdHka08uWn@OpenReview" class="panel paper" keywords="symbolic,regression,retrieval,augmented,rag,pre,neural,generation,expressions,trained">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NdHka08uWn" target="_blank" title="324/373"><span class="index notranslate">#324</span></a>
                <a id="title-NdHka08uWn@OpenReview" class="title-link" href="/venue/NdHka08uWn@OpenReview" target="_blank">RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regression</a>
                <a id="pdf-NdHka08uWn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NdHka08uWn@OpenReview', this)" data="https://openreview.net/pdf?id=NdHka08uWn">[PDF<sup id="pdf-stars-NdHka08uWn@OpenReview">7</sup>]</a>
                <a id="copy-NdHka08uWn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NdHka08uWn@OpenReview')">[Copy]</a>
                <a id="kimi-NdHka08uWn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NdHka08uWn@OpenReview', this)">[Kimi<sup id="kimi-stars-NdHka08uWn@OpenReview">5</sup>]</a>
                <a id="rel-NdHka08uWn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NdHka08uWn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NdHka08uWn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hengzhe Zhang" target="_blank">Hengzhe Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Chen" target="_blank">Qi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bing XUE" target="_blank">Bing XUE</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wolfgang Banzhaf" target="_blank">Wolfgang Banzhaf</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengjie Zhang" target="_blank">Mengjie Zhang</a>
            </p>
            <p id="summary-NdHka08uWn@OpenReview" class="summary">Symbolic regression is a key task in machine learning, aiming to discover mathematical expressions that best describe a dataset. While deep learning has increased interest in using neural networks for symbolic regression, many existing approaches rely on pre-trained models. These models require significant computational resources and struggle with regression tasks involving unseen functions and variables. A pre-training-free paradigm is needed to better integrate with search-based symbolic regression algorithms. To address these limitations, we propose a novel framework for symbolic regression that integrates evolutionary feature construction with a neural network, without the need for pre-training. Our approach adaptively generates symbolic trees that align with the desired semantics in real-time using a language model trained via online supervised learning, providing effective building blocks for feature construction. To mitigate hallucinations from the language model, we design a retrieval-augmented generation mechanism that explicitly leverages searched symbolic expressions. Additionally, we introduce a scale-invariant data augmentation technique that further improves the robustness and generalization of the model. Experimental results demonstrate that our framework achieves state-of-the-art accuracy across 25 regression algorithms and 120 regression tasks.</p>
            <p id="subjects-NdHka08uWn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-NdHka08uWn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NdHka08uWn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NdHka08uWn@OpenReview" onclick="foldPdfKimi('NdHka08uWn@OpenReview', this)" class="hr hr-fold">
        </div><div id="WKfb1xGXGx@OpenReview" class="panel paper" keywords="hair,perm,representation,strand,editing,textures,parametric,hairstyle,grooming,style">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WKfb1xGXGx" target="_blank" title="325/373"><span class="index notranslate">#325</span></a>
                <a id="title-WKfb1xGXGx@OpenReview" class="title-link" href="/venue/WKfb1xGXGx@OpenReview" target="_blank">Perm: A Parametric Representation for Multi-Style 3D Hair Modeling</a>
                <a id="pdf-WKfb1xGXGx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WKfb1xGXGx@OpenReview', this)" data="https://openreview.net/pdf?id=WKfb1xGXGx">[PDF<sup id="pdf-stars-WKfb1xGXGx@OpenReview">1</sup>]</a>
                <a id="copy-WKfb1xGXGx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WKfb1xGXGx@OpenReview')">[Copy]</a>
                <a id="kimi-WKfb1xGXGx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WKfb1xGXGx@OpenReview', this)">[Kimi<sup id="kimi-stars-WKfb1xGXGx@OpenReview">1</sup>]</a>
                <a id="rel-WKfb1xGXGx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WKfb1xGXGx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WKfb1xGXGx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chengan He" target="_blank">Chengan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Sun" target="_blank">Xin Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhixin Shu" target="_blank">Zhixin Shu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fujun Luan" target="_blank">Fujun Luan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soeren Pirk" target="_blank">Soeren Pirk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jorge Alejandro Amador Herrera" target="_blank">Jorge Alejandro Amador Herrera</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dominik L Michels" target="_blank">Dominik L Michels</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuanfeng Wang" target="_blank">Tuanfeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Zhang" target="_blank">Meng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Holly Rushmeier" target="_blank">Holly Rushmeier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhou" target="_blank">Yi Zhou</a>
            </p>
            <p id="summary-WKfb1xGXGx@OpenReview" class="summary">We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair grooming process. We conduct extensive experiments to validate the architecture design of \textsc{Perm}, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as single-view hair reconstruction, hairstyle editing, and hair-conditioned image generation.</p>
            <p id="subjects-WKfb1xGXGx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-WKfb1xGXGx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WKfb1xGXGx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WKfb1xGXGx@OpenReview" onclick="foldPdfKimi('WKfb1xGXGx@OpenReview', this)" class="hr hr-fold">
        </div><div id="2iCIHgE8KG@OpenReview" class="panel paper" keywords="gpfa,infinite,latent,factors,compositional,process,neural,manifolds,activities,behaviourally">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2iCIHgE8KG" target="_blank" title="326/373"><span class="index notranslate">#326</span></a>
                <a id="title-2iCIHgE8KG@OpenReview" class="title-link" href="/venue/2iCIHgE8KG@OpenReview" target="_blank">Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA</a>
                <a id="pdf-2iCIHgE8KG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2iCIHgE8KG@OpenReview', this)" data="https://openreview.net/pdf?id=2iCIHgE8KG">[PDF<sup id="pdf-stars-2iCIHgE8KG@OpenReview">2</sup>]</a>
                <a id="copy-2iCIHgE8KG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2iCIHgE8KG@OpenReview')">[Copy]</a>
                <a id="kimi-2iCIHgE8KG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2iCIHgE8KG@OpenReview', this)">[Kimi<sup id="kimi-stars-2iCIHgE8KG@OpenReview">1</sup>]</a>
                <a id="rel-2iCIHgE8KG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2iCIHgE8KG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2iCIHgE8KG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Changmin Yu" target="_blank">Changmin Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maneesh Sahani" target="_blank">Maneesh Sahani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Máté Lengyel" target="_blank">Máté Lengyel</a>
            </p>
            <p id="summary-2iCIHgE8KG@OpenReview" class="summary">Gaussian Process Factor Analysis (GPFA) is a powerful latent variable model for extracting low-dimensional manifolds underlying population neural activities. However, one limitation of standard GPFA models is that the number of latent factors needs to be pre-specified or selected through heuristic-based processes, and that all factors contribute at all times. We propose the infinite GPFA model, a fully Bayesian non-parametric extension of the classical GPFA by incorporating an Indian Buffet Process (IBP) prior over the factor loading process, such that it is possible to infer a potentially infinite set of latent factors, and the identity of those factors that contribute to neural firings in a compositional manner at each time point. Learning and inference in the infinite GPFA model is performed through variational expectation-maximisation, and we additionally propose scalable extensions based on sparse variational Gaussian Process methods. We empirically demonstrate that the infinite GPFA model correctly infers dynamically changing activations of latent factors on a synthetic dataset. By fitting the infinite GPFA model to population activities of hippocampal place cells during spatial navigation, we identify non-trivial and behaviourally meaningful dynamics in the neural encoding process.</p>
            <p id="subjects-2iCIHgE8KG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2iCIHgE8KG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2iCIHgE8KG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2iCIHgE8KG@OpenReview" onclick="foldPdfKimi('2iCIHgE8KG@OpenReview', this)" class="hr hr-fold">
        </div><div id="fGdF8Bq1FV@OpenReview" class="panel paper" keywords="dependent,vib,regularizer,mixture,expectation,bounds,prior,generalization,representation,gaussian">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fGdF8Bq1FV" target="_blank" title="327/373"><span class="index notranslate">#327</span></a>
                <a id="title-fGdF8Bq1FV@OpenReview" class="title-link" href="/venue/fGdF8Bq1FV@OpenReview" target="_blank">Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors</a>
                <a id="pdf-fGdF8Bq1FV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fGdF8Bq1FV@OpenReview', this)" data="https://openreview.net/pdf?id=fGdF8Bq1FV">[PDF<sup id="pdf-stars-fGdF8Bq1FV@OpenReview">2</sup>]</a>
                <a id="copy-fGdF8Bq1FV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fGdF8Bq1FV@OpenReview')">[Copy]</a>
                <a id="kimi-fGdF8Bq1FV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fGdF8Bq1FV@OpenReview', this)">[Kimi<sup id="kimi-stars-fGdF8Bq1FV@OpenReview">2</sup>]</a>
                <a id="rel-fGdF8Bq1FV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fGdF8Bq1FV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fGdF8Bq1FV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Milad Sefidgaran" target="_blank">Milad Sefidgaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abdellatif Zaidi" target="_blank">Abdellatif Zaidi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Piotr Krasnowski" target="_blank">Piotr Krasnowski</a>
            </p>
            <p id="summary-fGdF8Bq1FV@OpenReview" class="summary">We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and "test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the "structure'' and "simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a date-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB).</p>
            <p id="subjects-fGdF8Bq1FV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-fGdF8Bq1FV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fGdF8Bq1FV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fGdF8Bq1FV@OpenReview" onclick="foldPdfKimi('fGdF8Bq1FV@OpenReview', this)" class="hr hr-fold">
        </div><div id="avSocG0oFA@OpenReview" class="panel paper" keywords="pruning,dpp,delta,dare,darq,fine,tuned,rescaling,adamr,parameters">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=avSocG0oFA" target="_blank" title="328/373"><span class="index notranslate">#328</span></a>
                <a id="title-avSocG0oFA@OpenReview" class="title-link" href="/venue/avSocG0oFA@OpenReview" target="_blank">Revisiting Delta-Parameter Pruning For Fine-Tuned Models</a>
                <a id="pdf-avSocG0oFA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('avSocG0oFA@OpenReview', this)" data="https://openreview.net/pdf?id=avSocG0oFA">[PDF<sup id="pdf-stars-avSocG0oFA@OpenReview">3</sup>]</a>
                <a id="copy-avSocG0oFA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('avSocG0oFA@OpenReview')">[Copy]</a>
                <a id="kimi-avSocG0oFA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('avSocG0oFA@OpenReview', this)">[Kimi<sup id="kimi-stars-avSocG0oFA@OpenReview">1</sup>]</a>
                <a id="rel-avSocG0oFA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('avSocG0oFA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-avSocG0oFA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenlong Deng" target="_blank">Wenlong Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yize Zhao" target="_blank">Yize Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vala Vakilian" target="_blank">Vala Vakilian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minghui Chen" target="_blank">Minghui Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoxiao Li" target="_blank">Xiaoxiao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christos Thrampoulidis" target="_blank">Christos Thrampoulidis</a>
            </p>
            <p id="summary-avSocG0oFA@OpenReview" class="summary">Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters—the differences between fine-tuned and pre-trained model weights—while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters.To address these, we develop two algorithmic improvements: (1) DARq, which modifies the rescaling factor in DARE, leading to significant performance gains at high pruning rates (e.g., &gt;30% on COLA and SST2 for encoder models, with even larger improvements in decoder models), and (2) AdamR, an in-training modification that incorporates appropriate Delta regularization before applying DPP. We also demonstrate that DARq can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios.</p>
            <p id="subjects-avSocG0oFA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-avSocG0oFA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-avSocG0oFA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-avSocG0oFA@OpenReview" onclick="foldPdfKimi('avSocG0oFA@OpenReview', this)" class="hr hr-fold">
        </div><div id="pQsllTesiE@OpenReview" class="panel paper" keywords="stochastic,action,map,decision,environments,latent,actions,macro,learned,continuous">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pQsllTesiE" target="_blank" title="329/373"><span class="index notranslate">#329</span></a>
                <a id="title-pQsllTesiE@OpenReview" class="title-link" href="/venue/pQsllTesiE@OpenReview" target="_blank">Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</a>
                <a id="pdf-pQsllTesiE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pQsllTesiE@OpenReview', this)" data="https://openreview.net/pdf?id=pQsllTesiE">[PDF<sup id="pdf-stars-pQsllTesiE@OpenReview">1</sup>]</a>
                <a id="copy-pQsllTesiE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pQsllTesiE@OpenReview')">[Copy]</a>
                <a id="kimi-pQsllTesiE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pQsllTesiE@OpenReview', this)">[Kimi<sup id="kimi-stars-pQsllTesiE@OpenReview">3</sup>]</a>
                <a id="rel-pQsllTesiE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pQsllTesiE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pQsllTesiE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Baiting Luo" target="_blank">Baiting Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ava Pettet" target="_blank">Ava Pettet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aron Laszka" target="_blank">Aron Laszka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhishek Dubey" target="_blank">Abhishek Dubey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayan Mukhopadhyay" target="_blank">Ayan Mukhopadhyay</a>
            </p>
            <p id="summary-pQsllTesiE@OpenReview" class="summary">Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected through a stochastic behavior policy. We present \textit{Latent Macro Action Planner} (L-MAP), which addresses this challenge by learning a set of temporally extended macro-actions through a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs a (separate) learned prior model that acts as a latent transition model and allows efficient sampling of plausible actions. During planning, our approach accounts for stochasticity in both the environment and the behavior policy by using Monte Carlo tree search (MCTS). In offline RL settings, including stochastic continuous control tasks, L-MAP efficiently searches over discrete latent actions to yield high expected returns.Empirical results demonstrate that L-MAP maintains low decision latency despite increased action dimensionality. Notably, across tasks ranging from continuous control with inherently stochastic dynamics to high-dimensional robotic hand manipulation, L-MAP significantly outperforms existing model-based methods and performs on par with strong model-free actor-critic baselines, highlighting the effectiveness of the proposed approach in planning in complex and stochastic environments with high-dimensional action spaces.</p>
            <p id="subjects-pQsllTesiE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pQsllTesiE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pQsllTesiE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pQsllTesiE@OpenReview" onclick="foldPdfKimi('pQsllTesiE@OpenReview', this)" class="hr hr-fold">
        </div><div id="SuH5SdOXpe@OpenReview" class="panel paper" keywords="reprogramming,robustness,nrpm,representation,learning,robust,reprogrammed,adversarial,4open,322c">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SuH5SdOXpe" target="_blank" title="330/373"><span class="index notranslate">#330</span></a>
                <a id="title-SuH5SdOXpe@OpenReview" class="title-link" href="/venue/SuH5SdOXpe@OpenReview" target="_blank">Robustness Reprogramming for Representation Learning</a>
                <a id="pdf-SuH5SdOXpe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SuH5SdOXpe@OpenReview', this)" data="https://openreview.net/pdf?id=SuH5SdOXpe">[PDF<sup id="pdf-stars-SuH5SdOXpe@OpenReview">2</sup>]</a>
                <a id="copy-SuH5SdOXpe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SuH5SdOXpe@OpenReview')">[Copy]</a>
                <a id="kimi-SuH5SdOXpe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SuH5SdOXpe@OpenReview', this)">[Kimi<sup id="kimi-stars-SuH5SdOXpe@OpenReview">1</sup>]</a>
                <a id="rel-SuH5SdOXpe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SuH5SdOXpe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SuH5SdOXpe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhichao Hou" target="_blank">Zhichao Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=MohamadAli Torkamani" target="_blank">MohamadAli Torkamani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hamid Krim" target="_blank">Hamid Krim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaorui Liu" target="_blank">Xiaorui Liu</a>
            </p>
            <p id="summary-SuH5SdOXpe@OpenReview" class="summary">This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters?To explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness of our approaches.This work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics. Our implementation is available at https://anonymous.4open.science/r/NRPM-322C/</p>
            <p id="subjects-SuH5SdOXpe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SuH5SdOXpe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SuH5SdOXpe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SuH5SdOXpe@OpenReview" onclick="foldPdfKimi('SuH5SdOXpe@OpenReview', this)" class="hr hr-fold">
        </div><div id="OZVTqoli2N@OpenReview" class="panel paper" keywords="compositionality,incremental,modules,perspective,composed,demystify,model,staying,composable,pre">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OZVTqoli2N" target="_blank" title="331/373"><span class="index notranslate">#331</span></a>
                <a id="title-OZVTqoli2N@OpenReview" class="title-link" href="/venue/OZVTqoli2N@OpenReview" target="_blank">A Second-Order Perspective on Model Compositionality and Incremental Learning</a>
                <a id="pdf-OZVTqoli2N@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OZVTqoli2N@OpenReview', this)" data="https://openreview.net/pdf?id=OZVTqoli2N">[PDF<sup id="pdf-stars-OZVTqoli2N@OpenReview">1</sup>]</a>
                <a id="copy-OZVTqoli2N@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OZVTqoli2N@OpenReview')">[Copy]</a>
                <a id="kimi-OZVTqoli2N@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OZVTqoli2N@OpenReview', this)">[Kimi<sup id="kimi-stars-OZVTqoli2N@OpenReview">2</sup>]</a>
                <a id="rel-OZVTqoli2N@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OZVTqoli2N@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OZVTqoli2N@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Angelo Porrello" target="_blank">Angelo Porrello</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Bonicelli" target="_blank">Lorenzo Bonicelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pietro Buzzega" target="_blank">Pietro Buzzega</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Monica Millunzi" target="_blank">Monica Millunzi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simone Calderara" target="_blank">Simone Calderara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rita Cucchiara" target="_blank">Rita Cucchiara</a>
            </p>
            <p id="summary-OZVTqoli2N@OpenReview" class="summary">The fine-tuning of deep pre-trained models has revealed compositional properties, with multiple specialized modules that can be arbitrarily composed into a single, multi-task model. However, identifying the conditions that promote compositionality remains an open issue, with recent efforts concentrating mainly on linearized networks. We conduct a theoretical study that attempts to demystify compositionality in standard non-linear networks through the second-order Taylor approximation of the loss function. The proposed formulation highlights the importance of staying within the pre-training basin to achieve composable modules. Moreover, it provides the basis for two dual incremental training algorithms: the one from the perspective of multiple models trained individually, while the other aims to optimize the composed model as a whole. We probe their application in incremental classification tasks and highlight some valuable skills. In fact, the pool of incrementally learned modules not only supports the creation of an effective multi-task model but also enables unlearning and specialization in certain tasks.</p>
            <p id="subjects-OZVTqoli2N@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-OZVTqoli2N@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OZVTqoli2N@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OZVTqoli2N@OpenReview" onclick="foldPdfKimi('OZVTqoli2N@OpenReview', this)" class="hr hr-fold">
        </div><div id="kFsWpSxkFz@OpenReview" class="panel paper" keywords="metaurban,micromobility,urban,embodied,mobile,compositional,pedestrians,public,generalizability,streetscapes">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kFsWpSxkFz" target="_blank" title="332/373"><span class="index notranslate">#332</span></a>
                <a id="title-kFsWpSxkFz@OpenReview" class="title-link" href="/venue/kFsWpSxkFz@OpenReview" target="_blank">MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility</a>
                <a id="pdf-kFsWpSxkFz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kFsWpSxkFz@OpenReview', this)" data="https://openreview.net/pdf?id=kFsWpSxkFz">[PDF<sup id="pdf-stars-kFsWpSxkFz@OpenReview">2</sup>]</a>
                <a id="copy-kFsWpSxkFz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kFsWpSxkFz@OpenReview')">[Copy]</a>
                <a id="kimi-kFsWpSxkFz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kFsWpSxkFz@OpenReview', this)">[Kimi<sup id="kimi-stars-kFsWpSxkFz@OpenReview">5</sup>]</a>
                <a id="rel-kFsWpSxkFz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kFsWpSxkFz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kFsWpSxkFz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wayne Wu" target="_blank">Wayne Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Honglin He" target="_blank">Honglin He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jack He" target="_blank">Jack He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiran Wang" target="_blank">Yiran Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenda Duan" target="_blank">Chenda Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhizheng Liu" target="_blank">Zhizheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanyi Li" target="_blank">Quanyi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bolei Zhou" target="_blank">Bolei Zhou</a>
            </p>
            <p id="summary-kFsWpSxkFz@OpenReview" class="summary">Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. **Micromobility** enabled by AI for short-distance travel in public urban spaces plays a crucial component in the future transportation system. Ensuring the generalizability and safety of AI models maneuvering mobile machines is essential. In this work, we present **MetaUrban**, a *compositional* simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an *infinite* number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents’ appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and dataset will be publicly available.</p>
            <p id="subjects-kFsWpSxkFz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-kFsWpSxkFz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kFsWpSxkFz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kFsWpSxkFz@OpenReview" onclick="foldPdfKimi('kFsWpSxkFz@OpenReview', this)" class="hr hr-fold">
        </div><div id="A1ztozypga@OpenReview" class="panel paper" keywords="hymba,lms,heads,language,tokens,hybrid,head,architecture,memories,cache">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=A1ztozypga" target="_blank" title="333/373"><span class="index notranslate">#333</span></a>
                <a id="title-A1ztozypga@OpenReview" class="title-link" href="/venue/A1ztozypga@OpenReview" target="_blank">Hymba: A Hybrid-head Architecture for Small Language Models</a>
                <a id="pdf-A1ztozypga@OpenReview" class="title-pdf notranslate" onclick="togglePdf('A1ztozypga@OpenReview', this)" data="https://openreview.net/pdf?id=A1ztozypga">[PDF<sup id="pdf-stars-A1ztozypga@OpenReview">6</sup>]</a>
                <a id="copy-A1ztozypga@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('A1ztozypga@OpenReview')">[Copy]</a>
                <a id="kimi-A1ztozypga@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('A1ztozypga@OpenReview', this)">[Kimi<sup id="kimi-stars-A1ztozypga@OpenReview">5</sup>]</a>
                <a id="rel-A1ztozypga@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('A1ztozypga@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-A1ztozypga@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Dong" target="_blank">Xin Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonggan Fu" target="_blank">Yonggan Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shizhe Diao" target="_blank">Shizhe Diao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wonmin Byeon" target="_blank">Wonmin Byeon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=ZIJIA CHEN" target="_blank">ZIJIA CHEN</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ameya Mahabaleshwarkar" target="_blank">Ameya Mahabaleshwarkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shih-Yang Liu" target="_blank">Shih-Yang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthijs Van keirsbilck" target="_blank">Matthijs Van keirsbilck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min-Hung Chen" target="_blank">Min-Hung Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yoshi Suhara" target="_blank">Yoshi Suhara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingyan Celine Lin" target="_blank">Yingyan Celine Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Kautz" target="_blank">Jan Kautz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pavlo Molchanov" target="_blank">Pavlo Molchanov</a>
            </p>
            <p id="summary-A1ztozypga@OpenReview" class="summary">The transformative capabilities of language models (LMs) have intensified the demand for their deployment on everyday devices, necessitating efficient processing for on-device language tasks. To address this, we propose Hymba, a new family of small language models featuring a hybrid-head architecture that strategically integrates attention mechanisms with state space models (SSMs). This architecture leverages the strengths of both systems: attention heads provide high-resolution recall, akin to snapshot memories in the human brain, while SSM heads offer efficient context summarization, similar to fading memories. To further enhance Hymba's performance, we introduce learnable meta tokens that are prepended to input sequences and jointly trained with model weights during pretraining. These meta tokens act as a learned cache initialization during inference, modulating all subsequent tokens within the hybrid heads and boosting the model’s focus on salient information, similar to metamemory. Extensive experiments and ablation studies demonstrate that Hymba sets new state-of-the-art results for small LMs across various benchmarks and advances the accuracy-efficiency trade-offs of small LMs. For instance, Hymba-1.5B achieves comparable commonsense reasoning accuracy to LLaMA 3.2 3B while being 3.49x faster and offering a 14.72x reduction in cache size. All codes and models will be released upon acceptance.</p>
            <p id="subjects-A1ztozypga@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-A1ztozypga@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-A1ztozypga@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-A1ztozypga@OpenReview" onclick="foldPdfKimi('A1ztozypga@OpenReview', this)" class="hr hr-fold">
        </div><div id="6tyPSkshtF@OpenReview" class="panel paper" keywords="advantage,earlysettled,gap,ucb,regret,dependent,bonuses,mdps,bounds,hoeffding">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6tyPSkshtF" target="_blank" title="334/373"><span class="index notranslate">#334</span></a>
                <a id="title-6tyPSkshtF@OpenReview" class="title-link" href="/venue/6tyPSkshtF@OpenReview" target="_blank">Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition</a>
                <a id="pdf-6tyPSkshtF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6tyPSkshtF@OpenReview', this)" data="https://openreview.net/pdf?id=6tyPSkshtF">[PDF<sup id="pdf-stars-6tyPSkshtF@OpenReview">1</sup>]</a>
                <a id="copy-6tyPSkshtF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6tyPSkshtF@OpenReview')">[Copy]</a>
                <a id="kimi-6tyPSkshtF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6tyPSkshtF@OpenReview', this)">[Kimi<sup id="kimi-stars-6tyPSkshtF@OpenReview">2</sup>]</a>
                <a id="rel-6tyPSkshtF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6tyPSkshtF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6tyPSkshtF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhong Zheng" target="_blank">Zhong Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haochen Zhang" target="_blank">Haochen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingzhou Xue" target="_blank">Lingzhou Xue</a>
            </p>
            <p id="summary-6tyPSkshtF@OpenReview" class="summary">We study the gap-dependent bounds of two important algorithms for on-policy <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-248-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1557" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1558"><span class="mi" id="MathJax-Span-1559" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-248">Q</script>-learning for finite-horizon episodic tabular Markov Decision Processes (MDPs): UCB-Advantage (Zhang et al. 2020) and Q-EarlySettled-Advantage (Li et al. 2021). UCB-Advantage and Q-EarlySettled-Advantage improve upon the results based on Hoeffding-type bonuses and achieve the {almost optimal} <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-249-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1560" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.51em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1561"><span class="msqrt" id="MathJax-Span-1562"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-1563"><span class="mi" id="MathJax-Span-1564" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.68em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.003em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>T</mi></msqrt></math></span></span><script type="math/tex" id="MathJax-Element-249">\sqrt{T}</script>-type regret bound in the worst-case scenario, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-250-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1565" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1566"><span class="mi" id="MathJax-Span-1567" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-250">T</script> is the total number of steps. However, the benign structures of the MDPs such as a strictly positive suboptimality gap can significantly improve the regret. While gap-dependent regret bounds have been obtained for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-251-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1568" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1569"><span class="mi" id="MathJax-Span-1570" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-251">Q</script>-learning with Hoeffding-type bonuses, it remains an open question to establish gap-dependent regret bounds for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-252-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1571" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1572"><span class="mi" id="MathJax-Span-1573" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-252">Q</script>-learning using variance estimators in their bonuses and reference-advantage decomposition for variance reduction. We develop a novel error decompositionframework to prove gap-dependent regret bounds of UCB-Advantage and Q-EarlySettled-Advantage that are logarithmic in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-253-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1574" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1575"><span class="mi" id="MathJax-Span-1576" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-253">T</script> and improve upon existing ones for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-254-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1577" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1578"><span class="mi" id="MathJax-Span-1579" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-254">Q</script>-learning algorithms. Moreover, we establish the gap-dependent bound for the policy switching cost of UCB-Advantage and improve that under the worst-case MDPs. To our knowledge, this paper presents the first gap-dependent regret analysis for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-255-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1580" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1581"><span class="mi" id="MathJax-Span-1582" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-255">Q</script>-learning using variance estimators and reference-advantage decomposition and also provides the first gap-dependent analysis on policy switching cost for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-256-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1583" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1584"><span class="mi" id="MathJax-Span-1585" style="font-family: MathJax_Math-italic;">Q</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Q</mi></math></span></span><script type="math/tex" id="MathJax-Element-256">Q</script>-learning.</p>
            <p id="subjects-6tyPSkshtF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-6tyPSkshtF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6tyPSkshtF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6tyPSkshtF@OpenReview" onclick="foldPdfKimi('6tyPSkshtF@OpenReview', this)" class="hr hr-fold">
        </div><div id="h0ZfDIrj7T@OpenReview" class="panel paper" keywords="moa,llms,agents,alpacaeval,omni,language,mixture,capabilities,gpt,flask">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=h0ZfDIrj7T" target="_blank" title="335/373"><span class="index notranslate">#335</span></a>
                <a id="title-h0ZfDIrj7T@OpenReview" class="title-link" href="/venue/h0ZfDIrj7T@OpenReview" target="_blank">Mixture-of-Agents Enhances Large Language Model Capabilities</a>
                <a id="pdf-h0ZfDIrj7T@OpenReview" class="title-pdf notranslate" onclick="togglePdf('h0ZfDIrj7T@OpenReview', this)" data="https://openreview.net/pdf?id=h0ZfDIrj7T">[PDF<sup id="pdf-stars-h0ZfDIrj7T@OpenReview">6</sup>]</a>
                <a id="copy-h0ZfDIrj7T@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('h0ZfDIrj7T@OpenReview')">[Copy]</a>
                <a id="kimi-h0ZfDIrj7T@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('h0ZfDIrj7T@OpenReview', this)">[Kimi<sup id="kimi-stars-h0ZfDIrj7T@OpenReview">8</sup>]</a>
                <a id="rel-h0ZfDIrj7T@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('h0ZfDIrj7T@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-h0ZfDIrj7T@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junlin Wang" target="_blank">Junlin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jue Wang" target="_blank">Jue Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Athiwaratkun" target="_blank">Ben Athiwaratkun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ce Zhang" target="_blank">Ce Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Y Zou" target="_blank">James Y Zou</a>
            </p>
            <p id="summary-h0ZfDIrj7T@OpenReview" class="summary">Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, Arena-Hard, MT-Bench, and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs achieves a score of 65.1% on AlpacaEval 2.0 compared to 57.5% by GPT-4 Omni.</p>
            <p id="subjects-h0ZfDIrj7T@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-h0ZfDIrj7T@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-h0ZfDIrj7T@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-h0ZfDIrj7T@OpenReview" onclick="foldPdfKimi('h0ZfDIrj7T@OpenReview', this)" class="hr hr-fold">
        </div><div id="bhK7U37VW8@OpenReview" class="panel paper" keywords="autodan,turbo,jailbreak,strategies,teaming,success,1106,red,attack,persuasions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bhK7U37VW8" target="_blank" title="336/373"><span class="index notranslate">#336</span></a>
                <a id="title-bhK7U37VW8@OpenReview" class="title-link" href="/venue/bhK7U37VW8@OpenReview" target="_blank">AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs</a>
                <a id="pdf-bhK7U37VW8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bhK7U37VW8@OpenReview', this)" data="https://openreview.net/pdf?id=bhK7U37VW8">[PDF<sup id="pdf-stars-bhK7U37VW8@OpenReview">5</sup>]</a>
                <a id="copy-bhK7U37VW8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bhK7U37VW8@OpenReview')">[Copy]</a>
                <a id="kimi-bhK7U37VW8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bhK7U37VW8@OpenReview', this)">[Kimi<sup id="kimi-stars-bhK7U37VW8@OpenReview">5</sup>]</a>
                <a id="rel-bhK7U37VW8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bhK7U37VW8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bhK7U37VW8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaogeng Liu" target="_blank">Xiaogeng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiran Li" target="_blank">Peiran Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=G. Edward Suh" target="_blank">G. Edward Suh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yevgeniy Vorobeychik" target="_blank">Yevgeniy Vorobeychik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoqing Mao" target="_blank">Zhuoqing Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Somesh Jha" target="_blank">Somesh Jha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patrick McDaniel" target="_blank">Patrick McDaniel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Sun" target="_blank">Huan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Li" target="_blank">Bo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaowei Xiao" target="_blank">Chaowei Xiao</a>
            </p>
            <p id="summary-bhK7U37VW8@OpenReview" class="summary">Jailbreak attacks serve as essential red-teaming tools, proactively assessing whether LLMs can behave responsibly and safely in adversarial environments. Despite diverse strategies (e.g., cipher, low-resource language, persuasions, and so on) that have been proposed and shown success, these strategies are still manually designed, limiting their scope and effectiveness as a red-teaming tool. In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo.</p>
            <p id="subjects-bhK7U37VW8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-bhK7U37VW8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bhK7U37VW8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bhK7U37VW8@OpenReview" onclick="foldPdfKimi('bhK7U37VW8@OpenReview', this)" class="hr hr-fold">
        </div><div id="6RtRsg8ZV1@OpenReview" class="panel paper" keywords="mad,utd,update,policy,training,data,augmented,stabilizes,instability,resets">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6RtRsg8ZV1" target="_blank" title="337/373"><span class="index notranslate">#337</span></a>
                <a id="title-6RtRsg8ZV1@OpenReview" class="title-link" href="/venue/6RtRsg8ZV1@OpenReview" target="_blank">MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL</a>
                <a id="pdf-6RtRsg8ZV1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6RtRsg8ZV1@OpenReview', this)" data="https://openreview.net/pdf?id=6RtRsg8ZV1">[PDF<sup id="pdf-stars-6RtRsg8ZV1@OpenReview">4</sup>]</a>
                <a id="copy-6RtRsg8ZV1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6RtRsg8ZV1@OpenReview')">[Copy]</a>
                <a id="kimi-6RtRsg8ZV1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6RtRsg8ZV1@OpenReview', this)">[Kimi<sup id="kimi-stars-6RtRsg8ZV1@OpenReview">2</sup>]</a>
                <a id="rel-6RtRsg8ZV1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6RtRsg8ZV1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6RtRsg8ZV1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Claas Voelcker" target="_blank">Claas Voelcker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marcel Hussing" target="_blank">Marcel Hussing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=ERIC EATON" target="_blank">ERIC EATON</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amir-massoud Farahmand" target="_blank">Amir-massoud Farahmand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Igor Gilitschenski" target="_blank">Igor Gilitschenski</a>
            </p>
            <p id="summary-6RtRsg8ZV1@OpenReview" class="summary">Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning.</p>
            <p id="subjects-6RtRsg8ZV1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-6RtRsg8ZV1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6RtRsg8ZV1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6RtRsg8ZV1@OpenReview" onclick="foldPdfKimi('6RtRsg8ZV1@OpenReview', this)" class="hr hr-fold">
        </div><div id="tj5xJInWty@OpenReview" class="panel paper" keywords="graph,heterogeneous,privacy,thepuff,temporal,utility,terogeneous,tility,iciency,rivacy">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tj5xJInWty" target="_blank" title="338/373"><span class="index notranslate">#338</span></a>
                <a id="title-tj5xJInWty@OpenReview" class="title-link" href="/venue/tj5xJInWty@OpenReview" target="_blank">Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency</a>
                <a id="pdf-tj5xJInWty@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tj5xJInWty@OpenReview', this)" data="https://openreview.net/pdf?id=tj5xJInWty">[PDF<sup id="pdf-stars-tj5xJInWty@OpenReview">4</sup>]</a>
                <a id="copy-tj5xJInWty@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tj5xJInWty@OpenReview')">[Copy]</a>
                <a id="kimi-tj5xJInWty@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tj5xJInWty@OpenReview', this)">[Kimi<sup id="kimi-stars-tj5xJInWty@OpenReview">1</sup>]</a>
                <a id="rel-tj5xJInWty@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tj5xJInWty@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tj5xJInWty@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu He" target="_blank">Xinyu He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongqi Fu" target="_blank">Dongqi Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanghang Tong" target="_blank">Hanghang Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ross Maciejewski" target="_blank">Ross Maciejewski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingrui He" target="_blank">Jingrui He</a>
            </p>
            <p id="summary-tj5xJInWty@OpenReview" class="summary">Nowadays, Temporal Heterogeneous Graphs attract much research and industrial attention for building the next-generation Relational Deep Learning Models and Applications, due to their informative structures and features. While providing timely and precise services like personalized recommendations and question answering, this rich information also introduces extra exposure risk for each node in the graph, because the distinctive local topology, the abundant heterogeneous features, and the time dimension of the graph data are more prone to exposing sensitive information and narrow down the scope of victim candidates, which calls for well-defined protection techniques on graphs. To this end, we propose a **T**emporal **He**terogeneous Graph Generator balancing **P**rivacy, **U**tility, and E**ff**iciency, named **THePUff**. More specifically, we first propose a differential privacy algorithm to perturb the input temporal heterogeneous graph for protecting privacy, and then utilize both the perturbed graph and the original one in a generative adversarial setting for THePUff to learn and generate privacy-guaranteed and utility-preserved graph data in an efficient manner. We further propose 6 new metrics in the temporal setting to measure heterogeneous graph utility and privacy. Finally, based on temporal heterogeneous graph datasets with up to 1 million nodes and 20 million edges, the experiments show that THePUff generates utilizable temporal heterogeneous graphs with privacy protected, compared with state-of-the-art baselines.</p>
            <p id="subjects-tj5xJInWty@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tj5xJInWty@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tj5xJInWty@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tj5xJInWty@OpenReview" onclick="foldPdfKimi('tj5xJInWty@OpenReview', this)" class="hr hr-fold">
        </div><div id="dsHpulHpOK@OpenReview" class="panel paper" keywords="cell,control,populations,dosing,markovian,environments,reinforcement,memory,stressors,dynamics">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dsHpulHpOK" target="_blank" title="339/373"><span class="index notranslate">#339</span></a>
                <a id="title-dsHpulHpOK@OpenReview" class="title-link" href="/venue/dsHpulHpOK@OpenReview" target="_blank">Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics</a>
                <a id="pdf-dsHpulHpOK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dsHpulHpOK@OpenReview', this)" data="https://openreview.net/pdf?id=dsHpulHpOK">[PDF<sup id="pdf-stars-dsHpulHpOK@OpenReview">1</sup>]</a>
                <a id="copy-dsHpulHpOK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dsHpulHpOK@OpenReview')">[Copy]</a>
                <a id="kimi-dsHpulHpOK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dsHpulHpOK@OpenReview', this)">[Kimi<sup id="kimi-stars-dsHpulHpOK@OpenReview">2</sup>]</a>
                <a id="rel-dsHpulHpOK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dsHpulHpOK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dsHpulHpOK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Josiah Kratz" target="_blank">Josiah Kratz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Adamczyk" target="_blank">Jacob Adamczyk</a>
            </p>
            <p id="summary-dsHpulHpOK@OpenReview" class="summary">Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses significant challenges in driving cell populations toward extinction, and is thus an open question with great clinical significance. In this work, we focus on drug dosing in cell populations exhibiting phenotypic plasticity. For specific dynamical models switching between resistant and susceptible states, exact solutions are known. However, when the underlying system parameters are unknown, and for complex memory-based systems, obtaining the optimal solution is currently intractable. To address this challenge, we apply reinforcement learning (RL) to identify informed dosing strategies to control cell populations evolving under novel non-Markovian dynamics. We find that model-free deep RL is able to recover exact solutions and control cell populations even in the presence of long-range temporal dynamics. To further test our approach in more realistic settings, we demonstrate performant RL-based control strategies in environments with dynamic memory strength.</p>
            <p id="subjects-dsHpulHpOK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dsHpulHpOK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dsHpulHpOK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dsHpulHpOK@OpenReview" onclick="foldPdfKimi('dsHpulHpOK@OpenReview', this)" class="hr hr-fold">
        </div><div id="oI5tZaWkF9@OpenReview" class="panel paper" keywords="data,weighting,world,synthetic,generated,rethinking,real,approaches,text,classification">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oI5tZaWkF9" target="_blank" title="340/373"><span class="index notranslate">#340</span></a>
                <a id="title-oI5tZaWkF9@OpenReview" class="title-link" href="/venue/oI5tZaWkF9@OpenReview" target="_blank">Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification</a>
                <a id="pdf-oI5tZaWkF9@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oI5tZaWkF9@OpenReview', this)" data="https://openreview.net/pdf?id=oI5tZaWkF9">[PDF<sup id="pdf-stars-oI5tZaWkF9@OpenReview">5</sup>]</a>
                <a id="copy-oI5tZaWkF9@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oI5tZaWkF9@OpenReview')">[Copy]</a>
                <a id="kimi-oI5tZaWkF9@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oI5tZaWkF9@OpenReview', this)">[Kimi<sup id="kimi-stars-oI5tZaWkF9@OpenReview">6</sup>]</a>
                <a id="rel-oI5tZaWkF9@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oI5tZaWkF9@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oI5tZaWkF9@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hsun-Yu Kuo" target="_blank">Hsun-Yu Kuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin-Hsiang Liao" target="_blank">Yin-Hsiang Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Chieh Chao" target="_blank">Yu-Chieh Chao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Yun Ma" target="_blank">Wei-Yun Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pu-Jen Cheng" target="_blank">Pu-Jen Cheng</a>
            </p>
            <p id="summary-oI5tZaWkF9@OpenReview" class="summary">Synthetic data augmentation via large language models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring deficient outcomes while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs with using merely a little real-world data. We empirically assessed the effectiveness of our method on multiple text classification tasks, and the results showed leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator for model training.</p>
            <p id="subjects-oI5tZaWkF9@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-oI5tZaWkF9@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oI5tZaWkF9@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oI5tZaWkF9@OpenReview" onclick="foldPdfKimi('oI5tZaWkF9@OpenReview', this)" class="hr hr-fold">
        </div><div id="oSQiao9GqB@OpenReview" class="panel paper" keywords="lmms,interleave,llava,capabilities,multiimage,multi,image,video,tasks,scenarios">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oSQiao9GqB" target="_blank" title="341/373"><span class="index notranslate">#341</span></a>
                <a id="title-oSQiao9GqB@OpenReview" class="title-link" href="/venue/oSQiao9GqB@OpenReview" target="_blank">LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</a>
                <a id="pdf-oSQiao9GqB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oSQiao9GqB@OpenReview', this)" data="https://openreview.net/pdf?id=oSQiao9GqB">[PDF<sup id="pdf-stars-oSQiao9GqB@OpenReview">5</sup>]</a>
                <a id="copy-oSQiao9GqB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oSQiao9GqB@OpenReview')">[Copy]</a>
                <a id="kimi-oSQiao9GqB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oSQiao9GqB@OpenReview', this)">[Kimi<sup id="kimi-stars-oSQiao9GqB@OpenReview">4</sup>]</a>
                <a id="rel-oSQiao9GqB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oSQiao9GqB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oSQiao9GqB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Li" target="_blank">Feng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renrui Zhang" target="_blank">Renrui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhang" target="_blank">Hao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanhan Zhang" target="_blank">Yuanhan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Li" target="_blank">Bo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Li" target="_blank">Wei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zejun MA" target="_blank">Zejun MA</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunyuan Li" target="_blank">Chunyuan Li</a>
            </p>
            <p id="summary-oSQiao9GqB@OpenReview" class="summary">Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multiimage scenarios remains less explored. Additionally,prior LMM research separately tackles different scenarios, leaving it impossible to generalize cross scenarios with new emerging capabilities. To this end, we introduce LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To enable these capabilities, we regard the interleaved data format as a general template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4 primary domains with 14 tasks and 41 datasets. We also curate the LLaVAInterleave Bench to comprehensively evaluate the multiimage performance of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading results in multi-image, video, and 3D benchmarks, while maintaining the performance of single-image tasks. Besides, our model also exhibits several emerging capabilities, e.g., transferring tasks across different settings and modalities. Code will be available.</p>
            <p id="subjects-oSQiao9GqB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-oSQiao9GqB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oSQiao9GqB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oSQiao9GqB@OpenReview" onclick="foldPdfKimi('oSQiao9GqB@OpenReview', this)" class="hr hr-fold">
        </div><div id="fgUFZAxywx@OpenReview" class="panel paper" keywords="spherical,sliced,lssot,transport,optimal,distributions,slicing,embed,computational,linear">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fgUFZAxywx" target="_blank" title="342/373"><span class="index notranslate">#342</span></a>
                <a id="title-fgUFZAxywx@OpenReview" class="title-link" href="/venue/fgUFZAxywx@OpenReview" target="_blank">Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data</a>
                <a id="pdf-fgUFZAxywx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fgUFZAxywx@OpenReview', this)" data="https://openreview.net/pdf?id=fgUFZAxywx">[PDF<sup id="pdf-stars-fgUFZAxywx@OpenReview">1</sup>]</a>
                <a id="copy-fgUFZAxywx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fgUFZAxywx@OpenReview')">[Copy]</a>
                <a id="kimi-fgUFZAxywx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fgUFZAxywx@OpenReview', this)">[Kimi<sup id="kimi-stars-fgUFZAxywx@OpenReview">1</sup>]</a>
                <a id="rel-fgUFZAxywx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fgUFZAxywx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fgUFZAxywx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinran Liu" target="_blank">Xinran Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yikun Bai" target="_blank">Yikun Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rocio Diaz Martin" target="_blank">Rocio Diaz Martin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiwen Shi" target="_blank">Kaiwen Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ashkan Shahbazi" target="_blank">Ashkan Shahbazi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bennett Landman" target="_blank">Bennett Landman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Catie Chang" target="_blank">Catie Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soheil Kolouri" target="_blank">Soheil Kolouri</a>
            </p>
            <p id="summary-fgUFZAxywx@OpenReview" class="summary">Efficient comparison of spherical probability distributions becomes important in fields such as computer vision, geosciences, and medicine. Sliced optimal transport distances, such as spherical and stereographic spherical sliced Wasserstein distances, have recently been developed to address this need. These methods reduce the computational burden of optimal transport by slicing hyperspheres into one-dimensional projections, i.e., lines or circles. Concurrently, linear optimal transport has been proposed to embed distributions into <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-257-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1586" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1587"><span class="msubsup" id="MathJax-Span-1588"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1589" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.68em;"><span class="mn" id="MathJax-Span-1590" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-257">L^2</script> spaces, where the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-258-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1591" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1592"><span class="msubsup" id="MathJax-Span-1593"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1594" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.68em;"><span class="mn" id="MathJax-Span-1595" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-258">L^2</script> distance approximates the optimal transport distance, thereby simplifying comparisons across multiple distributions. In this work, we introduce the Linear Spherical Sliced Optimal Transport (LSSOT) framework, which utilizes slicing to embed spherical distributions into <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-259-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1596" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1597"><span class="msubsup" id="MathJax-Span-1598"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1599" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.68em;"><span class="mn" id="MathJax-Span-1600" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-259">L^2</script> spaces while preserving their intrinsic geometry, offering a computationally efficient metric for spherical probability measures. We establish the metricity of LSSOT and demonstrate its superior computational efficiency in applications such as cortical surface registration, 3D point cloud interpolation via gradient flow, and shape embedding. Our results demonstrate the significant computational benefits and high accuracy of LSSOT in these applications.</p>
            <p id="subjects-fgUFZAxywx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-fgUFZAxywx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fgUFZAxywx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fgUFZAxywx@OpenReview" onclick="foldPdfKimi('fgUFZAxywx@OpenReview', this)" class="hr hr-fold">
        </div><div id="tuu4de7HL1@OpenReview" class="panel paper" keywords="rshtr,subspace,varepsilon,random,guarantees,convergence,iterations,order,nonconvex,optimization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tuu4de7HL1" target="_blank" title="343/373"><span class="index notranslate">#343</span></a>
                <a id="title-tuu4de7HL1@OpenReview" class="title-link" href="/venue/tuu4de7HL1@OpenReview" target="_blank">Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization</a>
                <a id="pdf-tuu4de7HL1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tuu4de7HL1@OpenReview', this)" data="https://openreview.net/pdf?id=tuu4de7HL1">[PDF<sup id="pdf-stars-tuu4de7HL1@OpenReview">2</sup>]</a>
                <a id="copy-tuu4de7HL1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tuu4de7HL1@OpenReview')">[Copy]</a>
                <a id="kimi-tuu4de7HL1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tuu4de7HL1@OpenReview', this)">[Kimi<sup id="kimi-stars-tuu4de7HL1@OpenReview">1</sup>]</a>
                <a id="rel-tuu4de7HL1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tuu4de7HL1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tuu4de7HL1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rei Higuchi" target="_blank">Rei Higuchi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre-Louis Poirion" target="_blank">Pierre-Louis Poirion</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akiko Takeda" target="_blank">Akiko Takeda</a>
            </p>
            <p id="summary-tuu4de7HL1@OpenReview" class="summary">In recent years, random subspace methods have been actively studied for large-dimensional non-convex problems. Recent subspace methods have improved theoretical guarantees such as iteration complexity and local convergence rate while reducing computational costs by deriving descent directions in randomly selected low-dimensional subspaces. This paper proposes the Random Subspace Homogenized Trust Region (RSHTR) method with the best theoretical guarantees among random subspace algorithms for non-convex optimization. RSHTR achieves an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-260-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1601" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1602"><span class="mi" id="MathJax-Span-1603" style="font-family: MathJax_Math-italic;">ε</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi></math></span></span><script type="math/tex" id="MathJax-Element-260">\varepsilon</script>-approximate first-order stationary point in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-261-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1604" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.6em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1605"><span class="mi" id="MathJax-Span-1606" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-1607" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-1608"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1609" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1610"><span class="mrow" id="MathJax-Span-1611"><span class="mo" id="MathJax-Span-1612" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-1613" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span class="texatom" id="MathJax-Span-1614"><span class="mrow" id="MathJax-Span-1615"><span class="mo" id="MathJax-Span-1616" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-1617" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1618" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>3</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-261">O(\varepsilon^{-3/2})</script> iterations, converging locally at a linear rate. Furthermore, under rank-deficient conditions, RSHTR satisfies <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-262-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1619" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1620"><span class="mi" id="MathJax-Span-1621" style="font-family: MathJax_Math-italic;">ε</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi></math></span></span><script type="math/tex" id="MathJax-Element-262">\varepsilon</script>-approximate second-order necessary condition in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-263-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1622" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.6em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1623"><span class="mi" id="MathJax-Span-1624" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-1625" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-1626"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1627" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1628"><span class="mrow" id="MathJax-Span-1629"><span class="mo" id="MathJax-Span-1630" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-1631" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span class="texatom" id="MathJax-Span-1632"><span class="mrow" id="MathJax-Span-1633"><span class="mo" id="MathJax-Span-1634" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-1635" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1636" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>3</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-263">O(\varepsilon^{-3/2})</script> iterations and exhibits a local quadratic convergence. Experiments on real-world datasets verify the benefits of RSHTR.</p>
            <p id="subjects-tuu4de7HL1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tuu4de7HL1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tuu4de7HL1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tuu4de7HL1@OpenReview" onclick="foldPdfKimi('tuu4de7HL1@OpenReview', this)" class="hr hr-fold">
        </div><div id="PstM8YfhvI@OpenReview" class="panel paper" keywords="morphodiff,cellular,painting,interventions,morphology,biological,cell,responses,perturbational,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PstM8YfhvI" target="_blank" title="344/373"><span class="index notranslate">#344</span></a>
                <a id="title-PstM8YfhvI@OpenReview" class="title-link" href="/venue/PstM8YfhvI@OpenReview" target="_blank">MorphoDiff: Cellular Morphology Painting with Diffusion Models</a>
                <a id="pdf-PstM8YfhvI@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PstM8YfhvI@OpenReview', this)" data="https://openreview.net/pdf?id=PstM8YfhvI">[PDF<sup id="pdf-stars-PstM8YfhvI@OpenReview">5</sup>]</a>
                <a id="copy-PstM8YfhvI@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PstM8YfhvI@OpenReview')">[Copy]</a>
                <a id="kimi-PstM8YfhvI@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PstM8YfhvI@OpenReview', this)">[Kimi<sup id="kimi-stars-PstM8YfhvI@OpenReview">3</sup>]</a>
                <a id="rel-PstM8YfhvI@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PstM8YfhvI@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PstM8YfhvI@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeinab Navidi" target="_blank">Zeinab Navidi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Ma" target="_blank">Jun Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Esteban Miglietta" target="_blank">Esteban Miglietta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Liu" target="_blank">Le Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anne Carpenter" target="_blank">Anne Carpenter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beth Cimini" target="_blank">Beth Cimini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Haibe-Kains" target="_blank">Benjamin Haibe-Kains</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=BO WANG" target="_blank">BO WANG</a>
            </p>
            <p id="summary-PstM8YfhvI@OpenReview" class="summary">Understanding cellular responses to external stimuli is critical for parsing biological mechanisms and advancing therapeutic development. High-content image-based assays provide a cost-effective approach to examine cellular phenotypes induced by diverse interventions, which offers valuable insights into biological processes and cellular states. In this paper, we introduce MorphoDiff, a generative pipeline to predict high-resolution cell morphological responses under different conditions based on perturbation encoding. To the best of our knowledge, MorphoDiff is the first framework capable of producing guided, high-resolution predictions of cell morphology that generalize across both chemical and genetic interventions. The model integrates perturbation embeddings as guiding signals within a 2D latent diffusion model. The comprehensive computational, biological, and visual validations across three open-source Cell Painting datasets show that MorphoDiff can generate high-fidelity images and produce meaningful biology signals under various interventions. We envision the model will facilitate efficient in silico exploration of perturbational landscapes towards more effective drug discovery studies.</p>
            <p id="subjects-PstM8YfhvI@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PstM8YfhvI@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PstM8YfhvI@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PstM8YfhvI@OpenReview" onclick="foldPdfKimi('PstM8YfhvI@OpenReview', this)" class="hr hr-fold">
        </div><div id="qyU5s4fzLg@OpenReview" class="panel paper" keywords="seminfo,parsing,constituency,sentence,semantics,unsupervised,pcfg,maximizing,constituent,parsers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qyU5s4fzLg" target="_blank" title="345/373"><span class="index notranslate">#345</span></a>
                <a id="title-qyU5s4fzLg@OpenReview" class="title-link" href="/venue/qyU5s4fzLg@OpenReview" target="_blank">Improving Unsupervised Constituency Parsing via Maximizing Semantic Information</a>
                <a id="pdf-qyU5s4fzLg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qyU5s4fzLg@OpenReview', this)" data="https://openreview.net/pdf?id=qyU5s4fzLg">[PDF<sup id="pdf-stars-qyU5s4fzLg@OpenReview">2</sup>]</a>
                <a id="copy-qyU5s4fzLg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qyU5s4fzLg@OpenReview')">[Copy]</a>
                <a id="kimi-qyU5s4fzLg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qyU5s4fzLg@OpenReview', this)">[Kimi<sup id="kimi-stars-qyU5s4fzLg@OpenReview">2</sup>]</a>
                <a id="rel-qyU5s4fzLg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qyU5s4fzLg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qyU5s4fzLg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Chen" target="_blank">Junjie Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangheng He" target="_blank">Xiangheng He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yusuke Miyao" target="_blank">Yusuke Miyao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danushka Bollegala" target="_blank">Danushka Bollegala</a>
            </p>
            <p id="summary-qyU5s4fzLg@OpenReview" class="summary">Unsupervised constituency parsers organize phrases within a sentence into a tree-shaped syntactic constituent structure that reflects the organization of sentence semantics. However, the traditional objective of maximizing sentence log-likelihood (LL) does not explicitly account for the close relationship between the constituent structure and the semantics, resulting in a weak correlation between LL values and parsing accuracy.In this paper, we introduce a novel objective for training unsupervised parsers: maximizing the information between constituent structures and sentence semantics (SemInfo). We introduce a bag-of-substrings model to represent the semantics and apply the probability-weighted information metric to estimate the SemInfo.Additionally, we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG) induction, the state-of-the-art non-ensemble method for unsupervised constituency parsing. Experiments demonstrate that SemInfo correlates more strongly with parsing accuracy than LL.Our algorithm significantly enhances parsing accuracy by an average of 7.85 points across five PCFG variants and in four languages, achieving state-of-the-art level results in three of the four languages.</p>
            <p id="subjects-qyU5s4fzLg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qyU5s4fzLg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qyU5s4fzLg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qyU5s4fzLg@OpenReview" onclick="foldPdfKimi('qyU5s4fzLg@OpenReview', this)" class="hr hr-fold">
        </div><div id="VCbqXtS5YY@OpenReview" class="panel paper" keywords="reward,human,feedback,alignment,demonstrations,sft,demonstration,policy,model,data">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VCbqXtS5YY" target="_blank" title="346/373"><span class="index notranslate">#346</span></a>
                <a id="title-VCbqXtS5YY@OpenReview" class="title-link" href="/venue/VCbqXtS5YY@OpenReview" target="_blank">Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment</a>
                <a id="pdf-VCbqXtS5YY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VCbqXtS5YY@OpenReview', this)" data="https://openreview.net/pdf?id=VCbqXtS5YY">[PDF<sup id="pdf-stars-VCbqXtS5YY@OpenReview">1</sup>]</a>
                <a id="copy-VCbqXtS5YY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VCbqXtS5YY@OpenReview')">[Copy]</a>
                <a id="kimi-VCbqXtS5YY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VCbqXtS5YY@OpenReview', this)">[Kimi<sup id="kimi-stars-VCbqXtS5YY@OpenReview">3</sup>]</a>
                <a id="rel-VCbqXtS5YY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VCbqXtS5YY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VCbqXtS5YY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenliang Li" target="_blank">Chenliang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siliang Zeng" target="_blank">Siliang Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyi Liao" target="_blank">Zeyi Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxiang Li" target="_blank">Jiaxiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongyeop Kang" target="_blank">Dongyeop Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alfredo Garcia" target="_blank">Alfredo Garcia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyi Hong" target="_blank">Mingyi Hong</a>
            </p>
            <p id="summary-VCbqXtS5YY@OpenReview" class="summary">Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised fine-tuning (SFT) based upon large demonstrations data, (ii) a reward model (RM) is estimated based upon human feedback data, and (iii) reinforcement learning (RL) is used to further refine the SFT model by optimizing the estimated reward model. Demonstrations and human feedback data reflect human user preferences in different ways. As a result, the reward model estimate obtained from only human feedback data is likely not as accurate as a reward model estimate obtained from both demonstration and human feedback data. A policy model that optimizes the reward model estimate obtained from both demonstration and human feedback data will likely exhibit better alignment performance. We introduce a tractable algorithm for finding the reward and policy models and provide a finite-time performance guarantee. Additionally, we demonstrate the efficiency of the proposed solution with extensive experiments including alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithm by large margins, especially when the amounts of demonstration and preference data are unbalanced.</p>
            <p id="subjects-VCbqXtS5YY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-VCbqXtS5YY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VCbqXtS5YY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VCbqXtS5YY@OpenReview" onclick="foldPdfKimi('VCbqXtS5YY@OpenReview', this)" class="hr hr-fold">
        </div><div id="TId1SHe8JG@OpenReview" class="panel paper" keywords="calibration,uncertainty,aleatoric,higher,order,predictors,world,decompositions,provable,formal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TId1SHe8JG" target="_blank" title="347/373"><span class="index notranslate">#347</span></a>
                <a id="title-TId1SHe8JG@OpenReview" class="title-link" href="/venue/TId1SHe8JG@OpenReview" target="_blank">Provable Uncertainty Decomposition via Higher-Order Calibration</a>
                <a id="pdf-TId1SHe8JG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TId1SHe8JG@OpenReview', this)" data="https://openreview.net/pdf?id=TId1SHe8JG">[PDF<sup id="pdf-stars-TId1SHe8JG@OpenReview">3</sup>]</a>
                <a id="copy-TId1SHe8JG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TId1SHe8JG@OpenReview')">[Copy]</a>
                <a id="kimi-TId1SHe8JG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TId1SHe8JG@OpenReview', this)">[Kimi<sup id="kimi-stars-TId1SHe8JG@OpenReview">1</sup>]</a>
                <a id="rel-TId1SHe8JG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TId1SHe8JG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TId1SHe8JG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gustaf Ahdritz" target="_blank">Gustaf Ahdritz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aravind Gollakota" target="_blank">Aravind Gollakota</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Parikshit Gopalan" target="_blank">Parikshit Gopalan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charlotte Peale" target="_blank">Charlotte Peale</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Udi Wieder" target="_blank">Udi Wieder</a>
            </p>
            <p id="summary-TId1SHe8JG@OpenReview" class="summary">We give a principled method for decomposing the predictive uncertainty of a model into aleatoric and epistemic components with explicit semantics relating them to the real-world data distribution. While many works in the literature have proposed such decompositions, they lack the type of formal guarantees we provide. Our method is based on the new notion of higher-order calibration, which generalizes ordinary calibration to the setting of higher-order predictors that predict _mixtures_ over label distributions at every point. We show how to measure as well as achieve higher-order calibration using access to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-264-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1637" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1638"><span class="mi" id="MathJax-Span-1639" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-264">k</script>-snapshots, namely examples where each point has <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-265-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1640" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1641"><span class="mi" id="MathJax-Span-1642" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-265">k</script> independent conditional labels. Under higher-order calibration, the estimated aleatoric uncertainty at a point is guaranteed to match the real-world aleatoric uncertainty averaged over all points where the prediction is made. To our knowledge, this is the first formal guarantee of this type that places no assumptions whatsoever on the real-world data distribution. Importantly, higher-order calibration is also applicable to existing higher-order predictors such as Bayesian and ensemble models and provides a natural evaluation metric for such models. We demonstrate through experiments that our method produces meaningful uncertainty decompositions in tasks such as image classification.</p>
            <p id="subjects-TId1SHe8JG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TId1SHe8JG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TId1SHe8JG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TId1SHe8JG@OpenReview" onclick="foldPdfKimi('TId1SHe8JG@OpenReview', this)" class="hr hr-fold">
        </div><div id="IuU0wcO0mo@OpenReview" class="panel paper" keywords="decoding,brain,types,regions,sub,circuits,cellular,mice,distinctions,different">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IuU0wcO0mo" target="_blank" title="348/373"><span class="index notranslate">#348</span></a>
                <a id="title-IuU0wcO0mo@OpenReview" class="title-link" href="/venue/IuU0wcO0mo@OpenReview" target="_blank">Multi-session, multi-task neural decoding from distinct cell-types and brain regions</a>
                <a id="pdf-IuU0wcO0mo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IuU0wcO0mo@OpenReview', this)" data="https://openreview.net/pdf?id=IuU0wcO0mo">[PDF<sup id="pdf-stars-IuU0wcO0mo@OpenReview">2</sup>]</a>
                <a id="copy-IuU0wcO0mo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IuU0wcO0mo@OpenReview')">[Copy]</a>
                <a id="kimi-IuU0wcO0mo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IuU0wcO0mo@OpenReview', this)">[Kimi<sup id="kimi-stars-IuU0wcO0mo@OpenReview"></sup>]</a>
                <a id="rel-IuU0wcO0mo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IuU0wcO0mo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IuU0wcO0mo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mehdi Azabou" target="_blank">Mehdi Azabou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Krystal Pan" target="_blank">Krystal Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vinam Arora" target="_blank">Vinam Arora</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ian Knight" target="_blank">Ian Knight</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eva Dyer" target="_blank">Eva Dyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Blake A Richards" target="_blank">Blake A Richards</a>
            </p>
            <p id="summary-IuU0wcO0mo@OpenReview" class="summary">Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular sub-types, and the responses to different stimuli are diverse across regions and sub-types. It is unknown whether it is possible to pre-train and transfer brain decoding models between distinct tasks, cellular sub-types, and brain regions. To address these questions, we developed a multi-task transformer architecture and trained it on the entirety of the Allen Institute's Brain Observatory dataset. This dataset contains responses from over 100,000 neurons in 6 areas of the brains of mice, observed with two-photon calcium imaging, recorded while the mice observed different types of visual stimuli. Our results demonstrate that transfer is indeed possible -combining data from different sources is beneficial for a number of downstream decoding tasks. As well, we can transfer the model between regions and sub-types, demonstrating that there is in fact common information in diverse circuits that can be extracted by an appropriately designed model. Interestingly, we found that the model's latent representations showed clear distinctions between different brain regions and cellular sub-types, even though it was never given any information about these distinctions. Altogether, our work demonstrates that training a large-scale neural decoding model on diverse data is possible, and this provides a means of studying the differences and similarities between heterogeneous neural circuits.</p>
            <p id="subjects-IuU0wcO0mo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IuU0wcO0mo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IuU0wcO0mo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IuU0wcO0mo@OpenReview" onclick="foldPdfKimi('IuU0wcO0mo@OpenReview', this)" class="hr hr-fold">
        </div><div id="tfyHbvFZ0K@OpenReview" class="panel paper" keywords="knowledge,assumption,localization,storage,limitations,store,query,modification,accomplished,enter">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tfyHbvFZ0K" target="_blank" title="349/373"><span class="index notranslate">#349</span></a>
                <a id="title-tfyHbvFZ0K@OpenReview" class="title-link" href="/venue/tfyHbvFZ0K@OpenReview" target="_blank">Knowledge Localization: Mission Not Accomplished? Enter Query Localization!</a>
                <a id="pdf-tfyHbvFZ0K@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tfyHbvFZ0K@OpenReview', this)" data="https://openreview.net/pdf?id=tfyHbvFZ0K">[PDF<sup id="pdf-stars-tfyHbvFZ0K@OpenReview">3</sup>]</a>
                <a id="copy-tfyHbvFZ0K@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tfyHbvFZ0K@OpenReview')">[Copy]</a>
                <a id="kimi-tfyHbvFZ0K@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tfyHbvFZ0K@OpenReview', this)">[Kimi<sup id="kimi-stars-tfyHbvFZ0K@OpenReview">2</sup>]</a>
                <a id="rel-tfyHbvFZ0K@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tfyHbvFZ0K@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tfyHbvFZ0K@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuheng Chen" target="_blank">Yuheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Cao" target="_blank">Pengfei Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yubo Chen" target="_blank">Yubo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kang Liu" target="_blank">Kang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Zhao" target="_blank">Jun Zhao</a>
            </p>
            <p id="summary-tfyHbvFZ0K@OpenReview" class="summary">Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear.The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the **Knowledge Localization (KL)** assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons. However, this assumption has two limitations: first, it may be too rigid regarding knowledge storage, and second, it neglects the role of the attention module in knowledge expression. In this paper, we first re-examine the KL assumption and demonstrate that its limitations do indeed exist. To address these, we then present two new findings, each targeting one of the limitations: one focusing on knowledge storage and the other on knowledge expression.We summarize these findings as **Query Localization** assumption and argue that the KL assumption can be viewed as a simplification of the QL assumption. Based on QL assumption, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification, further validating our new assumption. We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously confirm our conclusions. Code will be made public soon.</p>
            <p id="subjects-tfyHbvFZ0K@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tfyHbvFZ0K@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tfyHbvFZ0K@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tfyHbvFZ0K@OpenReview" onclick="foldPdfKimi('tfyHbvFZ0K@OpenReview', this)" class="hr hr-fold">
        </div><div id="04qx93Viwj@OpenReview" class="panel paper" keywords="environmental,impact,textbf,water,developers,usage,development,creating,hardware,sim">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=04qx93Viwj" target="_blank" title="350/373"><span class="index notranslate">#350</span></a>
                <a id="title-04qx93Viwj@OpenReview" class="title-link" href="/venue/04qx93Viwj@OpenReview" target="_blank">Holistically Evaluating the Environmental Impact of Creating Language Models</a>
                <a id="pdf-04qx93Viwj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('04qx93Viwj@OpenReview', this)" data="https://openreview.net/pdf?id=04qx93Viwj">[PDF<sup id="pdf-stars-04qx93Viwj@OpenReview">2</sup>]</a>
                <a id="copy-04qx93Viwj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('04qx93Viwj@OpenReview')">[Copy]</a>
                <a id="kimi-04qx93Viwj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('04qx93Viwj@OpenReview', this)">[Kimi<sup id="kimi-stars-04qx93Viwj@OpenReview">1</sup>]</a>
                <a id="rel-04qx93Viwj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('04qx93Viwj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-04qx93Viwj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Morrison" target="_blank">Jacob Morrison</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Clara Na" target="_blank">Clara Na</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jared Fernandez" target="_blank">Jared Fernandez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Dettmers" target="_blank">Tim Dettmers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emma Strubell" target="_blank">Emma Strubell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jesse Dodge" target="_blank">Jesse Dodge</a>
            </p>
            <p id="summary-04qx93Viwj@OpenReview" class="summary">As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 7 billion active parameters, trained on up to 5 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-266-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;270 metric tons&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1643" style="width: 9.065em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1007.5em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1644"><span class="texatom" id="MathJax-Span-1645"><span class="mrow" id="MathJax-Span-1646"><span class="mtext" id="MathJax-Span-1647" style="font-family: MathJax_Main-bold;">270 metric tons</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">270 metric tons</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-266">\textbf{270 metric tons}</script> of carbon emissions, equivalent to powering about 53 homes in the United States for one year, and consumed <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-267-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;1.137 million liters of water&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1648" style="width: 16.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 13.388em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1013.34em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1649"><span class="texatom" id="MathJax-Span-1650"><span class="mrow" id="MathJax-Span-1651"><span class="mtext" id="MathJax-Span-1652" style="font-family: MathJax_Main-bold;">1.137 million liters of water</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">1.137 million liters of water</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-267">\textbf{1.137 million liters of water}</script>, equivalent to about 10 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-268-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1653" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1654"><span class="mo" id="MathJax-Span-1655" style="font-family: MathJax_Main;">∼</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo></math></span></span><script type="math/tex" id="MathJax-Element-268">\sim</script><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-269-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;80&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1656" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.1em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1657"><span class="texatom" id="MathJax-Span-1658"><span class="mrow" id="MathJax-Span-1659"><span class="mtext" id="MathJax-Span-1660" style="font-family: MathJax_Main-bold;">80</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">80</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-269">\textbf{80}</script>% of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-270-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1661" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1662"><span class="mo" id="MathJax-Span-1663" style="font-family: MathJax_Main;">∼</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo></math></span></span><script type="math/tex" id="MathJax-Element-270">\sim</script>15% and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-271-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1664" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1665"><span class="mo" id="MathJax-Span-1666" style="font-family: MathJax_Main;">∼</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo></math></span></span><script type="math/tex" id="MathJax-Element-271">\sim</script>85% of our hardware's maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large.</p>
            <p id="subjects-04qx93Viwj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-04qx93Viwj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-04qx93Viwj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-04qx93Viwj@OpenReview" onclick="foldPdfKimi('04qx93Viwj@OpenReview', this)" class="hr hr-fold">
        </div><div id="FtX6oAW7Dd@OpenReview" class="panel paper" keywords="pll,plench,algorithms,label,partial,systematically,evaluation,realistic,developed,selection">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FtX6oAW7Dd" target="_blank" title="351/373"><span class="index notranslate">#351</span></a>
                <a id="title-FtX6oAW7Dd@OpenReview" class="title-link" href="/venue/FtX6oAW7Dd@OpenReview" target="_blank">PLENCH: Realistic Evaluation of Deep Partial-Label Learning Algorithms</a>
                <a id="pdf-FtX6oAW7Dd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FtX6oAW7Dd@OpenReview', this)" data="https://openreview.net/pdf?id=FtX6oAW7Dd">[PDF<sup id="pdf-stars-FtX6oAW7Dd@OpenReview">2</sup>]</a>
                <a id="copy-FtX6oAW7Dd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FtX6oAW7Dd@OpenReview')">[Copy]</a>
                <a id="kimi-FtX6oAW7Dd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FtX6oAW7Dd@OpenReview', this)">[Kimi<sup id="kimi-stars-FtX6oAW7Dd@OpenReview">1</sup>]</a>
                <a id="rel-FtX6oAW7Dd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FtX6oAW7Dd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FtX6oAW7Dd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Wang" target="_blank">Wei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong-Dong Wu" target="_blank">Dong-Dong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jindong Wang" target="_blank">Jindong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gang Niu" target="_blank">Gang Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min-Ling Zhang" target="_blank">Min-Ling Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Masashi Sugiyama" target="_blank">Masashi Sugiyama</a>
            </p>
            <p id="summary-FtX6oAW7Dd@OpenReview" class="summary">Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We systematically investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.</p>
            <p id="subjects-FtX6oAW7Dd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FtX6oAW7Dd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FtX6oAW7Dd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FtX6oAW7Dd@OpenReview" onclick="foldPdfKimi('FtX6oAW7Dd@OpenReview', this)" class="hr hr-fold">
        </div><div id="lgsyLSsDRe@OpenReview" class="panel paper" keywords="embedding,mteb,retrieval,embed,training,contrastive,llms,curated,2024,generalist">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=lgsyLSsDRe" target="_blank" title="352/373"><span class="index notranslate">#352</span></a>
                <a id="title-lgsyLSsDRe@OpenReview" class="title-link" href="/venue/lgsyLSsDRe@OpenReview" target="_blank">NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models</a>
                <a id="pdf-lgsyLSsDRe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('lgsyLSsDRe@OpenReview', this)" data="https://openreview.net/pdf?id=lgsyLSsDRe">[PDF<sup id="pdf-stars-lgsyLSsDRe@OpenReview">3</sup>]</a>
                <a id="copy-lgsyLSsDRe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('lgsyLSsDRe@OpenReview')">[Copy]</a>
                <a id="kimi-lgsyLSsDRe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('lgsyLSsDRe@OpenReview', this)">[Kimi<sup id="kimi-stars-lgsyLSsDRe@OpenReview">2</sup>]</a>
                <a id="rel-lgsyLSsDRe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('lgsyLSsDRe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-lgsyLSsDRe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chankyu Lee" target="_blank">Chankyu Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rajarshi Roy" target="_blank">Rajarshi Roy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengyao Xu" target="_blank">Mengyao Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Raiman" target="_blank">Jonathan Raiman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Shoeybi" target="_blank">Mohammad Shoeybi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bryan Catanzaro" target="_blank">Bryan Catanzaro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Ping" target="_blank">Wei Ping</a>
            </p>
            <p id="summary-lgsyLSsDRe@OpenReview" class="summary">Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility.For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last &lt;EOS&gt; token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed- v1 model secured the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), across 56 embedding tasks. NV-Embed-v2 has reclaimed and maintained the top spot on MTEB since August 30, 2024, demonstrating the sustained effectiveness of the proposed methods over time. Additionally, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB.</p>
            <p id="subjects-lgsyLSsDRe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-lgsyLSsDRe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-lgsyLSsDRe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-lgsyLSsDRe@OpenReview" onclick="foldPdfKimi('lgsyLSsDRe@OpenReview', this)" class="hr hr-fold">
        </div><div id="3cgMU3TyyE@OpenReview" class="panel paper" keywords="conversation,planning,conversations,scope,llm,semantic,turn,llms,broaden,response">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3cgMU3TyyE" target="_blank" title="353/373"><span class="index notranslate">#353</span></a>
                <a id="title-3cgMU3TyyE@OpenReview" class="title-link" href="/venue/3cgMU3TyyE@OpenReview" target="_blank">Broaden your SCOPE! Efficient Conversation Planning for LLMs with Semantic Space</a>
                <a id="pdf-3cgMU3TyyE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3cgMU3TyyE@OpenReview', this)" data="https://openreview.net/pdf?id=3cgMU3TyyE">[PDF<sup id="pdf-stars-3cgMU3TyyE@OpenReview">2</sup>]</a>
                <a id="copy-3cgMU3TyyE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3cgMU3TyyE@OpenReview')">[Copy]</a>
                <a id="kimi-3cgMU3TyyE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3cgMU3TyyE@OpenReview', this)">[Kimi<sup id="kimi-stars-3cgMU3TyyE@OpenReview">5</sup>]</a>
                <a id="rel-3cgMU3TyyE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3cgMU3TyyE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3cgMU3TyyE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiliang Chen" target="_blank">Zhiliang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyuan Niu" target="_blank">Xinyuan Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuan-Sheng Foo" target="_blank">Chuan-Sheng Foo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bryan Kian Hsiang Low" target="_blank">Bryan Kian Hsiang Low</a>
            </p>
            <p id="summary-3cgMU3TyyE@OpenReview" class="summary">Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This gives the advantage of allowing the optimal LLM response to be selected at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget.</p>
            <p id="subjects-3cgMU3TyyE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3cgMU3TyyE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3cgMU3TyyE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3cgMU3TyyE@OpenReview" onclick="foldPdfKimi('3cgMU3TyyE@OpenReview', this)" class="hr hr-fold">
        </div><div id="rySLejeB1k@OpenReview" class="panel paper" keywords="pepper,pinwheel,orientation,coding,neuronal,salt,mammals,visual,spiking,retinotopy">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rySLejeB1k" target="_blank" title="354/373"><span class="index notranslate">#354</span></a>
                <a id="title-rySLejeB1k@OpenReview" class="title-link" href="/venue/rySLejeB1k@OpenReview" target="_blank">Emergent Orientation Maps —— Mechanisms, Coding Efficiency and Robustness</a>
                <a id="pdf-rySLejeB1k@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rySLejeB1k@OpenReview', this)" data="https://openreview.net/pdf?id=rySLejeB1k">[PDF<sup id="pdf-stars-rySLejeB1k@OpenReview">1</sup>]</a>
                <a id="copy-rySLejeB1k@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rySLejeB1k@OpenReview')">[Copy]</a>
                <a id="kimi-rySLejeB1k@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rySLejeB1k@OpenReview', this)">[Kimi<sup id="kimi-stars-rySLejeB1k@OpenReview">1</sup>]</a>
                <a id="rel-rySLejeB1k@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rySLejeB1k@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rySLejeB1k@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haixin Zhong" target="_blank">Haixin Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyu Wang" target="_blank">Haoyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Dai" target="_blank">Wei Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchao Huang" target="_blank">Yuchao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyi Huang" target="_blank">Mingyi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rubin Wang" target="_blank">Rubin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Roe" target="_blank">Anna Roe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuguo Yu" target="_blank">Yuguo Yu</a>
            </p>
            <p id="summary-rySLejeB1k@OpenReview" class="summary">Extensive experimental studies have shown that in lower mammals, neuronal orientation preference in the primary visual cortex is organized in a disordered "salt-and-pepper" pattern. In contrast, higher-order mammals display a continuous variation in orientation preference, forming structured pinwheel-like patterns. Despite these observations, the spiking mechanisms underlying the emergence of these distinct topological structures and their functional roles in visual processing remain poorly understood. To address this, we developed a self-evolving spiking neural network model with Hebbian plasticity, trained using physiological parameters characteristic of rodents, cats, and primates, including retinotopy, neuronal morphology, and connectivity patterns. Our results identify critical factors, such as the degree of input visual field overlap, neuronal density, and the balance between localized connectivity and long-range competition, that determine the emergence of either salt-and-pepper or pinwheel-like topologies. Furthermore, we demonstrate that pinwheel structures exhibit lower wiring costs and enhanced sparse coding capabilities compared to salt-and-pepper organizations. They also maintain greater coding robustness against noise in naturalistic visual stimuli. These findings suggest that such topological structures confer significant computational advantages in visual processing and highlight their potential application in the design of brain-inspired deep learning networks and algorithms.</p>
            <p id="subjects-rySLejeB1k@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-rySLejeB1k@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rySLejeB1k@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rySLejeB1k@OpenReview" onclick="foldPdfKimi('rySLejeB1k@OpenReview', this)" class="hr hr-fold">
        </div><div id="6NNA0MxhCH@OpenReview" class="panel paper" keywords="answer,mcqa,choice,formatted,vocabulary,symbol,ace,assemble,lms,layers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6NNA0MxhCH" target="_blank" title="355/373"><span class="index notranslate">#355</span></a>
                <a id="title-6NNA0MxhCH@OpenReview" class="title-link" href="/venue/6NNA0MxhCH@OpenReview" target="_blank">Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions</a>
                <a id="pdf-6NNA0MxhCH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6NNA0MxhCH@OpenReview', this)" data="https://openreview.net/pdf?id=6NNA0MxhCH">[PDF<sup id="pdf-stars-6NNA0MxhCH@OpenReview">3</sup>]</a>
                <a id="copy-6NNA0MxhCH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6NNA0MxhCH@OpenReview')">[Copy]</a>
                <a id="kimi-6NNA0MxhCH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6NNA0MxhCH@OpenReview', this)">[Kimi<sup id="kimi-stars-6NNA0MxhCH@OpenReview">2</sup>]</a>
                <a id="rel-6NNA0MxhCH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6NNA0MxhCH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6NNA0MxhCH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sarah Wiegreffe" target="_blank">Sarah Wiegreffe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oyvind Tafjord" target="_blank">Oyvind Tafjord</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonatan Belinkov" target="_blank">Yonatan Belinkov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanna Hajishirzi" target="_blank">Hanna Hajishirzi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ashish Sabharwal" target="_blank">Ashish Sabharwal</a>
            </p>
            <p id="summary-6NNA0MxhCH@OpenReview" class="summary">Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that prediction of a specific answer symbol is causally attributed to a few middle layers, and specifically their multi-head self-attention mechanisms. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that logit differences between answer choice tokens continue to grow over the course of training.</p>
            <p id="subjects-6NNA0MxhCH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-6NNA0MxhCH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6NNA0MxhCH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6NNA0MxhCH@OpenReview" onclick="foldPdfKimi('6NNA0MxhCH@OpenReview', this)" class="hr hr-fold">
        </div><div id="ixMBnOhFGd@OpenReview" class="panel paper" keywords="retrieval,seper,rag,perplexity,utility,generation,lens,semantic,retrieved,obscures">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ixMBnOhFGd" target="_blank" title="356/373"><span class="index notranslate">#356</span></a>
                <a id="title-ixMBnOhFGd@OpenReview" class="title-link" href="/venue/ixMBnOhFGd@OpenReview" target="_blank">SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction</a>
                <a id="pdf-ixMBnOhFGd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ixMBnOhFGd@OpenReview', this)" data="https://openreview.net/pdf?id=ixMBnOhFGd">[PDF<sup id="pdf-stars-ixMBnOhFGd@OpenReview">4</sup>]</a>
                <a id="copy-ixMBnOhFGd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ixMBnOhFGd@OpenReview')">[Copy]</a>
                <a id="kimi-ixMBnOhFGd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ixMBnOhFGd@OpenReview', this)">[Kimi<sup id="kimi-stars-ixMBnOhFGd@OpenReview">8</sup>]</a>
                <a id="rel-ixMBnOhFGd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ixMBnOhFGd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ixMBnOhFGd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Dai" target="_blank">Lu Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yijie Xu" target="_blank">Yijie Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinhui Ye" target="_blank">Jinhui Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Liu" target="_blank">Hao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Xiong" target="_blank">Hui Xiong</a>
            </p>
            <p id="summary-ixMBnOhFGd@OpenReview" class="summary">Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose *Semantic Perplexity (SePer)*, a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios.</p>
            <p id="subjects-ixMBnOhFGd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ixMBnOhFGd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ixMBnOhFGd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ixMBnOhFGd@OpenReview" onclick="foldPdfKimi('ixMBnOhFGd@OpenReview', this)" class="hr hr-fold">
        </div><div id="OdnqG1fYpo@OpenReview" class="panel paper" keywords="moner,moco,mri,motion,undersampled,inr,radial,iwuqing,correction,unsupervised">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OdnqG1fYpo" target="_blank" title="357/373"><span class="index notranslate">#357</span></a>
                <a id="title-OdnqG1fYpo@OpenReview" class="title-link" href="/venue/OdnqG1fYpo@OpenReview" target="_blank">Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation</a>
                <a id="pdf-OdnqG1fYpo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OdnqG1fYpo@OpenReview', this)" data="https://openreview.net/pdf?id=OdnqG1fYpo">[PDF<sup id="pdf-stars-OdnqG1fYpo@OpenReview">2</sup>]</a>
                <a id="copy-OdnqG1fYpo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OdnqG1fYpo@OpenReview')">[Copy]</a>
                <a id="kimi-OdnqG1fYpo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OdnqG1fYpo@OpenReview', this)">[Kimi<sup id="kimi-stars-OdnqG1fYpo@OpenReview">1</sup>]</a>
                <a id="rel-OdnqG1fYpo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OdnqG1fYpo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OdnqG1fYpo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Wu" target="_blank">Qing Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenhe Du" target="_blank">Chenhe Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanyu Tian" target="_blank">Xuanyu Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Yu" target="_blank">Jingyi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuyao Zhang" target="_blank">Yuyao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongjiang Wei" target="_blank">Hongjiang Wei</a>
            </p>
            <p id="summary-OdnqG1fYpo@OpenReview" class="summary">Motion correction (MoCo) in radial MRI is a particularly challenging problem due to the unpredictability of subject movement. Current state-of-the-art (SOTA) MoCo algorithms often rely on extensive high-quality MR images to pre-train neural networks, which constrains the solution space and leads to outstanding image reconstruction results. However, the need for large-scale datasets significantly increases costs and limits model generalization. In this work, we propose Moner, an unsupervised MoCo method that jointly reconstructs artifact-free MR images and estimates accurate motion from undersampled, rigid motion-corrupted k-space data, without requiring any training data. Our core idea is to leverage the continuous prior of implicit neural representation (INR) to constrain this ill-posed inverse problem, facilitating optimal solutions. Specifically, we integrate a quasi-static motion model into the INR, granting its ability to correct subject's motion. To stabilize model optimization, we reformulate radial MRI reconstruction as a back-projection problem using the Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash encoding strategy, significantly enhancing MoCo accuracy. Experiments on multiple MRI datasets show our Moner achieves performance comparable to SOTA MoCo techniques on in-domain data, while demonstrating significant improvements on out-of-domain data. The code is available at: https://github.com/iwuqing/Moner</p>
            <p id="subjects-OdnqG1fYpo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-OdnqG1fYpo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OdnqG1fYpo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OdnqG1fYpo@OpenReview" onclick="foldPdfKimi('OdnqG1fYpo@OpenReview', this)" class="hr hr-fold">
        </div><div id="ztzZDzgfrh@OpenReview" class="panel paper" keywords="hallucinations,redeep,rag,knowledge,ffns,retrieved,hallucination,external,parametric,copying">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ztzZDzgfrh" target="_blank" title="358/373"><span class="index notranslate">#358</span></a>
                <a id="title-ztzZDzgfrh@OpenReview" class="title-link" href="/venue/ztzZDzgfrh@OpenReview" target="_blank">ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability</a>
                <a id="pdf-ztzZDzgfrh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ztzZDzgfrh@OpenReview', this)" data="https://openreview.net/pdf?id=ztzZDzgfrh">[PDF<sup id="pdf-stars-ztzZDzgfrh@OpenReview">13</sup>]</a>
                <a id="copy-ztzZDzgfrh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ztzZDzgfrh@OpenReview')">[Copy]</a>
                <a id="kimi-ztzZDzgfrh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ztzZDzgfrh@OpenReview', this)">[Kimi<sup id="kimi-stars-ztzZDzgfrh@OpenReview">12</sup>]</a>
                <a id="rel-ztzZDzgfrh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ztzZDzgfrh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ztzZDzgfrh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongxiang Sun" target="_blank">Zhongxiang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoxue Zang" target="_blank">Xiaoxue Zang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zheng" target="_blank">Kai Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Xu" target="_blank">Jun Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Zhang" target="_blank">Xiao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijie Yu" target="_blank">Weijie Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Song" target="_blank">Yang Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Li" target="_blank">Han Li</a>
            </p>
            <p id="summary-ztzZDzgfrh@OpenReview" class="summary">Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) balance external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the **Knowledge FFNs** in LLMs overemphasize parametric knowledge in the residual stream, while **Copying Heads** fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose **ReDeEP**, a novel method that detects hallucinations by decoupling LLM’s utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.</p>
            <p id="subjects-ztzZDzgfrh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ztzZDzgfrh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ztzZDzgfrh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ztzZDzgfrh@OpenReview" onclick="foldPdfKimi('ztzZDzgfrh@OpenReview', this)" class="hr hr-fold">
        </div><div id="cfKZ5VrhXt@OpenReview" class="panel paper" keywords="rlhf,copo,preference,online,exploration,count,reward,llm,ucb,alignment">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cfKZ5VrhXt" target="_blank" title="359/373"><span class="index notranslate">#359</span></a>
                <a id="title-cfKZ5VrhXt@OpenReview" class="title-link" href="/venue/cfKZ5VrhXt@OpenReview" target="_blank">Online Preference Alignment for Language Models via Count-based Exploration</a>
                <a id="pdf-cfKZ5VrhXt@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cfKZ5VrhXt@OpenReview', this)" data="https://openreview.net/pdf?id=cfKZ5VrhXt">[PDF<sup id="pdf-stars-cfKZ5VrhXt@OpenReview">3</sup>]</a>
                <a id="copy-cfKZ5VrhXt@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cfKZ5VrhXt@OpenReview')">[Copy]</a>
                <a id="kimi-cfKZ5VrhXt@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cfKZ5VrhXt@OpenReview', this)">[Kimi<sup id="kimi-stars-cfKZ5VrhXt@OpenReview">4</sup>]</a>
                <a id="rel-cfKZ5VrhXt@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cfKZ5VrhXt@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cfKZ5VrhXt@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenjia Bai" target="_blank">Chenjia Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Zhang" target="_blank">Yang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuang Qiu" target="_blank">Shuang Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiaosheng Zhang" target="_blank">Qiaosheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kang Xu" target="_blank">Kang Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuelong Li" target="_blank">Xuelong Li</a>
            </p>
            <p id="summary-cfKZ5VrhXt@OpenReview" class="summary">Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.</p>
            <p id="subjects-cfKZ5VrhXt@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cfKZ5VrhXt@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cfKZ5VrhXt@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cfKZ5VrhXt@OpenReview" onclick="foldPdfKimi('cfKZ5VrhXt@OpenReview', this)" class="hr hr-fold">
        </div><div id="gye2U9uNXx@OpenReview" class="panel paper" keywords="thesaurus,llm,instruct,subjective,enthusiastic,humans,operational,semantics,ted,llms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gye2U9uNXx" target="_blank" title="360/373"><span class="index notranslate">#360</span></a>
                <a id="title-gye2U9uNXx@OpenReview" class="title-link" href="/venue/gye2U9uNXx@OpenReview" target="_blank">Uncovering Gaps in How Humans and LLMs Interpret Subjective Language</a>
                <a id="pdf-gye2U9uNXx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gye2U9uNXx@OpenReview', this)" data="https://openreview.net/pdf?id=gye2U9uNXx">[PDF<sup id="pdf-stars-gye2U9uNXx@OpenReview">1</sup>]</a>
                <a id="copy-gye2U9uNXx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gye2U9uNXx@OpenReview')">[Copy]</a>
                <a id="kimi-gye2U9uNXx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gye2U9uNXx@OpenReview', this)">[Kimi<sup id="kimi-stars-gye2U9uNXx@OpenReview">2</sup>]</a>
                <a id="rel-gye2U9uNXx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gye2U9uNXx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gye2U9uNXx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Erik Jones" target="_blank">Erik Jones</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arjun Patrawala" target="_blank">Arjun Patrawala</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Steinhardt" target="_blank">Jacob Steinhardt</a>
            </p>
            <p id="summary-gye2U9uNXx@OpenReview" class="summary">Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an *enthusiastic* blogpost, while developers might train models to be *helpful* and *harmless* using LLM-based edits. The LLM's *operational semantics* of such subjective phrases---how it adjusts its behavior when each phrase is included in the prompt---thus dictates how aligned it is with human intent. In this work, we uncover instances of *misalignment* between LLMs' actual operational semantics and what humans expect. Our method, TED (thesaurus error detector), first constructs a thesaurus that captures whether two phrases have similar operational semantics according to the LLM. It then elicits failures by unearthing disagreements between this thesaurus and a reference semantic thesaurus. TED routinely produces surprising instances of misalignment; for example, Mistral 7B Instruct produces more *harassing* outputs when it edits text to be *witty*, and Llama 3 8B Instruct produces *dishonest* articles when instructed to make the articles *enthusiastic*. Our results demonstrate that we can uncover unexpected LLM behavior by characterizing relationships between abstract concepts, rather than supervising individual outputs directly.</p>
            <p id="subjects-gye2U9uNXx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gye2U9uNXx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gye2U9uNXx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gye2U9uNXx@OpenReview" onclick="foldPdfKimi('gye2U9uNXx@OpenReview', this)" class="hr hr-fold">
        </div><div id="KZgo2YQbhc@OpenReview" class="panel paper" keywords="para,personalizing,target,rank,text,t2i,personalization,diffusionmodel,trainingdata,aligning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KZgo2YQbhc" target="_blank" title="361/373"><span class="index notranslate">#361</span></a>
                <a id="title-KZgo2YQbhc@OpenReview" class="title-link" href="/venue/KZgo2YQbhc@OpenReview" target="_blank">PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction</a>
                <a id="pdf-KZgo2YQbhc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KZgo2YQbhc@OpenReview', this)" data="https://openreview.net/pdf?id=KZgo2YQbhc">[PDF<sup id="pdf-stars-KZgo2YQbhc@OpenReview">5</sup>]</a>
                <a id="copy-KZgo2YQbhc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KZgo2YQbhc@OpenReview')">[Copy]</a>
                <a id="kimi-KZgo2YQbhc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KZgo2YQbhc@OpenReview', this)">[Kimi<sup id="kimi-stars-KZgo2YQbhc@OpenReview">5</sup>]</a>
                <a id="rel-KZgo2YQbhc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KZgo2YQbhc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KZgo2YQbhc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shangyu Chen" target="_blank">Shangyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zizheng Pan" target="_blank">Zizheng Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfei Cai" target="_blank">Jianfei Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dinh Phung" target="_blank">Dinh Phung</a>
            </p>
            <p id="summary-KZgo2YQbhc@OpenReview" class="summary">Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is chal-lenging as it typically struggles to make an appropriate trade-off between its trainingdata distribution and the target distribution, i.e., learning a novel concept with only afew target images to achieve personalization (aligning with the personalized target)while preserving text editability (aligning with diverse text prompts). In this paper,we propose PaRa, an effective and efficient Parameter Rank Reduction approachfor T2I model personalization by explicitly controlling the rank of the diffusionmodel parameters to restrict its initial diverse generation space into a small andwell-balanced target space. Our design is motivated by the fact that taming a T2Imodel toward a novel concept such as a specific art style implies a small generationspace. To this end, by reducing the rank of model parameters during finetuning, wecan effectively constrain the space of the denoising sampling trajectories towardsthe target. With comprehensive experiments, we show that PaRa achieves greatadvantages over existing finetuning approaches on single/multi-subject generationas well as single-image editing. Notably, compared to the prevailing fine-tuningtechnique LoRA, PaRa achieves better parameter efficiency (2× fewer learnableparameters) and much better target image alignment.</p>
            <p id="subjects-KZgo2YQbhc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-KZgo2YQbhc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KZgo2YQbhc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KZgo2YQbhc@OpenReview" onclick="foldPdfKimi('KZgo2YQbhc@OpenReview', this)" class="hr hr-fold">
        </div><div id="i8dYPGdB1C@OpenReview" class="panel paper" keywords="osma,submodular,textbf,osg,approximation,frac,algorithm,multi,communication,projection">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=i8dYPGdB1C" target="_blank" title="362/373"><span class="index notranslate">#362</span></a>
                <a id="title-i8dYPGdB1C@OpenReview" class="title-link" href="/venue/i8dYPGdB1C@OpenReview" target="_blank">Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency</a>
                <a id="pdf-i8dYPGdB1C@OpenReview" class="title-pdf notranslate" onclick="togglePdf('i8dYPGdB1C@OpenReview', this)" data="https://openreview.net/pdf?id=i8dYPGdB1C">[PDF<sup id="pdf-stars-i8dYPGdB1C@OpenReview"></sup>]</a>
                <a id="copy-i8dYPGdB1C@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('i8dYPGdB1C@OpenReview')">[Copy]</a>
                <a id="kimi-i8dYPGdB1C@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('i8dYPGdB1C@OpenReview', this)">[Kimi<sup id="kimi-stars-i8dYPGdB1C@OpenReview">3</sup>]</a>
                <a id="rel-i8dYPGdB1C@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('i8dYPGdB1C@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-i8dYPGdB1C@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qixin ZHANG" target="_blank">Qixin ZHANG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongqi Wan" target="_blank">Zongqi Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Yang" target="_blank">Yu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Shen" target="_blank">Li Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dacheng Tao" target="_blank">Dacheng Tao</a>
            </p>
            <p id="summary-i8dYPGdB1C@OpenReview" class="summary">Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm, are often hindered by their poor approximation guarantees and the rigid requirement for a fully connected communication graph. To address these challenges, we firstly present a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-272-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;MA-OSMA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1667" style="width: 7.034em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1668"><span class="texatom" id="MathJax-Span-1669"><span class="mrow" id="MathJax-Span-1670"><span class="mtext" id="MathJax-Span-1671" style="font-family: MathJax_Main-bold;">MA-OSMA</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">MA-OSMA</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-272">\textbf{MA-OSMA}</script> algorithm, which employs the multi-linear extension to transfer the discrete submodular maximization problem into a continuous optimization, thereby allowing us to reduce the strict dependence on a complete graph through consensus techniques. Moreover, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-273-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;MA-OSMA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1672" style="width: 7.034em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1673"><span class="texatom" id="MathJax-Span-1674"><span class="mrow" id="MathJax-Span-1675"><span class="mtext" id="MathJax-Span-1676" style="font-family: MathJax_Main-bold;">MA-OSMA</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">MA-OSMA</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-273">\textbf{MA-OSMA}</script> leverages a novel surrogate gradient to avoid sub-optimal stationary points. To eliminate the computationally intensive projection operations in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-274-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;MA-OSMA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1677" style="width: 7.034em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1678"><span class="texatom" id="MathJax-Span-1679"><span class="mrow" id="MathJax-Span-1680"><span class="mtext" id="MathJax-Span-1681" style="font-family: MathJax_Main-bold;">MA-OSMA</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">MA-OSMA</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-274">\textbf{MA-OSMA}</script>, we also introduce a projection-free <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-275-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;MA-OSEA&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1682" style="width: 6.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.42em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-1683"><span class="texatom" id="MathJax-Span-1684"><span class="mrow" id="MathJax-Span-1685"><span class="mtext" id="MathJax-Span-1686" style="font-family: MathJax_Main-bold;">MA-OSEA</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">MA-OSEA</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-275">\textbf{MA-OSEA}</script> algorithm, which effectively utilizes the KL divergence by mixing a uniform distribution. Theoretically, we confirm that both algorithms achieve a regret bound of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-276-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1687" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.836em, 1004.12em, 2.971em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1688"><span class="texatom" id="MathJax-Span-1689"><span class="mrow" id="MathJax-Span-1690"><span class="munderover" id="MathJax-Span-1691"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1692" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-1693" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1694" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-1695"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px;"><span style="position: absolute; clip: rect(2.867em, 1001.67em, 4.69em, -999.997em); top: -4.008em; left: 0.992em;"><span class="mrow" id="MathJax-Span-1696"><span class="mfrac" id="MathJax-Span-1697"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.461em, 1001.3em, 2.398em, -999.997em); top: -2.654em; left: 50%; margin-left: -0.674em;"><span class="mrow" id="MathJax-Span-1698"><span class="msubsup" id="MathJax-Span-1699"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.461em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1700" style="font-size: 70.7%; font-family: MathJax_Math-italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.029em; left: 0.471em;"><span class="texatom" id="MathJax-Span-1701"><span class="mrow" id="MathJax-Span-1702"><span class="mi" id="MathJax-Span-1703" style="font-size: 50%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mi" id="MathJax-Span-1704" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.461em, 1001.3em, 2.451em, -999.997em); top: -1.716em; left: 50%; margin-left: -0.674em;"><span class="mrow" id="MathJax-Span-1705"><span class="mn" id="MathJax-Span-1706" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-1707" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-1708" style="font-size: 70.7%; font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.41em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.409em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1001.62em, 3.961em, -999.997em); top: -4.841em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.888em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.419em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.711em, 1000.99em, 4.794em, -999.997em); top: -4.008em; left: 0em;"><span style="font-family: MathJax_Size2;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1709" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mfrac><mrow><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msub><mi>T</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow></mfrac></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-276">\widetilde{O}(\sqrt{\frac{C_{T}T}{1-\beta}})</script> against a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-277-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1710" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.992em, 1002.87em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1711"><span class="mo" id="MathJax-Span-1712" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-1713"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.409em, 1001.88em, 2.346em, -999.997em); top: -2.602em; left: 50%; margin-left: -0.935em;"><span class="mrow" id="MathJax-Span-1714"><span class="mn" id="MathJax-Span-1715" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-1716" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="msubsup" id="MathJax-Span-1717"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1718" style="font-size: 70.7%; font-family: MathJax_Math-italic;">e</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.393em; left: 0.315em;"><span class="texatom" id="MathJax-Span-1719"><span class="mrow" id="MathJax-Span-1720"><span class="mo" id="MathJax-Span-1721" style="font-size: 50%; font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-1722" style="font-size: 50%; font-family: MathJax_Math-italic;">c</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.32em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.154em;"><span class="mi" id="MathJax-Span-1723" style="font-size: 70.7%; font-family: MathJax_Math-italic;">c</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.98em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.982em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-1724" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mfrac><mrow><mn>1</mn><mo>−</mo><msup><mi>e</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mi>c</mi></mrow></msup></mrow><mi>c</mi></mfrac><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-277">(\frac{1-e^{-c}}{c})</script>-approximation to the best comparator in hindsight, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-278-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1725" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1726"><span class="msubsup" id="MathJax-Span-1727"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1728" style="font-family: MathJax_Math-italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="texatom" id="MathJax-Span-1729"><span class="mrow" id="MathJax-Span-1730"><span class="mi" id="MathJax-Span-1731" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-278">C_{T}</script> is the deviation of maximizer sequence, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-279-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1732" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.58em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1733"><span class="mi" id="MathJax-Span-1734" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></span></span><script type="math/tex" id="MathJax-Element-279">\beta</script> is the spectral gap of the network and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-280-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1735" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1736"><span class="mi" id="MathJax-Span-1737" style="font-family: MathJax_Math-italic;">c</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi></math></span></span><script type="math/tex" id="MathJax-Element-280">c</script> is the joint curvature of submodular objectives. This result significantly improves the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-281-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1738" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1002.19em, 2.711em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1739"><span class="mo" id="MathJax-Span-1740" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-1741"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-1742" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1001.2em, 2.346em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.57em;"><span class="mrow" id="MathJax-Span-1743"><span class="mn" id="MathJax-Span-1744" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-1745" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-1746" style="font-size: 70.7%; font-family: MathJax_Math-italic;">c</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.3em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.305em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-1747" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>c</mi></mrow></mfrac><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-281">(\frac{1}{1+c})</script>-approximation provided by the state-of-the-art OSG algorithm. Finally, we demonstrate the effectiveness of our proposed algorithms through simulation-based multi-target tracking.</p>
            <p id="subjects-i8dYPGdB1C@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-i8dYPGdB1C@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-i8dYPGdB1C@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-i8dYPGdB1C@OpenReview" onclick="foldPdfKimi('i8dYPGdB1C@OpenReview', this)" class="hr hr-fold">
        </div><div id="Fs9EabmQrJ@OpenReview" class="panel paper" keywords="embedllm,representations,language,routing,models,embedder,compact,downstream,llms,facilitate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Fs9EabmQrJ" target="_blank" title="363/373"><span class="index notranslate">#363</span></a>
                <a id="title-Fs9EabmQrJ@OpenReview" class="title-link" href="/venue/Fs9EabmQrJ@OpenReview" target="_blank">EmbedLLM: Learning Compact Representations of Large Language Models</a>
                <a id="pdf-Fs9EabmQrJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Fs9EabmQrJ@OpenReview', this)" data="https://openreview.net/pdf?id=Fs9EabmQrJ">[PDF<sup id="pdf-stars-Fs9EabmQrJ@OpenReview">4</sup>]</a>
                <a id="copy-Fs9EabmQrJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Fs9EabmQrJ@OpenReview')">[Copy]</a>
                <a id="kimi-Fs9EabmQrJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Fs9EabmQrJ@OpenReview', this)">[Kimi<sup id="kimi-stars-Fs9EabmQrJ@OpenReview">5</sup>]</a>
                <a id="rel-Fs9EabmQrJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Fs9EabmQrJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Fs9EabmQrJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Zhuang" target="_blank">Richard Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianhao Wu" target="_blank">Tianhao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaojin Wen" target="_blank">Zhaojin Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Li" target="_blank">Andrew Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiantao Jiao" target="_blank">Jiantao Jiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kannan Ramchandran" target="_blank">Kannan Ramchandran</a>
            </p>
            <p id="summary-Fs9EabmQrJ@OpenReview" class="summary">With hundreds of thousands of language models available on Huggingface today, efficiently evaluating and utilizing these models across various downstream tasks has become increasingly critical. Many existing methods repeatedly learn task-specific representations of Large Language Models (LLMs), which leads to inefficiencies in both time and computational resources. To address this, we propose EmbedLLM, a framework designed to learn compact vector representations of LLMs that facilitate downstream applications involving many models, such as model routing. We introduce an encoder-decoder approach for learning such embedding, along with a systematic framework to evaluate their effectiveness. Empirical results show that EmbedLLM outperforms prior methods in model routing. Additionally, we demonstrate that our method can forecast a model's performance on multiple benchmarks, without incurring additional inference cost. Extensive probing experiments validate that the learned embeddings capture key model characteristics, e.g. whether the model is specialized for coding tasks, even without being explicitly trained on them. We open source our dataset, code and embedder to facilitate further research and application.</p>
            <p id="subjects-Fs9EabmQrJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Fs9EabmQrJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fs9EabmQrJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fs9EabmQrJ@OpenReview" onclick="foldPdfKimi('Fs9EabmQrJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="rxVvRBgqmS@OpenReview" class="panel paper" keywords="pianomotion10m,piano,presses,hand,music,audios,dataset,motion,movements,playing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rxVvRBgqmS" target="_blank" title="364/373"><span class="index notranslate">#364</span></a>
                <a id="title-rxVvRBgqmS@OpenReview" class="title-link" href="/venue/rxVvRBgqmS@OpenReview" target="_blank">PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance</a>
                <a id="pdf-rxVvRBgqmS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rxVvRBgqmS@OpenReview', this)" data="https://openreview.net/pdf?id=rxVvRBgqmS">[PDF<sup id="pdf-stars-rxVvRBgqmS@OpenReview">3</sup>]</a>
                <a id="copy-rxVvRBgqmS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rxVvRBgqmS@OpenReview')">[Copy]</a>
                <a id="kimi-rxVvRBgqmS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rxVvRBgqmS@OpenReview', this)">[Kimi<sup id="kimi-stars-rxVvRBgqmS@OpenReview"></sup>]</a>
                <a id="rel-rxVvRBgqmS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rxVvRBgqmS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rxVvRBgqmS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qijun Gan" target="_blank">Qijun Gan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Wang" target="_blank">Song Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengtao Wu" target="_blank">Shengtao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianke Zhu" target="_blank">Jianke Zhu</a>
            </p>
            <p id="summary-rxVvRBgqmS@OpenReview" class="summary">Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The dataset and source code can be accessed at https://github.com/PianoMotion10M/PianoMotion10M.</p>
            <p id="subjects-rxVvRBgqmS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-rxVvRBgqmS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rxVvRBgqmS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rxVvRBgqmS@OpenReview" onclick="foldPdfKimi('rxVvRBgqmS@OpenReview', this)" class="hr hr-fold">
        </div><div id="ALzTQUgW8a@OpenReview" class="panel paper" keywords="magicpig,attention,lsh,topk,llm,sparse,sampling,110ms,computation,approximation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ALzTQUgW8a" target="_blank" title="365/373"><span class="index notranslate">#365</span></a>
                <a id="title-ALzTQUgW8a@OpenReview" class="title-link" href="/venue/ALzTQUgW8a@OpenReview" target="_blank">MagicPIG: LSH Sampling for Efficient LLM Generation</a>
                <a id="pdf-ALzTQUgW8a@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ALzTQUgW8a@OpenReview', this)" data="https://openreview.net/pdf?id=ALzTQUgW8a">[PDF<sup id="pdf-stars-ALzTQUgW8a@OpenReview">6</sup>]</a>
                <a id="copy-ALzTQUgW8a@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ALzTQUgW8a@OpenReview')">[Copy]</a>
                <a id="kimi-ALzTQUgW8a@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ALzTQUgW8a@OpenReview', this)">[Kimi<sup id="kimi-stars-ALzTQUgW8a@OpenReview">8</sup>]</a>
                <a id="rel-ALzTQUgW8a@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ALzTQUgW8a@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ALzTQUgW8a@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoming Chen" target="_blank">Zhuoming Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ranajoy Sadhukhan" target="_blank">Ranajoy Sadhukhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihao Ye" target="_blank">Zihao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Zhou" target="_blank">Yang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyu Zhang" target="_blank">Jianyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Nolte" target="_blank">Niklas Nolte</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuandong Tian" target="_blank">Yuandong Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthijs Douze" target="_blank">Matthijs Douze</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leon Bottou" target="_blank">Leon Bottou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Jia" target="_blank">Zhihao Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beidi Chen" target="_blank">Beidi Chen</a>
            </p>
            <p id="summary-ALzTQUgW8a@OpenReview" class="summary">Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, now becomes a bottleneck. Leveraging the common insight that attention is sparse, various dynamic sparse or TopK-based attention approximation methods have been proposed. In this paper, we first show that TopK attention itself suffers from a quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on CPU, which allows to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-282-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1.9&lt;/mn&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mn&gt;3.9&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1748" style="width: 5.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.638em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1004.48em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1749"><span class="mn" id="MathJax-Span-1750" style="font-family: MathJax_Main;">1.9</span><span class="mo" id="MathJax-Span-1751" style="font-family: MathJax_Main; padding-left: 0.263em;">∼</span><span class="mn" id="MathJax-Span-1752" style="font-family: MathJax_Main; padding-left: 0.263em;">3.9</span><span class="mo" id="MathJax-Span-1753" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.9</mn><mo>∼</mo><mn>3.9</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-282">1.9\sim3.9\times</script> across various GPU hardware and achieve 110ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens.</p>
            <p id="subjects-ALzTQUgW8a@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ALzTQUgW8a@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ALzTQUgW8a@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ALzTQUgW8a@OpenReview" onclick="foldPdfKimi('ALzTQUgW8a@OpenReview', this)" class="hr hr-fold">
        </div><div id="dGVZwyq5tV@OpenReview" class="panel paper" keywords="sparsity,teal,activation,llama,ctivation,fre,training,language,raining,mistral">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dGVZwyq5tV" target="_blank" title="366/373"><span class="index notranslate">#366</span></a>
                <a id="title-dGVZwyq5tV@OpenReview" class="title-link" href="/venue/dGVZwyq5tV@OpenReview" target="_blank">Training-Free Activation Sparsity in Large Language Models</a>
                <a id="pdf-dGVZwyq5tV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dGVZwyq5tV@OpenReview', this)" data="https://openreview.net/pdf?id=dGVZwyq5tV">[PDF<sup id="pdf-stars-dGVZwyq5tV@OpenReview">7</sup>]</a>
                <a id="copy-dGVZwyq5tV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dGVZwyq5tV@OpenReview')">[Copy]</a>
                <a id="kimi-dGVZwyq5tV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dGVZwyq5tV@OpenReview', this)">[Kimi<sup id="kimi-stars-dGVZwyq5tV@OpenReview">7</sup>]</a>
                <a id="rel-dGVZwyq5tV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dGVZwyq5tV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dGVZwyq5tV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=James Liu" target="_blank">James Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pragaash Ponnusamy" target="_blank">Pragaash Ponnusamy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianle Cai" target="_blank">Tianle Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=placeholder" target="_blank">placeholder</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yoon Kim" target="_blank">Yoon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Athiwaratkun" target="_blank">Ben Athiwaratkun</a>
            </p>
            <p id="summary-dGVZwyq5tV@OpenReview" class="summary">Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL (**T**raining-Fre**e** **A**ctivation Sparsity in **L**LMs), a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50\% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53× and 1.8× at 40\% and 50\% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.</p>
            <p id="subjects-dGVZwyq5tV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dGVZwyq5tV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dGVZwyq5tV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dGVZwyq5tV@OpenReview" onclick="foldPdfKimi('dGVZwyq5tV@OpenReview', this)" class="hr hr-fold">
        </div><div id="kam84eEmub@OpenReview" class="panel paper" keywords="layerdag,dags,dependencies,autoregressive,benchmarking,acyclic,computing,dag,layerwise,directed">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kam84eEmub" target="_blank" title="367/373"><span class="index notranslate">#367</span></a>
                <a id="title-kam84eEmub@OpenReview" class="title-link" href="/venue/kam84eEmub@OpenReview" target="_blank">LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation</a>
                <a id="pdf-kam84eEmub@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kam84eEmub@OpenReview', this)" data="https://openreview.net/pdf?id=kam84eEmub">[PDF<sup id="pdf-stars-kam84eEmub@OpenReview">4</sup>]</a>
                <a id="copy-kam84eEmub@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kam84eEmub@OpenReview')">[Copy]</a>
                <a id="kimi-kam84eEmub@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kam84eEmub@OpenReview', this)">[Kimi<sup id="kimi-stars-kam84eEmub@OpenReview"></sup>]</a>
                <a id="rel-kam84eEmub@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kam84eEmub@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kam84eEmub@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mufei Li" target="_blank">Mufei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viraj Shitole" target="_blank">Viraj Shitole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eli Chien" target="_blank">Eli Chien</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changhai Man" target="_blank">Changhai Man</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaodong Wang" target="_blank">Zhaodong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Srinivas" target="_blank">Srinivas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Zhang" target="_blank">Ying Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tushar Krishna" target="_blank">Tushar Krishna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pan Li" target="_blank">Pan Li</a>
            </p>
            <p id="summary-kam84eEmub@OpenReview" class="summary">Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes—a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms.</p>
            <p id="subjects-kam84eEmub@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-kam84eEmub@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kam84eEmub@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kam84eEmub@OpenReview" onclick="foldPdfKimi('kam84eEmub@OpenReview', this)" class="hr hr-fold">
        </div><div id="fxv0FfmDAg@OpenReview" class="panel paper" keywords="pruning,drop,distributionally,data,robust,uninformative,performance,tolerable,exceptionally,hungry">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fxv0FfmDAg" target="_blank" title="368/373"><span class="index notranslate">#368</span></a>
                <a id="title-fxv0FfmDAg@OpenReview" class="title-link" href="/venue/fxv0FfmDAg@OpenReview" target="_blank">DRoP: Distributionally Robust Data Pruning</a>
                <a id="pdf-fxv0FfmDAg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fxv0FfmDAg@OpenReview', this)" data="https://openreview.net/pdf?id=fxv0FfmDAg">[PDF<sup id="pdf-stars-fxv0FfmDAg@OpenReview">2</sup>]</a>
                <a id="copy-fxv0FfmDAg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fxv0FfmDAg@OpenReview')">[Copy]</a>
                <a id="kimi-fxv0FfmDAg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fxv0FfmDAg@OpenReview', this)">[Kimi<sup id="kimi-stars-fxv0FfmDAg@OpenReview">2</sup>]</a>
                <a id="rel-fxv0FfmDAg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fxv0FfmDAg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fxv0FfmDAg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Artem Vysogorets" target="_blank">Artem Vysogorets</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kartik Ahuja" target="_blank">Kartik Ahuja</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julia Kempe" target="_blank">Julia Kempe</a>
            </p>
            <p id="summary-fxv0FfmDAg@OpenReview" class="summary">In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. We present theoretical analysis of the classification risk in a mixture of Gaussians to argue that choosing appropriate class pruning ratios, coupled with random pruning within classes has potential to improve worst-class performance. We thus propose DRoP, a distributionally robust approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving distributional robustness at a tolerable drop of average performance as we prune more from the datasets.</p>
            <p id="subjects-fxv0FfmDAg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-fxv0FfmDAg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fxv0FfmDAg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fxv0FfmDAg@OpenReview" onclick="foldPdfKimi('fxv0FfmDAg@OpenReview', this)" class="hr hr-fold">
        </div><div id="NQqJPPCesd@OpenReview" class="panel paper" keywords="sliced,barycenter,mfswb,swb,marginal,fairness,wasserstein,marginals,surrogate,slicing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NQqJPPCesd" target="_blank" title="369/373"><span class="index notranslate">#369</span></a>
                <a id="title-NQqJPPCesd@OpenReview" class="title-link" href="/venue/NQqJPPCesd@OpenReview" target="_blank">Towards Marginal Fairness Sliced Wasserstein Barycenter</a>
                <a id="pdf-NQqJPPCesd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NQqJPPCesd@OpenReview', this)" data="https://openreview.net/pdf?id=NQqJPPCesd">[PDF<sup id="pdf-stars-NQqJPPCesd@OpenReview">2</sup>]</a>
                <a id="copy-NQqJPPCesd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NQqJPPCesd@OpenReview')">[Copy]</a>
                <a id="kimi-NQqJPPCesd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NQqJPPCesd@OpenReview', this)">[Kimi<sup id="kimi-stars-NQqJPPCesd@OpenReview">1</sup>]</a>
                <a id="rel-NQqJPPCesd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NQqJPPCesd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NQqJPPCesd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Khai Nguyen" target="_blank">Khai Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hai Nguyen" target="_blank">Hai Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nhat Ho" target="_blank">Nhat Ho</a>
            </p>
            <p id="summary-NQqJPPCesd@OpenReview" class="summary">The Sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems.</p>
            <p id="subjects-NQqJPPCesd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-NQqJPPCesd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NQqJPPCesd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NQqJPPCesd@OpenReview" onclick="foldPdfKimi('NQqJPPCesd@OpenReview', this)" class="hr hr-fold">
        </div><div id="UqYNPyotxL@OpenReview" class="panel paper" keywords="lmc,invariances,tree,invariance,connectivity,differentiable,ensembles,permutation,mode,achieving">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UqYNPyotxL" target="_blank" title="370/373"><span class="index notranslate">#370</span></a>
                <a id="title-UqYNPyotxL@OpenReview" class="title-link" href="/venue/UqYNPyotxL@OpenReview" target="_blank">Linear Mode Connectivity in Differentiable Tree Ensembles</a>
                <a id="pdf-UqYNPyotxL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UqYNPyotxL@OpenReview', this)" data="https://openreview.net/pdf?id=UqYNPyotxL">[PDF<sup id="pdf-stars-UqYNPyotxL@OpenReview">2</sup>]</a>
                <a id="copy-UqYNPyotxL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UqYNPyotxL@OpenReview')">[Copy]</a>
                <a id="kimi-UqYNPyotxL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UqYNPyotxL@OpenReview', this)">[Kimi<sup id="kimi-stars-UqYNPyotxL@OpenReview">1</sup>]</a>
                <a id="rel-UqYNPyotxL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UqYNPyotxL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UqYNPyotxL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ryuichi Kanoh" target="_blank">Ryuichi Kanoh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahito Sugiyama" target="_blank">Mahito Sugiyama</a>
            </p>
            <p id="summary-UqYNPyotxL@OpenReview" class="summary">Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.</p>
            <p id="subjects-UqYNPyotxL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-UqYNPyotxL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UqYNPyotxL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UqYNPyotxL@OpenReview" onclick="foldPdfKimi('UqYNPyotxL@OpenReview', this)" class="hr hr-fold">
        </div><div id="pPyJyeLriR@OpenReview" class="panel paper" keywords="unlearning,scalegun,certified,graph,node,propagation,scalable,edge,certifiable,embeddings">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pPyJyeLriR" target="_blank" title="371/373"><span class="index notranslate">#371</span></a>
                <a id="title-pPyJyeLriR@OpenReview" class="title-link" href="/venue/pPyJyeLriR@OpenReview" target="_blank">Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier</a>
                <a id="pdf-pPyJyeLriR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pPyJyeLriR@OpenReview', this)" data="https://openreview.net/pdf?id=pPyJyeLriR">[PDF<sup id="pdf-stars-pPyJyeLriR@OpenReview">3</sup>]</a>
                <a id="copy-pPyJyeLriR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pPyJyeLriR@OpenReview')">[Copy]</a>
                <a id="kimi-pPyJyeLriR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pPyJyeLriR@OpenReview', this)">[Kimi<sup id="kimi-stars-pPyJyeLriR@OpenReview">1</sup>]</a>
                <a id="rel-pPyJyeLriR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pPyJyeLriR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pPyJyeLriR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi" target="_blank">Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhewei Wei" target="_blank">Zhewei Wei</a>
            </p>
            <p id="summary-pPyJyeLriR@OpenReview" class="summary">Graph unlearning has emerged as a pivotal research area for ensuring privacy protection, given the widespread adoption of Graph Neural Networks (GNNs) in applications involving sensitive user data. Among existing studies, certified graph unlearning is distinguished by providing robust privacy guarantees. However, current certified graph unlearning methods are impractical for large-scale graphs because they necessitate the costly re-computation of graph propagation for each unlearning request. Although numerous scalable techniques have been developed to accelerate graph propagation for GNNs, their integration into certified graph unlearning remains uncertain as these scalable approaches introduce approximation errors into node embeddings. In contrast, certified graph unlearning demands bounded model error on exact node embeddings to maintain its certified guarantee. To address this challenge, we present ScaleGUN, the first approach to scale certified graph unlearning to billion-edge graphs. ScaleGUN integrates the approximate graph propagation technique into certified graph unlearning, offering certified guarantees for three unlearning scenarios: node feature, edge and node unlearning. Extensive experiments on real-world datasets demonstrate the efficiency and unlearning efficacy of ScaleGUN. Remarkably, ScaleGUN accomplishes <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-283-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x03B4;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1754" style="width: 8.44em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1006.93em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1755"><span class="mo" id="MathJax-Span-1756" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1757" style="font-family: MathJax_Math-italic;">ϵ</span><span class="mo" id="MathJax-Span-1758" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-1759" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">δ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1760" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-1761" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mo" id="MathJax-Span-1762" style="font-family: MathJax_Main; padding-left: 0.263em;">(</span><span class="mn" id="MathJax-Span-1763" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-1764" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-1765" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-1766" style="font-family: MathJax_Main;">10</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.992em;"><span class="texatom" id="MathJax-Span-1767"><span class="mrow" id="MathJax-Span-1768"><span class="mo" id="MathJax-Span-1769" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-1770" style="font-size: 70.7%; font-family: MathJax_Main;">4</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-1771" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>ϵ</mi><mo>,</mo><mi>δ</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><msup><mn>10</mn><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>4</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-283">(\epsilon,\delta)=(1,10^{-4})</script> certified unlearning on the billion-edge graph ogbn-papers100M in 20 seconds for a 5,000 random edge removal request -- of which only 5 seconds are required for updating the node embeddings -- compared to 1.91 hours for retraining and 1.89 hours for re-propagation. Our code is available at https://anonymous.4open.science/r/ScaleGUN-5921.</p>
            <p id="subjects-pPyJyeLriR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pPyJyeLriR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pPyJyeLriR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pPyJyeLriR@OpenReview" onclick="foldPdfKimi('pPyJyeLriR@OpenReview', this)" class="hr hr-fold">
        </div><div id="SjufxrSOYd@OpenReview" class="panel paper" keywords="iwns,graphon,mpnns,ign,igns,graphons,cut,invariant,transferability,gnns">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SjufxrSOYd" target="_blank" title="372/373"><span class="index notranslate">#372</span></a>
                <a id="title-SjufxrSOYd@OpenReview" class="title-link" href="/venue/SjufxrSOYd@OpenReview" target="_blank">Invariant Graphon Networks: Approximation and Cut Distance</a>
                <a id="pdf-SjufxrSOYd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SjufxrSOYd@OpenReview', this)" data="https://openreview.net/pdf?id=SjufxrSOYd">[PDF<sup id="pdf-stars-SjufxrSOYd@OpenReview">7</sup>]</a>
                <a id="copy-SjufxrSOYd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SjufxrSOYd@OpenReview')">[Copy]</a>
                <a id="kimi-SjufxrSOYd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SjufxrSOYd@OpenReview', this)">[Kimi<sup id="kimi-stars-SjufxrSOYd@OpenReview">3</sup>]</a>
                <a id="rel-SjufxrSOYd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SjufxrSOYd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SjufxrSOYd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Herbst" target="_blank">Daniel Herbst</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefanie Jegelka" target="_blank">Stefanie Jegelka</a>
            </p>
            <p id="summary-SjufxrSOYd@OpenReview" class="summary">Graph limit models, like graphons for limits of dense graphs, have recently been used as a tool to study size transferability of graph neural networks (GNNs). While most existing literature focuses on message passing GNNs (MPNNs), we attend to *Invariant Graph Networks* (IGNs), a powerful alternative GNN architecture. In this work, we generalize IGNs to graphons, introducing *Invariant Graphon Networks (IWNs)* which are defined using a subset of the IGN basis corresponding to bounded linear operators. Even with this restricted basis, we show that IWNs of order <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-284-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1772" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.19em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1773"><span class="mi" id="MathJax-Span-1774" style="font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-1775" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mn" id="MathJax-Span-1776" style="font-family: MathJax_Main; padding-left: 0.211em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo>+</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-284">k + 1</script> are at least as powerful as the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-285-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1777" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1778"><span class="mi" id="MathJax-Span-1779" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-285">k</script>-dimensional Weisfeiler-Leman (WL) test for graphon-signals and we establish universal approximation results for graphon-signals in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-286-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1780" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1781"><span class="msubsup" id="MathJax-Span-1782"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="texatom" id="MathJax-Span-1783"><span class="mrow" id="MathJax-Span-1784"><span class="mi" id="MathJax-Span-1785" style="font-family: MathJax_Caligraphic;">L</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.68em;"><span class="mi" id="MathJax-Span-1786" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">L</mi></mrow><mi>p</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-286">\mathcal{L}^p</script> distances using *signal-weighted homomorphism densities*. This significantly extends the prior work of Cai &amp; Wang (2022), showing that IWNs—a subset of their *IGN-small*—retain effectively the same expressivity as the full IGN basis in the limit. In contrast to their approach, our blueprint of IWNs also aligns better with the geometry of graphon space, for example facilitating comparability to MPNNs. We also highlight that, unlike other GNN architectures such as MPNNs, IWNs are *discontinuous with respect to cut distance*, which causes their lack of convergence and is inherently tied to the definition of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-287-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1787" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1788"><span class="mi" id="MathJax-Span-1789" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-287">k</script>-WL. Yet, their transferability remains comparable to MPNNs.</p>
            <p id="subjects-SjufxrSOYd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SjufxrSOYd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SjufxrSOYd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SjufxrSOYd@OpenReview" onclick="foldPdfKimi('SjufxrSOYd@OpenReview', this)" class="hr hr-fold">
        </div><div id="Bk13Qfu8Ru@OpenReview" class="panel paper" keywords="spurious,correlations,pruning,severing,settings,signal,samples,information,data,malfunction">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Bk13Qfu8Ru" target="_blank" title="373/373"><span class="index notranslate">#373</span></a>
                <a id="title-Bk13Qfu8Ru@OpenReview" class="title-link" href="/venue/Bk13Qfu8Ru@OpenReview" target="_blank">Severing Spurious Correlations with Data Pruning</a>
                <a id="pdf-Bk13Qfu8Ru@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Bk13Qfu8Ru@OpenReview', this)" data="https://openreview.net/pdf?id=Bk13Qfu8Ru">[PDF<sup id="pdf-stars-Bk13Qfu8Ru@OpenReview">2</sup>]</a>
                <a id="copy-Bk13Qfu8Ru@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Bk13Qfu8Ru@OpenReview')">[Copy]</a>
                <a id="kimi-Bk13Qfu8Ru@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Bk13Qfu8Ru@OpenReview', this)">[Kimi<sup id="kimi-stars-Bk13Qfu8Ru@OpenReview">3</sup>]</a>
                <a id="rel-Bk13Qfu8Ru@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Bk13Qfu8Ru@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Bk13Qfu8Ru@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Varun Mulchandani" target="_blank">Varun Mulchandani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jung-Eun Kim" target="_blank">Jung-Eun Kim</a>
            </p>
            <p id="summary-Bk13Qfu8Ru@OpenReview" class="summary">Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the formation of such correlations, recent studies propose approaches that yield promising results. These works, however, study settings where the strength of the spurious signal is significantly greater than that of the core, invariant signal, making it easier to detect the presence of spurious features in individual training samples and allow for further processing. In this paper, we identify new settings where the strength of the spurious signal is relatively weaker, making it difficult to detect any spurious information while continuing to have catastrophic consequences. We also learn that spurious correlations are formed primarily due to only a handful of all the samples containing the spurious feature and develop a novel data pruning technique that identifies and prunes small subsets of the training data that contain these samples. Our proposed technique does not require information regarding the sample-wise presence or nature of spurious information, or human intervention. Finally, we show that such data pruning attains state-of-the-art performance on previously studied settings where spurious information is identifiable.</p>
            <p id="subjects-Bk13Qfu8Ru@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Spotlight" target="_blank">ICLR.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Bk13Qfu8Ru@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bk13Qfu8Ru@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bk13Qfu8Ru@OpenReview" onclick="foldPdfKimi('Bk13Qfu8Ru@OpenReview', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-TeVAZXr3yv@OpenReview" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#TeVAZXr3yv@OpenReview">MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</a>
                    <a class="i-star" onclick="toggleAppStar('TeVAZXr3yv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TeVAZXr3yv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Im2neAMlre@OpenReview" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#Im2neAMlre@OpenReview">One slice is not enough: In search of stable conclusions in text-to-image evaluation</a>
                    <a class="i-star" onclick="toggleAppStar('Im2neAMlre@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Im2neAMlre@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-FPfCUJTsCn@OpenReview" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#FPfCUJTsCn@OpenReview">Differentiable Integer Linear Programming</a>
                    <a class="i-star" onclick="toggleAppStar('FPfCUJTsCn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FPfCUJTsCn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-FDnZFpHmU4@OpenReview" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#FDnZFpHmU4@OpenReview">Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling</a>
                    <a class="i-star" onclick="toggleAppStar('FDnZFpHmU4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FDnZFpHmU4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-3b9SKkRAKw@OpenReview" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#3b9SKkRAKw@OpenReview">LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('3b9SKkRAKw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3b9SKkRAKw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-OwpLQrpdwE@OpenReview" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#OwpLQrpdwE@OpenReview">Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels</a>
                    <a class="i-star" onclick="toggleAppStar('OwpLQrpdwE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OwpLQrpdwE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-oQ4igHyh3N@OpenReview" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#oQ4igHyh3N@OpenReview">TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters</a>
                    <a class="i-star" onclick="toggleAppStar('oQ4igHyh3N@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oQ4igHyh3N@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-xaYlO03tIk@OpenReview" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#xaYlO03tIk@OpenReview">Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion</a>
                    <a class="i-star" onclick="toggleAppStar('xaYlO03tIk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xaYlO03tIk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-nYpPAT4L3D@OpenReview" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#nYpPAT4L3D@OpenReview">Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('nYpPAT4L3D@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('nYpPAT4L3D@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-UvfI4grcM7@OpenReview" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#UvfI4grcM7@OpenReview">Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('UvfI4grcM7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UvfI4grcM7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Mn2qgIcIPS@OpenReview" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#Mn2qgIcIPS@OpenReview">Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs</a>
                    <a class="i-star" onclick="toggleAppStar('Mn2qgIcIPS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mn2qgIcIPS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-J9VogDTa1W@OpenReview" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#J9VogDTa1W@OpenReview">Systems with Switching Causal Relations: A Meta-Causal Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('J9VogDTa1W@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('J9VogDTa1W@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-QogcGNXJVw@OpenReview" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#QogcGNXJVw@OpenReview">The Computational Complexity of Circuit Discovery for Inner Interpretability</a>
                    <a class="i-star" onclick="toggleAppStar('QogcGNXJVw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QogcGNXJVw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-03OkC0LKDD@OpenReview" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#03OkC0LKDD@OpenReview">The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed Learning</a>
                    <a class="i-star" onclick="toggleAppStar('03OkC0LKDD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('03OkC0LKDD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-M7KyLjuN0A@OpenReview" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#M7KyLjuN0A@OpenReview">DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</a>
                    <a class="i-star" onclick="toggleAppStar('M7KyLjuN0A@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('M7KyLjuN0A@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-cTR17xl89h@OpenReview" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#cTR17xl89h@OpenReview">BodyGen: Advancing Towards Efficient Embodiment Co-Design</a>
                    <a class="i-star" onclick="toggleAppStar('cTR17xl89h@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cTR17xl89h@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-7BQkXXM8Fy@OpenReview" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#7BQkXXM8Fy@OpenReview">What Makes a Good Diffusion Planner for Decision Making?</a>
                    <a class="i-star" onclick="toggleAppStar('7BQkXXM8Fy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7BQkXXM8Fy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-mOpNrrV2zH@OpenReview" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#mOpNrrV2zH@OpenReview">CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph</a>
                    <a class="i-star" onclick="toggleAppStar('mOpNrrV2zH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mOpNrrV2zH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-wJv4AIt4sK@OpenReview" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#wJv4AIt4sK@OpenReview">Effective Interplay between Sparsity and Quantization: From Theory to Practice</a>
                    <a class="i-star" onclick="toggleAppStar('wJv4AIt4sK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wJv4AIt4sK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-7xCSK9BLPy@OpenReview" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#7xCSK9BLPy@OpenReview">Better Instruction-Following Through Minimum Bayes Risk</a>
                    <a class="i-star" onclick="toggleAppStar('7xCSK9BLPy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7xCSK9BLPy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-qtTIP5Gjc5@OpenReview" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#qtTIP5Gjc5@OpenReview">Demystifying the Token Dynamics of Deep Selective State Space Models</a>
                    <a class="i-star" onclick="toggleAppStar('qtTIP5Gjc5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qtTIP5Gjc5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-IwPXYk6BV9@OpenReview" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#IwPXYk6BV9@OpenReview">Enhancing Learning with Label Differential Privacy by Vector Approximation</a>
                    <a class="i-star" onclick="toggleAppStar('IwPXYk6BV9@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IwPXYk6BV9@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-SgymXhOEA5@OpenReview" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#SgymXhOEA5@OpenReview">Exploring the Camera bias of Person Re-identification</a>
                    <a class="i-star" onclick="toggleAppStar('SgymXhOEA5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SgymXhOEA5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-rdAbEn5DZt@OpenReview" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#rdAbEn5DZt@OpenReview">Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('rdAbEn5DZt@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rdAbEn5DZt@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-WcZLG8XxhD@OpenReview" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#WcZLG8XxhD@OpenReview">Learning-Augmented Frequent Directions</a>
                    <a class="i-star" onclick="toggleAppStar('WcZLG8XxhD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WcZLG8XxhD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-2o58Mbqkd2@OpenReview" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#2o58Mbqkd2@OpenReview">The Superposition of Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('2o58Mbqkd2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2o58Mbqkd2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xak8c9l1nu@OpenReview" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#xak8c9l1nu@OpenReview">Computational Explorations of Total Variation Distance</a>
                    <a class="i-star" onclick="toggleAppStar('xak8c9l1nu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xak8c9l1nu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hJ1BaJ5ELp@OpenReview" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#hJ1BaJ5ELp@OpenReview">Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation</a>
                    <a class="i-star" onclick="toggleAppStar('hJ1BaJ5ELp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hJ1BaJ5ELp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GjM61KRiTG@OpenReview" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#GjM61KRiTG@OpenReview">Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('GjM61KRiTG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GjM61KRiTG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-D042vFwJAM@OpenReview" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#D042vFwJAM@OpenReview">Physics-aligned field reconstruction with diffusion bridge</a>
                    <a class="i-star" onclick="toggleAppStar('D042vFwJAM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('D042vFwJAM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9WYMDgxDac@OpenReview" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#9WYMDgxDac@OpenReview">Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('9WYMDgxDac@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9WYMDgxDac@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xGs7Ch3Vyo@OpenReview" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#xGs7Ch3Vyo@OpenReview">Better autoregressive regression with LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('xGs7Ch3Vyo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xGs7Ch3Vyo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-VeMC6Bn0ZB@OpenReview" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#VeMC6Bn0ZB@OpenReview">Learning to Solve Differential Equation Constrained Optimization Problems</a>
                    <a class="i-star" onclick="toggleAppStar('VeMC6Bn0ZB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VeMC6Bn0ZB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ofuLWn8DFZ@OpenReview" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#ofuLWn8DFZ@OpenReview">Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning</a>
                    <a class="i-star" onclick="toggleAppStar('ofuLWn8DFZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ofuLWn8DFZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-S85PP4xjFD@OpenReview" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#S85PP4xjFD@OpenReview">ContraFusion: Contrastively Improving Compositional Understanding in Diffusion Models via Fine-Grained Negative Images</a>
                    <a class="i-star" onclick="toggleAppStar('S85PP4xjFD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('S85PP4xjFD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-scI9307PLG@OpenReview" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#scI9307PLG@OpenReview">Bundle Neural Network for message diffusion on graphs</a>
                    <a class="i-star" onclick="toggleAppStar('scI9307PLG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('scI9307PLG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sKYHBTAxVa@OpenReview" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#sKYHBTAxVa@OpenReview">LiveBench: A Challenging, Contamination-Free LLM Benchmark</a>
                    <a class="i-star" onclick="toggleAppStar('sKYHBTAxVa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sKYHBTAxVa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-JAMxRSXLFz@OpenReview" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#JAMxRSXLFz@OpenReview">Active Task Disambiguation with LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('JAMxRSXLFz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('JAMxRSXLFz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FxNNiUgtfa@OpenReview" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#FxNNiUgtfa@OpenReview">Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws</a>
                    <a class="i-star" onclick="toggleAppStar('FxNNiUgtfa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FxNNiUgtfa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ykuc5q381b@OpenReview" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#ykuc5q381b@OpenReview">BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('ykuc5q381b@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ykuc5q381b@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-yVQcr4qjD6@OpenReview" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#yVQcr4qjD6@OpenReview">Robust Function-Calling for On-Device Language Model via Function Masking</a>
                    <a class="i-star" onclick="toggleAppStar('yVQcr4qjD6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('yVQcr4qjD6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wmV4cIbgl6@OpenReview" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#wmV4cIbgl6@OpenReview">CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series</a>
                    <a class="i-star" onclick="toggleAppStar('wmV4cIbgl6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wmV4cIbgl6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wkHcXDv7cv@OpenReview" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#wkHcXDv7cv@OpenReview">Tuning Frequency Bias of State Space Models</a>
                    <a class="i-star" onclick="toggleAppStar('wkHcXDv7cv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wkHcXDv7cv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wg3rBImn3O@OpenReview" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#wg3rBImn3O@OpenReview">Provably Accurate Shapley Value Estimation via Leverage Score Sampling</a>
                    <a class="i-star" onclick="toggleAppStar('wg3rBImn3O@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wg3rBImn3O@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wN3KaUXA5X@OpenReview" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#wN3KaUXA5X@OpenReview">Diffusion On Syntax Trees For Program Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('wN3KaUXA5X@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wN3KaUXA5X@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wFD16gwpze@OpenReview" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#wFD16gwpze@OpenReview">Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra</a>
                    <a class="i-star" onclick="toggleAppStar('wFD16gwpze@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wFD16gwpze@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vVCHWVBsLH@OpenReview" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#vVCHWVBsLH@OpenReview">Decomposition Polyhedra of Piecewise Linear Functions</a>
                    <a class="i-star" onclick="toggleAppStar('vVCHWVBsLH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vVCHWVBsLH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-v9EjwMM55Y@OpenReview" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#v9EjwMM55Y@OpenReview">UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery</a>
                    <a class="i-star" onclick="toggleAppStar('v9EjwMM55Y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('v9EjwMM55Y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-twEvvkQqPS@OpenReview" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#twEvvkQqPS@OpenReview">Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems</a>
                    <a class="i-star" onclick="toggleAppStar('twEvvkQqPS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('twEvvkQqPS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sZQRUrvLn4@OpenReview" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#sZQRUrvLn4@OpenReview">Graph Neural Networks Can (Often) Count Substructures</a>
                    <a class="i-star" onclick="toggleAppStar('sZQRUrvLn4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sZQRUrvLn4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qxRoo7ULCo@OpenReview" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#qxRoo7ULCo@OpenReview">4K4DGen: Panoramic 4D Generation at 4K Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('qxRoo7ULCo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qxRoo7ULCo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qPx3i9sMxv@OpenReview" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#qPx3i9sMxv@OpenReview">Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation</a>
                    <a class="i-star" onclick="toggleAppStar('qPx3i9sMxv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qPx3i9sMxv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-q3EbOXb4y1@OpenReview" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#q3EbOXb4y1@OpenReview">Retri3D: 3D Neural Graphics Representation Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('q3EbOXb4y1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('q3EbOXb4y1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-p4cLtzk4oe@OpenReview" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#p4cLtzk4oe@OpenReview">Exploring Local Memorization in Diffusion Models via Bright Ending Attention</a>
                    <a class="i-star" onclick="toggleAppStar('p4cLtzk4oe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('p4cLtzk4oe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-o1Et3MogPw@OpenReview" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#o1Et3MogPw@OpenReview">Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a>
                    <a class="i-star" onclick="toggleAppStar('o1Et3MogPw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('o1Et3MogPw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-n9PDaFNi8t@OpenReview" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#n9PDaFNi8t@OpenReview">OS-ATLAS: Foundation Action Model for Generalist GUI Agents</a>
                    <a class="i-star" onclick="toggleAppStar('n9PDaFNi8t@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('n9PDaFNi8t@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-n0OtGl6VGb@OpenReview" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#n0OtGl6VGb@OpenReview">ThinK: Thinner Key Cache by Query-Driven Pruning</a>
                    <a class="i-star" onclick="toggleAppStar('n0OtGl6VGb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('n0OtGl6VGb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-m9RNBZewW2@OpenReview" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#m9RNBZewW2@OpenReview">Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('m9RNBZewW2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('m9RNBZewW2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-lzdFImKK8w@OpenReview" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#lzdFImKK8w@OpenReview">Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions</a>
                    <a class="i-star" onclick="toggleAppStar('lzdFImKK8w@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('lzdFImKK8w@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-l4fMj4Vnly@OpenReview" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#l4fMj4Vnly@OpenReview">ADIFF: Explaining audio difference using natural language</a>
                    <a class="i-star" onclick="toggleAppStar('l4fMj4Vnly@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('l4fMj4Vnly@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-q5EZ7gKcnW@OpenReview" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#q5EZ7gKcnW@OpenReview">Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision</a>
                    <a class="i-star" onclick="toggleAppStar('q5EZ7gKcnW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('q5EZ7gKcnW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kxFtMHItrf@OpenReview" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#kxFtMHItrf@OpenReview">Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('kxFtMHItrf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kxFtMHItrf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kpq3IIjUD3@OpenReview" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#kpq3IIjUD3@OpenReview">Learning local equivariant representations for quantum operators</a>
                    <a class="i-star" onclick="toggleAppStar('kpq3IIjUD3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kpq3IIjUD3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xsx3Fpo3UD@OpenReview" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#xsx3Fpo3UD@OpenReview">Advantage-Guided Distillation for Preference Alignment in Small Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('xsx3Fpo3UD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xsx3Fpo3UD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jXvwJ51vcK@OpenReview" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#jXvwJ51vcK@OpenReview">Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('jXvwJ51vcK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jXvwJ51vcK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jXLiDKsuDo@OpenReview" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#jXLiDKsuDo@OpenReview">SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('jXLiDKsuDo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jXLiDKsuDo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-iuxaCU3DI7@OpenReview" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#iuxaCU3DI7@OpenReview">Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data</a>
                    <a class="i-star" onclick="toggleAppStar('iuxaCU3DI7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('iuxaCU3DI7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ikkvC1UnnE@OpenReview" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#ikkvC1UnnE@OpenReview">Adaptive Batch Size for Privately Finding Second-Order Stationary Points</a>
                    <a class="i-star" onclick="toggleAppStar('ikkvC1UnnE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ikkvC1UnnE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-S5Yo6w3n3f@OpenReview" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#S5Yo6w3n3f@OpenReview">ODE-based Smoothing Neural Network for Reinforcement Learning Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('S5Yo6w3n3f@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('S5Yo6w3n3f@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hpCfPEvBsr@OpenReview" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#hpCfPEvBsr@OpenReview">MixEval-X: Any-to-any Evaluations from Real-world Data Mixture</a>
                    <a class="i-star" onclick="toggleAppStar('hpCfPEvBsr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hpCfPEvBsr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hUb2At2DsQ@OpenReview" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#hUb2At2DsQ@OpenReview">Rethinking and improving autoformalization: towards a faithful metric and a Dependency Retrieval-based approach</a>
                    <a class="i-star" onclick="toggleAppStar('hUb2At2DsQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hUb2At2DsQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gYWqxXE5RJ@OpenReview" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#gYWqxXE5RJ@OpenReview">ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language</a>
                    <a class="i-star" onclick="toggleAppStar('gYWqxXE5RJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gYWqxXE5RJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gWgaypDBs8@OpenReview" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#gWgaypDBs8@OpenReview">Representative Guidance: Diffusion Model Sampling with Consistency</a>
                    <a class="i-star" onclick="toggleAppStar('gWgaypDBs8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gWgaypDBs8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-friHAl5ofG@OpenReview" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#friHAl5ofG@OpenReview">Vision Language Models are In-Context Value Learners</a>
                    <a class="i-star" onclick="toggleAppStar('friHAl5ofG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('friHAl5ofG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eW4yh6HKz4@OpenReview" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#eW4yh6HKz4@OpenReview">CBQ: Cross-Block Quantization for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('eW4yh6HKz4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eW4yh6HKz4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dRXxFEY8ZE@OpenReview" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#dRXxFEY8ZE@OpenReview">$\texttt{BirdSet}$: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics</a>
                    <a class="i-star" onclick="toggleAppStar('dRXxFEY8ZE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dRXxFEY8ZE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dDpB23VbVa@OpenReview" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#dDpB23VbVa@OpenReview">Patch-Level Training for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('dDpB23VbVa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dDpB23VbVa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hNjCVVm0EQ@OpenReview" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#hNjCVVm0EQ@OpenReview">MamKO: Mamba-based Koopman operator for modeling and predictive control</a>
                    <a class="i-star" onclick="toggleAppStar('hNjCVVm0EQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hNjCVVm0EQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-y5einmJ0Yx@OpenReview" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#y5einmJ0Yx@OpenReview">GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation</a>
                    <a class="i-star" onclick="toggleAppStar('y5einmJ0Yx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('y5einmJ0Yx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fU8H4lzkIm@OpenReview" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#fU8H4lzkIm@OpenReview">PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems</a>
                    <a class="i-star" onclick="toggleAppStar('fU8H4lzkIm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fU8H4lzkIm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-d9aWa875kj@OpenReview" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#d9aWa875kj@OpenReview">Exact Certification of (Graph) Neural Networks Against Label Poisoning</a>
                    <a class="i-star" onclick="toggleAppStar('d9aWa875kj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('d9aWa875kj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-lqTILjL6lP@OpenReview" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#lqTILjL6lP@OpenReview">RESuM: A Rare Event Surrogate Model for Physics Detector Design</a>
                    <a class="i-star" onclick="toggleAppStar('lqTILjL6lP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('lqTILjL6lP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cv2iMNWCsh@OpenReview" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#cv2iMNWCsh@OpenReview">Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification</a>
                    <a class="i-star" onclick="toggleAppStar('cv2iMNWCsh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cv2iMNWCsh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cqsw28DuMW@OpenReview" style="display:none">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#cqsw28DuMW@OpenReview">TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('cqsw28DuMW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cqsw28DuMW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cWHonXThtM@OpenReview" style="display:none">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#cWHonXThtM@OpenReview">Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('cWHonXThtM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cWHonXThtM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jCPak79Kev@OpenReview" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#jCPak79Kev@OpenReview">AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies</a>
                    <a class="i-star" onclick="toggleAppStar('jCPak79Kev@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jCPak79Kev@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cD1kl2QKv1@OpenReview" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#cD1kl2QKv1@OpenReview">One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt</a>
                    <a class="i-star" onclick="toggleAppStar('cD1kl2QKv1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cD1kl2QKv1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bjxuqI4KwU@OpenReview" style="display:none">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#bjxuqI4KwU@OpenReview">Linear SCM Identification in the Presence of Confounders and Gaussian Noise</a>
                    <a class="i-star" onclick="toggleAppStar('bjxuqI4KwU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bjxuqI4KwU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bcTjW5kS4W@OpenReview" style="display:none">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#bcTjW5kS4W@OpenReview">NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('bcTjW5kS4W@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bcTjW5kS4W@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bW9fGYo44s@OpenReview" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#bW9fGYo44s@OpenReview">MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('bW9fGYo44s@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bW9fGYo44s@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bMC1t7eLRc@OpenReview" style="display:none">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#bMC1t7eLRc@OpenReview">Harnessing Diversity for Important Data Selection in Pretraining Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('bMC1t7eLRc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bMC1t7eLRc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-auZZ2gN0ZN@OpenReview" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#auZZ2gN0ZN@OpenReview">Dense Video Object Captioning from Disjoint Supervision</a>
                    <a class="i-star" onclick="toggleAppStar('auZZ2gN0ZN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('auZZ2gN0ZN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-agHddsQhsL@OpenReview" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#agHddsQhsL@OpenReview">Targeted Attack Improves Protection against Unauthorized Diffusion Customization</a>
                    <a class="i-star" onclick="toggleAppStar('agHddsQhsL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('agHddsQhsL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aZ1gNJu8wO@OpenReview" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#aZ1gNJu8wO@OpenReview">A Geometric Framework for Understanding Memorization in Generative Models</a>
                    <a class="i-star" onclick="toggleAppStar('aZ1gNJu8wO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aZ1gNJu8wO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aX7X9z3vQS@OpenReview" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#aX7X9z3vQS@OpenReview">Recovering Manifold Structure Using Ollivier Ricci Curvature</a>
                    <a class="i-star" onclick="toggleAppStar('aX7X9z3vQS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aX7X9z3vQS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aMBSY2ebPw@OpenReview" style="display:none">
                    <span class="i-index">#96</span>
                    <a class="i-title" href="#aMBSY2ebPw@OpenReview">Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?</a>
                    <a class="i-star" onclick="toggleAppStar('aMBSY2ebPw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aMBSY2ebPw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-t8qcGXaepr@OpenReview" style="display:none">
                    <span class="i-index">#97</span>
                    <a class="i-title" href="#t8qcGXaepr@OpenReview">Uncovering Overfitting in Large Language Model Editing</a>
                    <a class="i-star" onclick="toggleAppStar('t8qcGXaepr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('t8qcGXaepr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZGkfoufDaU@OpenReview" style="display:none">
                    <span class="i-index">#98</span>
                    <a class="i-title" href="#ZGkfoufDaU@OpenReview">Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('ZGkfoufDaU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZGkfoufDaU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YwJkv2YqBq@OpenReview" style="display:none">
                    <span class="i-index">#99</span>
                    <a class="i-title" href="#YwJkv2YqBq@OpenReview">Nesterov acceleration in benignly non-convex landscapes</a>
                    <a class="i-star" onclick="toggleAppStar('YwJkv2YqBq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YwJkv2YqBq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-e1wDDFmlVu@OpenReview" style="display:none">
                    <span class="i-index">#100</span>
                    <a class="i-title" href="#e1wDDFmlVu@OpenReview">Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts</a>
                    <a class="i-star" onclick="toggleAppStar('e1wDDFmlVu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('e1wDDFmlVu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Y2RW9EVwhT@OpenReview" style="display:none">
                    <span class="i-index">#101</span>
                    <a class="i-title" href="#Y2RW9EVwhT@OpenReview">Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</a>
                    <a class="i-star" onclick="toggleAppStar('Y2RW9EVwhT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Y2RW9EVwhT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-awvJBtB2op@OpenReview" style="display:none">
                    <span class="i-index">#102</span>
                    <a class="i-title" href="#awvJBtB2op@OpenReview">Generating Freeform Endoskeletal Robots</a>
                    <a class="i-star" onclick="toggleAppStar('awvJBtB2op@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('awvJBtB2op@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YhfrKB3Ah7@OpenReview" style="display:none">
                    <span class="i-index">#103</span>
                    <a class="i-title" href="#YhfrKB3Ah7@OpenReview">PABBO: Preferential Amortized Black-Box Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('YhfrKB3Ah7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YhfrKB3Ah7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WzCEiBILHu@OpenReview" style="display:none">
                    <span class="i-index">#104</span>
                    <a class="i-title" href="#WzCEiBILHu@OpenReview">Topological Schrödinger Bridge Matching</a>
                    <a class="i-star" onclick="toggleAppStar('WzCEiBILHu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WzCEiBILHu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-z8sxoCYgmd@OpenReview" style="display:none">
                    <span class="i-index">#105</span>
                    <a class="i-title" href="#z8sxoCYgmd@OpenReview">LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models</a>
                    <a class="i-star" onclick="toggleAppStar('z8sxoCYgmd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('z8sxoCYgmd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uqWM9hBDAE@OpenReview" style="display:none">
                    <span class="i-index">#106</span>
                    <a class="i-title" href="#uqWM9hBDAE@OpenReview">How Much is Unseen Depends Chiefly on Information About the Seen</a>
                    <a class="i-star" onclick="toggleAppStar('uqWM9hBDAE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uqWM9hBDAE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tv36j85SqR@OpenReview" style="display:none">
                    <span class="i-index">#107</span>
                    <a class="i-title" href="#Tv36j85SqR@OpenReview">Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding</a>
                    <a class="i-star" onclick="toggleAppStar('Tv36j85SqR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tv36j85SqR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-TtUh0TOlGX@OpenReview" style="display:none">
                    <span class="i-index">#108</span>
                    <a class="i-title" href="#TtUh0TOlGX@OpenReview">Regularization by Texts for Latent Diffusion Inverse Solvers</a>
                    <a class="i-star" onclick="toggleAppStar('TtUh0TOlGX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TtUh0TOlGX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-TlAdgeoDTo@OpenReview" style="display:none">
                    <span class="i-index">#109</span>
                    <a class="i-title" href="#TlAdgeoDTo@OpenReview">First-Person Fairness in Chatbots</a>
                    <a class="i-star" onclick="toggleAppStar('TlAdgeoDTo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TlAdgeoDTo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uREg3OHjLL@OpenReview" style="display:none">
                    <span class="i-index">#110</span>
                    <a class="i-title" href="#uREg3OHjLL@OpenReview">On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth</a>
                    <a class="i-star" onclick="toggleAppStar('uREg3OHjLL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uREg3OHjLL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SqZ0KY4qBD@OpenReview" style="display:none">
                    <span class="i-index">#111</span>
                    <a class="i-title" href="#SqZ0KY4qBD@OpenReview">Attention with Markov: A Curious Case of Single-layer Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('SqZ0KY4qBD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SqZ0KY4qBD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rDLgnYLM5b@OpenReview" style="display:none">
                    <span class="i-index">#112</span>
                    <a class="i-title" href="#rDLgnYLM5b@OpenReview">Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment</a>
                    <a class="i-star" onclick="toggleAppStar('rDLgnYLM5b@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rDLgnYLM5b@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SRpq5OBpED@OpenReview" style="display:none">
                    <span class="i-index">#113</span>
                    <a class="i-title" href="#SRpq5OBpED@OpenReview">Meta-Dynamical State Space Models for Integrative Neural Data Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('SRpq5OBpED@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SRpq5OBpED@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SOd07Qxkw4@OpenReview" style="display:none">
                    <span class="i-index">#114</span>
                    <a class="i-title" href="#SOd07Qxkw4@OpenReview">Improved Convergence Rate for Diffusion Probabilistic Models</a>
                    <a class="i-star" onclick="toggleAppStar('SOd07Qxkw4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SOd07Qxkw4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SG1R2H3fa1@OpenReview" style="display:none">
                    <span class="i-index">#115</span>
                    <a class="i-title" href="#SG1R2H3fa1@OpenReview">Revisiting Random Walks for Learning on Graphs</a>
                    <a class="i-star" onclick="toggleAppStar('SG1R2H3fa1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SG1R2H3fa1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SyVPiehSbg@OpenReview" style="display:none">
                    <span class="i-index">#116</span>
                    <a class="i-title" href="#SyVPiehSbg@OpenReview">Deep Learning Alternatives Of The Kolmogorov Superposition Theorem</a>
                    <a class="i-star" onclick="toggleAppStar('SyVPiehSbg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SyVPiehSbg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vi3DjUhFVm@OpenReview" style="display:none">
                    <span class="i-index">#117</span>
                    <a class="i-title" href="#vi3DjUhFVm@OpenReview">Test-time Alignment of Diffusion Models without Reward Over-optimization</a>
                    <a class="i-star" onclick="toggleAppStar('vi3DjUhFVm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vi3DjUhFVm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QOfswj7hij@OpenReview" style="display:none">
                    <span class="i-index">#118</span>
                    <a class="i-title" href="#QOfswj7hij@OpenReview">Online Neuro-Symbolic Predicate Invention for High-Level Planning</a>
                    <a class="i-star" onclick="toggleAppStar('QOfswj7hij@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QOfswj7hij@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tIBAOcAvn4@OpenReview" style="display:none">
                    <span class="i-index">#119</span>
                    <a class="i-title" href="#tIBAOcAvn4@OpenReview">Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors</a>
                    <a class="i-star" onclick="toggleAppStar('tIBAOcAvn4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tIBAOcAvn4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Q0zmmNNePz@OpenReview" style="display:none">
                    <span class="i-index">#120</span>
                    <a class="i-title" href="#Q0zmmNNePz@OpenReview">Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Q0zmmNNePz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Q0zmmNNePz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PpYy0dR3Qw@OpenReview" style="display:none">
                    <span class="i-index">#121</span>
                    <a class="i-title" href="#PpYy0dR3Qw@OpenReview">LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression</a>
                    <a class="i-star" onclick="toggleAppStar('PpYy0dR3Qw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PpYy0dR3Qw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PkpNRmBZ32@OpenReview" style="display:none">
                    <span class="i-index">#122</span>
                    <a class="i-title" href="#PkpNRmBZ32@OpenReview">Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions</a>
                    <a class="i-star" onclick="toggleAppStar('PkpNRmBZ32@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PkpNRmBZ32@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wHebuIb6IH@OpenReview" style="display:none">
                    <span class="i-index">#123</span>
                    <a class="i-title" href="#wHebuIb6IH@OpenReview">VLMaterial: Procedural Material Generation with Large Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('wHebuIb6IH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wHebuIb6IH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PUnD86UEK5@OpenReview" style="display:none">
                    <span class="i-index">#124</span>
                    <a class="i-title" href="#PUnD86UEK5@OpenReview">Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity</a>
                    <a class="i-star" onclick="toggleAppStar('PUnD86UEK5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PUnD86UEK5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-j4LITBSUjs@OpenReview" style="display:none">
                    <span class="i-index">#125</span>
                    <a class="i-title" href="#j4LITBSUjs@OpenReview">PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training</a>
                    <a class="i-star" onclick="toggleAppStar('j4LITBSUjs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('j4LITBSUjs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qgsXsqahMq@OpenReview" style="display:none">
                    <span class="i-index">#126</span>
                    <a class="i-title" href="#qgsXsqahMq@OpenReview">GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('qgsXsqahMq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qgsXsqahMq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ujpAYpFDEA@OpenReview" style="display:none">
                    <span class="i-index">#127</span>
                    <a class="i-title" href="#ujpAYpFDEA@OpenReview">Can Watermarked LLMs be Identified by Users via Crafted Prompts?</a>
                    <a class="i-star" onclick="toggleAppStar('ujpAYpFDEA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ujpAYpFDEA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-P7KRIiLM8T@OpenReview" style="display:none">
                    <span class="i-index">#128</span>
                    <a class="i-title" href="#P7KRIiLM8T@OpenReview">u-$\mu$P: The Unit-Scaled Maximal Update Parametrization</a>
                    <a class="i-star" onclick="toggleAppStar('P7KRIiLM8T@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('P7KRIiLM8T@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-P42DbV2nuV@OpenReview" style="display:none">
                    <span class="i-index">#129</span>
                    <a class="i-title" href="#P42DbV2nuV@OpenReview">Instance-dependent Early Stopping</a>
                    <a class="i-star" onclick="toggleAppStar('P42DbV2nuV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('P42DbV2nuV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-oCHsDpyawq@OpenReview" style="display:none">
                    <span class="i-index">#130</span>
                    <a class="i-title" href="#oCHsDpyawq@OpenReview">ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish</a>
                    <a class="i-star" onclick="toggleAppStar('oCHsDpyawq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oCHsDpyawq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-U3PBITXNG6@OpenReview" style="display:none">
                    <span class="i-index">#131</span>
                    <a class="i-title" href="#U3PBITXNG6@OpenReview">InverseBench: Benchmarking Plug-and-Play Diffusion Models for Scientific Inverse Problems</a>
                    <a class="i-star" onclick="toggleAppStar('U3PBITXNG6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('U3PBITXNG6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Oi47wc10sm@OpenReview" style="display:none">
                    <span class="i-index">#132</span>
                    <a class="i-title" href="#Oi47wc10sm@OpenReview">Programming Refusal with Conditional Activation Steering</a>
                    <a class="i-star" onclick="toggleAppStar('Oi47wc10sm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Oi47wc10sm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qtWjSboqfe@OpenReview" style="display:none">
                    <span class="i-index">#133</span>
                    <a class="i-title" href="#qtWjSboqfe@OpenReview">DEEM: Diffusion models serve as the eyes of large language models for image perception</a>
                    <a class="i-star" onclick="toggleAppStar('qtWjSboqfe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qtWjSboqfe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-yVeNBxwL5W@OpenReview" style="display:none">
                    <span class="i-index">#134</span>
                    <a class="i-title" href="#yVeNBxwL5W@OpenReview">MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers</a>
                    <a class="i-star" onclick="toggleAppStar('yVeNBxwL5W@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('yVeNBxwL5W@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OZbFRNhpwr@OpenReview" style="display:none">
                    <span class="i-index">#135</span>
                    <a class="i-title" href="#OZbFRNhpwr@OpenReview">SPA-BENCH: A COMPREHENSIVE BENCHMARK FOR SMARTPHONE AGENT EVALUATION</a>
                    <a class="i-star" onclick="toggleAppStar('OZbFRNhpwr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OZbFRNhpwr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-k3gCieTXeY@OpenReview" style="display:none">
                    <span class="i-index">#136</span>
                    <a class="i-title" href="#k3gCieTXeY@OpenReview">INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</a>
                    <a class="i-star" onclick="toggleAppStar('k3gCieTXeY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('k3gCieTXeY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jkUp3lybXf@OpenReview" style="display:none">
                    <span class="i-index">#137</span>
                    <a class="i-title" href="#jkUp3lybXf@OpenReview">Preference Optimization for Reasoning with Pseudo Feedback</a>
                    <a class="i-star" onclick="toggleAppStar('jkUp3lybXf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jkUp3lybXf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Nx4PMtJ1ER@OpenReview" style="display:none">
                    <span class="i-index">#138</span>
                    <a class="i-title" href="#Nx4PMtJ1ER@OpenReview">Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes</a>
                    <a class="i-star" onclick="toggleAppStar('Nx4PMtJ1ER@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Nx4PMtJ1ER@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-nGiGXLnKhl@OpenReview" style="display:none">
                    <span class="i-index">#139</span>
                    <a class="i-title" href="#nGiGXLnKhl@OpenReview">Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures</a>
                    <a class="i-star" onclick="toggleAppStar('nGiGXLnKhl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('nGiGXLnKhl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-N1L5TgtkAw@OpenReview" style="display:none">
                    <span class="i-index">#140</span>
                    <a class="i-title" href="#N1L5TgtkAw@OpenReview">Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</a>
                    <a class="i-star" onclick="toggleAppStar('N1L5TgtkAw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('N1L5TgtkAw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hBGavkf61a@OpenReview" style="display:none">
                    <span class="i-index">#141</span>
                    <a class="i-title" href="#hBGavkf61a@OpenReview">Diffusion Bridge AutoEncoders for Unsupervised Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('hBGavkf61a@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hBGavkf61a@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NTHMw8S1Ow@OpenReview" style="display:none">
                    <span class="i-index">#142</span>
                    <a class="i-title" href="#NTHMw8S1Ow@OpenReview">Towards Automated Knowledge Integration From Human-Interpretable Representations</a>
                    <a class="i-star" onclick="toggleAppStar('NTHMw8S1Ow@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NTHMw8S1Ow@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PiZtlzMWUj@OpenReview" style="display:none">
                    <span class="i-index">#143</span>
                    <a class="i-title" href="#PiZtlzMWUj@OpenReview">SoftCVI: Contrastive variational inference with self-generated soft labels</a>
                    <a class="i-star" onclick="toggleAppStar('PiZtlzMWUj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PiZtlzMWUj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wXSshrxlP4@OpenReview" style="display:none">
                    <span class="i-index">#144</span>
                    <a class="i-title" href="#wXSshrxlP4@OpenReview">GOPS: Learning Generative Object Priors for Unsupervised 3D Instance Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('wXSshrxlP4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wXSshrxlP4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gcouwCx7dG@OpenReview" style="display:none">
                    <span class="i-index">#145</span>
                    <a class="i-title" href="#gcouwCx7dG@OpenReview">Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency</a>
                    <a class="i-star" onclick="toggleAppStar('gcouwCx7dG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gcouwCx7dG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-MKEHCx25xp@OpenReview" style="display:none">
                    <span class="i-index">#146</span>
                    <a class="i-title" href="#MKEHCx25xp@OpenReview">WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</a>
                    <a class="i-star" onclick="toggleAppStar('MKEHCx25xp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MKEHCx25xp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-MGKDBuyv4p@OpenReview" style="display:none">
                    <span class="i-index">#147</span>
                    <a class="i-title" href="#MGKDBuyv4p@OpenReview">Mitigating Memorization in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('MGKDBuyv4p@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MGKDBuyv4p@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cnKhHxN3xj@OpenReview" style="display:none">
                    <span class="i-index">#148</span>
                    <a class="i-title" href="#cnKhHxN3xj@OpenReview">Wasserstein Distances, Neuronal Entanglement, and Sparsity</a>
                    <a class="i-star" onclick="toggleAppStar('cnKhHxN3xj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cnKhHxN3xj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-MFZjrTFE7h@OpenReview" style="display:none">
                    <span class="i-index">#149</span>
                    <a class="i-title" href="#MFZjrTFE7h@OpenReview">D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement</a>
                    <a class="i-star" onclick="toggleAppStar('MFZjrTFE7h@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MFZjrTFE7h@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-M8OGl34Pmg@OpenReview" style="display:none">
                    <span class="i-index">#150</span>
                    <a class="i-title" href="#M8OGl34Pmg@OpenReview">Following the Human Thread in Social Navigation</a>
                    <a class="i-star" onclick="toggleAppStar('M8OGl34Pmg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('M8OGl34Pmg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-M2SsqpxGtc@OpenReview" style="display:none">
                    <span class="i-index">#151</span>
                    <a class="i-title" href="#M2SsqpxGtc@OpenReview">CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation</a>
                    <a class="i-star" onclick="toggleAppStar('M2SsqpxGtc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('M2SsqpxGtc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lz0XW99tE0@OpenReview" style="display:none">
                    <span class="i-index">#152</span>
                    <a class="i-title" href="#Lz0XW99tE0@OpenReview">A Periodic Bayesian Flow for Material Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Lz0XW99tE0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lz0XW99tE0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LiUfN9h0Lx@OpenReview" style="display:none">
                    <span class="i-index">#153</span>
                    <a class="i-title" href="#LiUfN9h0Lx@OpenReview">Efficient and Accurate Explanation Estimation with Distribution Compression</a>
                    <a class="i-star" onclick="toggleAppStar('LiUfN9h0Lx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LiUfN9h0Lx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LbgIZpSUCe@OpenReview" style="display:none">
                    <span class="i-index">#154</span>
                    <a class="i-title" href="#LbgIZpSUCe@OpenReview">Nonlinear multiregion neural dynamics with parametric impulse response communication channels</a>
                    <a class="i-star" onclick="toggleAppStar('LbgIZpSUCe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LbgIZpSUCe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LXftdR11io@OpenReview" style="display:none">
                    <span class="i-index">#155</span>
                    <a class="i-title" href="#LXftdR11io@OpenReview">POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition</a>
                    <a class="i-star" onclick="toggleAppStar('LXftdR11io@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LXftdR11io@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LSp4KBhAom@OpenReview" style="display:none">
                    <span class="i-index">#156</span>
                    <a class="i-title" href="#LSp4KBhAom@OpenReview">LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models</a>
                    <a class="i-star" onclick="toggleAppStar('LSp4KBhAom@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LSp4KBhAom@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-szRmEM8Kx5@OpenReview" style="display:none">
                    <span class="i-index">#157</span>
                    <a class="i-title" href="#szRmEM8Kx5@OpenReview">Effective post-training embedding compression via temperature control in contrastive training</a>
                    <a class="i-star" onclick="toggleAppStar('szRmEM8Kx5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('szRmEM8Kx5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LBl7Hez0fF@OpenReview" style="display:none">
                    <span class="i-index">#158</span>
                    <a class="i-title" href="#LBl7Hez0fF@OpenReview">Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering</a>
                    <a class="i-star" onclick="toggleAppStar('LBl7Hez0fF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LBl7Hez0fF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KijslFbfOL@OpenReview" style="display:none">
                    <span class="i-index">#159</span>
                    <a class="i-title" href="#KijslFbfOL@OpenReview">Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction</a>
                    <a class="i-star" onclick="toggleAppStar('KijslFbfOL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KijslFbfOL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-K2jOacHUlO@OpenReview" style="display:none">
                    <span class="i-index">#160</span>
                    <a class="i-title" href="#K2jOacHUlO@OpenReview">Enhancing Large Language Models' Situated Faithfulness to External Contexts</a>
                    <a class="i-star" onclick="toggleAppStar('K2jOacHUlO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('K2jOacHUlO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IqHeDe2lbl@OpenReview" style="display:none">
                    <span class="i-index">#161</span>
                    <a class="i-title" href="#IqHeDe2lbl@OpenReview">Sparse components distinguish visual pathways &amp; their alignment to neural networks</a>
                    <a class="i-star" onclick="toggleAppStar('IqHeDe2lbl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IqHeDe2lbl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IUmj2dw5se@OpenReview" style="display:none">
                    <span class="i-index">#162</span>
                    <a class="i-title" href="#IUmj2dw5se@OpenReview">CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('IUmj2dw5se@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IUmj2dw5se@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IF0Q9KY3p2@OpenReview" style="display:none">
                    <span class="i-index">#163</span>
                    <a class="i-title" href="#IF0Q9KY3p2@OpenReview">Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression</a>
                    <a class="i-star" onclick="toggleAppStar('IF0Q9KY3p2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IF0Q9KY3p2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IC5RJvRoMp@OpenReview" style="display:none">
                    <span class="i-index">#164</span>
                    <a class="i-title" href="#IC5RJvRoMp@OpenReview">Streamlining Redundant Layers to Compress Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('IC5RJvRoMp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IC5RJvRoMp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hz4BYVY8YM@OpenReview" style="display:none">
                    <span class="i-index">#165</span>
                    <a class="i-title" href="#Hz4BYVY8YM@OpenReview">SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('Hz4BYVY8YM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hz4BYVY8YM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-HE5JmwniHm@OpenReview" style="display:none">
                    <span class="i-index">#166</span>
                    <a class="i-title" href="#HE5JmwniHm@OpenReview">DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimizaiton</a>
                    <a class="i-star" onclick="toggleAppStar('HE5JmwniHm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HE5JmwniHm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gj5JTAwdoy@OpenReview" style="display:none">
                    <span class="i-index">#167</span>
                    <a class="i-title" href="#Gj5JTAwdoy@OpenReview">Presto! Distilling Steps and Layers for Accelerating Music Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Gj5JTAwdoy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gj5JTAwdoy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-G6dMvRuhFr@OpenReview" style="display:none">
                    <span class="i-index">#168</span>
                    <a class="i-title" href="#G6dMvRuhFr@OpenReview">Grounding Video Models to Actions through Goal Conditioned Exploration</a>
                    <a class="i-star" onclick="toggleAppStar('G6dMvRuhFr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('G6dMvRuhFr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FhBT596F1X@OpenReview" style="display:none">
                    <span class="i-index">#169</span>
                    <a class="i-title" href="#FhBT596F1X@OpenReview">Learning Equivariant Non-Local Electron Density Functionals</a>
                    <a class="i-star" onclick="toggleAppStar('FhBT596F1X@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FhBT596F1X@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ExrEw8cVlU@OpenReview" style="display:none">
                    <span class="i-index">#170</span>
                    <a class="i-title" href="#ExrEw8cVlU@OpenReview">Poison-splat: Computation Cost Attack on 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('ExrEw8cVlU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ExrEw8cVlU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EbWf36quzd@OpenReview" style="display:none">
                    <span class="i-index">#171</span>
                    <a class="i-title" href="#EbWf36quzd@OpenReview">Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation</a>
                    <a class="i-star" onclick="toggleAppStar('EbWf36quzd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EbWf36quzd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EEgYUccwsV@OpenReview" style="display:none">
                    <span class="i-index">#172</span>
                    <a class="i-title" href="#EEgYUccwsV@OpenReview">AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials</a>
                    <a class="i-star" onclick="toggleAppStar('EEgYUccwsV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EEgYUccwsV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-E48QvQppIN@OpenReview" style="display:none">
                    <span class="i-index">#173</span>
                    <a class="i-title" href="#E48QvQppIN@OpenReview">Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences</a>
                    <a class="i-star" onclick="toggleAppStar('E48QvQppIN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('E48QvQppIN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-E1EHO0imOb@OpenReview" style="display:none">
                    <span class="i-index">#174</span>
                    <a class="i-title" href="#E1EHO0imOb@OpenReview">Scaling FP8 training to trillion-token LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('E1EHO0imOb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('E1EHO0imOb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dzh0hQPpuf@OpenReview" style="display:none">
                    <span class="i-index">#175</span>
                    <a class="i-title" href="#Dzh0hQPpuf@OpenReview">Student-Informed Teacher Training</a>
                    <a class="i-star" onclick="toggleAppStar('Dzh0hQPpuf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dzh0hQPpuf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DRiLWb8bJg@OpenReview" style="display:none">
                    <span class="i-index">#176</span>
                    <a class="i-title" href="#DRiLWb8bJg@OpenReview">Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation</a>
                    <a class="i-star" onclick="toggleAppStar('DRiLWb8bJg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DRiLWb8bJg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DC8bsa9bzY@OpenReview" style="display:none">
                    <span class="i-index">#177</span>
                    <a class="i-title" href="#DC8bsa9bzY@OpenReview">Estimating the Probabilities of Rare Outputs in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('DC8bsa9bzY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DC8bsa9bzY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cnwz9jONi5@OpenReview" style="display:none">
                    <span class="i-index">#178</span>
                    <a class="i-title" href="#Cnwz9jONi5@OpenReview">Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?</a>
                    <a class="i-star" onclick="toggleAppStar('Cnwz9jONi5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cnwz9jONi5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-BmG88rONaU@OpenReview" style="display:none">
                    <span class="i-index">#179</span>
                    <a class="i-title" href="#BmG88rONaU@OpenReview">Test-time Adaptation for Cross-modal Retrieval with Query Shift</a>
                    <a class="i-star" onclick="toggleAppStar('BmG88rONaU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BmG88rONaU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-BgxsmpVoOX@OpenReview" style="display:none">
                    <span class="i-index">#180</span>
                    <a class="i-title" href="#BgxsmpVoOX@OpenReview">Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('BgxsmpVoOX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BgxsmpVoOX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-BL4WBIfyrz@OpenReview" style="display:none">
                    <span class="i-index">#181</span>
                    <a class="i-title" href="#BL4WBIfyrz@OpenReview">Lightweight Neural App Control</a>
                    <a class="i-star" onclick="toggleAppStar('BL4WBIfyrz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BL4WBIfyrz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-AUCYptvAf3@OpenReview" style="display:none">
                    <span class="i-index">#182</span>
                    <a class="i-title" href="#AUCYptvAf3@OpenReview">Multi-Robot Motion Planning with Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('AUCYptvAf3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('AUCYptvAf3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-APojAzJQiq@OpenReview" style="display:none">
                    <span class="i-index">#183</span>
                    <a class="i-title" href="#APojAzJQiq@OpenReview">ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('APojAzJQiq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('APojAzJQiq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-AEFVa6VMu1@OpenReview" style="display:none">
                    <span class="i-index">#184</span>
                    <a class="i-title" href="#AEFVa6VMu1@OpenReview">Approximation algorithms for combinatorial optimization with predictions</a>
                    <a class="i-star" onclick="toggleAppStar('AEFVa6VMu1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('AEFVa6VMu1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-A6Y7AqlzLW@OpenReview" style="display:none">
                    <span class="i-index">#185</span>
                    <a class="i-title" href="#A6Y7AqlzLW@OpenReview">Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</a>
                    <a class="i-star" onclick="toggleAppStar('A6Y7AqlzLW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('A6Y7AqlzLW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9YNyiCJE3k@OpenReview" style="display:none">
                    <span class="i-index">#186</span>
                    <a class="i-title" href="#9YNyiCJE3k@OpenReview">OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents</a>
                    <a class="i-star" onclick="toggleAppStar('9YNyiCJE3k@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9YNyiCJE3k@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9UGfOJBuL8@OpenReview" style="display:none">
                    <span class="i-index">#187</span>
                    <a class="i-title" href="#9UGfOJBuL8@OpenReview">Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies</a>
                    <a class="i-star" onclick="toggleAppStar('9UGfOJBuL8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9UGfOJBuL8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9NfHbWKqMF@OpenReview" style="display:none">
                    <span class="i-index">#188</span>
                    <a class="i-title" href="#9NfHbWKqMF@OpenReview">SplatFormer: Point Transformer for Robust 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('9NfHbWKqMF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9NfHbWKqMF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8xxEBAtD7y@OpenReview" style="display:none">
                    <span class="i-index">#189</span>
                    <a class="i-title" href="#8xxEBAtD7y@OpenReview">Towards a Unified and Verified Understanding of Group-Operation Networks</a>
                    <a class="i-star" onclick="toggleAppStar('8xxEBAtD7y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8xxEBAtD7y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8bjspmAMBk@OpenReview" style="display:none">
                    <span class="i-index">#190</span>
                    <a class="i-title" href="#8bjspmAMBk@OpenReview">Quality Measures for Dynamic Graph Generative Models</a>
                    <a class="i-star" onclick="toggleAppStar('8bjspmAMBk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8bjspmAMBk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-7XgKAabsPp@OpenReview" style="display:none">
                    <span class="i-index">#191</span>
                    <a class="i-title" href="#7XgKAabsPp@OpenReview">Theory on Mixture-of-Experts in Continual Learning</a>
                    <a class="i-star" onclick="toggleAppStar('7XgKAabsPp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7XgKAabsPp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-7IzeL0kflu@OpenReview" style="display:none">
                    <span class="i-index">#192</span>
                    <a class="i-title" href="#7IzeL0kflu@OpenReview">Simplifying Deep Temporal Difference Learning</a>
                    <a class="i-star" onclick="toggleAppStar('7IzeL0kflu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7IzeL0kflu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-7ANDviElAo@OpenReview" style="display:none">
                    <span class="i-index">#193</span>
                    <a class="i-title" href="#7ANDviElAo@OpenReview">Graph Sparsification via Mixture of Graphs</a>
                    <a class="i-star" onclick="toggleAppStar('7ANDviElAo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7ANDviElAo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-78Nn4QJTEN@OpenReview" style="display:none">
                    <span class="i-index">#194</span>
                    <a class="i-title" href="#78Nn4QJTEN@OpenReview">When Attention Sink Emerges in Language Models: An Empirical View</a>
                    <a class="i-star" onclick="toggleAppStar('78Nn4QJTEN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('78Nn4QJTEN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-69Fp4dcmJN@OpenReview" style="display:none">
                    <span class="i-index">#195</span>
                    <a class="i-title" href="#69Fp4dcmJN@OpenReview">Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML</a>
                    <a class="i-star" onclick="toggleAppStar('69Fp4dcmJN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('69Fp4dcmJN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5FXKgOxmb2@OpenReview" style="display:none">
                    <span class="i-index">#196</span>
                    <a class="i-title" href="#5FXKgOxmb2@OpenReview">MAGNet: Motif-Agnostic Generation of Molecules from Scaffolds</a>
                    <a class="i-star" onclick="toggleAppStar('5FXKgOxmb2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5FXKgOxmb2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5BjQOUXq7i@OpenReview" style="display:none">
                    <span class="i-index">#197</span>
                    <a class="i-title" href="#5BjQOUXq7i@OpenReview">RegMix: Data Mixture as Regression for Language Model Pre-training</a>
                    <a class="i-star" onclick="toggleAppStar('5BjQOUXq7i@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5BjQOUXq7i@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5BSlakturs@OpenReview" style="display:none">
                    <span class="i-index">#198</span>
                    <a class="i-title" href="#5BSlakturs@OpenReview">Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds</a>
                    <a class="i-star" onclick="toggleAppStar('5BSlakturs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5BSlakturs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4gaySj8kvX@OpenReview" style="display:none">
                    <span class="i-index">#199</span>
                    <a class="i-title" href="#4gaySj8kvX@OpenReview">Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research</a>
                    <a class="i-star" onclick="toggleAppStar('4gaySj8kvX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4gaySj8kvX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4NTrco82W0@OpenReview" style="display:none">
                    <span class="i-index">#200</span>
                    <a class="i-title" href="#4NTrco82W0@OpenReview">Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks</a>
                    <a class="i-star" onclick="toggleAppStar('4NTrco82W0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4NTrco82W0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4HRRcqE9SU@OpenReview" style="display:none">
                    <span class="i-index">#201</span>
                    <a class="i-title" href="#4HRRcqE9SU@OpenReview">ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('4HRRcqE9SU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4HRRcqE9SU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-44cMlQSreK@OpenReview" style="display:none">
                    <span class="i-index">#202</span>
                    <a class="i-title" href="#44cMlQSreK@OpenReview">On Quantizing Neural Representation for Variable-Rate Video Coding</a>
                    <a class="i-star" onclick="toggleAppStar('44cMlQSreK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('44cMlQSreK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3ddi7Uss2A@OpenReview" style="display:none">
                    <span class="i-index">#203</span>
                    <a class="i-title" href="#3ddi7Uss2A@OpenReview">What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('3ddi7Uss2A@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3ddi7Uss2A@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3Oli4u6q3p@OpenReview" style="display:none">
                    <span class="i-index">#204</span>
                    <a class="i-title" href="#3Oli4u6q3p@OpenReview">RelitLRM: Generative Relightable Radiance for Large Reconstruction Models</a>
                    <a class="i-star" onclick="toggleAppStar('3Oli4u6q3p@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3Oli4u6q3p@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2kGKsyhtvh@OpenReview" style="display:none">
                    <span class="i-index">#205</span>
                    <a class="i-title" href="#2kGKsyhtvh@OpenReview">Towards hyperparameter-free optimization with differential privacy</a>
                    <a class="i-star" onclick="toggleAppStar('2kGKsyhtvh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2kGKsyhtvh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2hcfoCHKoB@OpenReview" style="display:none">
                    <span class="i-index">#206</span>
                    <a class="i-title" href="#2hcfoCHKoB@OpenReview">DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model</a>
                    <a class="i-star" onclick="toggleAppStar('2hcfoCHKoB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2hcfoCHKoB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2c7pfOqu9k@OpenReview" style="display:none">
                    <span class="i-index">#207</span>
                    <a class="i-title" href="#2c7pfOqu9k@OpenReview">DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference</a>
                    <a class="i-star" onclick="toggleAppStar('2c7pfOqu9k@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2c7pfOqu9k@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-28abpUEICJ@OpenReview" style="display:none">
                    <span class="i-index">#208</span>
                    <a class="i-title" href="#28abpUEICJ@OpenReview">CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations</a>
                    <a class="i-star" onclick="toggleAppStar('28abpUEICJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('28abpUEICJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1qP3lsatCR@OpenReview" style="display:none">
                    <span class="i-index">#209</span>
                    <a class="i-title" href="#1qP3lsatCR@OpenReview">NetMoE: Accelerating MoE Training through Dynamic Sample Placement</a>
                    <a class="i-star" onclick="toggleAppStar('1qP3lsatCR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1qP3lsatCR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1jcnvghayD@OpenReview" style="display:none">
                    <span class="i-index">#210</span>
                    <a class="i-title" href="#1jcnvghayD@OpenReview">Bayesian Optimization via Continual Variational Last Layer Training</a>
                    <a class="i-star" onclick="toggleAppStar('1jcnvghayD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1jcnvghayD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-11xgiMEI5o@OpenReview" style="display:none">
                    <span class="i-index">#211</span>
                    <a class="i-title" href="#11xgiMEI5o@OpenReview">OmniRe: Omni Urban Scene Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('11xgiMEI5o@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('11xgiMEI5o@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-10JOlFIPjt@OpenReview" style="display:none">
                    <span class="i-index">#212</span>
                    <a class="i-title" href="#10JOlFIPjt@OpenReview">In vivo cell-type and brain region classification via multimodal contrastive learning</a>
                    <a class="i-star" onclick="toggleAppStar('10JOlFIPjt@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('10JOlFIPjt@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0h6v4SpLCY@OpenReview" style="display:none">
                    <span class="i-index">#213</span>
                    <a class="i-title" href="#0h6v4SpLCY@OpenReview">Universal generalization guarantees for Wasserstein distributionally robust models</a>
                    <a class="i-star" onclick="toggleAppStar('0h6v4SpLCY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0h6v4SpLCY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0fhzSFsGUT@OpenReview" style="display:none">
                    <span class="i-index">#214</span>
                    <a class="i-title" href="#0fhzSFsGUT@OpenReview">PETRA: Parallel End-to-end Training with Reversible Architectures</a>
                    <a class="i-star" onclick="toggleAppStar('0fhzSFsGUT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0fhzSFsGUT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0fJfVOSUra@OpenReview" style="display:none">
                    <span class="i-index">#215</span>
                    <a class="i-title" href="#0fJfVOSUra@OpenReview">ThunderKittens: Simple, Fast, and $\textit{Adorable}$ Kernels</a>
                    <a class="i-star" onclick="toggleAppStar('0fJfVOSUra@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0fJfVOSUra@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0bmGL4q7vJ@OpenReview" style="display:none">
                    <span class="i-index">#216</span>
                    <a class="i-title" href="#0bmGL4q7vJ@OpenReview">Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage</a>
                    <a class="i-star" onclick="toggleAppStar('0bmGL4q7vJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0bmGL4q7vJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-00SnKBGTsz@OpenReview" style="display:none">
                    <span class="i-index">#217</span>
                    <a class="i-title" href="#00SnKBGTsz@OpenReview">DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback</a>
                    <a class="i-star" onclick="toggleAppStar('00SnKBGTsz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('00SnKBGTsz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sahQq2sH5x@OpenReview" style="display:none">
                    <span class="i-index">#218</span>
                    <a class="i-title" href="#sahQq2sH5x@OpenReview">Benchmarking Predictive Coding Networks -- Made Simple</a>
                    <a class="i-star" onclick="toggleAppStar('sahQq2sH5x@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sahQq2sH5x@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rpwGUtTeA5@OpenReview" style="display:none">
                    <span class="i-index">#219</span>
                    <a class="i-title" href="#rpwGUtTeA5@OpenReview">UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('rpwGUtTeA5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rpwGUtTeA5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qpXctF2aLZ@OpenReview" style="display:none">
                    <span class="i-index">#220</span>
                    <a class="i-title" href="#qpXctF2aLZ@OpenReview">Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('qpXctF2aLZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qpXctF2aLZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pPQPQ7Yd58@OpenReview" style="display:none">
                    <span class="i-index">#221</span>
                    <a class="i-title" href="#pPQPQ7Yd58@OpenReview">Control-oriented Clustering of Visual Latent Representation</a>
                    <a class="i-star" onclick="toggleAppStar('pPQPQ7Yd58@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pPQPQ7Yd58@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cznqgb4DNv@OpenReview" style="display:none">
                    <span class="i-index">#222</span>
                    <a class="i-title" href="#cznqgb4DNv@OpenReview">Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees</a>
                    <a class="i-star" onclick="toggleAppStar('cznqgb4DNv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cznqgb4DNv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UVnD9Ze6mF@OpenReview" style="display:none">
                    <span class="i-index">#223</span>
                    <a class="i-title" href="#UVnD9Ze6mF@OpenReview">AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories</a>
                    <a class="i-star" onclick="toggleAppStar('UVnD9Ze6mF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UVnD9Ze6mF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9GsgCUJtic@OpenReview" style="display:none">
                    <span class="i-index">#224</span>
                    <a class="i-title" href="#9GsgCUJtic@OpenReview">When do GFlowNets learn the right distribution?</a>
                    <a class="i-star" onclick="toggleAppStar('9GsgCUJtic@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9GsgCUJtic@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8oFvUBvF1u@OpenReview" style="display:none">
                    <span class="i-index">#225</span>
                    <a class="i-title" href="#8oFvUBvF1u@OpenReview">DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from One Demo</a>
                    <a class="i-star" onclick="toggleAppStar('8oFvUBvF1u@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8oFvUBvF1u@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3PDklqqqfN@OpenReview" style="display:none">
                    <span class="i-index">#226</span>
                    <a class="i-title" href="#3PDklqqqfN@OpenReview">Multi-Field Adaptive Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('3PDklqqqfN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3PDklqqqfN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-37EXtKCOkn@OpenReview" style="display:none">
                    <span class="i-index">#227</span>
                    <a class="i-title" href="#37EXtKCOkn@OpenReview">Learning Spatiotemporal Dynamical Systems from Point Process Observations</a>
                    <a class="i-star" onclick="toggleAppStar('37EXtKCOkn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('37EXtKCOkn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SOWZ59UyNc@OpenReview" style="display:none">
                    <span class="i-index">#228</span>
                    <a class="i-title" href="#SOWZ59UyNc@OpenReview">Lean-STaR: Learning to Interleave Thinking and Proving</a>
                    <a class="i-star" onclick="toggleAppStar('SOWZ59UyNc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SOWZ59UyNc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kuutidLf6R@OpenReview" style="display:none">
                    <span class="i-index">#229</span>
                    <a class="i-title" href="#kuutidLf6R@OpenReview">Diffusion Attribution Score: Which Training Sample Determines Your Generation?</a>
                    <a class="i-star" onclick="toggleAppStar('kuutidLf6R@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kuutidLf6R@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-RaR3ETzyKp@OpenReview" style="display:none">
                    <span class="i-index">#230</span>
                    <a class="i-title" href="#RaR3ETzyKp@OpenReview">Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance</a>
                    <a class="i-star" onclick="toggleAppStar('RaR3ETzyKp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('RaR3ETzyKp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-msEr27EejF@OpenReview" style="display:none">
                    <span class="i-index">#231</span>
                    <a class="i-title" href="#msEr27EejF@OpenReview">Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking</a>
                    <a class="i-star" onclick="toggleAppStar('msEr27EejF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('msEr27EejF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-l2zFn6TIQi@OpenReview" style="display:none">
                    <span class="i-index">#232</span>
                    <a class="i-title" href="#l2zFn6TIQi@OpenReview">Controlling Language and Diffusion Models by Transporting Activations</a>
                    <a class="i-star" onclick="toggleAppStar('l2zFn6TIQi@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('l2zFn6TIQi@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ws5phQki00@OpenReview" style="display:none">
                    <span class="i-index">#233</span>
                    <a class="i-title" href="#ws5phQki00@OpenReview">The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions</a>
                    <a class="i-star" onclick="toggleAppStar('ws5phQki00@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ws5phQki00@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Njx1NjHIx4@OpenReview" style="display:none">
                    <span class="i-index">#234</span>
                    <a class="i-title" href="#Njx1NjHIx4@OpenReview">Formation of Representations in Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('Njx1NjHIx4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Njx1NjHIx4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-U834XHJuqk@OpenReview" style="display:none">
                    <span class="i-index">#235</span>
                    <a class="i-title" href="#U834XHJuqk@OpenReview">Nonlinear Sequence Embedding by Monotone Variational Inequality</a>
                    <a class="i-star" onclick="toggleAppStar('U834XHJuqk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('U834XHJuqk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hwSmPOAmhk@OpenReview" style="display:none">
                    <span class="i-index">#236</span>
                    <a class="i-title" href="#hwSmPOAmhk@OpenReview">Understanding Factual Recall in Transformers via Associative Memories</a>
                    <a class="i-star" onclick="toggleAppStar('hwSmPOAmhk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hwSmPOAmhk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vWR3KuiQur@OpenReview" style="display:none">
                    <span class="i-index">#237</span>
                    <a class="i-title" href="#vWR3KuiQur@OpenReview">SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('vWR3KuiQur@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vWR3KuiQur@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uvHmnahyp1@OpenReview" style="display:none">
                    <span class="i-index">#238</span>
                    <a class="i-title" href="#uvHmnahyp1@OpenReview">SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints</a>
                    <a class="i-star" onclick="toggleAppStar('uvHmnahyp1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uvHmnahyp1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uuriavczkL@OpenReview" style="display:none">
                    <span class="i-index">#239</span>
                    <a class="i-title" href="#uuriavczkL@OpenReview">Counterfactual Realizability</a>
                    <a class="i-star" onclick="toggleAppStar('uuriavczkL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uuriavczkL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uSz2K30RRd@OpenReview" style="display:none">
                    <span class="i-index">#240</span>
                    <a class="i-title" href="#uSz2K30RRd@OpenReview">Weighted Point Cloud Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric</a>
                    <a class="i-star" onclick="toggleAppStar('uSz2K30RRd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uSz2K30RRd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-r3DF5sOo5B@OpenReview" style="display:none">
                    <span class="i-index">#241</span>
                    <a class="i-title" href="#r3DF5sOo5B@OpenReview">Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought</a>
                    <a class="i-star" onclick="toggleAppStar('r3DF5sOo5B@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('r3DF5sOo5B@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pZISppZSTv@OpenReview" style="display:none">
                    <span class="i-index">#242</span>
                    <a class="i-title" href="#pZISppZSTv@OpenReview">CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control</a>
                    <a class="i-star" onclick="toggleAppStar('pZISppZSTv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pZISppZSTv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pHOH8FVrTp@OpenReview" style="display:none">
                    <span class="i-index">#243</span>
                    <a class="i-title" href="#pHOH8FVrTp@OpenReview">No Need to Talk: Asynchronous Mixture of Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('pHOH8FVrTp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pHOH8FVrTp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-lvw3UgeVxS@OpenReview" style="display:none">
                    <span class="i-index">#244</span>
                    <a class="i-title" href="#lvw3UgeVxS@OpenReview">gRNAde: Geometric Deep Learning for 3D RNA inverse design</a>
                    <a class="i-star" onclick="toggleAppStar('lvw3UgeVxS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('lvw3UgeVxS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-lXRDQsiP2v@OpenReview" style="display:none">
                    <span class="i-index">#245</span>
                    <a class="i-title" href="#lXRDQsiP2v@OpenReview">Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction</a>
                    <a class="i-star" onclick="toggleAppStar('lXRDQsiP2v@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('lXRDQsiP2v@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-lJpqxFgWCM@OpenReview" style="display:none">
                    <span class="i-index">#246</span>
                    <a class="i-title" href="#lJpqxFgWCM@OpenReview">MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion</a>
                    <a class="i-star" onclick="toggleAppStar('lJpqxFgWCM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('lJpqxFgWCM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-k03mB41vyM@OpenReview" style="display:none">
                    <span class="i-index">#247</span>
                    <a class="i-title" href="#k03mB41vyM@OpenReview">Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('k03mB41vyM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('k03mB41vyM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hXm0Wu2U9K@OpenReview" style="display:none">
                    <span class="i-index">#248</span>
                    <a class="i-title" href="#hXm0Wu2U9K@OpenReview">Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('hXm0Wu2U9K@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hXm0Wu2U9K@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gTwRMU3lJ5@OpenReview" style="display:none">
                    <span class="i-index">#249</span>
                    <a class="i-title" href="#gTwRMU3lJ5@OpenReview">LoRA-Pro: Are Low-Rank Adapters Properly Optimized?</a>
                    <a class="i-star" onclick="toggleAppStar('gTwRMU3lJ5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gTwRMU3lJ5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gI0kPklUKS@OpenReview" style="display:none">
                    <span class="i-index">#250</span>
                    <a class="i-title" href="#gI0kPklUKS@OpenReview">Bilinear MLPs enable weight-based mechanistic interpretability</a>
                    <a class="i-star" onclick="toggleAppStar('gI0kPklUKS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gI0kPklUKS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fZK6AQXlUU@OpenReview" style="display:none">
                    <span class="i-index">#251</span>
                    <a class="i-title" href="#fZK6AQXlUU@OpenReview">Conformal Prediction Sets Can Cause Disparate Impact</a>
                    <a class="i-star" onclick="toggleAppStar('fZK6AQXlUU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fZK6AQXlUU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-f7KxfUrRSb@OpenReview" style="display:none">
                    <span class="i-index">#252</span>
                    <a class="i-title" href="#f7KxfUrRSb@OpenReview">Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model</a>
                    <a class="i-star" onclick="toggleAppStar('f7KxfUrRSb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('f7KxfUrRSb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dOAkHmsjRX@OpenReview" style="display:none">
                    <span class="i-index">#253</span>
                    <a class="i-title" href="#dOAkHmsjRX@OpenReview">Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling</a>
                    <a class="i-star" onclick="toggleAppStar('dOAkHmsjRX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dOAkHmsjRX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dEypApI1MZ@OpenReview" style="display:none">
                    <span class="i-index">#254</span>
                    <a class="i-title" href="#dEypApI1MZ@OpenReview">How Feature Learning Can Improve Neural Scaling Laws</a>
                    <a class="i-star" onclick="toggleAppStar('dEypApI1MZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dEypApI1MZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-csbf1p8xUq@OpenReview" style="display:none">
                    <span class="i-index">#255</span>
                    <a class="i-title" href="#csbf1p8xUq@OpenReview">X-ALMA: Plug &amp; Play Modules and Adaptive Rejection for Quality Translation at Scale</a>
                    <a class="i-star" onclick="toggleAppStar('csbf1p8xUq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('csbf1p8xUq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cmXWYolrlo@OpenReview" style="display:none">
                    <span class="i-index">#256</span>
                    <a class="i-title" href="#cmXWYolrlo@OpenReview">Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture</a>
                    <a class="i-star" onclick="toggleAppStar('cmXWYolrlo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cmXWYolrlo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cRR0oDFEBC@OpenReview" style="display:none">
                    <span class="i-index">#257</span>
                    <a class="i-title" href="#cRR0oDFEBC@OpenReview">Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('cRR0oDFEBC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cRR0oDFEBC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aD2uwhLbnA@OpenReview" style="display:none">
                    <span class="i-index">#258</span>
                    <a class="i-title" href="#aD2uwhLbnA@OpenReview">Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training</a>
                    <a class="i-star" onclick="toggleAppStar('aD2uwhLbnA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aD2uwhLbnA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yk87CwhBDx@OpenReview" style="display:none">
                    <span class="i-index">#259</span>
                    <a class="i-title" href="#Yk87CwhBDx@OpenReview">Can Large Language Models Understand Symbolic Graphics Programs?</a>
                    <a class="i-star" onclick="toggleAppStar('Yk87CwhBDx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yk87CwhBDx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YK9G4Htdew@OpenReview" style="display:none">
                    <span class="i-index">#260</span>
                    <a class="i-title" href="#YK9G4Htdew@OpenReview">Learning Transformer-based World Models with Contrastive Predictive Coding</a>
                    <a class="i-star" onclick="toggleAppStar('YK9G4Htdew@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YK9G4Htdew@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XgH1wfHSX8@OpenReview" style="display:none">
                    <span class="i-index">#261</span>
                    <a class="i-title" href="#XgH1wfHSX8@OpenReview">Algorithmic Phases of In-Context Learning</a>
                    <a class="i-star" onclick="toggleAppStar('XgH1wfHSX8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XgH1wfHSX8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XNA3Mnnbvb@OpenReview" style="display:none">
                    <span class="i-index">#262</span>
                    <a class="i-title" href="#XNA3Mnnbvb@OpenReview">DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</a>
                    <a class="i-star" onclick="toggleAppStar('XNA3Mnnbvb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XNA3Mnnbvb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-VaowElpVzd@OpenReview" style="display:none">
                    <span class="i-index">#263</span>
                    <a class="i-title" href="#VaowElpVzd@OpenReview">Co$^{\mathbf{3}}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('VaowElpVzd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VaowElpVzd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-VGQugiuCQs@OpenReview" style="display:none">
                    <span class="i-index">#264</span>
                    <a class="i-title" href="#VGQugiuCQs@OpenReview">Fair Clustering in the Sliding Window Model</a>
                    <a class="i-star" onclick="toggleAppStar('VGQugiuCQs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VGQugiuCQs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UL8b54P96G@OpenReview" style="display:none">
                    <span class="i-index">#265</span>
                    <a class="i-title" href="#UL8b54P96G@OpenReview">SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation</a>
                    <a class="i-star" onclick="toggleAppStar('UL8b54P96G@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UL8b54P96G@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-U67J0QNtzo@OpenReview" style="display:none">
                    <span class="i-index">#266</span>
                    <a class="i-title" href="#U67J0QNtzo@OpenReview">On Disentangled Training for Nonlinear Transform in Learned Image Compression</a>
                    <a class="i-star" onclick="toggleAppStar('U67J0QNtzo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('U67J0QNtzo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-TYSQYx9vwd@OpenReview" style="display:none">
                    <span class="i-index">#267</span>
                    <a class="i-title" href="#TYSQYx9vwd@OpenReview">Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations</a>
                    <a class="i-star" onclick="toggleAppStar('TYSQYx9vwd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TYSQYx9vwd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-TJo6aQb7mK@OpenReview" style="display:none">
                    <span class="i-index">#268</span>
                    <a class="i-title" href="#TJo6aQb7mK@OpenReview">Surprising Effectiveness of pretraining Ternary Language Model at Scale</a>
                    <a class="i-star" onclick="toggleAppStar('TJo6aQb7mK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TJo6aQb7mK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-THqWPzL00e@OpenReview" style="display:none">
                    <span class="i-index">#269</span>
                    <a class="i-title" href="#THqWPzL00e@OpenReview">TopoNets: High performing vision and language models with brain-like topography</a>
                    <a class="i-star" onclick="toggleAppStar('THqWPzL00e@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('THqWPzL00e@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SUc1UOWndp@OpenReview" style="display:none">
                    <span class="i-index">#270</span>
                    <a class="i-title" href="#SUc1UOWndp@OpenReview">Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient</a>
                    <a class="i-star" onclick="toggleAppStar('SUc1UOWndp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SUc1UOWndp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-S4dItvpvAv@OpenReview" style="display:none">
                    <span class="i-index">#271</span>
                    <a class="i-title" href="#S4dItvpvAv@OpenReview">How to Find the Exact Pareto Front for Multi-Objective MDPs?</a>
                    <a class="i-star" onclick="toggleAppStar('S4dItvpvAv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('S4dItvpvAv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-RInisw1yin@OpenReview" style="display:none">
                    <span class="i-index">#272</span>
                    <a class="i-title" href="#RInisw1yin@OpenReview">SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('RInisw1yin@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('RInisw1yin@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-R1hIXdST22@OpenReview" style="display:none">
                    <span class="i-index">#273</span>
                    <a class="i-title" href="#R1hIXdST22@OpenReview">Towards General-Purpose Model-Free Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('R1hIXdST22@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('R1hIXdST22@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QjSOgxJ0hp@OpenReview" style="display:none">
                    <span class="i-index">#274</span>
                    <a class="i-title" href="#QjSOgxJ0hp@OpenReview">Learning from End User Data with Shuffled Differential Privacy over Kernel Densities</a>
                    <a class="i-star" onclick="toggleAppStar('QjSOgxJ0hp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QjSOgxJ0hp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Q150eWkQ4I@OpenReview" style="display:none">
                    <span class="i-index">#275</span>
                    <a class="i-title" href="#Q150eWkQ4I@OpenReview">Spectral Compressive Imaging via Unmixing-driven Subspace Diffusion Refinement</a>
                    <a class="i-star" onclick="toggleAppStar('Q150eWkQ4I@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Q150eWkQ4I@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PGhiPGBf47@OpenReview" style="display:none">
                    <span class="i-index">#276</span>
                    <a class="i-title" href="#PGhiPGBf47@OpenReview">DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life</a>
                    <a class="i-star" onclick="toggleAppStar('PGhiPGBf47@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PGhiPGBf47@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OeBY9XqiTz@OpenReview" style="display:none">
                    <span class="i-index">#277</span>
                    <a class="i-title" href="#OeBY9XqiTz@OpenReview">Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('OeBY9XqiTz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OeBY9XqiTz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NGKQoaqLpo@OpenReview" style="display:none">
                    <span class="i-index">#278</span>
                    <a class="i-title" href="#NGKQoaqLpo@OpenReview">How new data pollutes LLM knowledge and how to dilute it</a>
                    <a class="i-star" onclick="toggleAppStar('NGKQoaqLpo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NGKQoaqLpo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-N4NhVN30ph@OpenReview" style="display:none">
                    <span class="i-index">#279</span>
                    <a class="i-title" href="#N4NhVN30ph@OpenReview">TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('N4NhVN30ph@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('N4NhVN30ph@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-MtDd7rWok1@OpenReview" style="display:none">
                    <span class="i-index">#280</span>
                    <a class="i-title" href="#MtDd7rWok1@OpenReview">Anti-Exposure Bias in Diffusion Models via Prompt Learning</a>
                    <a class="i-star" onclick="toggleAppStar('MtDd7rWok1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MtDd7rWok1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-MagmwodCAB@OpenReview" style="display:none">
                    <span class="i-index">#281</span>
                    <a class="i-title" href="#MagmwodCAB@OpenReview">3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image Generation</a>
                    <a class="i-star" onclick="toggleAppStar('MagmwodCAB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MagmwodCAB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LqTz13JS2P@OpenReview" style="display:none">
                    <span class="i-index">#282</span>
                    <a class="i-title" href="#LqTz13JS2P@OpenReview">Generalized Principal-Agent Problem with a Learning Agent</a>
                    <a class="i-star" onclick="toggleAppStar('LqTz13JS2P@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LqTz13JS2P@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-L14sqcrUC3@OpenReview" style="display:none">
                    <span class="i-index">#283</span>
                    <a class="i-title" href="#L14sqcrUC3@OpenReview">Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks</a>
                    <a class="i-star" onclick="toggleAppStar('L14sqcrUC3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('L14sqcrUC3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kpjvm2mB0K@OpenReview" style="display:none">
                    <span class="i-index">#284</span>
                    <a class="i-title" href="#Kpjvm2mB0K@OpenReview">Streaming Algorithms For $\ell_p$ Flows and $\ell_p$ Regression</a>
                    <a class="i-star" onclick="toggleAppStar('Kpjvm2mB0K@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kpjvm2mB0K@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IjQ2Jtemzy@OpenReview" style="display:none">
                    <span class="i-index">#285</span>
                    <a class="i-title" href="#IjQ2Jtemzy@OpenReview">Language Models Can Articulate Their Implicit Goals</a>
                    <a class="i-star" onclick="toggleAppStar('IjQ2Jtemzy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IjQ2Jtemzy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-INyi7qUdjZ@OpenReview" style="display:none">
                    <span class="i-index">#286</span>
                    <a class="i-title" href="#INyi7qUdjZ@OpenReview">Differential learning kinetics govern the transition from memorization to generalization during in-context learning</a>
                    <a class="i-star" onclick="toggleAppStar('INyi7qUdjZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('INyi7qUdjZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-HqLHY4TzGj@OpenReview" style="display:none">
                    <span class="i-index">#287</span>
                    <a class="i-title" href="#HqLHY4TzGj@OpenReview">Union-over-Intersections: Object Detection beyond Winner-Takes-All</a>
                    <a class="i-star" onclick="toggleAppStar('HqLHY4TzGj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HqLHY4TzGj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-H2Gxil855b@OpenReview" style="display:none">
                    <span class="i-index">#288</span>
                    <a class="i-title" href="#H2Gxil855b@OpenReview">Atlas Gaussians Diffusion for 3D Generation</a>
                    <a class="i-star" onclick="toggleAppStar('H2Gxil855b@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('H2Gxil855b@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GjfIZan5jN@OpenReview" style="display:none">
                    <span class="i-index">#289</span>
                    <a class="i-title" href="#GjfIZan5jN@OpenReview">Enhancing Pre-trained Representation Classifiability can Boost its Interpretability</a>
                    <a class="i-star" onclick="toggleAppStar('GjfIZan5jN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GjfIZan5jN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fk3eod9aaD@OpenReview" style="display:none">
                    <span class="i-index">#290</span>
                    <a class="i-title" href="#Fk3eod9aaD@OpenReview">In Search of Forgotten Domain Generalization</a>
                    <a class="i-star" onclick="toggleAppStar('Fk3eod9aaD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fk3eod9aaD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ywFOSIT9ik@OpenReview" style="display:none">
                    <span class="i-index">#291</span>
                    <a class="i-title" href="#ywFOSIT9ik@OpenReview">Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations</a>
                    <a class="i-star" onclick="toggleAppStar('ywFOSIT9ik@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ywFOSIT9ik@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EPHsIa0Ytg@OpenReview" style="display:none">
                    <span class="i-index">#292</span>
                    <a class="i-title" href="#EPHsIa0Ytg@OpenReview">Improved Approximation Algorithms for $k$-Submodular Maximization via Multilinear Extension</a>
                    <a class="i-star" onclick="toggleAppStar('EPHsIa0Ytg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EPHsIa0Ytg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-D1Y2XFgsPI@OpenReview" style="display:none">
                    <span class="i-index">#293</span>
                    <a class="i-title" href="#D1Y2XFgsPI@OpenReview">Imputation for prediction: beware of diminishing returns.</a>
                    <a class="i-star" onclick="toggleAppStar('D1Y2XFgsPI@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('D1Y2XFgsPI@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-CkgKSqZbuC@OpenReview" style="display:none">
                    <span class="i-index">#294</span>
                    <a class="i-title" href="#CkgKSqZbuC@OpenReview">$R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning</a>
                    <a class="i-star" onclick="toggleAppStar('CkgKSqZbuC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CkgKSqZbuC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Acvo2RGSCy@OpenReview" style="display:none">
                    <span class="i-index">#295</span>
                    <a class="i-title" href="#Acvo2RGSCy@OpenReview">DeLLMa: Decision Making Under Uncertainty with Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Acvo2RGSCy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Acvo2RGSCy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9ehJCZz4aM@OpenReview" style="display:none">
                    <span class="i-index">#296</span>
                    <a class="i-title" href="#9ehJCZz4aM@OpenReview">Learning Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations</a>
                    <a class="i-star" onclick="toggleAppStar('9ehJCZz4aM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9ehJCZz4aM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9bMZ29SPVx@OpenReview" style="display:none">
                    <span class="i-index">#297</span>
                    <a class="i-title" href="#9bMZ29SPVx@OpenReview">A CLIP-Powered Framework for Robust and Generalizable Data Selection</a>
                    <a class="i-star" onclick="toggleAppStar('9bMZ29SPVx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9bMZ29SPVx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-97rOQDPmk2@OpenReview" style="display:none">
                    <span class="i-index">#298</span>
                    <a class="i-title" href="#97rOQDPmk2@OpenReview">On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent</a>
                    <a class="i-star" onclick="toggleAppStar('97rOQDPmk2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('97rOQDPmk2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8oCrlOaYcc@OpenReview" style="display:none">
                    <span class="i-index">#299</span>
                    <a class="i-title" href="#8oCrlOaYcc@OpenReview">Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL</a>
                    <a class="i-star" onclick="toggleAppStar('8oCrlOaYcc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8oCrlOaYcc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8UFG9D8xeU@OpenReview" style="display:none">
                    <span class="i-index">#300</span>
                    <a class="i-title" href="#8UFG9D8xeU@OpenReview">Direct Post-Training Preference Alignment of Multi-Agent Motion Generation Model with Implicit Feedback from Demonstrations</a>
                    <a class="i-star" onclick="toggleAppStar('8UFG9D8xeU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8UFG9D8xeU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-7nyJBVCTGQ@OpenReview" style="display:none">
                    <span class="i-index">#301</span>
                    <a class="i-title" href="#7nyJBVCTGQ@OpenReview">LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning</a>
                    <a class="i-star" onclick="toggleAppStar('7nyJBVCTGQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7nyJBVCTGQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-71pur4y8gs@OpenReview" style="display:none">
                    <span class="i-index">#302</span>
                    <a class="i-title" href="#71pur4y8gs@OpenReview">TabWak: A Watermark for Tabular Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('71pur4y8gs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('71pur4y8gs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-60i0ksMAhd@OpenReview" style="display:none">
                    <span class="i-index">#303</span>
                    <a class="i-title" href="#60i0ksMAhd@OpenReview">BlendRL: A Framework for Merging Symbolic and Neural Policy Learning</a>
                    <a class="i-star" onclick="toggleAppStar('60i0ksMAhd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('60i0ksMAhd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5ZEbpBYGwH@OpenReview" style="display:none">
                    <span class="i-index">#304</span>
                    <a class="i-title" href="#5ZEbpBYGwH@OpenReview">COPER: Correlation-based Permutations for Multi-View Clustering</a>
                    <a class="i-star" onclick="toggleAppStar('5ZEbpBYGwH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5ZEbpBYGwH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4ub9gpx9xw@OpenReview" style="display:none">
                    <span class="i-index">#305</span>
                    <a class="i-title" href="#4ub9gpx9xw@OpenReview">Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations</a>
                    <a class="i-star" onclick="toggleAppStar('4ub9gpx9xw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4ub9gpx9xw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4FVGowGzQb@OpenReview" style="display:none">
                    <span class="i-index">#306</span>
                    <a class="i-title" href="#4FVGowGzQb@OpenReview">Preference Optimization as Probabilistic Inference</a>
                    <a class="i-star" onclick="toggleAppStar('4FVGowGzQb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4FVGowGzQb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-48WAZhwHHw@OpenReview" style="display:none">
                    <span class="i-index">#307</span>
                    <a class="i-title" href="#48WAZhwHHw@OpenReview">Planning in Natural Language Improves LLM Search for Code Generation</a>
                    <a class="i-star" onclick="toggleAppStar('48WAZhwHHw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('48WAZhwHHw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3fl1SENSYO@OpenReview" style="display:none">
                    <span class="i-index">#308</span>
                    <a class="i-title" href="#3fl1SENSYO@OpenReview">Unleashing the Potential of Diffusion Models for Incomplete Data Imputation</a>
                    <a class="i-star" onclick="toggleAppStar('3fl1SENSYO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3fl1SENSYO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2pNLknCTvG@OpenReview" style="display:none">
                    <span class="i-index">#309</span>
                    <a class="i-title" href="#2pNLknCTvG@OpenReview">uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed MABs</a>
                    <a class="i-star" onclick="toggleAppStar('2pNLknCTvG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2pNLknCTvG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1xzqz73hvL@OpenReview" style="display:none">
                    <span class="i-index">#310</span>
                    <a class="i-title" href="#1xzqz73hvL@OpenReview">High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws</a>
                    <a class="i-star" onclick="toggleAppStar('1xzqz73hvL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1xzqz73hvL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1Iuw1jcIrf@OpenReview" style="display:none">
                    <span class="i-index">#311</span>
                    <a class="i-title" href="#1Iuw1jcIrf@OpenReview">MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code</a>
                    <a class="i-star" onclick="toggleAppStar('1Iuw1jcIrf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1Iuw1jcIrf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0yvZm2AjUr@OpenReview" style="display:none">
                    <span class="i-index">#312</span>
                    <a class="i-title" href="#0yvZm2AjUr@OpenReview">Monitoring Latent World States in Language Models with Propositional Probes</a>
                    <a class="i-star" onclick="toggleAppStar('0yvZm2AjUr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0yvZm2AjUr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-yfW1x7uBS5@OpenReview" style="display:none">
                    <span class="i-index">#313</span>
                    <a class="i-title" href="#yfW1x7uBS5@OpenReview">Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI</a>
                    <a class="i-star" onclick="toggleAppStar('yfW1x7uBS5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('yfW1x7uBS5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xsELpEPn4A@OpenReview" style="display:none">
                    <span class="i-index">#314</span>
                    <a class="i-title" href="#xsELpEPn4A@OpenReview">JudgeLM: Fine-tuned Large Language Models are Scalable Judges</a>
                    <a class="i-star" onclick="toggleAppStar('xsELpEPn4A@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xsELpEPn4A@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xQBRrtQM8u@OpenReview" style="display:none">
                    <span class="i-index">#315</span>
                    <a class="i-title" href="#xQBRrtQM8u@OpenReview">Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control</a>
                    <a class="i-star" onclick="toggleAppStar('xQBRrtQM8u@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xQBRrtQM8u@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uTqnyF0JNR@OpenReview" style="display:none">
                    <span class="i-index">#316</span>
                    <a class="i-title" href="#uTqnyF0JNR@OpenReview">IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning</a>
                    <a class="i-star" onclick="toggleAppStar('uTqnyF0JNR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uTqnyF0JNR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vQhn4wrQ6j@OpenReview" style="display:none">
                    <span class="i-index">#317</span>
                    <a class="i-title" href="#vQhn4wrQ6j@OpenReview">Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('vQhn4wrQ6j@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vQhn4wrQ6j@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-m9wG6ai2Xk@OpenReview" style="display:none">
                    <span class="i-index">#318</span>
                    <a class="i-title" href="#m9wG6ai2Xk@OpenReview">MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations</a>
                    <a class="i-star" onclick="toggleAppStar('m9wG6ai2Xk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('m9wG6ai2Xk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mkDam1xIzW@OpenReview" style="display:none">
                    <span class="i-index">#319</span>
                    <a class="i-title" href="#mkDam1xIzW@OpenReview">Probabilistic Geometric Principal Component Analysis with application to neural data</a>
                    <a class="i-star" onclick="toggleAppStar('mkDam1xIzW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mkDam1xIzW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-l6QnSQizmN@OpenReview" style="display:none">
                    <span class="i-index">#320</span>
                    <a class="i-title" href="#l6QnSQizmN@OpenReview">Online Reinforcement Learning in Non-Stationary Context-Driven Environments</a>
                    <a class="i-star" onclick="toggleAppStar('l6QnSQizmN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('l6QnSQizmN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZV7CLf0RHK@OpenReview" style="display:none">
                    <span class="i-index">#321</span>
                    <a class="i-title" href="#ZV7CLf0RHK@OpenReview">Fine-tuning with Reserved Majority for Noise Reduction</a>
                    <a class="i-star" onclick="toggleAppStar('ZV7CLf0RHK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZV7CLf0RHK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-h8yg0hT96f@OpenReview" style="display:none">
                    <span class="i-index">#322</span>
                    <a class="i-title" href="#h8yg0hT96f@OpenReview">Bayesian Experimental Design Via Contrastive Diffusions</a>
                    <a class="i-star" onclick="toggleAppStar('h8yg0hT96f@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('h8yg0hT96f@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hwnObmOTrV@OpenReview" style="display:none">
                    <span class="i-index">#323</span>
                    <a class="i-title" href="#hwnObmOTrV@OpenReview">Modeling Complex System Dynamics with Flow Matching Across Time and Conditions</a>
                    <a class="i-star" onclick="toggleAppStar('hwnObmOTrV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hwnObmOTrV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NdHka08uWn@OpenReview" style="display:none">
                    <span class="i-index">#324</span>
                    <a class="i-title" href="#NdHka08uWn@OpenReview">RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regression</a>
                    <a class="i-star" onclick="toggleAppStar('NdHka08uWn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NdHka08uWn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WKfb1xGXGx@OpenReview" style="display:none">
                    <span class="i-index">#325</span>
                    <a class="i-title" href="#WKfb1xGXGx@OpenReview">Perm: A Parametric Representation for Multi-Style 3D Hair Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('WKfb1xGXGx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WKfb1xGXGx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2iCIHgE8KG@OpenReview" style="display:none">
                    <span class="i-index">#326</span>
                    <a class="i-title" href="#2iCIHgE8KG@OpenReview">Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA</a>
                    <a class="i-star" onclick="toggleAppStar('2iCIHgE8KG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2iCIHgE8KG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fGdF8Bq1FV@OpenReview" style="display:none">
                    <span class="i-index">#327</span>
                    <a class="i-title" href="#fGdF8Bq1FV@OpenReview">Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors</a>
                    <a class="i-star" onclick="toggleAppStar('fGdF8Bq1FV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fGdF8Bq1FV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-avSocG0oFA@OpenReview" style="display:none">
                    <span class="i-index">#328</span>
                    <a class="i-title" href="#avSocG0oFA@OpenReview">Revisiting Delta-Parameter Pruning For Fine-Tuned Models</a>
                    <a class="i-star" onclick="toggleAppStar('avSocG0oFA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('avSocG0oFA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pQsllTesiE@OpenReview" style="display:none">
                    <span class="i-index">#329</span>
                    <a class="i-title" href="#pQsllTesiE@OpenReview">Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</a>
                    <a class="i-star" onclick="toggleAppStar('pQsllTesiE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pQsllTesiE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SuH5SdOXpe@OpenReview" style="display:none">
                    <span class="i-index">#330</span>
                    <a class="i-title" href="#SuH5SdOXpe@OpenReview">Robustness Reprogramming for Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('SuH5SdOXpe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SuH5SdOXpe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OZVTqoli2N@OpenReview" style="display:none">
                    <span class="i-index">#331</span>
                    <a class="i-title" href="#OZVTqoli2N@OpenReview">A Second-Order Perspective on Model Compositionality and Incremental Learning</a>
                    <a class="i-star" onclick="toggleAppStar('OZVTqoli2N@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OZVTqoli2N@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kFsWpSxkFz@OpenReview" style="display:none">
                    <span class="i-index">#332</span>
                    <a class="i-title" href="#kFsWpSxkFz@OpenReview">MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility</a>
                    <a class="i-star" onclick="toggleAppStar('kFsWpSxkFz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kFsWpSxkFz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-A1ztozypga@OpenReview" style="display:none">
                    <span class="i-index">#333</span>
                    <a class="i-title" href="#A1ztozypga@OpenReview">Hymba: A Hybrid-head Architecture for Small Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('A1ztozypga@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('A1ztozypga@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-6tyPSkshtF@OpenReview" style="display:none">
                    <span class="i-index">#334</span>
                    <a class="i-title" href="#6tyPSkshtF@OpenReview">Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition</a>
                    <a class="i-star" onclick="toggleAppStar('6tyPSkshtF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6tyPSkshtF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-h0ZfDIrj7T@OpenReview" style="display:none">
                    <span class="i-index">#335</span>
                    <a class="i-title" href="#h0ZfDIrj7T@OpenReview">Mixture-of-Agents Enhances Large Language Model Capabilities</a>
                    <a class="i-star" onclick="toggleAppStar('h0ZfDIrj7T@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('h0ZfDIrj7T@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bhK7U37VW8@OpenReview" style="display:none">
                    <span class="i-index">#336</span>
                    <a class="i-title" href="#bhK7U37VW8@OpenReview">AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('bhK7U37VW8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bhK7U37VW8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-6RtRsg8ZV1@OpenReview" style="display:none">
                    <span class="i-index">#337</span>
                    <a class="i-title" href="#6RtRsg8ZV1@OpenReview">MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL</a>
                    <a class="i-star" onclick="toggleAppStar('6RtRsg8ZV1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6RtRsg8ZV1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tj5xJInWty@OpenReview" style="display:none">
                    <span class="i-index">#338</span>
                    <a class="i-title" href="#tj5xJInWty@OpenReview">Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency</a>
                    <a class="i-star" onclick="toggleAppStar('tj5xJInWty@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tj5xJInWty@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dsHpulHpOK@OpenReview" style="display:none">
                    <span class="i-index">#339</span>
                    <a class="i-title" href="#dsHpulHpOK@OpenReview">Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('dsHpulHpOK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dsHpulHpOK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-oI5tZaWkF9@OpenReview" style="display:none">
                    <span class="i-index">#340</span>
                    <a class="i-title" href="#oI5tZaWkF9@OpenReview">Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification</a>
                    <a class="i-star" onclick="toggleAppStar('oI5tZaWkF9@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oI5tZaWkF9@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-oSQiao9GqB@OpenReview" style="display:none">
                    <span class="i-index">#341</span>
                    <a class="i-title" href="#oSQiao9GqB@OpenReview">LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</a>
                    <a class="i-star" onclick="toggleAppStar('oSQiao9GqB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oSQiao9GqB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fgUFZAxywx@OpenReview" style="display:none">
                    <span class="i-index">#342</span>
                    <a class="i-title" href="#fgUFZAxywx@OpenReview">Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data</a>
                    <a class="i-star" onclick="toggleAppStar('fgUFZAxywx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fgUFZAxywx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tuu4de7HL1@OpenReview" style="display:none">
                    <span class="i-index">#343</span>
                    <a class="i-title" href="#tuu4de7HL1@OpenReview">Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('tuu4de7HL1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tuu4de7HL1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PstM8YfhvI@OpenReview" style="display:none">
                    <span class="i-index">#344</span>
                    <a class="i-title" href="#PstM8YfhvI@OpenReview">MorphoDiff: Cellular Morphology Painting with Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('PstM8YfhvI@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PstM8YfhvI@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qyU5s4fzLg@OpenReview" style="display:none">
                    <span class="i-index">#345</span>
                    <a class="i-title" href="#qyU5s4fzLg@OpenReview">Improving Unsupervised Constituency Parsing via Maximizing Semantic Information</a>
                    <a class="i-star" onclick="toggleAppStar('qyU5s4fzLg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qyU5s4fzLg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-VCbqXtS5YY@OpenReview" style="display:none">
                    <span class="i-index">#346</span>
                    <a class="i-title" href="#VCbqXtS5YY@OpenReview">Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('VCbqXtS5YY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VCbqXtS5YY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-TId1SHe8JG@OpenReview" style="display:none">
                    <span class="i-index">#347</span>
                    <a class="i-title" href="#TId1SHe8JG@OpenReview">Provable Uncertainty Decomposition via Higher-Order Calibration</a>
                    <a class="i-star" onclick="toggleAppStar('TId1SHe8JG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TId1SHe8JG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IuU0wcO0mo@OpenReview" style="display:none">
                    <span class="i-index">#348</span>
                    <a class="i-title" href="#IuU0wcO0mo@OpenReview">Multi-session, multi-task neural decoding from distinct cell-types and brain regions</a>
                    <a class="i-star" onclick="toggleAppStar('IuU0wcO0mo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IuU0wcO0mo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tfyHbvFZ0K@OpenReview" style="display:none">
                    <span class="i-index">#349</span>
                    <a class="i-title" href="#tfyHbvFZ0K@OpenReview">Knowledge Localization: Mission Not Accomplished? Enter Query Localization!</a>
                    <a class="i-star" onclick="toggleAppStar('tfyHbvFZ0K@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tfyHbvFZ0K@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-04qx93Viwj@OpenReview" style="display:none">
                    <span class="i-index">#350</span>
                    <a class="i-title" href="#04qx93Viwj@OpenReview">Holistically Evaluating the Environmental Impact of Creating Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('04qx93Viwj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('04qx93Viwj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FtX6oAW7Dd@OpenReview" style="display:none">
                    <span class="i-index">#351</span>
                    <a class="i-title" href="#FtX6oAW7Dd@OpenReview">PLENCH: Realistic Evaluation of Deep Partial-Label Learning Algorithms</a>
                    <a class="i-star" onclick="toggleAppStar('FtX6oAW7Dd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FtX6oAW7Dd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-lgsyLSsDRe@OpenReview" style="display:none">
                    <span class="i-index">#352</span>
                    <a class="i-title" href="#lgsyLSsDRe@OpenReview">NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models</a>
                    <a class="i-star" onclick="toggleAppStar('lgsyLSsDRe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('lgsyLSsDRe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3cgMU3TyyE@OpenReview" style="display:none">
                    <span class="i-index">#353</span>
                    <a class="i-title" href="#3cgMU3TyyE@OpenReview">Broaden your SCOPE! Efficient Conversation Planning for LLMs with Semantic Space</a>
                    <a class="i-star" onclick="toggleAppStar('3cgMU3TyyE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3cgMU3TyyE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rySLejeB1k@OpenReview" style="display:none">
                    <span class="i-index">#354</span>
                    <a class="i-title" href="#rySLejeB1k@OpenReview">Emergent Orientation Maps —— Mechanisms, Coding Efficiency and Robustness</a>
                    <a class="i-star" onclick="toggleAppStar('rySLejeB1k@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rySLejeB1k@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-6NNA0MxhCH@OpenReview" style="display:none">
                    <span class="i-index">#355</span>
                    <a class="i-title" href="#6NNA0MxhCH@OpenReview">Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions</a>
                    <a class="i-star" onclick="toggleAppStar('6NNA0MxhCH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6NNA0MxhCH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ixMBnOhFGd@OpenReview" style="display:none">
                    <span class="i-index">#356</span>
                    <a class="i-title" href="#ixMBnOhFGd@OpenReview">SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction</a>
                    <a class="i-star" onclick="toggleAppStar('ixMBnOhFGd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ixMBnOhFGd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OdnqG1fYpo@OpenReview" style="display:none">
                    <span class="i-index">#357</span>
                    <a class="i-title" href="#OdnqG1fYpo@OpenReview">Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation</a>
                    <a class="i-star" onclick="toggleAppStar('OdnqG1fYpo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OdnqG1fYpo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ztzZDzgfrh@OpenReview" style="display:none">
                    <span class="i-index">#358</span>
                    <a class="i-title" href="#ztzZDzgfrh@OpenReview">ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability</a>
                    <a class="i-star" onclick="toggleAppStar('ztzZDzgfrh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ztzZDzgfrh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cfKZ5VrhXt@OpenReview" style="display:none">
                    <span class="i-index">#359</span>
                    <a class="i-title" href="#cfKZ5VrhXt@OpenReview">Online Preference Alignment for Language Models via Count-based Exploration</a>
                    <a class="i-star" onclick="toggleAppStar('cfKZ5VrhXt@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cfKZ5VrhXt@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gye2U9uNXx@OpenReview" style="display:none">
                    <span class="i-index">#360</span>
                    <a class="i-title" href="#gye2U9uNXx@OpenReview">Uncovering Gaps in How Humans and LLMs Interpret Subjective Language</a>
                    <a class="i-star" onclick="toggleAppStar('gye2U9uNXx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gye2U9uNXx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KZgo2YQbhc@OpenReview" style="display:none">
                    <span class="i-index">#361</span>
                    <a class="i-title" href="#KZgo2YQbhc@OpenReview">PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction</a>
                    <a class="i-star" onclick="toggleAppStar('KZgo2YQbhc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KZgo2YQbhc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-i8dYPGdB1C@OpenReview" style="display:none">
                    <span class="i-index">#362</span>
                    <a class="i-title" href="#i8dYPGdB1C@OpenReview">Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency</a>
                    <a class="i-star" onclick="toggleAppStar('i8dYPGdB1C@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('i8dYPGdB1C@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fs9EabmQrJ@OpenReview" style="display:none">
                    <span class="i-index">#363</span>
                    <a class="i-title" href="#Fs9EabmQrJ@OpenReview">EmbedLLM: Learning Compact Representations of Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Fs9EabmQrJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fs9EabmQrJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rxVvRBgqmS@OpenReview" style="display:none">
                    <span class="i-index">#364</span>
                    <a class="i-title" href="#rxVvRBgqmS@OpenReview">PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance</a>
                    <a class="i-star" onclick="toggleAppStar('rxVvRBgqmS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rxVvRBgqmS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ALzTQUgW8a@OpenReview" style="display:none">
                    <span class="i-index">#365</span>
                    <a class="i-title" href="#ALzTQUgW8a@OpenReview">MagicPIG: LSH Sampling for Efficient LLM Generation</a>
                    <a class="i-star" onclick="toggleAppStar('ALzTQUgW8a@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ALzTQUgW8a@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dGVZwyq5tV@OpenReview" style="display:none">
                    <span class="i-index">#366</span>
                    <a class="i-title" href="#dGVZwyq5tV@OpenReview">Training-Free Activation Sparsity in Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('dGVZwyq5tV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dGVZwyq5tV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kam84eEmub@OpenReview" style="display:none">
                    <span class="i-index">#367</span>
                    <a class="i-title" href="#kam84eEmub@OpenReview">LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation</a>
                    <a class="i-star" onclick="toggleAppStar('kam84eEmub@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kam84eEmub@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fxv0FfmDAg@OpenReview" style="display:none">
                    <span class="i-index">#368</span>
                    <a class="i-title" href="#fxv0FfmDAg@OpenReview">DRoP: Distributionally Robust Data Pruning</a>
                    <a class="i-star" onclick="toggleAppStar('fxv0FfmDAg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fxv0FfmDAg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NQqJPPCesd@OpenReview" style="display:none">
                    <span class="i-index">#369</span>
                    <a class="i-title" href="#NQqJPPCesd@OpenReview">Towards Marginal Fairness Sliced Wasserstein Barycenter</a>
                    <a class="i-star" onclick="toggleAppStar('NQqJPPCesd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NQqJPPCesd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UqYNPyotxL@OpenReview" style="display:none">
                    <span class="i-index">#370</span>
                    <a class="i-title" href="#UqYNPyotxL@OpenReview">Linear Mode Connectivity in Differentiable Tree Ensembles</a>
                    <a class="i-star" onclick="toggleAppStar('UqYNPyotxL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UqYNPyotxL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pPyJyeLriR@OpenReview" style="display:none">
                    <span class="i-index">#371</span>
                    <a class="i-title" href="#pPyJyeLriR@OpenReview">Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier</a>
                    <a class="i-star" onclick="toggleAppStar('pPyJyeLriR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pPyJyeLriR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SjufxrSOYd@OpenReview" style="display:none">
                    <span class="i-index">#372</span>
                    <a class="i-title" href="#SjufxrSOYd@OpenReview">Invariant Graphon Networks: Approximation and Cut Distance</a>
                    <a class="i-star" onclick="toggleAppStar('SjufxrSOYd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SjufxrSOYd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bk13Qfu8Ru@OpenReview" style="display:none">
                    <span class="i-index">#373</span>
                    <a class="i-title" href="#Bk13Qfu8Ru@OpenReview">Severing Spurious Correlations with Data Pruning</a>
                    <a class="i-star" onclick="toggleAppStar('Bk13Qfu8Ru@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bk13Qfu8Ru@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>