<html><head>
    <title>ICCV.2025 - Highlight | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for ICCV.2025 - Highlight, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body id="venue"><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">ICCV.2025 - Highlight</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 263
    </p>
    <div class="papers">
        <div id="Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="panel paper" keywords="vdm,flashvdm,generation,vectset,diffusion,vae,dit,hunyuan3d,shape,decoding">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation_ICCV_2025_paper.html" target="_blank" title="1/263"><span class="index notranslate">#1</span></a>
                <a id="title-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="title-link" href="/venue/Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" target="_blank">Unleashing Vecset Diffusion Model for Fast Shape Generation</a>
                <a id="pdf-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF">12</sup>]</a>
                <a id="copy-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeqiang Lai" target="_blank">Zeqiang Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunfei Zhao" target="_blank">Yunfei Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zibo Zhao" target="_blank">Zibo Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haolin Liu" target="_blank">Haolin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fuyun Wang" target="_blank">Fuyun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huiwen Shi" target="_blank">Huiwen Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianghui Yang" target="_blank">Xianghui Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingxiang Lin" target="_blank">Qingxiang Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingwei Huang" target="_blank">Jingwei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhong Liu" target="_blank">Yuhong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Jiang" target="_blank">Jie Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunchao Guo" target="_blank">Chunchao Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyu Yue" target="_blank">Xiangyu Yue</a>
            </p>
            <p id="summary-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="summary">3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vectset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles at high-speed generation. Challenges exist because of not only difficulties in accelerating diffusion sampling but also VAE decoding in VDM -- areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps, while maintaining comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation technique. For VAE, we introduce a lightning vectset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding,, and Efficient Network Design. By exploiting the locality of vectset and the sparsity of shape surface in the volume, the proposed decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to the current state-of-the-art open-source shape generation model Hunyuan3D-2, resulting in Hunyuan3D-2 Turbo. Through systematic evaluation for both generation and reconstruction, we demonstrate that our model outperforms existing fast 3D generation methods by a significant margin, achieving comparable performance to the state-of-the-art models while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are publicly available at https://github.com/Tencent-Hunyuan/FlashVDM.</p>
            <p id="subjects-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" onclick="foldPdfKimi('Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="panel paper" keywords="depth,data,sim2real,real,stage,simulation,captured,diffusion,simulated,synthetic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth_ICCV_2025_paper.html" target="_blank" title="2/263"><span class="index notranslate">#2</span></a>
                <a id="title-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="title-link" href="/venue/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" target="_blank">Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion</a>
                <a id="pdf-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mutian Xu" target="_blank">Mutian Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chongjie Ye" target="_blank">Chongjie Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haolin Liu" target="_blank">Haolin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yushuang Wu" target="_blank">Yushuang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Chang" target="_blank">Jiahao Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoguang Han" target="_blank">Xiaoguang Han</a>
            </p>
            <p id="summary-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="summary">3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes StableDiffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns.</p>
            <p id="subjects-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" onclick="foldPdfKimi('Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="panel paper" keywords="udc,vit,video,videos,display,dataset,datasets,world,udcs,degraded">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras_ICCV_2025_paper.html" target="_blank" title="3/263"><span class="index notranslate">#3</span></a>
                <a id="title-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="title-link" href="/venue/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" target="_blank">UDC-VIT: A Real-World Video Dataset for Under-Display Cameras</a>
                <a id="pdf-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kyusu Ahn" target="_blank">Kyusu Ahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=JiSoo Kim" target="_blank">JiSoo Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangik Lee" target="_blank">Sangik Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=HyunGyu Lee" target="_blank">HyunGyu Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Byeonghyun Ko" target="_blank">Byeonghyun Ko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chanwoo Park" target="_blank">Chanwoo Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaejin Lee" target="_blank">Jaejin Lee</a>
            </p>
            <p id="summary-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="summary">Even though an Under-Display Camera (UDC) is an advanced imaging system, the display panel significantly degrades captured images or videos, introducing low transmittance, blur, noise, and flare issues. Tackling such issues is challenging because of the complex degradation of UDCs, including diverse flare patterns. However, no dataset contains videos of real-world UDC degradation. In this paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike existing datasets, UDC-VIT exclusively includes human motions for facial recognition. We propose a video-capturing system to acquire clean and UDC-degraded videos of the same scene simultaneously. Then, we align a pair of captured videos frame by frame, using discrete Fourier transform (DFT). We compare UDC-VIT with six representative UDC still image datasets and two existing UDC video datasets. Using six deep-learning models, we compare UDC-VIT and an existing synthetic UDC video dataset. The results indicate the ineffectiveness of models trained on earlier synthetic UDC video datasets, as they do not reflect the actual characteristics of UDC-degraded videos. We also demonstrate the importance of effective UDC restoration by evaluating face recognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT is available at our official GitHub repository.</p>
            <p id="subjects-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" onclick="foldPdfKimi('Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="panel paper" keywords="driver,attention,llada,cognitive,explainable,driving,prediction,allocation,deeper,autonomous">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction_ICCV_2025_paper.html" target="_blank" title="4/263"><span class="index notranslate">#4</span></a>
                <a id="title-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="title-link" href="/venue/Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" target="_blank">Where, What, Why: Towards Explainable Driver Attention Prediction</a>
                <a id="pdf-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF">11</sup>]</a>
                <a id="copy-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Zhou" target="_blank">Yuchen Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayu Tang" target="_blank">Jiayu Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyan Xiao" target="_blank">Xiaoyan Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueyao Lin" target="_blank">Yueyao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linkai Liu" target="_blank">Linkai Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zipeng Guo" target="_blank">Zipeng Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Fei" target="_blank">Hao Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaobo Xia" target="_blank">Xiaobo Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Gou" target="_blank">Chao Gou</a>
            </p>
            <p id="summary-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="summary">Modeling task-driven attention in driving is a fundamental challenge for both autonomous vehicles and cognitive science. Existing methods primarily predict where drivers look by generating spatial heatmaps, but fail to capture the cognitive motivations behind attention allocation in specific contexts, which limits deeper understanding of attention mechanisms. To bridge this gap, we introduce Explainable Driver Attention Prediction, a novel task paradigm that jointly predicts spatial attention regions (where), parses attended semantics (what), and provides cognitive reasoning for attention allocation (why). To support this, we present W3DA, the first large-scale explainable driver attention dataset. It enriches existing benchmarks with detailed semantic and causal annotations across diverse driving scenarios, including normal conditions, safety-critical situations, and traffic accidents. We further propose LLada, a Large Language model-driven framework for driver attention prediction, which unifies pixel modeling, semantic parsing, and cognitive reasoning within an end-to-end architecture. Extensive experiments demonstrate the effectiveness of LLada, exhibiting robust generalization across datasets and driving conditions. This work serves as a key step toward a deeper understanding of driver attention mechanisms, with significant implications for autonomous driving, intelligent driver training, and human-computer interaction.</p>
            <p id="subjects-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" onclick="foldPdfKimi('Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="panel paper" keywords="vrm,distillation,knowledge,relation,virtual,matching,inter,teacher,spurious,affinity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching_ICCV_2025_paper.html" target="_blank" title="5/263"><span class="index notranslate">#5</span></a>
                <a id="title-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="title-link" href="/venue/Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" target="_blank">VRM: Knowledge Distillation via Virtual Relation Matching</a>
                <a id="pdf-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weijia Zhang" target="_blank">Weijia Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Xie" target="_blank">Fei Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weidong Cai" target="_blank">Weidong Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Ma" target="_blank">Chao Ma</a>
            </p>
            <p id="summary-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="summary">Knowledge distillation (KD) aims to transfer the knowledge of a more capable yet cumbersome teacher model to a lightweight student model. In recent years, relation-based KD methods have fallen behind, as their instance-matching counterparts dominate in performance. In this paper, we revive relational KD by identifying and tackling several key issues in relation-based methods, including their susceptibility to overfitting and spurious responses. Specifically, we transfer novelly constructed affinity graphs that compactly encapsulate a wealth of beneficial inter-sample, inter-class, and inter-view correlations by exploiting virtual views and relations as a new kind of knowledge. As a result, the student has access to richer guidance signals and stronger regularisation throughout the distillation process. To further mitigate the adverse impact of spurious responses, we prune the affinity graphs by dynamically detaching redundant and unreliable edges. Extensive experiments on CIFAR-100, ImageNet, and MS-COCO datasets demonstrate the superior performance of the proposed virtual relation matching (VRM) method, where it consistently sets new state-of-the-art records over a range of models, architectures, tasks, and set-ups. For instance, VRM for the first time hits 74.0% accuracy for ResNet50-to-MobileNetV2 distillation on ImageNet, and improves DeiT-T by 14.44% on CIFAR-100 with a ResNet56 teacher.</p>
            <p id="subjects-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="panel paper" keywords="objectmate,object,insertion,subject,composing,photorealistic,scene,driven,generation,lighting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation_ICCV_2025_paper.html" target="_blank" title="6/263"><span class="index notranslate">#6</span></a>
                <a id="title-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="title-link" href="/venue/Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" target="_blank">ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation</a>
                <a id="pdf-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Winter" target="_blank">Daniel Winter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Asaf Shul" target="_blank">Asaf Shul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matan Cohen" target="_blank">Matan Cohen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dana Berman" target="_blank">Dana Berman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yael Pritch" target="_blank">Yael Pritch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Rav-Acha" target="_blank">Alex Rav-Acha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yedid Hoshen" target="_blank">Yedid Hoshen</a>
            </p>
            <p id="summary-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="summary">This paper introduces a tuning-free method for both object insertion and subject-driven generation. The task involves composing an object, given multiple views, into a scene specified by either an image or text. Existing methods struggle to fully meet the task's challenging objectives: (i) seamlessly composing the object into the scene with photorealistic pose and lighting, and (ii) preserving the object's identity. We hypothesize that achieving these goals requires large-scale supervision, but manually collecting sufficient data is simply too expensive. The key observation in this paper is that many mass-produced objects recur across multiple images of large unlabeled datasets, in different scenes, poses, and lighting conditions. We use this observation to create massive supervision by retrieving sets of diverse views of the same object. This powerful paired dataset enables us to train a straightforward text-to-image diffusion architecture to map the object and scene descriptions to the composite image. We compare our method, ObjectMate, with state-of-the-art methods for object insertion and subject-driven generation, using a single or multiple references. Empirically, ObjectMate achieves superior identity preservation and more photorealistic composition. Differently from many other multi-reference methods, ObjectMate does not require slow test-time tuning.</p>
            <p id="subjects-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" onclick="foldPdfKimi('Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="panel paper" keywords="unziplora,style,loras,subject,lora,recontextualization,image,dreambooth,variations,recombining">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image_ICCV_2025_paper.html" target="_blank" title="7/263"><span class="index notranslate">#7</span></a>
                <a id="title-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="title-link" href="/venue/Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" target="_blank">UnZipLoRA: Separating Content and Style from a Single Image</a>
                <a id="pdf-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF">19</sup>]</a>
                <a id="copy-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Liu" target="_blank">Chang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viraj Shah" target="_blank">Viraj Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aiyu Cui" target="_blank">Aiyu Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Svetlana Lazebnik" target="_blank">Svetlana Lazebnik</a>
            </p>
            <p id="summary-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="summary">This paper introduces UnZipLoRA, a method for decomposing an image into its constituent subject and style, represented as two distinct LoRAs (Low-Rank Adaptations). Unlike existing personalization techniques that focus on either subject or style in isolation, or require separate training sets for each, UnZipLoRA disentangles these elements from a single image by training both the LoRAs simultaneously. UnZipLoRA ensures that the resulting LoRAs are compatible, i.e., they can be seamlessly combined using direct addition. UnZipLoRA enables independent manipulation and recontextualization of subject and style, including generating variations of each, applying the extracted style to new subjects, and recombining them to reconstruct the original image or create novel variations. To address the challenge of subject and style entanglement, UnZipLoRA employs a novel prompt separation technique, as well as column and block separation strategies to accurately preserve the characteristics of subject and style, and ensure compatibility between the learned LoRAs. Evaluation with human studies and quantitative metrics demonstrates UnZipLoRA's effectiveness compared to other state-of-the-art methods, including DreamBooth-LoRA, Inspiration Tree, and B-LoRA.</p>
            <p id="subjects-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" onclick="foldPdfKimi('Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="panel paper" keywords="vit3d,trajvit,video,tokenization,tokens,token,panoptic,grounded,trajectory,encoder">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object_ICCV_2025_paper.html" target="_blank" title="8/263"><span class="index notranslate">#8</span></a>
                <a id="title-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="title-link" href="/venue/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" target="_blank">One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory</a>
                <a id="pdf-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenhao Zheng" target="_blank">Chenhao Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jieyu Zhang" target="_blank">Jieyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammadreza Salehi" target="_blank">Mohammadreza Salehi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqi Gao" target="_blank">Ziqi Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vishnu Iyengar" target="_blank">Vishnu Iyengar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Norimasa Kobori" target="_blank">Norimasa Kobori</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Kong" target="_blank">Quan Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ranjay Krishna" target="_blank">Ranjay Krishna</a>
            </p>
            <p id="summary-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="summary">Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 20x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.</p>
            <p id="subjects-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" onclick="foldPdfKimi('Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="panel paper" keywords="physnap,sdfs,articulated,diffusion,plausibility,guiding,cloud,alignment,objects,point">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment_ICCV_2025_paper.html" target="_blank" title="9/263"><span class="index notranslate">#9</span></a>
                <a id="title-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="title-link" href="/venue/Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" target="_blank">Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints</a>
                <a id="pdf-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jens U. Kreber" target="_blank">Jens U. Kreber</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joerg Stueckler" target="_blank">Joerg Stueckler</a>
            </p>
            <p id="summary-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="summary">Articulated objects are an important type of interactable objects in everyday environments. In this paper, we propose PhysNAP, a novel diffusion model-based approach for generating articulated objects that aligns them with partial point clouds and improves their physical plausibility. The model represents part shapes by signed distance functions (SDFs). We guide the reverse diffusion process using a point cloud alignment loss computed using the predicted SDFs. Additionally, we impose non-penetration and mobility constraints based on the part SDFs for guiding the model to generate more physically plausible objects. We also make our diffusion approach category-aware to further improve point cloud alignment if category information is available. We evaluate the generative ability and constraint consistency of samples generated with PhysNAP using the PartNet-Mobility dataset. We also compare it with an unguided baseline diffusion model and demonstrate that PhysNAP can improve constraint consistency and provides a tradeoff with generative ability.</p>
            <p id="subjects-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" onclick="foldPdfKimi('Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="panel paper" keywords="lidar,localization,raloc,rotation,scr,outdoor,awareness,dataset,robotcar,canonicalization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness_ICCV_2025_paper.html" target="_blank" title="10/263"><span class="index notranslate">#10</span></a>
                <a id="title-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="title-link" href="/venue/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" target="_blank">RALoc: Enhancing Outdoor LiDAR Localization via Rotation Awareness</a>
                <a id="pdf-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuyang Yang" target="_blank">Yuyang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen Li" target="_blank">Wen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Ao" target="_blank">Sheng Ao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingshan Xu" target="_blank">Qingshan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangshu Yu" target="_blank">Shangshu Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Guo" target="_blank">Yu Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin Zhou" target="_blank">Yin Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siqi Shen" target="_blank">Siqi Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Wang" target="_blank">Cheng Wang</a>
            </p>
            <p id="summary-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="summary">LiDAR localization is a fundamental task in autonomous driving and robotics. Scene Coordinate Regression (SCR) exhibits leading pose accuracy, achieving impressive results in learning-based localization. We observe that the real-world LiDAR scans captured from different viewpoints usually result in the catastrophic collapse of SCR. However, existing LiDAR localization methods have largely overlooked the issue of rotation sensitivity in SCR. In this paper, we present RALoc, an outdoor LiDAR localization method with rotation awareness to achieve accurate localization. The key to our approach is to design a Point Cloud Canonicalization module, which leverages a powerful equivariant key feature aggregation to transform the input LiDAR scan towards a consistent orientation, effectively eliminating the adverse effects of rotation. This proposed module has promising scalability and can be seamlessly integrated with the existing LiDAR localization network. Moreover, we propose the Bidirectional LiDAR Localization (BiLiLo) dataset as a benchmark to evaluate the performance of various methods in large outdoor scenes with significant rotation changes. Extensive experiments show that RALoc significantly improves localization performance in scenarios with large rotation changes, and also achieves competitive performance in the Oxford Radar RobotCar dataset. Our code and dataset will be released upon acceptance.</p>
            <p id="subjects-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" onclick="foldPdfKimi('Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="panel paper" keywords="dreamlayer,layer,generation,image,harmonization,multi,attention,inter,coherent,layers">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model_ICCV_2025_paper.html" target="_blank" title="11/263"><span class="index notranslate">#11</span></a>
                <a id="title-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="title-link" href="/venue/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" target="_blank">DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Model</a>
                <a id="pdf-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF">12</sup>]</a>
                <a id="copy-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junjia Huang" target="_blank">Junjia Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengxiang Yan" target="_blank">Pengxiang Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinhang Cai" target="_blank">Jinhang Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiyang Liu" target="_blank">Jiyang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhao Wang" target="_blank">Zhao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yitong Wang" target="_blank">Yitong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinglong Wu" target="_blank">Xinglong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guanbin Li" target="_blank">Guanbin Li</a>
            </p>
            <p id="summary-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="summary">Text-driven image generation using diffusion models has recently gained significant attention. To enable more flexible image manipulation and editing, recent research has expanded from single image generation to transparent layer generation and multi-layer compositions. However, existing approaches often fail to provide a thorough exploration of multi-layer structures, leading to inconsistent inter-layer interactions, such as occlusion relationships, spatial layout, and shadowing. In this paper, we introduce DreamLayer, a novel framework that enables coherent text-driven generation of multiple image layers, by explicitly modeling the relationship between transparent foreground and background layers. DreamLayer incorporates three key components, i.e., Context-Aware Cross-Attention (CACA) for global-local information exchange, Layer-Shared Self-Attention (LSSA) for establishing robust inter-layer connections, and Information Retained Harmonization (IRH) for refining fusion details at the latent level.By leveraging a coherent full-image context, DreamLayer builds inter-layer connections through attention mechanisms and applies a harmonization step to achieve seamless layer fusion. To facilitate research in multi-layer generation, we construct a high-quality, diverse multi-layer dataset including 400k samples. Extensive experiments and user studies demonstrate that DreamLayer generates more coherent and well-aligned layers, with broad applicability, including latent-space image editing and image-to-layer decomposition.</p>
            <p id="subjects-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" onclick="foldPdfKimi('Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="panel paper" keywords="occupancy,autoocc,ended,splatting,vision,annotation,semantic,open,language,gaussian">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian_ICCV_2025_paper.html" target="_blank" title="12/263"><span class="index notranslate">#12</span></a>
                <a id="title-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="title-link" href="/venue/Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" target="_blank">AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting</a>
                <a id="pdf-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Zhou" target="_blank">Xiaoyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingqi Wang" target="_blank">Jingqi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongtao Wang" target="_blank">Yongtao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufei Wei" target="_blank">Yufei Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Dong" target="_blank">Nan Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Hsuan Yang" target="_blank">Ming-Hsuan Yang</a>
            </p>
            <p id="summary-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="summary">Obtaining high-quality 3D semantic occupancy from raw sensor data remains an essential yet challenging task, often requiring extensive manual labeling. In this work, we propose AutoOcc, a vision-centric automated pipeline for open-ended semantic occupancy annotation that integrates differentiable Gaussian splatting guided by vision-language models. We formulate the open-ended semantic 3D occupancy reconstruction task to automatically generate scene occupancy by combining attention maps from vision-language models and foundation vision models. We devise semantic-aware Gaussians as intermediate geometric descriptors and propose a cumulative Gaussian-to-voxel splatting algorithm that enables effective and efficient occupancy annotation. Our framework outperforms existing automated occupancy annotation methods without human labels. AutoOcc also enables open-ended semantic occupancy auto-labeling, achieving robust performance in both static and dynamically complex scenarios.</p>
            <p id="subjects-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" onclick="foldPdfKimi('Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="panel paper" keywords="mtl,task,rep,mto,representation,saliency,complementary,entropybased,optimizer,unleashing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task_ICCV_2025_paper.html" target="_blank" title="13/263"><span class="index notranslate">#13</span></a>
                <a id="title-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="title-link" href="/venue/Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" target="_blank">Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</a>
                <a id="pdf-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zedong Wang" target="_blank">Zedong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Li" target="_blank">Siyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Xu" target="_blank">Dan Xu</a>
            </p>
            <p id="summary-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="summary">Despite the promise of Multi-Task Learning (MTL) in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts through optimizer-centric loss scaling and gradient manipulation, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizer designs, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropybased penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting (EW) policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law (PL) exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing.</p>
            <p id="subjects-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="panel paper" keywords="optimizable,registration,distance,affine,grassmannian,cost,subspace,geodesic,function,joomeok">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance_ICCV_2025_paper.html" target="_blank" title="14/263"><span class="index notranslate">#14</span></a>
                <a id="title-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="title-link" href="/venue/Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" target="_blank">Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold</a>
                <a id="pdf-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jaeho Shin" target="_blank">Jaeho Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyeonjae Gil" target="_blank">Hyeonjae Gil</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junwoo Jang" target="_blank">Junwoo Jang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maani Ghaffari" target="_blank">Maani Ghaffari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayoung Kim" target="_blank">Ayoung Kim</a>
            </p>
            <p id="summary-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="summary">Affine Grassmannian has been favored for expressing proximity between lines and planes due to its theoretical exactness in measuring distances among features. Despite this advantage, the existing method can only measure the proximity without yielding the distance as an explicit function of rigid body transformation. Thus, an optimizable distance function on the manifold has remained underdeveloped, stifling its application in registration problems. This paper is the first to explicitly derive an optimizable cost function between two Grassmannian features with respect to rigid body transformation (R and t). Specifically, we present a rigorous mathematical proof demonstrating that the bases of high-dimensional linear subspaces can serve as an explicit representation of the cost. Finally, we propose an optimizable cost function based on the transformed bases that can be applied to the registration problem of any affine subspace. Compared to vector parameter-based approaches, our method is able to find a globally optimal solution by directly minimizing the geodesic distance which is agnostic to representation ambiguity. The resulting cost function and its extension to the inlier-set maximizing Branch-and-Bound (BnB) solver have been demonstrated to improve the convergence of existing solutions or outperform them in various computer vision tasks. The code is available on https://github.com/joomeok/GrassmannRegistration.</p>
            <p id="subjects-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" onclick="foldPdfKimi('Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="panel paper" keywords="clip,modality,gap,continual,linlany,mindthegap,compensating,pre,trained,learning">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap_ICCV_2025_paper.html" target="_blank" title="15/263"><span class="index notranslate">#15</span></a>
                <a id="title-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="title-link" href="/venue/Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" target="_blank">Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning</a>
                <a id="pdf-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF">9</sup>]</a>
                <a id="copy-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linlan Huang" target="_blank">Linlan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xusheng Cao" target="_blank">Xusheng Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haori Lu" target="_blank">Haori Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Meng" target="_blank">Yifan Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Yang" target="_blank">Fei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xialei Liu" target="_blank">Xialei Liu</a>
            </p>
            <p id="summary-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="summary">Continual learning aims to enable models to learn sequentially from continuously incoming data while retaining performance on previously learned tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting strong capabilities across various downstream tasks, there has been growing interest in leveraging CLIP for continual learning in such scenarios. Most existing works overlook the inherent modality gap in CLIP, a key factor in its generalization and adaptability. In this paper, we analyze the variations in the modality gap during the fine-tuning of vision-language pre-trained models. Our observations reveal that the modality gap effectively reflects the extent to which pre-trained knowledge is preserved. Based on these insights, we propose a simple yet effective method, MG-CLIP, that improves CLIP's performance in class-incremental learning. Our approach leverages modality gap preservation to mitigate forgetting and modality gap compensation to enhance the capacity for new data, introducing a novel modality-gap-based perspective for continual learning. Extensive experiments on multiple benchmarks demonstrate that our method outperforms existing approaches without requiring additional replay data. Our code is available at https://github.com/linlany/MindtheGap.</p>
            <p id="subjects-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" onclick="foldPdfKimi('Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="panel paper" keywords="buffer,registration,scenes,voxel,shot,cloud,diverse,generalization,manual,search">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.html" target="_blank" title="16/263"><span class="index notranslate">#16</span></a>
                <a id="title-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="title-link" href="/venue/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" target="_blank">BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes</a>
                <a id="pdf-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Minkyun Seo" target="_blank">Minkyun Seo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyungtae Lim" target="_blank">Hyungtae Lim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kanghee Lee" target="_blank">Kanghee Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Carlone" target="_blank">Luca Carlone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaesik Park" target="_blank">Jaesik Park</a>
            </p>
            <p id="summary-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="summary">Recent advances in deep learning-based point cloud registration have improved generalization, yet most methods still require retraining or manual parameter tuning for each new environment. In this paper, we identify three key factors limiting generalization: (a) reliance on environment-specific voxel size and search radius, (b) poor out-of-domain robustness of learning-based keypoint detectors,and (c) raw coordinate usage, which exacerbates scale discrepancies. To address these issues, we present a zero-shot registration pipeline called BUFFER-X by (a) adaptively determining voxel size/search radii, (b) using farthest point sampling to bypass learned detectors, and (c) leveraging patch-wise scale normalization for consistent coordinate bounds. In particular, we present a multi-scale patch-based descriptor generation and a hierarchical inlier search across scales to improve robustness in diverse scenes. We also propose a novel generalizability benchmark using 11 datasets that cover various indoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X achieves substantial generalization without prior information or manual parameter tuning for the test datasets. Our code is available at https://github.com/MIT-SPARK/BUFFER-X.</p>
            <p id="subjects-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" onclick="foldPdfKimi('Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="panel paper" keywords="spike,noise,restoration,diffusion,light,degradation,low,restoring,deterministic,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration_ICCV_2025_paper.html" target="_blank" title="17/263"><span class="index notranslate">#17</span></a>
                <a id="title-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="title-link" href="/venue/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" target="_blank">Noise-Modeled Diffusion Models for Low-Light Spike Image Restoration</a>
                <a id="pdf-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruonan Liu" target="_blank">Ruonan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Zhu" target="_blank">Lin Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xijie Xiang" target="_blank">Xijie Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lizhi Wang" target="_blank">Lizhi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hua Huang" target="_blank">Hua Huang</a>
            </p>
            <p id="summary-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="summary">Spike-based imaging, inspired by the human visual system, offers several advantages, including high temporal resolution and low power consumption, but suffers from significant image degradation in low-light conditions due to noise interference. Restoring spike images under such conditions poses a significant challenge, as traditional frame-based or spike-based techniques are ill-suited to handle such severe noise and unique noise characteristics. This paper proposes a novel approach for restoring low-light spike images using noise-modeled diffusion models. By establishing a noise-embedded spike imaging model under low light, we model the forward diffusion process as the degradation of spike images with proportional and residual terms and incorporate deterministic and non-deterministic components with reverse shifting, enabling the model to capture the distinctive spike noise structure. Additionally, we utilize region mask image, dark current map and spike density value as conditions to further guide the restoration process by providing prompts for degradation regions, deterministic parameters and noise intensity, respectively. Experimental results demonstrate that our method significantly outperforms existing spike-based reconstruction and diffusion-based image restoration methods in both quantitative performance and visual quality. The code and dataset are available at https://github.com/BIT-Vision/SpikeDiffusion.</p>
            <p id="subjects-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" onclick="foldPdfKimi('Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="panel paper" keywords="rasterization,radiant,splatting,ray,tracing,foam,rendering,differentiable,hardware,eschew">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing_ICCV_2025_paper.html" target="_blank" title="18/263"><span class="index notranslate">#18</span></a>
                <a id="title-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="title-link" href="/venue/Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" target="_blank">Radiant Foam: Real-Time Differentiable Ray Tracing</a>
                <a id="pdf-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shrisudhan Govindarajan" target="_blank">Shrisudhan Govindarajan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Rebain" target="_blank">Daniel Rebain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwang Moo Yi" target="_blank">Kwang Moo Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Tagliasacchi" target="_blank">Andrea Tagliasacchi</a>
            </p>
            <p id="summary-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="summary">Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementation of light transport phenomena like reflection and refraction much more difficult. We propose a novel scene representation which avoids these approximations, but keeps the efficiency and reconstruction quality of splatting by leveraging a decades-old efficient volumetric mesh ray tracing algorithm which has been largely overlooked in recent computer vision research. The resulting model, which we name Radiant Foam, achieves rendering speed and quality comparable to Gaussian Splatting, without the constraints of rasterization. Unlike ray traced Gaussian models that use hardware ray tracing acceleration, our method requires no special hardware or APIs beyond the standard features of a programmable GPU.</p>
            <p id="subjects-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" onclick="foldPdfKimi('Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="panel paper" keywords="retracker,tracking,online,matching,occlusions,retracking,points,viewpoint,decoder,pretrain">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking_ICCV_2025_paper.html" target="_blank" title="19/263"><span class="index notranslate">#19</span></a>
                <a id="title-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="title-link" href="/venue/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" target="_blank">ReTracker: Exploring Image Matching for Robust Online Any Point Tracking</a>
                <a id="pdf-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongli Tan" target="_blank">Dongli Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyi He" target="_blank">Xingyi He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sida Peng" target="_blank">Sida Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiqing Gong" target="_blank">Yiqing Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Zhu" target="_blank">Xing Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaming Sun" target="_blank">Jiaming Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruizhen Hu" target="_blank">Ruizhen Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujun Shen" target="_blank">Yujun Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hujun Bao" target="_blank">Hujun Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaowei Zhou" target="_blank">Xiaowei Zhou</a>
            </p>
            <p id="summary-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="summary">This paper aims to establish correspondences for a set of 2D query points across a video sequence in an online manner. Recent methods leverage future frames to achieve smooth point tracking at the current frame, but they still struggle to find points with significant viewpoint changes after long-term occlusions and inherently cannot achieve online tracking. To overcome these challenges, we develop a novel online tracking framework, named ReTracker, that integrates two advances in image matching with tracking-specific designs. First, a decoder network with a global receptive field is incorporated with a temporal attention module to robustly track points undergoing large location changes. Second, the decoder network is adapted to pretrain on large-scale two-view matching data, which offers significantly greater diversity and volume than tracking data, to learn general matching priors. This pretraining strategy effectively enhances our tracker's ability to handle viewpoint and appearance variations after long-term occlusions. Experiments demonstrate that our method outperforms recent online trackers across multiple benchmarks and achieves competitive or superior performance compared to offline methods. Furthermore, we collect an ego-centric, occlusion-heavy dataset to illustrate the retracking capabilities of our approach. The code and dataset will be released for the reproducibility.</p>
            <p id="subjects-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" onclick="foldPdfKimi('Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="panel paper" keywords="correspondences,shutter,point,motion,cameras,asynchronous,rolling,solver,linear,structure">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous_ICCV_2025_paper.html" target="_blank" title="20/263"><span class="index notranslate">#20</span></a>
                <a id="title-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="title-link" href="/venue/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" target="_blank">A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks</a>
                <a id="pdf-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Su" target="_blank">Hang Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunlong Feng" target="_blank">Yunlong Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Gehrig" target="_blank">Daniel Gehrig</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Panfeng Jiang" target="_blank">Panfeng Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling Gao" target="_blank">Ling Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xavier Lagorce" target="_blank">Xavier Lagorce</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laurent Kneip" target="_blank">Laurent Kneip</a>
            </p>
            <p id="summary-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="summary">Structure and continuous motion estimation from point correspondences is a fundamental problem in computer vision that has been powered by well-known algorithms such as the familiar 5-point or 8-point algorithm. However, despite their acclaim, these algorithms are limited to processing point correspondences originating from a pair of views each one representing an instantaneous capture of the scene. Yet, in the case of rolling shutter cameras, or more recently, event cameras, this synchronization breaks down. In this work, we present a unified approach for structure and linear motion estimation from 2D point correspondences with arbitrary timestamps, from an arbitrary set of views. By formulating the problem in terms of first-order dynamics and leveraging a constant velocity motion model, we derive a novel, linear point incidence relation allowing for the efficient recovery of both linear velocity and 3D points with predictable degeneracies and solution multiplicities. Owing to its general formulation, it can handle correspondences from a wide range of sensing modalities such as global shutter, rolling shutter, and event cameras, and can even combine correspondences from different collocated sensors. We validate the effectiveness of our solver on both simulated and real-world data, where we show consistent improvement across all modalities when compared to recent approaches. We believe our work opens the door to efficient structure and motion estimation from asynchronous data.</p>
            <p id="subjects-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" onclick="foldPdfKimi('Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="panel paper" keywords="lrm,reconstruction,long,gaussian,splats,arthurhero,llrm,feed,960x540,dl3dv">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats_ICCV_2025_paper.html" target="_blank" title="21/263"><span class="index notranslate">#21</span></a>
                <a id="title-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="title-link" href="/venue/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" target="_blank">Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats</a>
                <a id="pdf-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Ziwen" target="_blank">Chen Ziwen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tan" target="_blank">Hao Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sai Bi" target="_blank">Sai Bi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fujun Luan" target="_blank">Fujun Luan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yicong Hong" target="_blank">Yicong Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Fuxin" target="_blank">Li Fuxin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zexiang Xu" target="_blank">Zexiang Xu</a>
            </p>
            <p id="summary-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="summary">We propose Long-LRM, a feed-forward 3D Gaussian reconstruction model for instant, high-resolution, 360deg wide-coverage, scene-level reconstruction. Specifically, it takes in 32 input images at a resolution of 960x540 and produces the Gaussian reconstruction in just 1 second on a single A100 GPU. To handle the long sequence of 250K tokens brought by the large input size, Long-LRM features a mixture of the recent Mamba2 blocks and the classical transformer blocks, enhanced by a light-weight token merging module and Gaussian pruning steps that balance between quality and efficiency. We evaluate Long-LRM on the large-scale DL3DV benchmark and Tanks&amp;Temples, demonstrating reconstruction quality comparable to the optimization-based methods while achieving an 800x speedup w.r.t. the optimization-based approaches and an input size at least 60x larger than the previous feed-forward approaches. We conduct extensive ablation studies on our model design choices for both rendering quality and computation efficiency. We also explore Long-LRM's compatibility with other Gaussian variants such as 2D GS, which enhances Long-LRM's ability in geometry reconstruction. Project page: http://arthurhero.github.io/projects/llrm/</p>
            <p id="subjects-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" onclick="foldPdfKimi('Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="panel paper" keywords="textbook,interleaved,instructional,pretraining,videos,multimodal,knowledge,vlm,text,vlms">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining_ICCV_2025_paper.html" target="_blank" title="22/263"><span class="index notranslate">#22</span></a>
                <a id="title-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="title-link" href="/venue/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" target="_blank">2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</a>
                <a id="pdf-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Zhang" target="_blank">Wenqi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Zhang" target="_blank">Hang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Li" target="_blank">Xin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiashuo Sun" target="_blank">Jiashuo Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongliang Shen" target="_blank">Yongliang Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiming Lu" target="_blank">Weiming Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deli Zhao" target="_blank">Deli Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueting Zhuang" target="_blank">Yueting Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lidong Bing" target="_blank">Lidong Bing</a>
            </p>
            <p id="summary-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="summary">Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Code and dataset are available on https://multimodal-interleaved-textbook.github.io.</p>
            <p id="subjects-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="panel paper" keywords="informativeness,representativeness,datatailor,collaborative,uniqueness,tuning,mastering,data,modal,instruction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness_ICCV_2025_paper.html" target="_blank" title="23/263"><span class="index notranslate">#23</span></a>
                <a id="title-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="title-link" href="/venue/Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" target="_blank">Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness</a>
                <a id="pdf-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qifan Yu" target="_blank">Qifan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhebei Shen" target="_blank">Zhebei Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongqi Yue" target="_blank">Zhongqi Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Wu" target="_blank">Yang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bosheng Qin" target="_blank">Bosheng Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqiao Zhang" target="_blank">Wenqiao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunfei Li" target="_blank">Yunfei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juncheng Li" target="_blank">Juncheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siliang Tang" target="_blank">Siliang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueting Zhuang" target="_blank">Yueting Zhuang</a>
            </p>
            <p id="summary-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="summary">Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principles--informativeness, uniqueness, and representativeness--for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 101.3% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the "Less is More" philosophy in MLLM development. The code and data is available in this URL.</p>
            <p id="subjects-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" onclick="foldPdfKimi('Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="panel paper" keywords="specialized,cot,thought,bootstrapping,mllms,grounded,chain,charts,multimodal,data">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation_ICCV_2025_paper.html" target="_blank" title="24/263"><span class="index notranslate">#24</span></a>
                <a id="title-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="title-link" href="/venue/Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" target="_blank">Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation</a>
                <a id="pdf-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaer Xia" target="_blank">Jiaer Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingkui Tong" target="_blank">Bingkui Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhang Zang" target="_blank">Yuhang Zang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Shao" target="_blank">Rui Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiyang Zhou" target="_blank">Kaiyang Zhou</a>
            </p>
            <p id="summary-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="summary">Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with chain-of-thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation.</p>
            <p id="subjects-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" onclick="foldPdfKimi('Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="panel paper" keywords="depth,completion,shot,prompt,test,zero,monocular,tuning,foundation,sensor">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion_ICCV_2025_paper.html" target="_blank" title="25/263"><span class="index notranslate">#25</span></a>
                <a id="title-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="title-link" href="/venue/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" target="_blank">Test-Time Prompt Tuning for Zero-Shot Depth Completion</a>
                <a id="pdf-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chanhwi Jeong" target="_blank">Chanhwi Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Inhwan Bae" target="_blank">Inhwan Bae</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin-Hwi Park" target="_blank">Jin-Hwi Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hae-Gon Jeon" target="_blank">Hae-Gon Jeon</a>
            </p>
            <p id="summary-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="summary">Zero-shot depth completion with metric scales poses significant challenges, primarily due to performance limitations such as domain specificity and sensor characteristics. One recent emerging solution is to integrate monocular depth foundation models into depth completion frameworks, yet these efforts still face issues with suboptimal performance and often require further adaptation to the target task. Surprisingly, we find that a simple test-time training, which fine-tunes monocular depth foundation models on sparse depth measurements from sensors just as it is, yields reasonable results. However, this test-time training obviously incurs high computational costs and introduces biases towards specific conditions, making it impractical for real-world scenarios. In this paper, we introduce a new approach toward parameter-efficient zero-shot depth completion. Our key idea of this work is to leverage visual prompt tuning, achieving sensor-specific depth scale adaptation without forgetting foundational knowledge. Experimental results on diverse datasets demonstrate that our approach outperforms relevant state-of-the-art methods, showing superior generalization and efficiency. Our source code is available in the supplementary materials.</p>
            <p id="subjects-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" onclick="foldPdfKimi('Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
    <div id="Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="panel paper" keywords="lora,stolenlora,extraction,attacks,peft,model,vulnerability,defense,synthetic,adapted">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data_ICCV_2025_paper.html" target="_blank" title="26/263"><span class="index notranslate">#26</span></a>
                <a id="title-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="title-link" href="/venue/Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" target="_blank">StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data</a>
                <a id="pdf-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yixu Wang" target="_blank">Yixu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Teng" target="_blank">Yan Teng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingchun Wang" target="_blank">Yingchun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingjun Ma" target="_blank">Xingjun Ma</a>
            </p>
            <p id="summary-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="summary">Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries.Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods.We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.</p>
            <p id="subjects-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" onclick="foldPdfKimi('Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="panel paper" keywords="consensus,congcd,deconstruction,multiplex,lytang63,dissecting,category,discovery,human,contextual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction_ICCV_2025_paper.html" target="_blank" title="27/263"><span class="index notranslate">#27</span></a>
                <a id="title-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="title-link" href="/venue/Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" target="_blank">Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction</a>
                <a id="pdf-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Luyao Tang" target="_blank">Luyao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kunze Huang" target="_blank">Kunze Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaoqi Chen" target="_blank">Chaoqi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Yuan" target="_blank">Yuxuan Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxin Li" target="_blank">Chenxin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaotong Tu" target="_blank">Xiaotong Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinghao Ding" target="_blank">Xinghao Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Huang" target="_blank">Yue Huang</a>
            </p>
            <p id="summary-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="summary">Human perceptual systems excel at inducing and recognizing objects across both known and novel categories, a capability far beyond current machine learning frameworks. While generalized category discovery (GCD) aims to bridge this gap, existing methods predominantly focus on optimizing objective functions. We present an orthogonal solution, inspired by the human cognitive process for novel object understanding: decomposing objects into visual primitives and establishing cross-knowledge comparisons. We propose ConGCD, which establishes primitive-oriented representations through high-level semantic reconstruction, binding intra-class shared attributes via deconstruction. Mirroring human preference diversity in visual processing, where distinct individuals leverage dominant or contextual cues, we implement dominant and contextual consensus units to capture class-discriminative patterns and inherent distributional invariants, respectively. A consensus scheduler dynamically optimizes activation pathways, with final predictions emerging through multiplex consensus integration. Extensive evaluations across coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a consensus-aware paradigm. Code is available at: https://github.com/lytang63/ConGCD.</p>
            <p id="subjects-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" onclick="foldPdfKimi('Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="panel paper" keywords="ssl,visual,clip,vqa,language,vision,pretraining,metaclip,supervised,multimodal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Scaling_Language-Free_Visual_Representation_Learning_ICCV_2025_paper.html" target="_blank" title="28/263"><span class="index notranslate">#28</span></a>
                <a id="title-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="title-link" href="/venue/Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" target="_blank">Scaling Language-Free Visual Representation Learning</a>
                <a id="pdf-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Scaling_Language-Free_Visual_Representation_Learning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=David Fan" target="_blank">David Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengbang Tong" target="_blank">Shengbang Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiachen Zhu" target="_blank">Jiachen Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Koustuv Sinha" target="_blank">Koustuv Sinha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuang Liu" target="_blank">Zhuang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlei Chen" target="_blank">Xinlei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Rabbat" target="_blank">Michael Rabbat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Ballas" target="_blank">Nicolas Ballas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yann LeCun" target="_blank">Yann LeCun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amir Bar" target="_blank">Amir Bar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saining Xie" target="_blank">Saining Xie</a>
            </p>
            <p id="summary-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="summary">Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning. Code and models are at https://github.com/facebookresearch/webssl.</p>
            <p id="subjects-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" onclick="foldPdfKimi('Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="panel paper" keywords="object,detection,diffrefine,domain,densification,due,false,points,proposals,densify">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.html" target="_blank" title="29/263"><span class="index notranslate">#29</span></a>
                <a id="title-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="title-link" href="/venue/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" target="_blank">DiffRefine: Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection</a>
                <a id="pdf-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sangyun Shin" target="_blank">Sangyun Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhang He" target="_blank">Yuhang He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Hou" target="_blank">Xinyu Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Hodgson" target="_blank">Samuel Hodgson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Markham" target="_blank">Andrew Markham</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niki Trigoni" target="_blank">Niki Trigoni</a>
            </p>
            <p id="summary-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="summary">The robustness of 3D object detection in large-scale outdoor point clouds degrades significantly when deployed in an unseen environment due to domain shifts. To minimize the domain gap, existing works on domain adaptive detection focuses on several factors, including point density, object shape and sizes, to reduce the false negative detections. However, the adaptation results indicate that there are still remaining challenges. We argue that this is due to the challenge in recognizing comparably less distinctive region on object surface due to sparsity, occlusion, etc. In this work, we aim to reinforce those features by generating points on object surface to make them straightforwardly recognizable. We draw our motivation from a common observation that detection proposals already contain the accurate bounding boxes, but with relatively low objectness score predictions, which lead to false negatives. Given these box proposals, we densify sparse object points with a diffusion approach. As a result, our model DiffRefine can act as a simple additional module before second-stage refinement, where most existing detection models for two-stage detection can use. Experimental results on domain adaptive detection show competitive performance, especially on vanishing points due to distance on various detection architectures.</p>
            <p id="subjects-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" onclick="foldPdfKimi('Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="panel paper" keywords="adversaries,adversarial,teacher,decision,robustness,distillation,shot,vlms,optima,confound">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths_ICCV_2025_paper.html" target="_blank" title="30/263"><span class="index notranslate">#30</span></a>
                <a id="title-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="title-link" href="/venue/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" target="_blank">Confound from All Sides, Distill with Resilience: Multi-Objective Adversarial Paths to Zero-Shot Robustness</a>
                <a id="pdf-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junhao Dong" target="_blank">Junhao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiao Liu" target="_blank">Jiao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinghua Qu" target="_blank">Xinghua Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yew-Soon Ong" target="_blank">Yew-Soon Ong</a>
            </p>
            <p id="summary-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="summary">Adversarially robust knowledge distillation transfers the robustness of a large-scale teacher model to a lightweight student while preserving natural performance. However, foundation Vision-Language Models (VLMs) also demand the transfer of zero-shot inference capabilities. We find that standard robust distillation using untargeted adversarial examples fails to transfer out-of-distribution (zero-shot) robustness, as these adversaries primarily push inputs away from their original distribution, exploring a limited portion of the teacher's decision space and missing more diverse failure modes. A natural solution is to generate multiple targeted adversaries that traverse diverse paths across decision boundaries. Thus, these adversaries probe a broader region of the teacher's decision surface. However, naive targeted adversary optimization often converges to local optima within a single category's decision region, limiting the diversity. To address this, we propose a Multi-Objective Optimization (MOO)-based adversarial distillation framework that transfers robustness from large VLMs to lightweight ones by exploiting adversaries with two main objectives: misclassification and category-level adversarial diversity. Theoretically, we show that optimizing for diversity mitigates adversarial collapse into local optima, ensuring adversaries span multiple decision regions and capture the teacher's generalizable robust features. Extensive experiments demonstrate the superiority of our method over state-of-the-art adversarial learning across diverse scenarios.</p>
            <p id="subjects-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" onclick="foldPdfKimi('Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="panel paper" keywords="volumetricsmpl,volumetric,nbw,human,meshes,body,interactions,neural,scenes,coap">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts_ICCV_2025_paper.html" target="_blank" title="31/263"><span class="index notranslate">#31</span></a>
                <a id="title-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="title-link" href="/venue/Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" target="_blank">VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions</a>
                <a id="pdf-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marko Mihajlovic" target="_blank">Marko Mihajlovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Zhang" target="_blank">Siwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gen Li" target="_blank">Gen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaifeng Zhao" target="_blank">Kaifeng Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lea Muller" target="_blank">Lea Muller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Tang" target="_blank">Siyu Tang</a>
            </p>
            <p id="summary-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="summary">Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains.</p>
            <p id="subjects-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" onclick="foldPdfKimi('Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="panel paper" keywords="shape,geometrically,matchings,consistent,matching,globally,shapes,hyper,consistency,practice">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching_ICCV_2025_paper.html" target="_blank" title="32/263"><span class="index notranslate">#32</span></a>
                <a id="title-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="title-link" href="/venue/Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" target="_blank">Fast Globally Optimal and Geometrically Consistent 3D Shape Matching</a>
                <a id="pdf-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Roetzer" target="_blank">Paul Roetzer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Bernard" target="_blank">Florian Bernard</a>
            </p>
            <p id="summary-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="summary">Geometric consistency, i.e. the preservation of neighbourhoods, is a natural and strong prior in 3D shape matching. Geometrically consistent matchings are crucial for many downstream applications, such as texture transfer or statistical shape modelling. Yet, in practice, geometric consistency is often overlooked, or only achieved under severely limiting assumptions (e.g. a good initialisation). In this work, we propose a novel formalism for computing globally optimal and geometrically consistent matchings between 3D shapes which is scalable in practice. Our key idea is to represent the surface of the source shape as a collection of cyclic graphs, which are then consistently matched to the target shape. Mathematically, we construct a hyper product graph (between source and target shape), and then cast 3D shape matching as a minimum-cost circulation flow problem in this hyper graph, which yields global geometrically consistent matchings between both shapes. We empirically show that our formalism is efficiently solvable and that it leads to high-quality results. Our code is publicly available.</p>
            <p id="subjects-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" onclick="foldPdfKimi('Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="panel paper" keywords="dov,evasion,copyright,evading,ood,oversimplistic,provenance,datasets,thefts,dataset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks_ICCV_2025_paper.html" target="_blank" title="33/263"><span class="index notranslate">#33</span></a>
                <a id="title-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="title-link" href="/venue/Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" target="_blank">Evading Data Provenance in Deep Neural Networks</a>
                <a id="pdf-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyu Zhu" target="_blank">Hongyu Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sichu Liang" target="_blank">Sichu Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenwen Wang" target="_blank">Wenwen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuomeng Zhang" target="_blank">Zhuomeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangqi Li" target="_blank">Fangqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shi-Lin Wang" target="_blank">Shi-Lin Wang</a>
            </p>
            <p id="summary-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="summary">Modern over-parameterized deep models are highly data-dependent, with large scale general-purpose and domain-specific datasets serving as the bedrock for rapid advancements. However, many datasets are proprietary or contain sensitive information, making unrestricted model training problematic. In the open world where data thefts cannot be fully prevented, Dataset Ownership Verification (DOV) has emerged as a promising method to protect copyright by detecting unauthorized model training and tracing illicit activities. Due to its diversity and superior stealth, evading DOV is considered extremely challenging. However, this paper identifies that previous studies have relied on oversimplistic evasion attacks for evaluation, leading to a false sense of security. We introduce a unified evasion framework, in which a teacher model first learns from the copyright dataset and then transfers task-relevant yet identifier-independent domain knowledge to a surrogate student using an out-of-distribution (OOD) dataset as the intermediary. Leveraging Vision-Language Models and Large Language Models, we curate the most informative and reliable subsets from the OOD gallery set as the final transfer set, and propose selectively transferring task-oriented knowledge to achieve a better trade-off between generalization and evasion effectiveness. Experiments across diverse datasets covering eleven DOV methods demonstrate our approach simultaneously eliminates all copyright identifiers and significantly outperforms nine state-of-the-art evasion attacks in both generalization and effectiveness, with moderate computational overhead. As a proof of concept, we reveal key vulnerabilities in current DOV methods, highlighting the need for long-term development to enhance practicality.</p>
            <p id="subjects-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" onclick="foldPdfKimi('Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="panel paper" keywords="domain,diffusion,fitness,generalization,domains,transferability,detectors,improving,performance,tasks">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness_ICCV_2025_paper.html" target="_blank" title="34/263"><span class="index notranslate">#34</span></a>
                <a id="title-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="title-link" href="/venue/He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" target="_blank">Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability</a>
                <a id="pdf-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Boyong He" target="_blank">Boyong He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxiang Ji" target="_blank">Yuxiang Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoyue Tan" target="_blank">Zhuoyue Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liaoni Wu" target="_blank">Liaoni Wu</a>
            </p>
            <p id="summary-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="summary">Detectors often suffer from performance drop due to domain gap between training and testing data. Recent methods explore diffusion models applied to domain generalization (DG) and adaptation (DA) tasks, but still struggle with large inference costs and have not yet fully leveraged the capabilities of diffusion models. We propose to tackle these problems by extracting intermediate features from a single-step diffusion process, improving feature collection and fusion to reduce inference time by 75% while enhancing performance on source domains (i.e., Fitness). Then, we construct an object-centered auxiliary branch by applying box-masked images with class prompts to extract robust and domain-invariant features that focus on object. We also apply consistency loss to align the auxiliary and ordinary branch, balancing fitness and generalization while preventing overfitting and improving performance on target domains (i.e., Generalization). Furthermore, within a unified framework, standard detectors are guided by diffusion detectors through feature-level and object-level alignment on source domains (for DG) and unlabeled target domains (for DA), thereby improving cross-domain detection performance (i.e., Transferability). Our method achieves competitive results on 3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO generalization benchmark demonstrate that our method maintains significant advantages and show remarkable efficiency in large domain shifts and low-data scenarios. Our work shows the superiority of applying diffusion models to domain generalized and adaptive detection tasks and offers valuable insights for visual perception tasks across diverse domains.</p>
            <p id="subjects-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" onclick="foldPdfKimi('He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="panel paper" keywords="distillation,knowledge,distill,adaptive,sampling,kdas,work,faster,overlooks,quality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling_ICCV_2025_paper.html" target="_blank" title="35/263"><span class="index notranslate">#35</span></a>
                <a id="title-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="title-link" href="/venue/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" target="_blank">What to Distill? Fast Knowledge Distillation with Adaptive Sampling</a>
                <a id="pdf-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF">7</sup>]</a>
                <a id="copy-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Byungchul Chae" target="_blank">Byungchul Chae</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seonyeong Heo" target="_blank">Seonyeong Heo</a>
            </p>
            <p id="summary-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="summary">Knowledge Distillation (KD) has been established as an effective technique for reducing the resource requirements of models when tackling computer vision tasks. Prior work has studied how to distill the knowledge of a teacher model better, but it overlooks how data affects the distillation result. This work examines the impact of data in knowledge distillation from two perspectives: (i) quantity of knowledge and (ii) quality of knowledge. Our examination finds that faster knowledge distillation can be achieved by using data with a large amount of high-quality knowledge in distillation. Based on the findings, this work proposes an efficient adaptive sampling method called KDAS for faster knowledge distillation, which enhances the distillation efficiency by selecting and applying 'good' samples for the distillation. This work shows that our adaptive sampling methods can effectively accelerate the training efficiency of a student model when combined with existing KD methods.</p>
            <p id="subjects-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" onclick="foldPdfKimi('Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="panel paper" keywords="demosaicing,rgb,multispectral,images,camera,cameras,dual,demosaiced,mosaiced,spectral">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tedla_Multispectral_Demosaicing_via_Dual_Cameras_ICCV_2025_paper.html" target="_blank" title="36/263"><span class="index notranslate">#36</span></a>
                <a id="title-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="title-link" href="/venue/Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" target="_blank">Multispectral Demosaicing via Dual Cameras</a>
                <a id="pdf-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Tedla_Multispectral_Demosaicing_via_Dual_Cameras_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=SaiKiran Tedla" target="_blank">SaiKiran Tedla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyong Lee" target="_blank">Junyong Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beixuan Yang" target="_blank">Beixuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahmoud Afifi" target="_blank">Mahmoud Afifi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael S. Brown" target="_blank">Michael S. Brown</a>
            </p>
            <p id="summary-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="summary">Multispectral (MS) images capture detailed scene information across a wide range of spectral bands, making them invaluable for applications requiring rich spectral data. Integrating MS imaging into multi-camera devices, such as smartphones, has the potential to enhance both spectral applications and RGB image quality. A critical step in processing MS data is demosaicing, which reconstructs color information from the mosaic MS images captured by the camera. This paper proposes a method for MS image demosaicing specifically designed for dual-camera setups where both RGB and MS cameras capture the same scene. Our approach leverages co-captured RGB images, which typically have higher spatial fidelity, to guide the demosaicing of lower-fidelity MS images. We introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB and MS mosaiced images with ground-truth demosaiced outputs - that enables training and evaluation of our method. Experimental results demonstrate that our method achieves state-of-the-art accuracy compared to existing techniques.</p>
            <p id="subjects-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" onclick="foldPdfKimi('Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="panel paper" keywords="layout,scenescript,infilling,scene,layouts,human,loop,local,task,correction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling_ICCV_2025_paper.html" target="_blank" title="37/263"><span class="index notranslate">#37</span></a>
                <a id="title-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="title-link" href="/venue/Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" target="_blank">Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling</a>
                <a id="pdf-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Xie" target="_blank">Christopher Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Armen Avetisyan" target="_blank">Armen Avetisyan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Henry Howard-Jenkins" target="_blank">Henry Howard-Jenkins</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yawar Siddiqui" target="_blank">Yawar Siddiqui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julian Straub" target="_blank">Julian Straub</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Newcombe" target="_blank">Richard Newcombe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vasileios Balntas" target="_blank">Vasileios Balntas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Engel" target="_blank">Jakob Engel</a>
            </p>
            <p id="summary-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="summary">We present a novel human-in-the-loop approach to estimate 3D scene layout that uses human feedback from an egocentric standpoint. We study this approach through introduction of a novel local correction task, where users identify local errors and prompt a model to automatically correct them. Building on SceneScript, a state-of-the-art framework for 3D scene layout estimation that leverages structured language, we propose a solution that structures this problem as "infilling", a task studied in natural language processing. We train a multi-task version of SceneScript that maintains performance on global predictions while significantly improving its local correction ability. We integrate this into a human-in-the-loop system, enabling a user to iteratively refine scene layout estimates via a low-friction "one-click fix" workflow. Our system enables the final refined layout to diverge from the training distribution, allowing for more accurate modelling of complex layouts.</p>
            <p id="subjects-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" onclick="foldPdfKimi('Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="panel paper" keywords="gnc,graduated,sac,outliers,convexity,robust,shape,initialization,annealing,scorings">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity_ICCV_2025_paper.html" target="_blank" title="38/263"><span class="index notranslate">#38</span></a>
                <a id="title-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="title-link" href="/venue/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" target="_blank">SAC-GNC: SAmple Consensus for adaptive Graduated Non-Convexity</a>
                <a id="pdf-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Valter Piedade" target="_blank">Valter Piedade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chitturi Sidhartha" target="_blank">Chitturi Sidhartha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jos Gaspar" target="_blank">Jos Gaspar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Venu Madhav Govindu" target="_blank">Venu Madhav Govindu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pedro Miraldo" target="_blank">Pedro Miraldo</a>
            </p>
            <p id="summary-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="summary">Outliers are ubiquitous in geometric vision contexts such as pose estimation and mapping, leading to inaccurate estimates. While robust loss functions can tackle outliers, it is challenging to make the estimation robust to the choice of initialization and to estimate the appropriate robust loss shape parameter that allows distinguishing inliers from outliers. Graduated non-convexity (GNC) often mitigates these issues. However, typical GNC uses a fixed annealing factor to update the shape parameter, which can lead to low-quality or inefficient estimates. This paper proposes a novel approach to adaptively anneal the shape parameter within a GNC framework. We developed a search strategy that incorporates a sampling of annealing choices and model scorings to select the most promising shape parameter at each GNC iteration. Additionally, we propose new stopping criteria and an initialization technique that improves performance for diverse data, and we show the benefits of combining discrete and continuous robust estimation strategies. We evaluate our method using synthetic and real-world data in two problems: 3D registration and pose graph optimization in SLAM sequences. Our results demonstrate greater efficiency and robustness compared to previous GNC schemes. Code and other resources are available at https://www.merl.com/research/highlights/sac-gnc.</p>
            <p id="subjects-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" onclick="foldPdfKimi('Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="panel paper" keywords="egocentric,person,vision,activities,tracking,object,human,really,challenging,segmentation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision_ICCV_2025_paper.html" target="_blank" title="39/263"><span class="index notranslate">#39</span></a>
                <a id="title-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="title-link" href="/venue/Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" target="_blank">Is Tracking Really More Challenging in First Person Egocentric Vision?</a>
                <a id="pdf-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Dunnhofer" target="_blank">Matteo Dunnhofer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaira Manigrasso" target="_blank">Zaira Manigrasso</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Micheloni" target="_blank">Christian Micheloni</a>
            </p>
            <p id="summary-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="summary">Visual object tracking and segmentation are becoming fundamental tasks for understanding human activities in egocentric vision. Recent research has benchmarked state-of-the-art methods and concluded that first person egocentric vision presents challenges compared to previously studied domains. However, these claims are based on evaluations conducted across significantly different scenarios. Many of the challenging characteristics attributed to egocentric vision are also present in third person videos of human-object activities. This raises a critical question: how much of the observed performance drop stems from the unique first person viewpoint inherent to egocentric vision versus the domain of human-object activities? To address this question, we introduce a new benchmark study designed to disentangle such factors. Our evaluation strategy enables a more precise separation of challenges related to the first person perspective from those linked to the broader domain of human-object activity understanding. By doing so, we provide deeper insights into the true sources of difficulty in egocentric tracking and segmentation, facilitating more targeted advancements on this task.</p>
            <p id="subjects-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" onclick="foldPdfKimi('Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="panel paper" keywords="gui,history,simpagent,context,agent,simplification,modeling,agents,element,unrelated">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification_ICCV_2025_paper.html" target="_blank" title="40/263"><span class="index notranslate">#40</span></a>
                <a id="title-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="title-link" href="/venue/Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" target="_blank">Less is More: Empowering GUI Agent with Context-Aware Simplification</a>
                <a id="pdf-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gongwei Chen" target="_blank">Gongwei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xurui Zhou" target="_blank">Xurui Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Shao" target="_blank">Rui Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yibo Lyu" target="_blank">Yibo Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiwen Zhou" target="_blank">Kaiwen Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Wang" target="_blank">Shuai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Li" target="_blank">Wentao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinchuan Li" target="_blank">Yinchuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongang Qi" target="_blank">Zhongang Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liqiang Nie" target="_blank">Liqiang Nie</a>
            </p>
            <p id="summary-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="summary">The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agents and summarize: **1) the high-density and loose-relation of element context** highlight the existence of many unrelated elements and their negative influence; **2) the high redundancy of history context** reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed **SimpAgent**. To mitigate potential interference from numerous unrelated elements, we introduce a **masking-based element pruning** method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a **consistency-guided history compression** module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent.</p>
            <p id="subjects-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="panel paper" keywords="cotracker3,trackers,videos,real,simpler,synthetic,tracking,pseudo,labelling,better">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos_ICCV_2025_paper.html" target="_blank" title="41/263"><span class="index notranslate">#41</span></a>
                <a id="title-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="title-link" href="/venue/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" target="_blank">CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos</a>
                <a id="pdf-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nikita Karaev" target="_blank">Nikita Karaev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuri Makarov" target="_blank">Yuri Makarov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyuan Wang" target="_blank">Jianyuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Natalia Neverova" target="_blank">Natalia Neverova</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Rupprecht" target="_blank">Christian Rupprecht</a>
            </p>
            <p id="summary-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="summary">We introduce CoTracker3, a new state-of-the-art point tracker. With CoTracker3, we revisit the design of recent trackers, removing components and reducing the number of parameters while also improving performance. We also explore the interplay of synthetic and real data. Recent trackers are trained on synthetic videos due to the difficulty of collecting tracking annotations for real data. However, this can result in suboptimal performance due to the statistical gap between synthetic and real videos. We thus suggest using off-the-shelf trackers as teachers, annotating real videos with pseudo-labels. Compared to other recent attempts at using real data for learning trackers, this scheme is much simpler and achieves better results using 1,000 times less data. CoTracker3 is available in online (causal) and offline variants and is particularly robust to occlusions.</p>
            <p id="subjects-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" onclick="foldPdfKimi('Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="panel paper" keywords="scenemi,betweening,hsi,scene,keyframe,modeling,motion,human,gimo,scenes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction_ICCV_2025_paper.html" target="_blank" title="42/263"><span class="index notranslate">#42</span></a>
                <a id="title-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="title-link" href="/venue/Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" target="_blank">SceneMI: Motion In-betweening for Modeling Human-Scene Interaction</a>
                <a id="pdf-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Inwoo Hwang" target="_blank">Inwoo Hwang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bing Zhou" target="_blank">Bing Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Young Min Kim" target="_blank">Young Min Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Wang" target="_blank">Jian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuan Guo" target="_blank">Chuan Guo</a>
            </p>
            <p id="summary-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="summary">Modeling human-scene interactions (HSI) is essential for understanding and simulating everyday human behaviors. Recent approaches utilizing generative modeling have made progress in this domain; however, they are limited in controllability and flexibility for real-world applications. To address these challenges, we propose reformulating the HSI modeling problem as Scene-aware Motion In-betweening---a more tractable and practical task. We introduce SceneMI, a framework that supports several practical applications, including keyframe-guided character animation in 3D scenes and enhancing the motion quality of imperfect HSI data. SceneMI employs dual scene descriptors to comprehensively encode global and local scene context. Furthermore, our framework leverages the inherent denoising nature of diffusion models to generalize on noisy keyframes. Experimental results demonstrate SceneMI's effectiveness in scene-aware keyframe in-betweening and generalization to the real-world GIMO dataset, where motions and scenes are acquired by noisy IMU sensors and smartphones. We further showcase SceneMI's applicability in HSI reconstruction from monocular videos.</p>
            <p id="subjects-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" onclick="foldPdfKimi('Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="panel paper" keywords="estimation,resolution,motion,range,flow,datasets,relayflow,long,optical,displacements">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution_ICCV_2025_paper.html" target="_blank" title="43/263"><span class="index notranslate">#43</span></a>
                <a id="title-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="title-link" href="/venue/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" target="_blank">Learning Large Motion Estimation from Intermediate Representations with a High-Resolution Optical Flow Dataset Featuring Long-Range Dynamic Motion</a>
                <a id="pdf-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hoonhee Cho" target="_blank">Hoonhee Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhwan Jeong" target="_blank">Yuhwan Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuk-Jin Yoon" target="_blank">Kuk-Jin Yoon</a>
            </p>
            <p id="summary-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="summary">With advancements in sensor and display technologies, high-resolution imagery is becoming increasingly prevalent in diverse applications. As a result, optical flow estimation needs to adapt to larger image resolutions, where even moderate movements lead to substantial pixel displacements, making long-range motion estimation more critical than ever. However, existing datasets primarily focus on short-range flow in low-resolution settings, limiting the generalization of models to high-resolution scenarios with large displacements. Additionally, there is a lack of suitable datasets for evaluating model capacity in long-range motion estimation, further hindering progress in this area. To address this, we introduce RelayFlow-4K, high-resolution 4K optical flow dataset designed to capture diverse motion patterns, including long-range intermediate frame flows. While such datasets provide valuable training resources, long-range estimation remains challenging due to increased matching ambiguity. Simply incorporating these datasets does not inherently improve performance. To this end, we propose a novel training framework that integrates matching cost distillation and incremental time-step learning to refine cost volume estimation and stabilize training. Additionally, we leverage the distance map, which measures the distance from unmatched regions to their nearest matched pixels, improving occlusion handling. Our approach significantly enhances long-range optical flow estimation in high-resolution settings. Our datasets and code are available at https://github.com/Chohoonhee/RelayFlow-4K.</p>
            <p id="subjects-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" onclick="foldPdfKimi('Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="panel paper" keywords="embodied,videoagent,egocentric,video,memory,understanding,openeqa,vq3d,scene,scenes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors_ICCV_2025_paper.html" target="_blank" title="44/263"><span class="index notranslate">#44</span></a>
                <a id="title-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="title-link" href="/venue/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" target="_blank">Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding</a>
                <a id="pdf-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Fan" target="_blank">Yue Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojian Ma" target="_blank">Xiaojian Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rongpeng Su" target="_blank">Rongpeng Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Guo" target="_blank">Jun Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rujie Wu" target="_blank">Rujie Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Chen" target="_blank">Xi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Li" target="_blank">Qing Li</a>
            </p>
            <p id="summary-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="summary">This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 6.5% on Ego4D-VQ3D, 2.6% on OpenEQA, and 15.3% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public.</p>
            <p id="subjects-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" onclick="foldPdfKimi('Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="panel paper" keywords="corvid,reasoning,mllms,cot,multimodal,thought,287k,mcot,chain,architecturally">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning_ICCV_2025_paper.html" target="_blank" title="45/263"><span class="index notranslate">#45</span></a>
                <a id="title-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="title-link" href="/venue/Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" target="_blank">Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning</a>
                <a id="pdf-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingjing Jiang" target="_blank">Jingjing Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Ma" target="_blank">Chao Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xurui Song" target="_blank">Xurui Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwang Zhang" target="_blank">Hanwang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Luo" target="_blank">Jun Luo</a>
            </p>
            <p id="summary-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="summary">Recent advancements in multimodal large language models (MLLMs) have demonstrated exceptional performance in multimodal perception and understanding. However, leading open-source MLLMs exhibit significant limitations in complex and structured reasoning, particularly in tasks requiring deep reasoning for decision-making and problem-solving. In this work, we present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning capabilities. Architecturally, Corvid incorporates a hybrid vision encoder for informative visual representation and a meticulously designed connector (GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT reasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality multimodal CoT instruction-following dataset, refined and standardized from diverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid with a two-stage CoT-formatted training approach to progressively enhance its step-by-step reasoning abilities. Furthermore, we propose an effective inference-time scaling strategy that enables Corvid to mitigate over-reasoning and under-reasoning through self-verification. Extensive experiments demonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art MLLMs with similar parameter scales, with notable strengths in mathematical reasoning and science problem-solving. Project page: https://mm-vl.github.io/corvid.</p>
            <p id="subjects-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" onclick="foldPdfKimi('Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="panel paper" keywords="stabledepth,depth,video,flickering,scene,monocular,13x,scale,consistent,invariant">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth_ICCV_2025_paper.html" target="_blank" title="46/263"><span class="index notranslate">#46</span></a>
                <a id="title-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="title-link" href="/venue/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" target="_blank">StableDepth: Scene-Consistent and Scale-Invariant Monocular Depth</a>
                <a id="pdf-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF">13</sup>]</a>
                <a id="copy-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Zhang" target="_blank">Zheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lihe Yang" target="_blank">Lihe Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Yang" target="_blank">Tianyu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaohui Yu" target="_blank">Chaohui Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyang Guo" target="_blank">Xiaoyang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixing Lao" target="_blank">Yixing Lao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengshuang Zhao" target="_blank">Hengshuang Zhao</a>
            </p>
            <p id="summary-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="summary">Recent advances in monocular depth estimation significantly improve robustness and accuracy. However, relative depth models exhibit flickering and 3D inconsistency in video data, limiting 3D reconstruction applications. We introduce StableDepth, a scene-consistent and scale-invariant depth estimation method achieving scene-level 3D consistency. Our dual-decoder architecture learns from large-scale unlabeled video data, enhancing generalization and reducing flickering. Unlike previous methods requiring full video sequences, StableDepth enables online inference at 13x faster speed, achieving significant improvements across benchmarks with comparable temporal consistency to video diffusion-based estimators.</p>
            <p id="subjects-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="panel paper" keywords="coordinates,front,correspondences,back,dense,bop,object,surface,pose,hccepose">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D_ICCV_2025_paper.html" target="_blank" title="47/263"><span class="index notranslate">#47</span></a>
                <a id="title-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="title-link" href="/venue/Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" target="_blank">HccePose(BF): Predicting Front &amp; Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation</a>
                <a id="pdf-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yulin Wang" target="_blank">Yulin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengting Hu" target="_blank">Mengting Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongli Li" target="_blank">Hongli Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Luo" target="_blank">Chen Luo</a>
            </p>
            <p id="summary-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="summary">In pose estimation for seen objects, a prevalent pipeline involves using neural networks to predict dense 3D coordinates of the object surface on 2D images, which are then used to establish dense 2D-3D correspondences. However, current methods primarily focus on more efficient encoding techniques to improve the precision of predicted 3D coordinates on the object's front surface, overlooking the potential benefits of incorporating the back surface and interior of the object. To better utilize the full surface and interior of the object, this study predicts 3D coordinates of both the object's front and back surfaces and densely samples 3D coordinates between them. This process creates ultra-dense 2D-3D correspondences, effectively enhancing pose estimation accuracy based on the Perspective-n-Point (PnP) algorithm. Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to provide a more accurate and efficient representation of front and back surface coordinates. Experimental results show that, compared to existing state-of-the-art (SOTA) methods on the BOP website, the proposed approach outperforms across seven classic BOP core datasets.</p>
            <p id="subjects-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" onclick="foldPdfKimi('Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="panel paper" keywords="point,maps,dynamic,dpm,dust3r,reconstruction,tasks,scenes,extrinsics,intrinsics">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction_ICCV_2025_paper.html" target="_blank" title="48/263"><span class="index notranslate">#48</span></a>
                <a id="title-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="title-link" href="/venue/Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" target="_blank">Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction</a>
                <a id="pdf-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Edgar Sucar" target="_blank">Edgar Sucar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihang Lai" target="_blank">Zihang Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eldar Insafutdinov" target="_blank">Eldar Insafutdinov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>
            </p>
            <p id="summary-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="summary">DUSt3R has recently demonstrated that many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing 3D scenes, and establishing image correspondences, can be reduced to predicting a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. While this formulation is elegant and powerful, it is limited to static scenes. To overcome this limitation, we introduce the concept of Dynamic Point Maps (DPM), which extends standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key insight is that, when time is introduced, several possible spatial and temporal references can be used to define the point maps. We identify a minimal subset of these combinations that can be regressed by a network to solve the aforementioned tasks. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks, including video depth prediction, dynamic point cloud reconstruction, 3D scene flow, and object pose tracking, achieving state-of-the-art performance.</p>
            <p id="subjects-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" onclick="foldPdfKimi('Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="panel paper" keywords="illumination,light,consistency,view,low,enhancement,lighting,vcnet,scene,images">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement_ICCV_2025_paper.html" target="_blank" title="49/263"><span class="index notranslate">#49</span></a>
                <a id="title-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" target="_blank">Exploring View Consistency for Scene-Adaptive Low-Light Light Field Image Enhancement</a>
                <a id="pdf-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuo Zhang" target="_blank">Shuo Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Gao" target="_blank">Chen Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youfang Lin" target="_blank">Youfang Lin</a>
            </p>
            <p id="summary-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="summary">Light Field (LF) images captured under low illumination conditions typically exhibit low quality. Recent learning-based methods for low-light LF enhancement are generally tailored to specific illumination inputs, limiting their performance in real-world scenes. Moreover, how to maintain the inherent view-consistency in the enhanced images also remain as a difficult problem. In this paper, we propose to explore the view consistency for scene-adaptive low-light LF enhancement. We first analyze the view consistency for LF illumination maps and design a self-supervised view-consistent loss to keep the consistency between the illumination maps of different views in LFs. To enhance the model's perception of illumination, we combine both global and local information to estimate the illumination map, which is easily plugged into other models. Subsequently, we use the illumination maps to light up the low-light LF images and restore the corruption to produce the final enhanced image. Extensive experiments demonstrate that our View-Consistenct Network (VCNet) outperforms state-of-the-art methods on real-world low-light LF datasets in both fixed lighting conditions and dynamic lighting conditions. Our proposed illumination adjustment is also demonstrated that can comprehensively improve the performance of existing methods in terms of both image quality and view consistency.</p>
            <p id="subjects-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="panel paper" keywords="eventups,lighting,uncalibrated,event,illumination,stereo,photometric,ups,camera,bandwidth">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera_ICCV_2025_paper.html" target="_blank" title="50/263"><span class="index notranslate">#50</span></a>
                <a id="title-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="title-link" href="/venue/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" target="_blank">EventUPS: Uncalibrated Photometric Stereo Using an Event Camera</a>
                <a id="pdf-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinxiu Liang" target="_blank">Jinxiu Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bohan Yu" target="_blank">Bohan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siqi Yang" target="_blank">Siqi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haotian Zhuang" target="_blank">Haotian Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jieji Ren" target="_blank">Jieji Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiqi Duan" target="_blank">Peiqi Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>
            </p>
            <p id="summary-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="summary">We present EventUPS, the first uncalibrated photometric stereo (UPS) method using an event camera--a neuromorphic sensor that asynchronously detects brightness changes with microsecond resolution. Traditional frame-based UPS methods are hindered by high bandwidth demands and limited use in dynamic scenes. These methods require dense image correspondence under varying illumination and are incompatible with the fundamentally different sensing paradigm of event data. Our approach introduces three key innovations: an augmented null space formulation that directly relates each event to joint constraints on surface normals and lighting, naturally handling ambient illumination; a continuous parameterization of time-varying illumination that connects asynchronous events to synchronized lighting estimation; and a lighting fixture with known relative geometry that reduces ambiguity to a convex-concave uncertainty. We validate EventUPS using a custom-built LED lighting system. Experimental results show that our method achieves accuracy surpassing its frame-based counterpart while requiring only 5% of the data bandwidth.</p>
            <p id="subjects-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" onclick="foldPdfKimi('Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="panel paper" keywords="part,engine,ziqi,data,foundation,object,datasets,300x,segmentation,annotates">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Find_Any_Part_in_3D_ICCV_2025_paper.html" target="_blank" title="51/263"><span class="index notranslate">#51</span></a>
                <a id="title-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="title-link" href="/venue/Ma_Find_Any_Part_in_3D@ICCV2025@CVF" target="_blank">Find Any Part in 3D</a>
                <a id="pdf-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ma_Find_Any_Part_in_3D@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_Find_Any_Part_in_3D_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ma_Find_Any_Part_in_3D@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ma_Find_Any_Part_in_3D@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ma_Find_Any_Part_in_3D@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ma_Find_Any_Part_in_3D@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ma_Find_Any_Part_in_3D@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqi Ma" target="_blank">Ziqi Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yisong Yue" target="_blank">Yisong Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgia Gkioxari" target="_blank">Georgia Gkioxari</a>
            </p>
            <p id="summary-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="summary">Why don't we have foundation models in 3D yet? A key limitation is data scarcity. For 3D object part segmentation, existing datasets are small in size and lack diversity. We show that it is possible to break this data barrier by building a data engine powered by 2D foundation models. Our data engine automatically annotates any number of object parts: 1755x more unique part types than existing datasets combined. By training on our annotated data with a simple contrastive objective, we obtain an open-world model that generalizes to any part in any object based on any text query. Even when evaluated zero-shot, we outperform existing methods on the datasets they train on. We achieve 260% improvement in mIoU and boost speed by 6x to 300x. Our scaling analysis confirms that this generalization stems from the data scale, which underscores the impact of our data engine. Finally, to advance general-category open-world 3D part segmentation, we release a benchmark covering a wide range of objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/</p>
            <p id="subjects-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" onclick="foldPdfKimi('Ma_Find_Any_Part_in_3D@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="panel paper" keywords="3dt,advdreamer,variations,adv,vlms,world,vlm,real,naturalness,robustness">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D_ICCV_2025_paper.html" target="_blank" title="52/263"><span class="index notranslate">#52</span></a>
                <a id="title-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="title-link" href="/venue/Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" target="_blank">AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?</a>
                <a id="pdf-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shouwei Ruan" target="_blank">Shouwei Ruan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanqing Liu" target="_blank">Hanqing Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Huang" target="_blank">Yao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoqi Wang" target="_blank">Xiaoqi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caixin Kang" target="_blank">Caixin Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Su" target="_blank">Hang Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinpeng Dong" target="_blank">Yinpeng Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingxing Wei" target="_blank">Xingxing Wei</a>
            </p>
            <p id="summary-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="summary">Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations, we propose AdvDreamer, the first framework capable of generating physically reproducible Adversarial 3D Transformation (Adv-3DT) samples from single-view observations. In AdvDreamer, we integrate three key innovations: Firstly, to characterize real-world 3D variations with limited prior knowledge precisely, we design a zero-shot Monocular Pose Manipulation pipeline built upon generative 3D priors. Secondly, to ensure the visual quality of worst-case Adv-3DT samples, we propose Naturalness Reward Model that provides continuous naturalness regularization during adversarial optimization, effectively preventing convergence to hallucinated or unnatural elements. Thirdly, to enable systematic evaluation across diverse VLM architectures and visual-language tasks, we introduce the Inverse Semantic Probability loss as the adversarial optimization objective, which solely operates in the fundamental visual-textual alignment space. Based on the captured Adv-3DT samples with high aggressiveness and transferability, we establish MM3DTBench, the first VQA benchmark dataset tailored to evaluate VLM robustness under challenging 3D variations. Extensive evaluations of representative VLMs with varying architectures reveal that real-world 3D variations can pose severe threats to model performance across various tasks.</p>
            <p id="subjects-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" onclick="foldPdfKimi('Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="panel paper" keywords="intensity,event,flow,optical,cameras,unsupervised,appearance,motion,estimation,tub">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event_ICCV_2025_paper.html" target="_blank" title="53/263"><span class="index notranslate">#53</span></a>
                <a id="title-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="title-link" href="/venue/Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" target="_blank">Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras</a>
                <a id="pdf-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuang Guo" target="_blank">Shuang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Friedhelm Hamann" target="_blank">Friedhelm Hamann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guillermo Gallego" target="_blank">Guillermo Gallego</a>
            </p>
            <p id="summary-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="summary">Event cameras rely on motion to obtain information about scene appearance. This means that appearance and motion are inherently linked: either both are present and recorded in the event data, or neither is captured. Previous works treat the recovery of these two visual quantities as separate tasks, which does not fit with the above-mentioned nature of event cameras and overlooks the inherent relations between them. We propose an unsupervised learning framework that jointly estimates optical flow (motion) and image intensity (appearance) using a single network. From the data generation model, we newly derive the event-based photometric error as a function of optical flow and image intensity. This error is further combined with the contrast maximization framework to form a comprehensive loss function that provides proper constraints for both flow and intensity estimation. Exhaustive experiments show our method's state-of-the-art performance: in optical flow estimation, it reduces EPE by 20% and AE by 25% compared to unsupervised approaches, while delivering competitive intensity estimation results, particularly in high dynamic range scenarios. Our method also achieves shorter inference time than all other optical flow methods and many of the image reconstruction methods, while they output only one quantity. Project page: https://github.com/tub-rip/E2FAI</p>
            <p id="subjects-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" onclick="foldPdfKimi('Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="panel paper" keywords="cobl,object,objects,layers,prompting,amodally,layering,tabletops,shot,representation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting_ICCV_2025_paper.html" target="_blank" title="54/263"><span class="index notranslate">#54</span></a>
                <a id="title-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="title-link" href="/venue/Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" target="_blank">CObL: Toward Zero-Shot Ordinal Layering without User Prompting</a>
                <a id="pdf-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aneel Damaraju" target="_blank">Aneel Damaraju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dean Hazineh" target="_blank">Dean Hazineh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Todd Zickler" target="_blank">Todd Zickler</a>
            </p>
            <p id="summary-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="summary">Vision benefits from grouping pixels into objects and understanding their spatial relationships, both laterally and in depth. We capture this with a scene representation comprising an occlusion-ordered stack of "object layers," each containing an isolated and amodally-completed object. To infer this representation from an image, we introduce a diffusion-based architecture named Concurrent Object Layers (CObL). CObL generates a stack of object layers in parallel, using Stable Diffusion as a prior for natural objects and inference-time guidance to ensure the inferred layers composite back to the input image. We train CObL using a few thousand synthetically-generated images of multi-object tabletop scenes, and we find that it zero-shot generalizes to photographs of real-world tabletops with varying numbers of novel objects. In contrast to recent models for amodal object completion, CObL reconstructs multiple occluded objects without user prompting and without knowing the number of objects beforehand. Unlike previous models for unsupervised object-centric representation learning, CObL is not limited to the world it was trained in.</p>
            <p id="subjects-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" onclick="foldPdfKimi('Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="panel paper" keywords="taxonomy,hierarchical,recognition,material,appearance,materials,depth,taxonomic,dataset,maps">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance_ICCV_2025_paper.html" target="_blank" title="55/263"><span class="index notranslate">#55</span></a>
                <a id="title-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="title-link" href="/venue/Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" target="_blank">Hierarchical Material Recognition from Local Appearance</a>
                <a id="pdf-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Beveridge" target="_blank">Matthew Beveridge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shree K. Nayar" target="_blank">Shree K. Nayar</a>
            </p>
            <p id="summary-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="summary">We introduce a taxonomy of materials for hierarchical recognition from local appearance. Our taxonomy is motivated by vision applications and is arranged according to the physical traits of materials. We contribute a diverse, in-the-wild dataset with images and depth maps of the taxonomy classes. Utilizing the taxonomy and dataset, we present a method for hierarchical material recognition based on graph attention networks. Our model leverages the taxonomic proximity between classes and achieves state-of-the-art performance. We demonstrate the model's potential to generalize to adverse, real-world imaging conditions, and that novel views rendered using the depth maps can enhance this capability. Finally, we show the model's capacity to rapidly learn new materials in a few-shot learning setting.</p>
            <p id="subjects-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" onclick="foldPdfKimi('Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="panel paper" keywords="memfof,memory,frame,estimation,1080p,flow,gpu,resolution,optical,fullhd">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation_ICCV_2025_paper.html" target="_blank" title="56/263"><span class="index notranslate">#56</span></a>
                <a id="title-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="title-link" href="/venue/Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" target="_blank">MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation</a>
                <a id="pdf-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vladislav Bargatin" target="_blank">Vladislav Bargatin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Egor Chistov" target="_blank">Egor Chistov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Yakovenko" target="_blank">Alexander Yakovenko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dmitriy Vatolin" target="_blank">Dmitriy Vatolin</a>
            </p>
            <p id="summary-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="summary">Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at: https://github.com/msu-video-group/memfof.</p>
            <p id="subjects-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" onclick="foldPdfKimi('Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="panel paper" keywords="etch,tightness,clothed,body,fitting,equivariant,cloth,loose,clothing,humans">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness_ICCV_2025_paper.html" target="_blank" title="57/263"><span class="index notranslate">#57</span></a>
                <a id="title-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="title-link" href="/venue/Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" target="_blank">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a>
                <a id="pdf-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Boqian Li" target="_blank">Boqian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiwen Feng" target="_blank">Haiwen Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Cai" target="_blank">Zeyu Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael J. Black" target="_blank">Michael J. Black</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuliang Xiu" target="_blank">Yuliang Xiu</a>
            </p>
            <p id="summary-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="summary">Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% 89.8%) in one-shot (or out-of-distribution) settings ( 1% data). Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at boqian-li.github.io/ETCH.</p>
            <p id="subjects-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" onclick="foldPdfKimi('Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="panel paper" keywords="mesh,mags,adsorbed,net,gaussians,rmd,rgd,splatting,simulation,representation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian_ICCV_2025_paper.html" target="_blank" title="58/263"><span class="index notranslate">#58</span></a>
                <a id="title-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="title-link" href="/venue/Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" target="_blank">MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting</a>
                <a id="pdf-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaojie Ma" target="_blank">Shaojie Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yawei Luo" target="_blank">Yawei Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Yang" target="_blank">Wei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>
            </p>
            <p id="summary-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="summary">3D reconstruction and simulation, although interrelated, have distinct objectives: reconstruction requires a flexible 3D representation that can adapt to diverse scenes, while simulation needs a structured representation to model motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians to roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D representation. Such representation harnesses both the rendering flexibility of 3D Gaussians and the structured property of meshes. To achieve this, we introduce RMD-Net, a network that learns motion priors from video data to refine mesh deformations, alongside RGD-Net, which models the relative displacement between the mesh and Gaussians to enhance rendering fidelity under mesh constraints. To generalize to novel, user-defined deformations beyond input video without reliance on temporal data, we propose MPE-Net, which leverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to the universality of meshes, MaGS is compatible with various deformation priors such as ARAP, SMPL, and soft physics simulation. Extensive experiments on the D-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves state-of-the-art performance in both reconstruction and simulation.</p>
            <p id="subjects-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" onclick="foldPdfKimi('Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="panel paper" keywords="fuxi,rtm,weather,forecasting,radiative,physical,prediction,transfer,3320,guided">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling_ICCV_2025_paper.html" target="_blank" title="59/263"><span class="index notranslate">#59</span></a>
                <a id="title-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="title-link" href="/venue/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" target="_blank">FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling</a>
                <a id="pdf-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qiusheng Huang" target="_blank">Qiusheng Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohui Zhong" target="_blank">Xiaohui Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Fan" target="_blank">Xu Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Li" target="_blank">Hao Li</a>
            </p>
            <p id="summary-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="summary">Similar to conventional video generation, current deep learning-based weather prediction frameworks often lack explicit physical constraints, leading to unphysical outputs that limit their reliability for operational forecasting. Among various physical processes requiring proper representation, radiation plays a fundamental role as it drives Earth's weather and climate systems. However, accurate simulation of radiative transfer processes remains challenging for traditional numerical weather prediction (NWP) models due to their inherent complexity and high computational costs. Here, we propose FuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance weather forecast accuracy while enforcing physical consistency. FuXi-RTM integrates a primary forecasting model (FuXi) with a fixed deep learning-based radiative transfer model (DLRTM) surrogate that efficiently replaces conventional radiation parameterization schemes. This represents the first deep learning-based weather forecasting framework to explicitly incorporate physical process modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM outperforms its unconstrained counterpart in 88.51% of 3320 variable and lead time combinations, with improvements in radiative flux predictions. By incorporating additional physical processes, FuXi-RTM paves the way for next-generation weather forecasting systems that are both accurate and physically consistent.</p>
            <p id="subjects-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" onclick="foldPdfKimi('Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="panel paper" keywords="diorama,scene,world,unleashing,interactability,shot,scenes,rgb,annotations,indoor">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling_ICCV_2025_paper.html" target="_blank" title="60/263"><span class="index notranslate">#60</span></a>
                <a id="title-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="title-link" href="/venue/Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" target="_blank">Diorama: Unleashing Zero-shot Single-view 3D Indoor Scene Modeling</a>
                <a id="pdf-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qirui Wu" target="_blank">Qirui Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Denys Iliash" target="_blank">Denys Iliash</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Ritchie" target="_blank">Daniel Ritchie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manolis Savva" target="_blank">Manolis Savva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angel X. Chang" target="_blank">Angel X. Chang</a>
            </p>
            <p id="summary-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="summary">Reconstructing structured 3D scenes from RGB images using CAD objects unlocks efficient and compact scene representations that maintain compositionality and interactability. Existing works propose training-heavy methods relying on either expensive yet inaccurate real-world annotations or controllable yet monotonous synthetic data that do not generalize well to unseen objects or domains. We present Diorama, the first zero-shot open-world system that holistically models 3D scenes from single-view RGB observations without requiring end-to-end training or human annotations. We show the feasibility of our approach by decomposing the problem into subtasks and introduce better solutions to each: architecture reconstruction, 3D shape retrieval, object pose estimation, and scene layout optimization. We evaluate our system on both synthetic and real-world data to show we significantly outperform baselines from prior work. We also demonstrate generalization to real-world internet images and the text-to-scene task.</p>
            <p id="subjects-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" onclick="foldPdfKimi('Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="panel paper" keywords="bokeh,bokehlicious,rendering,aperture,realbokeh,controllable,realdof,professional,photorealistic,apertures">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures_ICCV_2025_paper.html" target="_blank" title="61/263"><span class="index notranslate">#61</span></a>
                <a id="title-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="title-link" href="/venue/Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" target="_blank">Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures</a>
                <a id="pdf-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Seizinger" target="_blank">Tim Seizinger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florin-Alexandru Vasluianu" target="_blank">Florin-Alexandru Vasluianu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marcos V. Conde" target="_blank">Marcos V. Conde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongwei Wu" target="_blank">Zongwei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Radu Timofte" target="_blank">Radu Timofte</a>
            </p>
            <p id="summary-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="summary">Bokeh rendering methods play a key role in creating the visually appealing, softly blurred backgrounds seen in professional photography. While recent learning-based approaches show promising results, generating realistic Bokeh with controllable strength remains challenging. Existing methods require additional inputs and suffer from unrealistic Bokeh reproduction due to reliance on synthetic data. In this work, we propose Bokehlicious, a highly efficient network that provides intuitive control over Bokeh strength through an Aperture-Aware Attention mechanism, mimicking the physical lens aperture. To further address the lack of high-quality real-world data, we present RealBokeh, a novel dataset featuring 23,000 high-resolution (24-MP) images captured by professional photographers, covering diverse scenes with varied aperture and focal length settings. Evaluations on both our new RealBokeh and established Bokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA methods while significantly reducing computational cost and exhibiting strong zero-shot generalization. Our method and dataset further extend to defocus deblurring, achieving competitive results on the RealDOF benchmark. Our code and data will be public.</p>
            <p id="subjects-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" onclick="foldPdfKimi('Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="panel paper" keywords="progait,prosthesis,transfemoral,gait,prosthetic,dataset,amputations,legs,video,tasks">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis_ICCV_2025_paper.html" target="_blank" title="62/263"><span class="index notranslate">#62</span></a>
                <a id="title-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="title-link" href="/venue/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" target="_blank">ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users</a>
                <a id="pdf-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyu Yin" target="_blank">Xiangyu Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyuan Yang" target="_blank">Boyuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weichen Liu" target="_blank">Weichen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiyao Xue" target="_blank">Qiyao Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abrar Alamri" target="_blank">Abrar Alamri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Goeran Fiedler" target="_blank">Goeran Fiedler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Gao" target="_blank">Wei Gao</a>
            </p>
            <p id="summary-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="summary">Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. The ProGait dataset is available at https://huggingface.co/datasets/ericyxy98/ProGait, and the source codes of our benchmark tasks are available at https://github.com/pittisl/ProGait.</p>
            <p id="subjects-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" onclick="foldPdfKimi('Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="panel paper" keywords="wonderplay,video,physics,dynamic,scene,scenes,solver,generator,single,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and_ICCV_2025_paper.html" target="_blank" title="63/263"><span class="index notranslate">#63</span></a>
                <a id="title-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="title-link" href="/venue/Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" target="_blank">WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions</a>
                <a id="pdf-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zizhang Li" target="_blank">Zizhang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong-Xing Yu" target="_blank">Hong-Xing Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Liu" target="_blank">Wei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin Yang" target="_blank">Yin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Herrmann" target="_blank">Charles Herrmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gordon Wetzstein" target="_blank">Gordon Wetzstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>
            </p>
            <p id="summary-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="summary">WonderPlay is a novel framework integrating physics simulation with video generation for generating action-conditioned dynamic 3D scenes from a single image. Our hybrid generative simulator first uses a physics solver to simulate coarse 3D dynamics, which subsequently conditions a video generator to produce a video with finer, more realistic motion. The generated video is then used to update the simulated dynamic 3D scene, closing the loop between the physics solver and the video generator. This approach enables intuitive user control to be combined with the accurate dynamics of physics-based simulators and the expressivity of diffusion-based video generators. Experimental results demonstrate that WonderPlay enables users to interact with various scenes of diverse content, including cloth, sand, snow, liquid, smoke, elasticity, and rigid bodies -- all using a single image input. Code will be made public.</p>
            <p id="subjects-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" onclick="foldPdfKimi('Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="panel paper" keywords="chrome,clothed,multiview,occlusion,reconstruction,occluded,human,consistency,resilience,monocular">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a_ICCV_2025_paper.html" target="_blank" title="64/263"><span class="index notranslate">#64</span></a>
                <a id="title-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="title-link" href="/venue/Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" target="_blank">CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image</a>
                <a id="pdf-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Arindam Dutta" target="_blank">Arindam Dutta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Zheng" target="_blank">Meng Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongpai Gao" target="_blank">Zhongpai Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Planche" target="_blank">Benjamin Planche</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anwesa Choudhuri" target="_blank">Anwesa Choudhuri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Terrence Chen" target="_blank">Terrence Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit K. Roy-Chowdhury" target="_blank">Amit K. Roy-Chowdhury</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyan Wu" target="_blank">Ziyan Wu</a>
            </p>
            <p id="summary-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="summary">Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions.</p>
            <p id="subjects-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" onclick="foldPdfKimi('Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="panel paper" keywords="assembly,combinative,matching,shape,interlocking,parts,geometric,identical,shapes,surface">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Combinative_Matching_for_Geometric_Shape_Assembly_ICCV_2025_paper.html" target="_blank" title="65/263"><span class="index notranslate">#65</span></a>
                <a id="title-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="title-link" href="/venue/Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" target="_blank">Combinative Matching for Geometric Shape Assembly</a>
                <a id="pdf-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Combinative_Matching_for_Geometric_Shape_Assembly_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nahyuk Lee" target="_blank">Nahyuk Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juhong Min" target="_blank">Juhong Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junhong Lee" target="_blank">Junhong Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunghyun Park" target="_blank">Chunghyun Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minsu Cho" target="_blank">Minsu Cho</a>
            </p>
            <p id="summary-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="summary">This paper introduces a new shape-matching methodology, combinative matching, to combine interlocking parts for geometric shape assembly. Previous methods for geometric assembly typically rely on aligning parts by finding identical surfaces between the parts as in conventional shape matching and registration. Specifically, we explicitly model two distinct properties of interlocking shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method thus learns to establish correspondences across regions where their surface shapes appear identical but their volumes occupy the inverted space to each other. To facilitate this process, we also learn to align regions in rotation by estimating their shape orientations via equivariant neural networks. The proposed approach significantly reduces local ambiguities in matching and allows a robust combination of parts in assembly. Experimental results on geometric assembly benchmarks demonstrate the efficacy of our method, consistently outperforming the state of the art.</p>
            <p id="subjects-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" onclick="foldPdfKimi('Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="panel paper" keywords="depth,flashdepth,streaming,video,estimation,resolution,2044x1148,eyeline,across,fps">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.html" target="_blank" title="66/263"><span class="index notranslate">#66</span></a>
                <a id="title-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="title-link" href="/venue/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" target="_blank">FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution</a>
                <a id="pdf-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gene Chou" target="_blank">Gene Chou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Xian" target="_blank">Wenqi Xian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guandao Yang" target="_blank">Guandao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohamed Abdelfattah" target="_blank">Mohamed Abdelfattah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bharath Hariharan" target="_blank">Bharath Hariharan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Snavely" target="_blank">Noah Snavely</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ning Yu" target="_blank">Ning Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Debevec" target="_blank">Paul Debevec</a>
            </p>
            <p id="summary-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="summary">A versatile video depth estimation model should be consistent and accurate across frames, produce high-resolution depth maps, and support real-time streaming. We propose a method, FlashDepth, that satisfies all three requirements, performing depth estimation for a 2044x1148 streaming video at 24 FPS. We show that, with careful modifications to pretrained single-image depth models, these capabilities are enabled with relatively little data and training. We validate our approach across multiple unseen datasets against state-of-the-art depth models, and find that our method outperforms them in terms of boundary sharpness and speed by a significant margin, while maintaining competitive accuracy. We hope our model will enable various applications that require high-resolution depth, such as visual effects editing, and online decision-making, such as robotics. We release all code and model weights at https://github.com/Eyeline-Research/FlashDepth.</p>
            <p id="subjects-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" onclick="foldPdfKimi('Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="panel paper" keywords="human,immersive,physically,plausible,interaction,motion,real,humanoid,robot,interhuman">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible_ICCV_2025_paper.html" target="_blank" title="67/263"><span class="index notranslate">#67</span></a>
                <a id="title-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="title-link" href="/venue/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" target="_blank">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a>
                <a id="pdf-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiyang Ji" target="_blank">Kaiyang Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Shi" target="_blank">Ye Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Jin" target="_blank">Zichen Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kangyi Chen" target="_blank">Kangyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lan Xu" target="_blank">Lan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuexin Ma" target="_blank">Yuexin Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Yu" target="_blank">Jingyi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingya Wang" target="_blank">Jingya Wang</a>
            </p>
            <p id="summary-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="summary">Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.</p>
            <p id="subjects-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" onclick="foldPdfKimi('Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="panel paper" keywords="experts,anomaly,vad,splatting,splatters,events,temporal,gaussian,supervision,moe">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new_ICCV_2025_paper.html" target="_blank" title="68/263"><span class="index notranslate">#68</span></a>
                <a id="title-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="title-link" href="/venue/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" target="_blank">Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection</a>
                <a id="pdf-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Giacomo D' Amicantonio" target="_blank">Giacomo D' Amicantonio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Snehashis Majhi" target="_blank">Snehashis Majhi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Kong" target="_blank">Quan Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Garattoni" target="_blank">Lorenzo Garattoni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gianpiero Francesca" target="_blank">Gianpiero Francesca</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francois Bremond" target="_blank">Francois Bremond</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Egor Bondarev" target="_blank">Egor Bondarev</a>
            </p>
            <p id="summary-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="summary">Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.</p>
            <p id="subjects-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" onclick="foldPdfKimi('Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="panel paper" keywords="dancer,dance,music,human,videos,token,musical,transformer,pose,hands">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation_ICCV_2025_paper.html" target="_blank" title="69/263"><span class="index notranslate">#69</span></a>
                <a id="title-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="title-link" href="/venue/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" target="_blank">X-Dancer: Expressive Music to Human Dance Video Generation</a>
                <a id="pdf-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyuan Chen" target="_blank">Zeyuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyi Xu" target="_blank">Hongyi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guoxian Song" target="_blank">Guoxian Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=You Xie" target="_blank">You Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxu Zhang" target="_blank">Chenxu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Chen" target="_blank">Xin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Wang" target="_blank">Chao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Chang" target="_blank">Di Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linjie Luo" target="_blank">Linjie Luo</a>
            </p>
            <p id="summary-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="summary">We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. See our project page for more results: https://zeyuan-chen.com/X-Dancer/.</p>
            <p id="subjects-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" onclick="foldPdfKimi('Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="panel paper" keywords="faceq,customization,face,restoration,bench,aigfs,quality,eval,491k,evaluation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation_ICCV_2025_paper.html" target="_blank" title="70/263"><span class="index notranslate">#70</span></a>
                <a id="title-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="title-link" href="/venue/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" target="_blank">F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration</a>
                <a id="pdf-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Liu" target="_blank">Lu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huiyu Duan" target="_blank">Huiyu Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Hu" target="_blank">Qiang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liu Yang" target="_blank">Liu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunlei Cai" target="_blank">Chunlei Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianxiao Ye" target="_blank">Tianxiao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huayu Liu" target="_blank">Huayu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyun Zhang" target="_blank">Xiaoyun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangtao Zhai" target="_blank">Guangtao Zhai</a>
            </p>
            <p id="summary-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="summary">Recent artificial intelligence (AI) generative models have demonstrated remarkable capabilities in image production, and have been widely applied to face image generation, customization, and restoration. However, many AI-generated faces (AIGFs) still suffer from issues such as unique distortions, unrealistic details, and unexpected identity shifts, underscoring the need for a comprehensive quality evaluation method for AIGFs. To this end, we introduce **FaceQ**, the first comprehensive AI-generated Face image database with fine-grained Quality annotations aligned with human preferences, which consists of 12K images and 491K ratings across multiple dimensions. Using the FaceQ database, we establish **F-Bench**, a benchmark for comparing and evaluating face generation, customization, and restoration models, highlighting strengths and weaknesses across various prompts and evaluation dimensions. Additionally, we assess the performance of existing image quality assessment (IQA) methods on FaceQ, and further propose a large multimodal model (LMM) based Face quality Evaluator (**F-Eval**) to accurately assess the multi-dimensional quality of generated faces in a one-for-all manner. Extensive experimental results demonstrate the state-of-the-art performance of our F-Eval.</p>
            <p id="subjects-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" onclick="foldPdfKimi('Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="panel paper" keywords="temporal,pixel,deepfake,frequency,wise,artifacts,video,spatial,inconsistencies,detection">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection_ICCV_2025_paper.html" target="_blank" title="71/263"><span class="index notranslate">#71</span></a>
                <a id="title-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="title-link" href="/venue/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" target="_blank">Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection</a>
                <a id="pdf-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Taehoon Kim" target="_blank">Taehoon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jongwook Choi" target="_blank">Jongwook Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonghyun Jeong" target="_blank">Yonghyun Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haeun Noh" target="_blank">Haeun Noh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaejun Yoo" target="_blank">Jaejun Yoo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungryul Baek" target="_blank">Seungryul Baek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jongwon Choi" target="_blank">Jongwon Choi</a>
            </p>
            <p id="summary-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="summary">We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. The traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect pixel-wise temporal artifacts. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios.</p>
            <p id="subjects-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" onclick="foldPdfKimi('Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="panel paper" keywords="clothed,avatars,disentangled,avatar,layeravatar,representation,layered,generating,olivia23333,generation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation_ICCV_2025_paper.html" target="_blank" title="72/263"><span class="index notranslate">#72</span></a>
                <a id="title-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" target="_blank">Disentangled Clothed Avatar Generation with Layered Representation</a>
                <a id="pdf-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weitian Zhang" target="_blank">Weitian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yichao Yan" target="_blank">Yichao Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sijing Wu" target="_blank">Sijing Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manwen Liao" target="_blank">Manwen Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaokang Yang" target="_blank">Xiaokang Yang</a>
            </p>
            <p id="summary-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="summary">Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. While existing methods have made progress in creating animatable digital avatars, generating avatars with disentangled components (e.g., body, hair, and clothes) has long been a challenge. In this paper, we propose LayerAvatar, a novel feed-forward diffusion-based method capable of generating high-quality component-disentangled clothed avatars in seconds. We propose a layered UV feature plane representation, where components are distributed in different layers of the Gaussian-based UV feature plane with corresponding semantic labels. This representation can be effectively learned with current feed-forward generation pipelines, facilitating component disentanglement and enhancing details of generated avatars. Based on the well-designed representation, we train a single-stage diffusion model and introduce constrain terms to mitigate the severe occlusion issue of the innermost human body layer. Extensive experiments demonstrate the superior performances of our method in generating highly detailed and disentangled clothed avatars. In addition, we explore its applications in component transfer. The project page is available at https://olivia23333.github.io/LayerAvatar.</p>
            <p id="subjects-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="panel paper" keywords="fingerprints,generative,riemannian,regurgitative,definition,attribution,models,model,heightening,modalities">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Song_Riemannian-Geometric_Fingerprints_of_Generative_Models_ICCV_2025_paper.html" target="_blank" title="73/263"><span class="index notranslate">#73</span></a>
                <a id="title-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="title-link" href="/venue/Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" target="_blank">Riemannian-Geometric Fingerprints of Generative Models</a>
                <a id="pdf-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Song_Riemannian-Geometric_Fingerprints_of_Generative_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hae Jin Song" target="_blank">Hae Jin Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laurent Itti" target="_blank">Laurent Itti</a>
            </p>
            <p id="summary-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="summary">Recent breakthroughs and rapid integration of generative models (GMs) have sparked interest in the problem of model attribution and their fingerprints. For instance, service providers need reliable methods of authenticating their models to protect their IP, while users and law enforcement seek to verify the source of generated content for accountability and trust. In addition, a growing threat of model collapse is arising, as more model-generated data are being fed back into sources (e.g., YouTube) that are often harvested for training ("regurgitative training"), heightening the need to differentiate synthetic from human data. Yet, a gap still exists in understanding generative models' fingerprints, we believe, stemming from the lack of a formal framework that can define, represent, and analyze the fingerprints in a principled way. To address this gap, we take a geometric approach and propose a new definition of artifact and fingerprint of generative models using Riemannian geometry, which allows us to leverage the rich theory of differential geometry. Our new definition generalizes previous work (Song et al, 2024) to non-Euclidean manifolds by learning Riemannian metrics from data and replacing the Euclidean distances and nearest-neighbor search with geodesic distances and kNN-based Riemannian center of mass. We apply our theory to a new gradient-based algorithm for computing the fingerprints in practice. Results show that it is more effective in distinguishing a large array of generative models, spanning across 4 different datasets in 2 different resolutions (64x64, 256x256), 27 model architectures, and 2 modalities (Vision, Vision-Language). Using our proposed definition can significantly improve the performance on model attribution, as well as a generalization to unseen datasets, model types, and modalities, suggesting its efficacy in practice.</p>
            <p id="subjects-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" onclick="foldPdfKimi('Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="panel paper" keywords="irregularly,isp2hrnet,pixels,sampled,image,regular,grid,irregular,resolution,gradient">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled_ICCV_2025_paper.html" target="_blank" title="74/263"><span class="index notranslate">#74</span></a>
                <a id="title-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="title-link" href="/venue/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" target="_blank">ISP2HRNet: Learning to Reconstruct High Resolution Image from Irregularly Sampled Pixels via Hierarchical Gradient Learning</a>
                <a id="pdf-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanlin Wang" target="_blank">Yuanlin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiqin Xiong" target="_blank">Ruiqin Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Zhao" target="_blank">Rui Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Wang" target="_blank">Jin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaopeng Fan" target="_blank">Xiaopeng Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiejun Huang" target="_blank">Tiejun Huang</a>
            </p>
            <p id="summary-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="summary">While image signals are typically defined on a regular 2D grid, there are scenarios where they are only available at irregular positions. In such cases, reconstructing a complete image on regular grid is essential. This paper introduces ISP2HRNet, an end-to-end network designed to reconstruct high resolution image from irregularly sampled pixels that do not fall on a regular grid. To handle the challenges brought by irregular sampling, we propose an architecture to extract gradient structure hierarchically and learn continuous image representation. Specifically, we derive image gradient for each irregularly sampled pixel and further learn higher order gradient structural features according to the geometric and photometric information at the vertices of neighboring triangles. To convert the features from irregular pixels to regular grid, we propose a dual branch content-dependent weight generator to adaptively fuse the information from neighboring irregular pixels. Subsequently, an encoder captures deep structural details on regular grid and forms latent codes. Implicit neural representation parameterized by multi-layer perceptron decodes the latent codes and coordinates to pixel values for generating high resolution image. Experimental results demonstrate that the proposed network effectively solves the problem of high resolution image reconstruction from irregularly sampled pixels and achieves promising results. The source code is available at https://github.com/yuanlinwang/ISP2HRNet.</p>
            <p id="subjects-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" onclick="foldPdfKimi('Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="panel paper" keywords="keypoint,skeleton,seeker,behaviouris,anomaliesin,methodson,performanceon,taskin,applicationssuch,ubnormal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video_ICCV_2025_paper.html" target="_blank" title="75/263"><span class="index notranslate">#75</span></a>
                <a id="title-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="title-link" href="/venue/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" target="_blank">Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection</a>
                <a id="pdf-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anja Deli" target="_blank">Anja Deli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matej Grcic" target="_blank">Matej Grcic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sinia egvi" target="_blank">Sinia egvi</a>
            </p>
            <p id="summary-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="summary">Detecting anomalous human behaviouris an important visual taskin safety-critical applicationssuch as healthcare monitoring,workplace safety,or public surveillance.In these contexts,abnormalities are often reflectedwith unusual human poses.Thus, we propose SeeKer,a method for detecting anomaliesin sequences of human skeletons.Our method formulates the skeleton sequence densitythrough autoregressive factorization at the keypoint level.The corresponding conditional distributionsrepresent probable keypoint locations given prior skeletal motion.We formulate the joint distribution of the considered skeletonas causal prediction of conditional Gaussiansacross its constituent keypoints.A skeleton is flagged as anomalous if its keypoint locations surprise our model(i.e. receive a low density).In practice, our anomaly score is a weighted sum of per-keypoint log-conditionals,where the weights account for the confidence of the underlying keypoint detector.Despite its conceptual simplicity,SeeKer surpasses all previous methodson the UBnormal and MSAD-HR datasetswhile delivering competitive performanceon the ShanghaiTech dataset.</p>
            <p id="subjects-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" onclick="foldPdfKimi('Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="panel paper" keywords="gamefactory,game,action,videos,control,generalizable,domain,generative,creating,interactive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos_ICCV_2025_paper.html" target="_blank" title="76/263"><span class="index notranslate">#76</span></a>
                <a id="title-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="title-link" href="/venue/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" target="_blank">GameFactory: Creating New Games with Generative Interactive Videos</a>
                <a id="pdf-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiwen Yu" target="_blank">Jiwen Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiran Qin" target="_blank">Yiran Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xintao Wang" target="_blank">Xintao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Wan" target="_blank">Pengfei Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Zhang" target="_blank">Di Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xihui Liu" target="_blank">Xihui Liu</a>
            </p>
            <p id="summary-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="summary">Generative videos have the potential to revolutionize game development by autonomously creating new content. In this paper, we present GameFactory, a framework for action-controlled scene-generalizable game video generation. We first address the fundamental challenge of action controllability by introducing GF-Minecraft, an action-annotated game video dataset without human bias, and developing an action control module that enables precise control over both keyboard and mouse inputs. We further extend to support autoregressive generation for unlimited-length interactive videos.More importantly, GameFactory tackles the critical challenge of scene-generalizable action control, which most existing methods fail to address. To enable the creation of entirely new and diverse games beyond fixed styles and scenes, we leverage the open-domain generative priors from pre-trained video diffusion models. To bridge the domain gap between open-domain priors and small-scale game datasets, we propose a multi-phase training strategy with a domain adapter that decouples game style learning from action control. This decoupling ensures that action control learning is no longer bound to specific game styles, thereby achieving scene-generalizable action control. Experimental results demonstrate that GameFactory effectively generates open-domain action-controllable game videos, representing a significant step forward in AI-driven game generation.</p>
            <p id="subjects-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" onclick="foldPdfKimi('Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="panel paper" keywords="genmo,motion,generalist,generation,estimation,human,motions,diverse,handles,tasks">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_GENMO_A_GENeralist_Model_for_Human_MOtion_ICCV_2025_paper.html" target="_blank" title="77/263"><span class="index notranslate">#77</span></a>
                <a id="title-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="title-link" href="/venue/Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" target="_blank">GENMO: A GENeralist Model for Human MOtion</a>
                <a id="pdf-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_GENMO_A_GENeralist_Model_for_Human_MOtion_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiefeng Li" target="_blank">Jiefeng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinkun Cao" target="_blank">Jinkun Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haotian Zhang" target="_blank">Haotian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Davis Rempe" target="_blank">Davis Rempe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Kautz" target="_blank">Jan Kautz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Umar Iqbal" target="_blank">Umar Iqbal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Yuan" target="_blank">Ye Yuan</a>
            </p>
            <p id="summary-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="summary">Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.</p>
            <p id="subjects-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" onclick="foldPdfKimi('Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="panel paper" keywords="crowded,video,drones,maps,scenes,density,moving,inflow,counting,shared">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Video_Individual_Counting_for_Moving_Drones_ICCV_2025_paper.html" target="_blank" title="78/263"><span class="index notranslate">#78</span></a>
                <a id="title-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="title-link" href="/venue/Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" target="_blank">Video Individual Counting for Moving Drones</a>
                <a id="pdf-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Video_Individual_Counting_for_Moving_Drones_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yaowu Fan" target="_blank">Yaowu Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jia Wan" target="_blank">Jia Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Han" target="_blank">Tao Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antoni B. Chan" target="_blank">Antoni B. Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andy J. Ma" target="_blank">Andy J. Ma</a>
            </p>
            <p id="summary-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="summary">Video Individual Counting (VIC) has received increasing attention for its importance in intelligent video surveillance. Existing works are limited in two aspects, i.e., dataset and method. Previous datasets are captured with fixed or rarely moving cameras with relatively sparse individuals, restricting evaluation for a highly varying view and time in crowded scenes. Existing methods rely on localization followed by association or classification, which struggle under dense and dynamic conditions due to inaccurate localization of small targets. To address these issues, we introduce the MovingDroneCrowd Dataset, featuring videos captured by fast-moving drones in crowded scenes under diverse illuminations, shooting heights and angles. We further propose a Shared Density map-guided Network (SDNet) using a Depth-wise Cross-Frame Attention (DCFA) module to directly estimate shared density maps between consecutive frames, from which the inflow and outflow density maps are derived by subtracting the shared density maps from the global density maps. The inflow density maps across frames are summed up to obtain the number of unique pedestrians in a video. Experiments on our datasets and publicly available ones show the superiority of our method over the state of the arts in highly dynamic and complex crowded scenes. Our dataset and codes have been released publicly.</p>
            <p id="subjects-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" onclick="foldPdfKimi('Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="panel paper" keywords="visual,mllms,mllm,chronicles,multimodal,images,frequent,city,ended,llms">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of_ICCV_2025_paper.html" target="_blank" title="79/263"><span class="index notranslate">#79</span></a>
                <a id="title-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="title-link" href="/venue/Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" target="_blank">Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</a>
                <a id="pdf-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Boyang Deng" target="_blank">Boyang Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Songyou Peng" target="_blank">Songyou Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyle Genova" target="_blank">Kyle Genova</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gordon Wetzstein" target="_blank">Gordon Wetzstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Snavely" target="_blank">Noah Snavely</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonidas Guibas" target="_blank">Leonidas Guibas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Funkhouser" target="_blank">Thomas Funkhouser</a>
            </p>
            <p id="summary-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="summary">We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.).</p>
            <p id="subjects-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" onclick="foldPdfKimi('Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="panel paper" keywords="multimodal,byte,visual,encoding,language,understanding,tokens,pair,vision,mllms">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding_ICCV_2025_paper.html" target="_blank" title="80/263"><span class="index notranslate">#80</span></a>
                <a id="title-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" target="_blank">Unified Multimodal Understanding via Byte-Pair Visual Encoding</a>
                <a id="pdf-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wanpeng Zhang" target="_blank">Wanpeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yicheng Feng" target="_blank">Yicheng Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Luo" target="_blank">Hao Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yijiang Li" target="_blank">Yijiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihao Yue" target="_blank">Zihao Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sipeng Zheng" target="_blank">Sipeng Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongqing Lu" target="_blank">Zongqing Lu</a>
            </p>
            <p id="summary-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="summary">Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.</p>
            <p id="subjects-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="panel paper" keywords="tiling,artifacts,normalization,offs,kreshuklab,trade,segmentation,microscopy,images,stitched">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation_ICCV_2025_paper.html" target="_blank" title="81/263"><span class="index notranslate">#81</span></a>
                <a id="title-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="title-link" href="/venue/Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" target="_blank">Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images</a>
                <a id="pdf-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Elena Buglakova" target="_blank">Elena Buglakova</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anwai Archit" target="_blank">Anwai Archit</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edoardo D'Imprima" target="_blank">Edoardo D'Imprima</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julia Mahamid" target="_blank">Julia Mahamid</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Constantin Pape" target="_blank">Constantin Pape</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Kreshuk" target="_blank">Anna Kreshuk</a>
            </p>
            <p id="summary-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="summary">Segmentation of very large images is a common problem in microscopy, medical imaging or remote sensing. The problem is usually addressed by sliding window inference, which can theoretically lead to seamlessly stitched predictions. However, in practice many of the popular pipelines still suffer from tiling artifacts. We investigate the root cause of these issues and show that they stem from the normalization layers within the neural networks. We propose indicators to detect normalization issues and further explore the trade-offs between artifact-free and high-quality predictions, using three diverse microscopy datasets as examples. Finally, we propose to use BatchRenorm as the most suitable normalization strategy, which effectively removes tiling artifacts and enhances transfer performance, thereby improving the reusability of trained networks for new datasets. The code is available at https://github.com/kreshuklab/no_tiling_artifacts.</p>
            <p id="subjects-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="panel paper" keywords="chart,captions,chartcap,hallucination,extraneous,charts,caption,565k,dense,informative">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning_ICCV_2025_paper.html" target="_blank" title="82/263"><span class="index notranslate">#82</span></a>
                <a id="title-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="title-link" href="/venue/Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" target="_blank">ChartCap: Mitigating Hallucination of Dense Chart Captioning</a>
                <a id="pdf-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junyoung Lim" target="_blank">Junyoung Lim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaewoo Ahn" target="_blank">Jaewoo Ahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gunhee Kim" target="_blank">Gunhee Kim</a>
            </p>
            <p id="summary-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="summary">Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.</p>
            <p id="subjects-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" onclick="foldPdfKimi('Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="panel paper" keywords="uniphys,motion,control,diffusion,character,physics,flexible,physically,plausible,planner">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based_ICCV_2025_paper.html" target="_blank" title="83/263"><span class="index notranslate">#83</span></a>
                <a id="title-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="title-link" href="/venue/Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" target="_blank">UniPhys: Unified Planner and Controller with Diffusion for Flexible Physics-Based Character Control</a>
                <a id="pdf-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Wu" target="_blank">Yan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Korrawe Karunratanakul" target="_blank">Korrawe Karunratanakul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyi Luo" target="_blank">Zhengyi Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Tang" target="_blank">Siyu Tang</a>
            </p>
            <p id="summary-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="summary">Generating natural and physically plausible character motion remains challenging, particularly for long-horizon control with diverse guidance signals. While prior work combines high-level diffusion-based motion planners with low-level physics controllers, these systems suffer from domain gaps that degrade motion quality and require task-specific fine-tuning.To tackle this problem, we introduce UniPhys, a diffusion-based behavior cloning framework that unifies motion planning and control into a single model. UniPhys enables flexible, expressive character motion conditioned on multi-modal inputs such as text, trajectories, and goals. To address accumulated prediction errors over long sequences, UniPhys is trained with the Diffusion Forcing paradigm, learning to denoise noisy motion histories and handle discrepancies introduced by the physics simulator. This design allows UniPhys to robustly generate physically plausible, long-horizon motions. Through guided sampling, UniPhys generalizes to a wide range of control signals, including unseen ones, without requiring task-specific fine-tuning. Experiments show that UniPhys outperforms prior methods in motion naturalness, generalization, and robustness across diverse control tasks.</p>
            <p id="subjects-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" onclick="foldPdfKimi('Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="panel paper" keywords="motion,motionmillion,zero,shot,eval,million,generation,human,generalization,sequences">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data_ICCV_2025_paper.html" target="_blank" title="84/263"><span class="index notranslate">#84</span></a>
                <a id="title-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="title-link" href="/venue/Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" target="_blank">Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</a>
                <a id="pdf-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ke Fan" target="_blank">Ke Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shunlin Lu" target="_blank">Shunlin Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minyue Dai" target="_blank">Minyue Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runyi Yu" target="_blank">Runyi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lixing Xiao" target="_blank">Lixing Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyang Dou" target="_blank">Zhiyang Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junting Dong" target="_blank">Junting Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lizhuang Ma" target="_blank">Lizhuang Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingbo Wang" target="_blank">Jingbo Wang</a>
            </p>
            <p id="summary-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="summary">Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion--the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation.</p>
            <p id="subjects-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" onclick="foldPdfKimi('Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="panel paper" keywords="biometrics,disenq,activity,former,cues,disen,additional,identity,motion,misidentifications">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics_ICCV_2025_paper.html" target="_blank" title="85/263"><span class="index notranslate">#85</span></a>
                <a id="title-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="title-link" href="/venue/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" target="_blank">DisenQ: Disentangling Q-Former for Activity-Biometrics</a>
                <a id="pdf-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shehreen Azad" target="_blank">Shehreen Azad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yogesh Singh Rawat" target="_blank">Yogesh Singh Rawat</a>
            </p>
            <p id="summary-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="summary">In this work, we address activity-biometrics, which involves identifying individuals across diverse set of activities. Unlike traditional person identification, this setting introduces additional challenges as identity cues become entangled with motion dynamics and appearance variations, making biometrics feature learning more complex. While additional visual data like pose and/or silhouette help, they often struggle from extraction inaccuracies. To overcome this, we propose a multimodal language-guided framework that replaces reliance on additional visual data with structured textual supervision. At its core, we introduce **DisenQ** (**Disen**tangling **Q**-Former), a unified querying transformer that disentangles biometrics, motion, and non-biometrics features by leveraging structured language guidance. This ensures identity cues remain independent of appearance and motion variations, preventing misidentifications. We evaluate our approach on three activity-based video benchmarks, achieving state-of-the-art performance. Additionally, we demonstrate strong generalization to complex real-world scenario with competitive performance on a traditional video-based identification benchmark, showing the effectiveness of our framework.</p>
            <p id="subjects-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" onclick="foldPdfKimi('Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="panel paper" keywords="mesh,meshllm,meshes,serialized,llms,understand,empowering,1500k,progressively,language">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate_ICCV_2025_paper.html" target="_blank" title="86/263"><span class="index notranslate">#86</span></a>
                <a id="title-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="title-link" href="/venue/Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" target="_blank">MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh</a>
                <a id="pdf-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuangkang Fang" target="_blank">Shuangkang Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=I-Chao Shen" target="_blank">I-Chao Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufeng Wang" target="_blank">Yufeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi-Hsuan Tsai" target="_blank">Yi-Hsuan Tsai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuchang Zhou" target="_blank">Shuchang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenrui Ding" target="_blank">Wenrui Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Takeo Igarashi" target="_blank">Takeo Igarashi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Hsuan Yang" target="_blank">Ming-Hsuan Yang</a>
            </p>
            <p id="summary-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="summary">We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50x larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes.</p>
            <p id="subjects-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" onclick="foldPdfKimi('Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="panel paper" keywords="grasp,dexvlg,dexterous,language,aligned,vision,dexgraspnet,poses,objects,part">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale_ICCV_2025_paper.html" target="_blank" title="87/263"><span class="index notranslate">#87</span></a>
                <a id="title-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="title-link" href="/venue/He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" target="_blank">DexVLG: Dexterous Vision-Language-Grasp Model at Scale</a>
                <a id="pdf-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei He" target="_blank">Jiawei He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danshi Li" target="_blank">Danshi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinqiang Yu" target="_blank">Xinqiang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zekun Qi" target="_blank">Zekun Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenyao Zhang" target="_blank">Wenyao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Chen" target="_blank">Jiayi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxiang Zhang" target="_blank">Zhaoxiang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhizheng Zhang" target="_blank">Zhizheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Yi" target="_blank">Li Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Wang" target="_blank">He Wang</a>
            </p>
            <p id="summary-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="summary">As large models gain traction, vision-language models are enabling robots to tackle increasingly complex tasks. However, limited by the difficulty of data collection, progress has mainly focused on controlling simple gripper end-effectors. There is little research on functional grasping with large models for human-like dexterous hands. In this paper, we introduce DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction aligned with language instructions using single-view RGBD input. To accomplish this, we generate a dataset of 170 million dexterous grasp poses mapped to semantic parts across 174,000 objects in simulation, paired with detailed part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used to train a VLM with a flow-matching-based pose head producing instruction-aligned grasp poses for tabletop objects. To evaluate DexVLG's performance, we create benchmarks in simulations and conduct real-world experiments. Extensive experiments demonstrate DexVLG's strong zero-shot generalization capabilities, achieving an over 76% zero-shot execution success rate and state-of-the-art part-grasp accuracy in simulation, as well as successful part-aligned grasps on physical objects in real-world scenarios.</p>
            <p id="subjects-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" onclick="foldPdfKimi('He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="panel paper" keywords="discord,discrete,continuous,rectified,motion,tokens,decoding,smoother,flow,conditioning">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding_ICCV_2025_paper.html" target="_blank" title="88/263"><span class="index notranslate">#88</span></a>
                <a id="title-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="title-link" href="/venue/Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" target="_blank">DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding</a>
                <a id="pdf-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jungbin Cho" target="_blank">Jungbin Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junwan Kim" target="_blank">Junwan Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jisoo Kim" target="_blank">Jisoo Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minseo Kim" target="_blank">Minseo Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingu Kang" target="_blank">Mingu Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sungeun Hong" target="_blank">Sungeun Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tae-Hyun Oh" target="_blank">Tae-Hyun Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youngjae Yu" target="_blank">Youngjae Yu</a>
            </p>
            <p id="summary-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="summary">Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this discord between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Code and checkpoints will be released.</p>
            <p id="subjects-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" onclick="foldPdfKimi('Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="panel paper" keywords="wildlife,animalclue,feces,species,animal,indirect,recognizing,footprints,traces,identification">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces_ICCV_2025_paper.html" target="_blank" title="89/263"><span class="index notranslate">#89</span></a>
                <a id="title-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="title-link" href="/venue/Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" target="_blank">AnimalClue: Recognizing Animals by their Traces</a>
                <a id="pdf-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Risa Shinoda" target="_blank">Risa Shinoda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nakamasa Inoue" target="_blank">Nakamasa Inoue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Iro Laina" target="_blank">Iro Laina</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Rupprecht" target="_blank">Christian Rupprecht</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hirokatsu Kataoka" target="_blank">Hirokatsu Kataoka</a>
            </p>
            <p id="summary-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="summary">Wildlife observation plays an important role in biodiversity conservation, necessitating robust methodologies for monitoring wildlife populations and interspecies interactions. Recent advances in computer vision have significantly contributed to automating fundamental wildlife observation tasks, such as animal detection and species identification. However, accurately identifying species from indirect evidence like footprints and feces remains relatively underexplored, despite its importance in contributing to wildlife monitoring. To bridge this gap, we introduce AnimalClue, the first large-scale dataset for species identification from images of indirect evidence. Our dataset consists of 159,605 bounding boxes encompassing five categories of indirect clues: footprints, feces, eggs, bones, and feathers. It covers 968 species, 200 families, and 65 orders. Each image is annotated with species-level labels, bounding boxes or segmentation masks, and fine-grained trait information, including activity patterns and habitat preferences. Unlike existing datasets primarily focused on direct visual features (e.g., animal appearances), AnimalClue presents unique challenges for classification, detection, and instance segmentation tasks due to the need for recognizing more detailed and subtle visual features. In our experiments, we extensively evaluate representative vision models and identify key challenges in animal identification from their traces. Our dataset and code are available at https://dahlian00.github.io/AnimalCluePage/.</p>
            <p id="subjects-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" onclick="foldPdfKimi('Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="panel paper" keywords="ominicontrol,dit,control,image,transformer,conditioning,architectural,tasks,tokens,aligned">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer_ICCV_2025_paper.html" target="_blank" title="90/263"><span class="index notranslate">#90</span></a>
                <a id="title-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="title-link" href="/venue/Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" target="_blank">OminiControl: Minimal and Universal Control for Diffusion Transformer</a>
                <a id="pdf-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenxiong Tan" target="_blank">Zhenxiong Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Songhua Liu" target="_blank">Songhua Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyi Yang" target="_blank">Xingyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiaochu Xue" target="_blank">Qiaochu Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinchao Wang" target="_blank">Xinchao Wang</a>
            </p>
            <p id="summary-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="summary">We present OminiControl, a novel approach that rethinks how image conditions are integrated into Diffusion Transformer (DiT) architectures. Current image conditioning methods either introduce substantial parameter overhead or handle only specific control tasks effectively, limiting their practical versatility. OminiControl addresses these limitations through three key innovations: (1) a minimal architectural design that leverages the DiT's own VAE encoder and transformer blocks, requiring just 0.1% additional parameters; (2) a unified sequence processing strategy that combines condition tokens with image tokens for flexible token interactions; and (3) a dynamic position encoding mechanism that adapts to both spatially-aligned and non-aligned control tasks. Our extensive experiments show that this streamlined approach not only matches but surpasses the performance of specialized methods across multiple conditioning tasks. To overcome data limitations in subject-driven generation, we also introduce Subjects200K, a large-scale dataset of identity-consistent image pairs synthesized using DiT models themselves. This work demonstrates that effective image control can be achieved without architectural complexity, opening new possibilities for efficient and versatile image generation systems.</p>
            <p id="subjects-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" onclick="foldPdfKimi('Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="panel paper" keywords="reflow,straighten,vrfno,rectified,images,step,viscous,couplings,limitations,flow">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization_ICCV_2025_paper.html" target="_blank" title="91/263"><span class="index notranslate">#91</span></a>
                <a id="title-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="title-link" href="/venue/Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" target="_blank">Straighten Viscous Rectified Flow via Noise Optimization</a>
                <a id="pdf-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jimin Dai" target="_blank">Jimin Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiexi Yan" target="_blank">Jiexi Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Yang" target="_blank">Jian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Luo" target="_blank">Lei Luo</a>
            </p>
            <p id="summary-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="summary">The Reflow operation aims to straighten the inference trajectories of the rectified flow during training by constructing deterministic couplings between noises and images, thereby improving the quality of generated images in single-step or few-step generation. However, we identify critical limitations in Reflow, particularly its inability to rapidly generate high-quality images due to a distribution gap between images in its constructed deterministic couplings and real images. To address these shortcomings, we propose a novel alternative called Straighten Viscous Rectified Flow via Noise Optimization (VRFNO), which is a joint training framework integrating an encoder and a neural velocity field. VRFNO introduces two key innovations: (1) a historical velocity term that enhances trajectory distinction, enabling the model to more accurately predict the velocity of the current trajectory, and (2) the noise optimization through reparameterization to form optimized couplings with real images which are then utilized for training, effectively mitigating errors caused by Reflow's limitations. Comprehensive experiments on synthetic data and real datasets with varying resolutions show that VRFNO significantly mitigates the limitations of Reflow, achieving state-of-the-art performance in both one-step and few-step generation tasks.</p>
            <p id="subjects-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" onclick="foldPdfKimi('Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="panel paper" keywords="traceability,consumers,fingerprints,providers,service,fingerprint,fingerprinting,image,level,t2i">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models_ICCV_2025_paper.html" target="_blank" title="92/263"><span class="index notranslate">#92</span></a>
                <a id="title-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="title-link" href="/venue/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" target="_blank">Scalable Dual Fingerprinting for Hierarchical Attribution of Text-to-Image Models</a>
                <a id="pdf-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianwei Fei" target="_blank">Jianwei Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunshu Dai" target="_blank">Yunshu Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peipeng Yu" target="_blank">Peipeng Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Kong" target="_blank">Zhe Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiantao Zhou" target="_blank">Jiantao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihua Xia" target="_blank">Zhihua Xia</a>
            </p>
            <p id="summary-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="summary">The commercialization of generative artificial intelligence (GenAI) has led to a multi-level ecosystem involving model developers, service providers, and consumers. Thus, ensuring traceability is crucial, as service providers may violate intellectual property rights (IPR), and consumers may generate harmful content. However, existing methods are limited to single-level attribution scenarios and cannot simultaneously trace across multiple levels. To this end, we introduce a scalable dual fingerprinting method for text-to-image (T2I) models, to achieve traceability of both service providers and consumers. Specifically, we propose 2-headed Fingerprint-Informed Low-Rank Adaptation (FI-LoRA), where each head is controlled by a binary fingerprint and capable of introducing the fingerprints into generated images. In practice, one FI-LoRA head is used by the developer to assign a unique fingerprint to each service provider, while the other is made available to service providers for embedding consumer-specific fingerprints during image generation. Our method does not merely embed two fingerprints within the generated image but instead allows independent control over them at developer level and business level, enabling simultaneous traceability of businesses and consumers. Experiments show that our method applies to various image generation and editing tasks of multiple T2I models, and can achieve over 99.9% extraction accuracy for both fingerprints. Our method also demonstrates good robustness against both image-level attacks and white-box model-level attacks. We hope our work provides a unified solution for developers to implement multi-tiered traceability of their models and hierarchical control over model distribution and content generation.</p>
            <p id="subjects-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" onclick="foldPdfKimi('Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="panel paper" keywords="summdiff,summaries,video,summarization,subjectivity,task,diffusion,regressed,innate,multiple">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion_ICCV_2025_paper.html" target="_blank" title="93/263"><span class="index notranslate">#93</span></a>
                <a id="title-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="title-link" href="/venue/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" target="_blank">SummDiff: Generative Modeling of Video Summarization with Diffusion</a>
                <a id="pdf-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kwanseok Kim" target="_blank">Kwanseok Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaehoon Hahm" target="_blank">Jaehoon Hahm</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sumin Kim" target="_blank">Sumin Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinhwan Sul" target="_blank">Jinhwan Sul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Byunghak Kim" target="_blank">Byunghak Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joonseok Lee" target="_blank">Joonseok Lee</a>
            </p>
            <p id="summary-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="summary">Video summarization is a task of shortening a video by choosing a subset of frames while preserving its essential moments. Despite the innate subjectivity of the task, previous works have deterministically regressed to an averaged frame score over multiple raters, ignoring the inherent subjectivity of what constitutes a "good" summary. We propose a novel problem formulation by framing video summarization as a conditional generation task, allowing a model to learn the distribution of good summaries and to generate multiple plausible summaries that better reflect varying human perspectives. Adopting diffusion models for the first time in video summarization, our proposed method, SummDiff, dynamically adapts to visual contexts and generates multiple candidate summaries conditioned on the input video. Extensive experiments demonstrate that SummDiff not only achieves the state-of-the-art performance on various benchmarks but also produces summaries that closely align with individual annotator preferences. Moreover, we provide a deeper insight with novel metrics from an analysis of the knapsack, which is an important last step of generating summaries but has been overlooked in evaluation.</p>
            <p id="subjects-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" onclick="foldPdfKimi('Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="panel paper" keywords="iqa,quality,adapter,images,diffusion,image,generation,conditioning,models,assessment">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based_ICCV_2025_paper.html" target="_blank" title="94/263"><span class="index notranslate">#94</span></a>
                <a id="title-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="title-link" href="/venue/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" target="_blank">IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models</a>
                <a id="pdf-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Khaled Abud" target="_blank">Khaled Abud</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Lavrushkin" target="_blank">Sergey Lavrushkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexey Kirillov" target="_blank">Alexey Kirillov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dmitriy Vatolin" target="_blank">Dmitriy Vatolin</a>
            </p>
            <p id="summary-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="summary">Diffusion-based models have recently revolutionized image generation, achieving unprecedented levels of fidelity. However, consistent generation of high-quality images remains challenging partly due to the lack of conditioning mechanisms for perceptual quality. In this work, we propose methods to integrate image quality assessment (IQA) models into diffusion-based generators, enabling quality-aware image generation. We show that diffusion models can learn complex qualitative relationships from both IQA models' outputs and internal activations. First, we experiment with gradient-based guidance to optimize image quality directly and show this method has limited generalizability. To address this, we introduce IQA-Adapter, a novel framework that conditions generation on target quality levels by learning the implicit relationship between images and quality scores. When conditioned on high target quality, IQA-Adapter can shift the distribution of generated images towards a higher-quality subdomain, and, inversely, it can be used as a degradation model, generating progressively more distorted images when provided with a lower-quality signal. Under high-quality condition, IQA-Adapter achieves up to a 10% improvement across multiple objective metrics, as confirmed by a user preference study, while preserving generative diversity and content. Furthermore, we extend IQA-Adapter to a reference-based conditioning scenario, utilizing the rich activation space of IQA models to transfer highly specific, content-agnostic qualitative features between images.</p>
            <p id="subjects-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" onclick="foldPdfKimi('Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="panel paper" keywords="dropletvideo,camera,spatio,video,temporal,consistency,generation,plot,movements,integral">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent_ICCV_2025_paper.html" target="_blank" title="95/263"><span class="index notranslate">#95</span></a>
                <a id="title-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="title-link" href="/venue/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" target="_blank">DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation</a>
                <a id="pdf-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Runze Zhang" target="_blank">Runze Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guoguang Du" target="_blank">Guoguang Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaochuan Li" target="_blank">Xiaochuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Jia" target="_blank">Qi Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Jin" target="_blank">Liang Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Liu" target="_blank">Lu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingjing Wang" target="_blank">Jingjing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cong Xu" target="_blank">Cong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenhua Guo" target="_blank">Zhenhua Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaqian Zhao" target="_blank">Yaqian Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoli Gong" target="_blank">Xiaoli Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rengang Li" target="_blank">Rengang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoyu Fan" target="_blank">Baoyu Fan</a>
            </p>
            <p id="summary-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="summary">Spatio-temporal consistency is a critical topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a camera-movement description after a prompt without constraining its outcomes. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to model development. Initially, we constructed DropletVideo-10M, which comprises 10 million videos that feature dynamic camera motion and object actions. With an average length of 206 words, the captions offer detailed accounts of camera movements. Following this, we developed the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The work has been open-sourced: https://dropletx.github.io/.</p>
            <p id="subjects-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="panel paper" keywords="editing,reflow,mid,rectified,text,step,reflex,real,image,images">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via_ICCV_2025_paper.html" target="_blank" title="96/263"><span class="index notranslate">#96</span></a>
                <a id="title-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="title-link" href="/venue/Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" target="_blank">ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation</a>
                <a id="pdf-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jimyeong Kim" target="_blank">Jimyeong Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jungwon Park" target="_blank">Jungwon Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yeji Song" target="_blank">Yeji Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nojun Kwak" target="_blank">Nojun Kwak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wonjong Rhee" target="_blank">Wonjong Rhee</a>
            </p>
            <p id="summary-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="summary">Rectified Flow text-to-image models surpass diffusion models in image quality and text alignment, but adapting ReFlow for real-image editing remains challenging. We propose a new real-image editing method for ReFlow by analyzing the intermediate representations of multimodal transformer blocks and identifying three key features. To extract these features from real images with sufficient structural preservation, we leverage mid-step latent, which is inverted only up to the mid-step. We then adapt attention during injection to improve editability and enhance alignment to the target text. Our method is training-free, requires no user-provided mask, and can be applied even without a source prompt. Extensive experiments on two benchmarks with nine baselines demonstrate its superior performance over prior methods, further validated by human evaluations confirming a strong user preference for our approach.</p>
            <p id="subjects-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" onclick="foldPdfKimi('Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="panel paper" keywords="style,drag,aware,drop,insertion,insert,magic,personalization,subjectplop,domain">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop_ICCV_2025_paper.html" target="_blank" title="97/263"><span class="index notranslate">#97</span></a>
                <a id="title-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="title-link" href="/venue/Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" target="_blank">Magic Insert: Style-Aware Drag-and-Drop</a>
                <a id="pdf-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nataniel Ruiz" target="_blank">Nataniel Ruiz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanzhen Li" target="_blank">Yuanzhen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neal Wadhwa" target="_blank">Neal Wadhwa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yael Pritch" target="_blank">Yael Pritch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Rubinstein" target="_blank">Michael Rubinstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David E. Jacobs" target="_blank">David E. Jacobs</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shlomi Fruchter" target="_blank">Shlomi Fruchter</a>
            </p>
            <p id="summary-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="summary">We present Magic Insert, a method to drag-and-drop subjects from a user-provided image into a target image of a different style in a plausible manner while matching the style of the target image. This work formalizes our version of the problem of style-aware drag-and-drop and proposes to tackle it by decomposing it into two sub-problems: style-aware personalization and realistic object insertion in stylized images. For style-aware personalization, we cast our method as a weight-and-text-embedding finetuning method with inference-time module-targeted style injection. For subject insertion, we propose Bootstrapped Domain Adaption (BDA) to adapt a domain-specific photorealistic object insertion model to the domain of diverse artistic styles. Overall, the method significantly outperforms traditional and state-of-the-art approaches that struggle with quality, subject fidelity and harmonious stylization. Finally, we present a new dataset, SubjectPlop, to facilitate evaluation and future progress in this area.</p>
            <p id="subjects-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" onclick="foldPdfKimi('Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="panel paper" keywords="quantization,ptq,qat,outlier,aware,region,performance,outliers,post,super">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution_ICCV_2025_paper.html" target="_blank" title="98/263"><span class="index notranslate">#98</span></a>
                <a id="title-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="title-link" href="/venue/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" target="_blank">Outlier-Aware Post-Training Quantization for Image Super-Resolution</a>
                <a id="pdf-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hailing Wang" target="_blank">Hailing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianglin Lu" target="_blank">Jianglin Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yitian Zhang" target="_blank">Yitian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Fu" target="_blank">Yun Fu</a>
            </p>
            <p id="summary-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="summary">Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75x speedup.</p>
            <p id="subjects-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="panel paper" keywords="edit360,editing,edits,assets,view,anchor,diffusion,multi,modifications,views">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle_ICCV_2025_paper.html" target="_blank" title="99/263"><span class="index notranslate">#99</span></a>
                <a id="title-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="title-link" href="/venue/Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" target="_blank">Edit360: 2D Image Edits to 3D Assets from Any Angle</a>
                <a id="pdf-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junchao Huang" target="_blank">Junchao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinting Hu" target="_blank">Xinting Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaoshuai Shi" target="_blank">Shaoshuai Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuotao Tian" target="_blank">Zhuotao Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Jiang" target="_blank">Li Jiang</a>
            </p>
            <p id="summary-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="summary">Recent advances in diffusion models have significantly improved image generation and editing, but extending these capabilities to 3D assets remains challenging, especially for fine-grained edits that require multi-view consistency. Existing methods typically restrict editing to predetermined viewing angles, severely limiting their flexibility and practical applications.We introduce Edit360, a tuning-free framework that extends 2D modifications to multi-view consistent 3D editing. Built upon video diffusion models, Edit360 enables user-specific editing from arbitrary viewpoints while ensuring structural coherence across all views. The framework selects anchor views for 2D modifications and propagates edits across the entire 360-degree range. To achieve this, Edit360 introduces a novel Anchor-View Editing Propagation mechanism, which effectively aligns and merges multi-view information within the latent and attention spaces of diffusion models. The resulting edited multi-view sequences facilitate the reconstruction of high-quality 3D assets, enabling customizable 3D content creation.</p>
            <p id="subjects-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" onclick="foldPdfKimi('Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="panel paper" keywords="distillation,dmd2,sd3,adversarial,adm,score,step,discriminators,matching,pre">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and_ICCV_2025_paper.html" target="_blank" title="100/263"><span class="index notranslate">#100</span></a>
                <a id="title-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="title-link" href="/venue/Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" target="_blank">Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis</a>
                <a id="pdf-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yanzuo Lu" target="_blank">Yanzuo Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxi Ren" target="_blank">Yuxi Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Xia" target="_blank">Xin Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shanchuan Lin" target="_blank">Shanchuan Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Wang" target="_blank">Xing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuefeng Xiao" target="_blank">Xuefeng Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andy J. Ma" target="_blank">Andy J. Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohua Xie" target="_blank">Xiaohua Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian-Huang Lai" target="_blank">Jian-Huang Lai</a>
            </p>
            <p id="summary-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="summary">Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators.Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications.To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner.In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces.Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage.By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time.Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.</p>
            <p id="subjects-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" onclick="foldPdfKimi('Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="panel paper" keywords="video,image,diffusion,generation,representations,walt,visual,understanding,objectives,uncharted">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations_ICCV_2025_paper.html" target="_blank" title="101/263"><span class="index notranslate">#101</span></a>
                <a id="title-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="title-link" href="/venue/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" target="_blank">From Image to Video: An Empirical Study of Diffusion Representations</a>
                <a id="pdf-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pedro Vlez" target="_blank">Pedro Vlez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luisa F. Polana" target="_blank">Luisa F. Polana</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuhan Zhang" target="_blank">Chuhan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Kabra" target="_blank">Rishabh Kabra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anurag Arnab" target="_blank">Anurag Arnab</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mehdi S. M. Sajjadi" target="_blank">Mehdi S. M. Sajjadi</a>
            </p>
            <p id="summary-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="summary">Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis.This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we analyze the performance of latent image and video diffusion representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. For the most informative comparison, we utilize the same model architecture, WALT, across image and video generation objectives. Our results show that video generation pre-training consistently outperforms its image counterpart, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning.</p>
            <p id="subjects-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" onclick="foldPdfKimi('Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="panel paper" keywords="acquisition,visual,encoders,corruptions,traces,semantic,ryan,caesar,ramos,clip">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_ICCV_2025_paper.html" target="_blank" title="102/263"><span class="index notranslate">#102</span></a>
                <a id="title-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="title-link" href="/venue/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" target="_blank">Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</a>
                <a id="pdf-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Ramos" target="_blank">Ryan Ramos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vladan Stojni" target="_blank">Vladan Stojni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giorgos Kordopatis-Zilos" target="_blank">Giorgos Kordopatis-Zilos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuta Nakashima" target="_blank">Yuta Nakashima</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giorgos Tolias" target="_blank">Giorgos Tolias</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noa Garcia" target="_blank">Noa Garcia</a>
            </p>
            <p id="summary-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="summary">Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions. We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces</p>
            <p id="subjects-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" onclick="foldPdfKimi('Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="panel paper" keywords="ragdiffusion,clothing,faithful,hallucinations,generation,garment,texture,rag,structure,slle">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation_ICCV_2025_paper.html" target="_blank" title="103/263"><span class="index notranslate">#103</span></a>
                <a id="title-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="title-link" href="/venue/Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" target="_blank">RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation</a>
                <a id="pdf-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhan Li" target="_blank">Yuhan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianfeng Tan" target="_blank">Xianfeng Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxiang Shang" target="_blank">Wenxiang Shang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yubo Wu" target="_blank">Yubo Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Wang" target="_blank">Jian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanhong Chen" target="_blank">Xuanhong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhang" target="_blank">Yi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hangcheng Zhu" target="_blank">Hangcheng Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingbing Ni" target="_blank">Bingbing Ni</a>
            </p>
            <p id="summary-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="summary">Standard clothing asset generation involves restoring forward-facing flat-lay garment images displayed on a clear background by extracting clothing information from diverse real-world contexts, which presents significant challenges due to highly standardized structure sampling distributions and clothing semantic absence in complex scenarios. Existing models have limited spatial perception, often exhibiting structural hallucinations and texture distortion in this high-specification generative task. To address this issue, we propose a novel Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance structure determinacy and mitigate hallucinations by assimilating knowledge from language models and external databases. RAGDiffusion consists of two processes: (1) Retrieval-based structure aggregation, which employs contrastive learning and a Structure Locally Linear Embedding (SLLE) to derive global structure and spatial landmarks, providing both soft and hard guidance to counteract structural ambiguities; and (2) Omni-level faithful garment generation, which introduces a coarse-to-fine texture alignment that ensures fidelity in pattern and detail components within the diffusing. Extensive experiments on challenging real-world datasets demonstrate that RAGDiffusion synthesizes structurally and texture-faithful clothing assets with significant performance improvements, representing a pioneering effort in high-specification faithful generation with RAG to confront intrinsic hallucinations and enhance fidelity.</p>
            <p id="subjects-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" onclick="foldPdfKimi('Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="panel paper" keywords="episodic,memory,temporally,essential,vcil,incremental,semantic,ucf,video,101">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning_ICCV_2025_paper.html" target="_blank" title="104/263"><span class="index notranslate">#104</span></a>
                <a id="title-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="title-link" href="/venue/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" target="_blank">ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning</a>
                <a id="pdf-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jongseo Lee" target="_blank">Jongseo Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyungho Bae" target="_blank">Kyungho Bae</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyle Min" target="_blank">Kyle Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gyeong-Moon Park" target="_blank">Gyeong-Moon Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwoo Choi" target="_blank">Jinwoo Choi</a>
            </p>
            <p id="summary-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="summary">In this work, we tackle the problem of video class-incremental learning (VCIL). Many existing VCIL methods mitigate catastrophic forgetting by rehearsal training with a few temporally dense samples stored in episodic memory, which is memory-inefficient. Alternatively, some methods store temporally sparse samples, sacrificing essential temporal information and thereby resulting in inferior performance. To address this trade-off between memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL consists of episodic memory for storing temporally sparse features and semantic memory for storing general knowledge represented by learnable prompts. We introduce a novel memory retrieval (MR) module that integrates episodic memory and semantic prompts through cross-attention, enabling the retrieval of temporally dense features from temporally sparse features. We rigorously validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced memory, ESSENTIAL achieves favorable performance on the benchmarks.</p>
            <p id="subjects-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" onclick="foldPdfKimi('Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="panel paper" keywords="noisequery,noise,generation,t2i,guidance,implicit,goal,silent,text,assistant">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image_ICCV_2025_paper.html" target="_blank" title="105/263"><span class="index notranslate">#105</span></a>
                <a id="title-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="title-link" href="/venue/Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" target="_blank">The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation</a>
                <a id="pdf-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoyu Wang" target="_blank">Ruoyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huayang Huang" target="_blank">Huayang Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Zhu" target="_blank">Ye Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Olga Russakovsky" target="_blank">Olga Russakovsky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Wu" target="_blank">Yu Wu</a>
            </p>
            <p id="summary-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="summary">In this work, we introduce NoiseQuery as a novel method for enhanced noise initialization in versatile goal-driven text-to-image (T2I) generation. Specifically, we propose to leverage an aligned Gaussian noise as implicit guidance to complement explicit user-defined inputs, such as text prompts, for better generation quality and controllability. Unlike existing noise optimization methods designed for specific models, our approach is grounded in a fundamental examination of the generic finite-step noise scheduler design in diffusion formulation, allowing better generalization across different diffusion-based architectures in a tuning-free manner. This model-agnostic nature allows us to construct a reusable noise library compatible with multiple T2I models and enhancement techniques, serving as a foundational layer for more effective generation. Extensive experiments demonstrate that NoiseQuery enables fine-grained control and yields significant performance boosts not only over high-level semantics but also over low-level visual attributes, which are typically difficult to specify through text alone, with seamless integration into current workflows with minimal computational overhead.</p>
            <p id="subjects-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" onclick="foldPdfKimi('Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="panel paper" keywords="gfpack,packing,irregular,gradient,utilization,feasibility,attention,rotation,timhsue,approaches">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing_ICCV_2025_paper.html" target="_blank" title="106/263"><span class="index notranslate">#106</span></a>
                <a id="title-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="title-link" href="/venue/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" target="_blank">GFPack++: Attention-Driven Gradient Fields for Optimizing 2D Irregular Packing</a>
                <a id="pdf-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyang Xue" target="_blank">Tianyang Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Lu" target="_blank">Lin Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Liu" target="_blank">Yang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingdong Wu" target="_blank">Mingdong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Dong" target="_blank">Hao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanbin Zhang" target="_blank">Yanbin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renmin Han" target="_blank">Renmin Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoquan Chen" target="_blank">Baoquan Chen</a>
            </p>
            <p id="summary-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="summary">2D irregular packing is a classic combinatorial optimization problem with various applications, such as material utilization and texture atlas generation. Due to its NP-hard nature, conventional numerical approaches typically encounter slow convergence and high computational costs. Previous research GFPack introduced a generative method for gradient-based packing, providing early evidence of its feasibility but faced limitations such as insufficient rotation support, poor boundary adaptability, and high overlap ratios. In this paper, we propose GFPack++, a deeply investigated framework that adopts attention-based geometry and relation encoding, enabling more comprehensive modeling of complex packing relationships. We further design a constrained gradient and a weighting function to enhance both the feasibility of the produced solutions and the learning effectiveness. Experimental results on multiple datasets demonstrate that GFPack++ achieves higher space utilization, supports continuous rotation, generalizes well to arbitrary boundaries, and infers orders of magnitude faster than previous approaches. Codes for this paper are at https://github.com/TimHsue/GFPack-pp.</p>
            <p id="subjects-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" onclick="foldPdfKimi('Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="panel paper" keywords="lora,clora,dog,generation,image,concepts,cat,models,attention,loras">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation_ICCV_2025_paper.html" target="_blank" title="107/263"><span class="index notranslate">#107</span></a>
                <a id="title-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="title-link" href="/venue/Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" target="_blank">Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation</a>
                <a id="pdf-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tuna Han Salih Meral" target="_blank">Tuna Han Salih Meral</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enis Simsar" target="_blank">Enis Simsar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Tombari" target="_blank">Federico Tombari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pinar Yanardag" target="_blank">Pinar Yanardag</a>
            </p>
            <p id="summary-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="summary">Low-Rank Adaptation (LoRA) has emerged as a powerful and popular technique for personalization, enabling efficient adaptation of pre-trained image generation models for specific tasks without comprehensive retraining. While employing individual pre-trained LoRA models excels at representing single concepts, such as those representing a specific dog or a cat, utilizing multiple LoRA models to capture a variety of concepts in a single image still poses a significant challenge. Existing methods often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). We introduce CLoRA, a training-free approach that addresses these limitations by updating the attention maps of multiple LoRA models at test-time, and leveraging the attention maps to create semantic masks for fusing latent representations. This enables the generation of composite images that accurately reflect the characteristics of each LoRA. Our comprehensive qualitative and quantitative evaluations demonstrate that CLoRA significantly outperforms existing methods in multi-concept image generation using LoRAs.</p>
            <p id="subjects-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" onclick="foldPdfKimi('Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="panel paper" keywords="covisibility,comatch,subpixel,covisible,tokens,bilateral,level,zizhuoli,dense,matching">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching_ICCV_2025_paper.html" target="_blank" title="108/263"><span class="index notranslate">#108</span></a>
                <a id="title-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="title-link" href="/venue/Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" target="_blank">CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching</a>
                <a id="pdf-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zizhuo Li" target="_blank">Zizhuo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Lu" target="_blank">Yifan Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Tang" target="_blank">Linfeng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shihua Zhang" target="_blank">Shihua Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Ma" target="_blank">Jiayi Ma</a>
            </p>
            <p id="summary-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="summary">This prospective study proposes CoMatch, a novel semi-dense image matcher with dynamic covisibility awareness and bilateral subpixel accuracy. Firstly, observing that modeling context interaction over the entire coarse feature map elicits highly redundant computation due to the neighboring representation similarity of tokens, a covisibility-guided token condenser is introduced to adaptively aggregate tokens in light of their covisibility scores that are dynamically estimated, thereby ensuring computational efficiency while improving the representational capacity of aggregated tokens simultaneously. Secondly, considering that feature interaction with non-covisible areas is distracting, which may degrade feature distinctiveness, a covisibility-assisted attention mechanism is deployed to selectively suppress irrelevant message broadcast from non-covisible reduced tokens, resulting in robust and compact attention to relevant rather than all ones. Thirdly, we find that at the fine-level stage, current methods adjust only the target view's keypoints to subpixel level, while those in the source view remain restricted at the coarse level and thus not informative enough, detrimental to keypoint location-sensitive usages. A simple yet potent fine correlation module is developed to refine matching candidates in both source and target views to subpixel level, attaining attractive performance improvement. Thorough experimentation across an array of public benchmarks affirms CoMatch's promising accuracy, efficiency, and generalizability. Code is available at https://github.com/ZizhuoLi/CoMatch.</p>
            <p id="subjects-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" onclick="foldPdfKimi('Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="panel paper" keywords="editing,malicious,dct,image,purification,jpeg,adversarial,shield,images,noise">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing_ICCV_2025_paper.html" target="_blank" title="109/263"><span class="index notranslate">#109</span></a>
                <a id="title-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="title-link" href="/venue/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" target="_blank">DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing</a>
                <a id="pdf-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aniruddha Bala" target="_blank">Aniruddha Bala</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohit Chowdhury" target="_blank">Rohit Chowdhury</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohan Jaiswal" target="_blank">Rohan Jaiswal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddharth Roheda" target="_blank">Siddharth Roheda</a>
            </p>
            <p id="summary-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="summary">Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques.</p>
            <p id="subjects-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" onclick="foldPdfKimi('Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="panel paper" keywords="face,exif,generated,supervised,photographic,pretext,faces,tags,self,serving">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection_ICCV_2025_paper.html" target="_blank" title="110/263"><span class="index notranslate">#110</span></a>
                <a id="title-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="title-link" href="/venue/Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" target="_blank">Bi-Level Optimization for Self-Supervised AI-Generated Face Detection</a>
                <a id="pdf-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mian Zou" target="_blank">Mian Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Zhong" target="_blank">Nan Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baosheng Yu" target="_blank">Baosheng Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yibing Zhan" target="_blank">Yibing Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kede Ma" target="_blank">Kede Ma</a>
            </p>
            <p id="summary-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="summary">AI-generated face detectors trained via supervised learning typically rely on synthesized images from specific generators, limiting their generalization to emerging generative techniques. To overcome this limitation, we introduce a self-supervised method based on bi-level optimization. In the inner loop, we pretrain a vision encoder only on photographic face images using a set of linearly weighted pretext tasks: classification of categorical exchangeable image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of artificial face manipulations. The outer loop then optimizes the relative weights of these pretext tasks to enhance the coarse-grained detection of manipulated faces, serving as a proxy task for identifying AI-generated faces. In doing so, it aligns self-supervised learning more closely with the ultimate goal of AI-generated face detection. Once pretrained, the encoder remains fixed, and AI-generated faces are detected either as anomalies under a Gaussian mixture model fitted to photographic face features or by a lightweight two-layer perceptron serving as a binary classifier. Extensive experiments demonstrate that our detectors significantly outperform existing approaches in both one-class and binary classification settings, exhibiting strong generalization to unseen generators.</p>
            <p id="subjects-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" onclick="foldPdfKimi('Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="panel paper" keywords="shape,editing,shapes,localized,edits,edited,blending,inpainting,innacurate,blended">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing_ICCV_2025_paper.html" target="_blank" title="111/263"><span class="index notranslate">#111</span></a>
                <a id="title-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="title-link" href="/venue/Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" target="_blank">Blended Point Cloud Diffusion for Localized Text-guided Shape Editing</a>
                <a id="pdf-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Etai Sella" target="_blank">Etai Sella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noam Atia" target="_blank">Noam Atia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ron Mokady" target="_blank">Ron Mokady</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hadar Averbuch-Elor" target="_blank">Hadar Averbuch-Elor</a>
            </p>
            <p id="summary-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="summary">Natural language offers a highly intuitive interface for enabling localized fine-grained edits of 3D shapes. However, prior works face challenges in preserving global coherence while locally modifying the input 3D shape. In this work, we introduce an inpainting-based framework for editing shapes represented as point clouds. Our approach leverages foundation 3D diffusion models for achieving localized shape edits, adding structural guidance in the form of a partial conditional shape, ensuring that other regions correctly preserve the shape's identity. Furthermore, to encourage identity preservation also within the local edited region, we propose an inference-time coordinate blending algorithm which balances reconstruction of the full shape with inpainting at a progression of noise levels during the inference process. Our coordinate blending algorithm seamlessly blends the original shape with its edited version, enabling a fine-grained editing of 3D shapes, all while circumventing the need for computationally expensive and often innacurate inversion. Extensive experiments show that our method outperforms alternative techniques across a wide range of metrics that evaluate both fidelity to the original shape and also adherence to the textual description. We will release our code and trained models.</p>
            <p id="subjects-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" onclick="foldPdfKimi('Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="panel paper" keywords="dlf,generative,branch,latent,compressing,fidelity,clic2020,illm,extreme,compression">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion_ICCV_2025_paper.html" target="_blank" title="112/263"><span class="index notranslate">#112</span></a>
                <a id="title-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="title-link" href="/venue/Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" target="_blank">DLF: Extreme Image Compression with Dual-generative Latent Fusion</a>
                <a id="pdf-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Naifu Xue" target="_blank">Naifu Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyang Jia" target="_blank">Zhaoyang Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Li" target="_blank">Jiahao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Li" target="_blank">Bin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Zhang" target="_blank">Yuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Lu" target="_blank">Yan Lu</a>
            </p>
            <p id="summary-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="summary">Recent studies in extreme image compression have achieved remarkable performance by compressing the tokens from generative tokenizers. However, these methods often prioritize clustering common semantics within the dataset, while overlooking the diverse details of individual objects. Consequently, this results in suboptimal reconstruction fidelity, especially at low bitrates. To address this issue, we introduce a Dual-generative Latent Fusion (DLF) paradigm. DLF decomposes the latent into semantic and detail elements, compressing them through two distinct branches. The semantic branch clusters high-level information into compact tokens, while the detail branch encodes perceptually critical details to enhance the overall fidelity. Additionally, we propose a cross-branch interactive design to reduce redundancy between the two branches, thereby minimizing the overall bit cost. Experimental results demonstrate the impressive reconstruction quality of DLF even below 0.01 bits per pixel (bpp). On the CLIC2020 test set, our method achieves bitrate savings of up to 27.93% on LPIPS and 53.55% on DISTS compared to MS-ILLM. Furthermore, DLF surpasses recent diffusion-based codecs in visual fidelity while maintaining a comparable level of generative realism. Project: https://dlfcodec.github.io/</p>
            <p id="subjects-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" onclick="foldPdfKimi('Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="panel paper" keywords="purification,customization,protective,antipure,guidance,perturbation,imperceptible,diffusion,workflow,resistant">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to_ICCV_2025_paper.html" target="_blank" title="113/263"><span class="index notranslate">#113</span></a>
                <a id="title-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="title-link" href="/venue/Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" target="_blank">Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification</a>
                <a id="pdf-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenkui Yang" target="_blank">Wenkui Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Cao" target="_blank">Jie Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junxian Duan" target="_blank">Junxian Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran He" target="_blank">Ran He</a>
            </p>
            <p id="summary-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="summary">Diffusion models like Stable Diffusion have become prominent in visual synthesis tasks due to their powerful customization capabilities, which also introduce significant security risks, including deepfakes and copyright infringement. In response, a class of methods known as protective perturbation emerged, which mitigates image misuse by injecting imperceptible adversarial noise. However, purification can remove protective perturbations, thereby exposing images again to the risk of malicious forgery. In this work, we formalize the anti-purification task, highlighting challenges that hinder existing approaches, and propose a simple diagnostic protective perturbation named AntiPure. AntiPure exposes vulnerabilities of purification within the "purification-customization" workflow, owing to two guidance mechanisms: 1) Patch-wise Frequency Guidance, which reduces the model's influence over high-frequency components in the purified image, and 2) Erroneous Timestep Guidance, which disrupts the model's denoising strategy across different timesteps. With additional guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings, achieving effective post-customization distortion. Experiments show that, as a stress test for purification, AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods within the purification-customization workflow.</p>
            <p id="subjects-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" onclick="foldPdfKimi('Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="panel paper" keywords="gradnorm,laic,clustering,nouns,gradients,assisted,strategies,filtering,rigorous,positiveness">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image_ICCV_2025_paper.html" target="_blank" title="114/263"><span class="index notranslate">#114</span></a>
                <a id="title-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="title-link" href="/venue/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" target="_blank">On the Provable Importance of Gradients for Autonomous Language-Assisted Image Clustering</a>
                <a id="pdf-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Peng" target="_blank">Bo Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Lu" target="_blank">Jie Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangquan Zhang" target="_blank">Guangquan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Fang" target="_blank">Zhen Fang</a>
            </p>
            <p id="summary-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="summary">This paper investigates the recently emerged problem of Language-assisted Image Clustering (LaIC), where textual semantics are leveraged to improve the discriminability of visual representations to facilitate image clustering. Due to the unavailability of true class names, one of core challenges of LaIC lies in how to filter positive nouns, i.e., those semantically close to the images of interest, from unlabeled wild corpus data. Existing filtering strategies are predominantly based on the off-the-shelf feature space learned by CLIP; however, despite being intuitive, these strategies lack a rigorous theoretical foundation. To fill this gap, we propose a novel gradient-based framework, termed as GradNorm, which is theoretically guaranteed and shows strong empirical performance. In particular, we measure the positiveness of each noun based on the magnitude of gradients back-propagated from the cross-entropy between the predicted target distribution and the softmax output. Theoretically, we provide a rigorous error bound to quantify the separability of positive nouns by GradNorm and prove that GradNorm naturally subsumes existing filtering strategies as extremely special cases of itself. Empirically, extensive experiments show that GradNorm achieves the state-of-the-art clustering performance on various benchmarks.</p>
            <p id="subjects-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" onclick="foldPdfKimi('Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="panel paper" keywords="captionsmiths,caption,captioning,language,descriptiveness,flexibly,pattern,captions,controlling,smoothly">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning_ICCV_2025_paper.html" target="_blank" title="115/263"><span class="index notranslate">#115</span></a>
                <a id="title-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="title-link" href="/venue/Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" target="_blank">CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning</a>
                <a id="pdf-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kuniaki Saito" target="_blank">Kuniaki Saito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Donghyun Kim" target="_blank">Donghyun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwanyong Park" target="_blank">Kwanyong Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Atsushi Hashimoto" target="_blank">Atsushi Hashimoto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yoshitaka Ushiku" target="_blank">Yoshitaka Ushiku</a>
            </p>
            <p id="summary-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="summary">An image captioning model flexibly switching its language pattern, e.g., descriptiveness and length, should be useful since it can be applied to diverse applications. However, despite the dramatic improvement in generative vision-language models, fine-grained control over the properties of generated captions is not easy due to two reasons: (i) existing models are not given the properties as a condition during training and (ii) existing models cannot smoothly transition its language pattern from one state to the other. Given this challenge, we propose a new approach, CaptionSmiths, to acquire a single captioning model that can handle diverse language patterns. First, our approach quantifies three properties of each caption, length, descriptiveness, and uniqueness of a word, as continuous scalar values, without human annotation. Given the values, we represent the conditioning via interpolation between two endpoint vectors corresponding to the extreme states, e.g., one for a very short caption and one for a very long caption. Empirical results demonstrate that the resulting model can smoothly change the properties of the output captions and show higher lexical alignment than baselines. For instance, CaptionSmiths reduces the error in controlling caption length by 506% despite better lexical alignment. Code will be available on https://github.com/omronsinicx/captionsmiths.</p>
            <p id="subjects-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" onclick="foldPdfKimi('Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="panel paper" keywords="72b,regionfocus,screenspot,grounding,qwen2,gui,visual,test,scaling,pro">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding_ICCV_2025_paper.html" target="_blank" title="116/263"><span class="index notranslate">#116</span></a>
                <a id="title-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="title-link" href="/venue/Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" target="_blank">Visual Test-time Scaling for GUI Agent Grounding</a>
                <a id="pdf-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tiange Luo" target="_blank">Tiange Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lajanugen Logeswaran" target="_blank">Lajanugen Logeswaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Justin Johnson" target="_blank">Justin Johnson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Honglak Lee" target="_blank">Honglak Lee</a>
            </p>
            <p id="summary-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="summary">We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+% on Screenspot-pro and 24+% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS-72B and Qwen2.5-VL-72B, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code is publicly available at https://github.com/tiangeluo/RegionFocus.</p>
            <p id="subjects-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" onclick="foldPdfKimi('Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="panel paper" keywords="gecko,wsi,pretraining,mil,concept,gigapixel,modalities,contrastive,patch,wsis">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology_ICCV_2025_paper.html" target="_blank" title="117/263"><span class="index notranslate">#117</span></a>
                <a id="title-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="title-link" href="/venue/Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" target="_blank">GECKO: Gigapixel Vision-Concept Contrastive Pretraining in Histopathology</a>
                <a id="pdf-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Saarthak Kapse" target="_blank">Saarthak Kapse</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pushpak Pati" target="_blank">Pushpak Pati</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Srikar Yellapragada" target="_blank">Srikar Yellapragada</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Srijan Das" target="_blank">Srijan Das</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rajarsi R. Gupta" target="_blank">Rajarsi R. Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joel Saltz" target="_blank">Joel Saltz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dimitris Samaras" target="_blank">Dimitris Samaras</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Prasanna" target="_blank">Prateek Prasanna</a>
            </p>
            <p id="summary-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="summary">Pretraining a Multiple Instance Learning (MIL) aggregator enables the derivation of Whole Slide Image (WSI)-level embeddings from patch-level representations without supervision. While recent multimodal MIL pretraining approaches leveraging auxiliary modalities have demonstrated performance gains over unimodal WSI pretraining, the acquisition of these additional modalities necessitates extensive clinical profiling. This requirement increases costs and limits scalability in existing WSI datasets lacking such paired modalities. To address this, we propose Gigapixel Vision-Concept Knowledge Contrastive pretraining (GECKO), which aligns WSIs with a Concept Prior derived from the available WSIs. First, we derive an inherently interpretable concept prior by computing the similarity between each WSI patch and textual descriptions of predefined pathology concepts. GECKO then employs a dual-branch MIL network: one branch aggregates patch embeddings into a WSI-level deep embedding, while the other aggregates the concept prior to a corresponding WSI-level concept embedding. Both aggregated embeddings are aligned using a contrastive objective, thereby pretraining the entire dual-branch MIL model. Moreover, when auxiliary modalities such as transcriptomics data are available, GECKO seamlessly integrates them. Across five diverse tasks, GECKO consistently outperforms prior unimodal and multimodal pretraining approaches while also delivering clinically meaningful interpretability that bridges the gap between computational models and pathology expertise. Code is made available at github.com/bmi-imaginelab/GECKO</p>
            <p id="subjects-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" onclick="foldPdfKimi('Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="panel paper" keywords="medsam,segmentation,foundation,updates,adaptation,medical,parametric,lesions,catastrophic,forgetting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates_ICCV_2025_paper.html" target="_blank" title="118/263"><span class="index notranslate">#118</span></a>
                <a id="title-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="title-link" href="/venue/Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" target="_blank">Test-time Adaptation for Foundation Medical Segmentation Model Without Parametric Updates</a>
                <a id="pdf-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kecheng Chen" target="_blank">Kecheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Luo" target="_blank">Xinyu Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiexin Qin" target="_blank">Tiexin Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Liu" target="_blank">Jie Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Liu" target="_blank">Hui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Victor Ho Fun Lee" target="_blank">Victor Ho Fun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong Yan" target="_blank">Hong Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoliang Li" target="_blank">Haoliang Li</a>
            </p>
            <p id="summary-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="summary">Foundation medical segmentation models, with MedSAM being the most popular, have achieved promising performance across organs and lesions. However, MedSAM still suffers from compromised performance on specific lesions with intricate structures and appearance, as well as bounding box prompt-induced perturbations. Although current test-time adaptation (TTA) methods for medical image segmentation may tackle this issue, partial (e.g., batch normalization) or whole parametric updates restrict their effectiveness due to limited update signals or catastrophic forgetting in large models. Meanwhile, these approaches ignore the computational complexity during adaptation, which is particularly significant for modern foundation models. To this end, our theoretical analyses reveal that directly refining image embeddings is feasible to approach the same goal as parametric updates under the MedSAM architecture, which enables us to realize high computational efficiency and segmentation performance without the risk of catastrophic forgetting. Under this framework, we propose to encourage maximizing factorized conditional probabilities of the posterior prediction probability using a proposed distribution-approximated latent conditional random field loss combined with an entropy minimization loss. Experiments show that we achieve about 3% Dice score improvements across three datasets while reducing computational complexity by over 7 times.</p>
            <p id="subjects-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="panel paper" keywords="segmentation,outliers,feature,outlier,vocabulary,attention,semantic,sfp,purification,propagated">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic_ICCV_2025_paper.html" target="_blank" title="119/263"><span class="index notranslate">#119</span></a>
                <a id="title-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="title-link" href="/venue/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" target="_blank">Feature Purification Matters: Suppressing Outlier Propagation for Training-Free Open-Vocabulary Semantic Segmentation</a>
                <a id="pdf-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuo Jin" target="_blank">Shuo Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyue Yu" target="_blank">Siyue Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingfeng Zhang" target="_blank">Bingfeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingjie Sun" target="_blank">Mingjie Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Dong" target="_blank">Yi Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jimin Xiao" target="_blank">Jimin Xiao</a>
            </p>
            <p id="summary-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="summary">Training-free open-vocabulary semantic segmentation has advanced with vision-language models like CLIP, which exhibit strong zero-shot abilities. However, CLIP's attention mechanism often wrongly emphasises specific image tokens, namely outliers, which results in irrelevant over-activation. Existing approaches struggle with these outliers that arise in intermediate layers and propagate through the model, ultimately degrading spatial perception. In this paper, we propose a Self-adaptive Feature Purifier framework (SFP) to suppress propagated outliers and enhance semantic representations for open-vocabulary semantic segmentation. Specifically, based on an in-depth analysis of attention responses between image and class tokens, we design a self-adaptive outlier mitigator to detect and mitigate outliers at each layer for propagated feature purification. In addition, we introduce a semantic-aware attention enhancer to augment attention intensity in semantically relevant regions, which strengthens the purified feature to focus on objects. Further, we introduce a hierarchical attention integrator to aggregate multi-layer attention maps to refine spatially coherent feature representations for final segmentation. Our proposed SFP enables robust outlier suppression and object-centric feature representation, leading to a more precise segmentation. Extensive experiments show that our method achieves state-of-the-art performance and surpasses existing methods by an average of 4.6% mIoU on eight segmentation benchmarks. The code will be released.</p>
            <p id="subjects-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" onclick="foldPdfKimi('Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="panel paper" keywords="person,diffusion,diffps,search,suboptimal,sfan,uncropped,prw,prior,knowledge">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search_ICCV_2025_paper.html" target="_blank" title="120/263"><span class="index notranslate">#120</span></a>
                <a id="title-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="title-link" href="/venue/Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" target="_blank">Leveraging Prior Knowledge of Diffusion Model for Person Search</a>
                <a id="pdf-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Giyeol Kim" target="_blank">Giyeol Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sooyoung Yang" target="_blank">Sooyoung Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jihyong Oh" target="_blank">Jihyong Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Myungjoo Kang" target="_blank">Myungjoo Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chanho Eom" target="_blank">Chanho Eom</a>
            </p>
            <p id="summary-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="summary">Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.</p>
            <p id="subjects-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" onclick="foldPdfKimi('Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="panel paper" keywords="geo4d,video,reconstruction,leveraging,modalities,geometric,repurpose,generators,dynamic,trained">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction_ICCV_2025_paper.html" target="_blank" title="121/263"><span class="index notranslate">#121</span></a>
                <a id="title-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="title-link" href="/venue/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" target="_blank">Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction</a>
                <a id="pdf-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeren Jiang" target="_blank">Zeren Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuanxia Zheng" target="_blank">Chuanxia Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Iro Laina" target="_blank">Iro Laina</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Diane Larlus" target="_blank">Diane Larlus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>
            </p>
            <p id="summary-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="summary">We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic priors captured by large-scale pre-trained video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, disparity, and ray maps. We propose a new multi-modal alignment algorithm to align and fuse these modalities, as well as a sliding window approach at inference time, thus enabling robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods.</p>
            <p id="subjects-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" onclick="foldPdfKimi('Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="panel paper" keywords="rppg,chroma,gaussian,rhythmguassian,physiological,ggm,motion,generalizable,remote,signals">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement_ICCV_2025_paper.html" target="_blank" title="122/263"><span class="index notranslate">#122</span></a>
                <a id="title-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="title-link" href="/venue/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" target="_blank">RhythmGuassian: Repurposing Generalizable Gaussian Model For Remote Physiological Measurement</a>
                <a id="pdf-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Lu" target="_blank">Hao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuting Zhang" target="_blank">Yuting Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Tang" target="_blank">Jiaqi Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Fu" target="_blank">Bowen Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhang Ge" target="_blank">Wenhang Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Wei" target="_blank">Wei Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaishun Wu" target="_blank">Kaishun Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingcong Chen" target="_blank">Yingcong Chen</a>
            </p>
            <p id="summary-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="summary">Remote Photoplethysmography (rPPG) enables non-contact extraction of physiological signals, providing significant advantages in medical monitoring, emotion recognition, and face anti-spoofing. However, the extraction of reliable rPPG signals is hindered by motion variations in real-world environments, leading to entanglement issue. To address the challenge, we employ the Generalizable Gaussian Model (GGM) to disentangle geometry and chroma components with 4D Gaussian representations. Employing the GGM for robust rPPG estimation is non-trivial. Firstly, there are no camera parameters in the dataset, resulting in the inability to render video from 4D Gaussian. The "4D virtual camera" is proposed to construct extra Gaussian parameters to describe view and motion changes, giving the ability to render video with the fixed virtual camera parameters. Further, the chroma component is still not explicitly decoupled in 4D Gaussian representation. Explicit motion modeling (EMM) is designed to decouple the motion variation in an unsupervised manner. Explicit chroma modeling (ECM) is tailored to decouple specular, physiological, and noise signals, respectively. To validate our approach, we expand existing rPPG datasets to include various motion and illumination interference scenarios, demonstrating the effectiveness of our method in real-world settings. The code is available at https://github.com/LuPaoPao/RhythmGuassian.</p>
            <p id="subjects-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" onclick="foldPdfKimi('Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="panel paper" keywords="cameras,matrices,fundamental,viewing,recovery,represent,noisy,uncalibrated,graph,available">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices_ICCV_2025_paper.html" target="_blank" title="123/263"><span class="index notranslate">#123</span></a>
                <a id="title-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="title-link" href="/venue/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" target="_blank">On the Recovery of Cameras from Fundamental Matrices</a>
                <a id="pdf-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rakshith Madhavan" target="_blank">Rakshith Madhavan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federica Arrigoni" target="_blank">Federica Arrigoni</a>
            </p>
            <p id="summary-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="summary">The viewing graph is a compact tool to encode the geometry of multiple views: nodes represent uncalibrated cameras and edges represent fundamental matrices (when available). Most research focuses on theoretical analyses, exploring for which viewing graphs it is possible (in principle) to retrieve cameras from fundamental matrices, in the sense that the problem admits a unique solution for noiseless data. However, the practical task of recovering cameras from noisy fundamental matrices is still open, as available methods are limited to special graphs (such as those covered by triplets). In this paper, we develop the first method that can deal with the recovery of cameras from noisy fundamental matrices in a general viewing graph. Experimental results demonstrate the promise of the proposed approach on a variety of synthetic and real scenarios.</p>
            <p id="subjects-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" onclick="foldPdfKimi('Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="panel paper" keywords="vlms,encoder,evev2,vision,free,language,baaivision,modalities,excavating,improved">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models_ICCV_2025_paper.html" target="_blank" title="124/263"><span class="index notranslate">#124</span></a>
                <a id="title-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="title-link" href="/venue/Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" target="_blank">EVEv2: Improved Baselines for Encoder-Free Vision-Language Models</a>
                <a id="pdf-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haiwen Diao" target="_blank">Haiwen Diao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaotong Li" target="_blank">Xiaotong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufeng Cui" target="_blank">Yufeng Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueze Wang" target="_blank">Yueze Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoge Deng" target="_blank">Haoge Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ting Pan" target="_blank">Ting Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxuan Wang" target="_blank">Wenxuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huchuan Lu" target="_blank">Huchuan Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlong Wang" target="_blank">Xinlong Wang</a>
            </p>
            <p id="summary-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="summary">Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.</p>
            <p id="subjects-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" onclick="foldPdfKimi('Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="panel paper" keywords="fscd,exemplars,template,rpine,shot,pattern,matching,regression,object,dubbed">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression_ICCV_2025_paper.html" target="_blank" title="125/263"><span class="index notranslate">#125</span></a>
                <a id="title-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="title-link" href="/venue/Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" target="_blank">Few-Shot Pattern Detection via Template Matching and Regression</a>
                <a id="pdf-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Eunchan Jo" target="_blank">Eunchan Jo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dahyun Kang" target="_blank">Dahyun Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanghyun Kim" target="_blank">Sanghyun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunseon Choi" target="_blank">Yunseon Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minsu Cho" target="_blank">Minsu Cho</a>
            </p>
            <p id="summary-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="summary">We address the problem of few-shot pattern detection, which aims to detect all instances of a given pattern, typically represented by a few exemplars, from an input image. Although similar problems have been studied in few-shot object counting and detection (FSCD), previous methods and their benchmarks have narrowed patterns of interest to object categories and often fail to localize non-object patterns. In this work, we propose a simple yet effective detector based on template matching and regression, dubbed TMR. While previous FSCD methods typically represent target exemplars as spatially collapsed prototypes and lose structural information, we revisit classic template matching and regression. It effectively preserves and leverages the spatial layout of exemplars through a minimalistic structure with a small number of learnable convolutional or projection layers on top of a frozen backbone. We also introduce a new dataset, dubbed RPINE, which covers a wider range of patterns than existing object-centric datasets. Our method outperforms the state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation.</p>
            <p id="subjects-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" onclick="foldPdfKimi('Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="panel paper" keywords="scanpath,scangaze,gaze,eye,searcher,volumes,volumetric,dataset,publicly,radiologist">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling_ICCV_2025_paper.html" target="_blank" title="126/263"><span class="index notranslate">#126</span></a>
                <a id="title-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="title-link" href="/venue/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" target="_blank">CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling</a>
                <a id="pdf-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Trong Thang Pham" target="_blank">Trong Thang Pham</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akash Awasthi" target="_blank">Akash Awasthi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saba Khan" target="_blank">Saba Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Esteban Duran Marti" target="_blank">Esteban Duran Marti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tien-Phat Nguyen" target="_blank">Tien-Phat Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Khoa Vo" target="_blank">Khoa Vo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minh Tran" target="_blank">Minh Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Son Nguyen" target="_blank">Son Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cuong Tran" target="_blank">Cuong Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuki Ikebe" target="_blank">Yuki Ikebe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anh Totti Nguyen" target="_blank">Anh Totti Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anh Nguyen" target="_blank">Anh Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhigang Deng" target="_blank">Zhigang Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carol C. Wu" target="_blank">Carol C. Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hien Nguyen" target="_blank">Hien Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ngan Le" target="_blank">Ngan Le</a>
            </p>
            <p id="summary-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="summary">Understanding radiologists' eye movement during Computed Tomography (CT) reading is crucial for developing effective interpretable computer-aided diagnosis systems. However, CT research in this area has been limited by the lack of publicly available eye-tracking datasets and the three-dimensional complexity of CT volumes. To address these challenges, we present the first publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to process CT volumes and generate radiologist-like 3D fixation sequences, overcoming the limitations of current scanpath predictors that only handle 2D inputs. Since deep learning models benefit from a pretraining step, we develop a pipeline that converts existing 2D gaze datasets into 3D gaze data to pretrain CT-Searcher. Through both qualitative and quantitative evaluations on CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a comprehensive assessment framework for 3D scanpath prediction in medical imaging.</p>
            <p id="subjects-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" onclick="foldPdfKimi('Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="panel paper" keywords="weaveseg,weaving,nuclei,adaptive,contrast,feature,iterative,segmentation,spectral,ambiguous">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation_ICCV_2025_paper.html" target="_blank" title="127/263"><span class="index notranslate">#127</span></a>
                <a id="title-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" target="_blank">WeaveSeg: Iterative Contrast-weaving and Spectral Feature-refining for Nuclei Instance Segmentation</a>
                <a id="pdf-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajia Li" target="_blank">Jiajia Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huisi Wu" target="_blank">Huisi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Qin" target="_blank">Jing Qin</a>
            </p>
            <p id="summary-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="summary">histopathology images is a fundamental task in computational pathology. It is also a very challenging task due to complex nuclei morphologies, ambiguous boundaries, and staining variations. Existing methods often struggle to precisely delineate overlapping nuclei and handle class imbalance. We introduce WeaveSeg, a novel deep learning model for nuclei instance segmentation that significantly improves segmentation performance via synergistic integration of adaptive spectral feature refinement and iterative contrast-weaving. WeaveSeg features an adaptive spectral detail refinement (SAR) module for multi-scale feature enhancement via adaptive frequency component fusion, and an iterative contrast-weaving (ICW) module that progressively refines features through integrating contrastive attention, decoupled semantic context, and adaptive gating. Furthermore, we introduce a specialized uncertainty loss to explicitly model ambiguous regions, and a novel local contrast-based self-adaptive adjustment mechanism to accommodate dynamic feature distributions. Extensive experiments on MoNuSeg and CoNSeP demonstrate WeaveSeg's SOTA performance over existing models. Code will be publicly available.</p>
            <p id="subjects-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="panel paper" keywords="saliency,dataset,bias,datasets,freeview,mit300,cat2000,gap,adapting,tuebingen">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kummerer_Modeling_Saliency_Dataset_Bias_ICCV_2025_paper.html" target="_blank" title="128/263"><span class="index notranslate">#128</span></a>
                <a id="title-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="title-link" href="/venue/Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" target="_blank">Modeling Saliency Dataset Bias</a>
                <a id="pdf-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kummerer_Modeling_Saliency_Dataset_Bias_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matthias Kmmerer" target="_blank">Matthias Kmmerer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harneet Singh Khanuja" target="_blank">Harneet Singh Khanuja</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthias Bethge" target="_blank">Matthias Bethge</a>
            </p>
            <p id="summary-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="summary">Recent advances in image-based saliency prediction are approaching gold standard performance levels on existing benchmarks. Despite this success, we show that predicting fixations across multiple saliency datasets remains challenging due to dataset bias. We find a significant performance drop (around 40%) when models trained on one dataset are applied to another. Surprisingly, increasing dataset diversity does not resolve this inter-dataset gap, with close to 60% attributed to dataset-specific biases. To address this remaining generalization gap, we propose a novel architecture extending a mostly dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters that govern interpretable mechanisms such as multi-scale structure, center bias, and fixation spread. Adapting only these parameters to new data accounts for more than 75% of the generalization gap, with a large fraction of the improvement achieved with as few as 50 samples. Our model sets a new state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark (MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from unrelated datasets, but with a substantial boost when adapting to the respective training datasets. The model also provides valuable insights into spatial saliency properties, revealing complex multi-scale effects that combine both absolute and relative sizes.</p>
            <p id="subjects-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" onclick="foldPdfKimi('Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="panel paper" keywords="pointmil,interpretable,biomedical,classification,cloud,drug,point,instance,interpretability,macc">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.html" target="_blank" title="129/263"><span class="index notranslate">#129</span></a>
                <a id="title-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="title-link" href="/venue/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" target="_blank">Interpretable point cloud classification using multiple instance learning</a>
                <a id="pdf-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF"></sup>]</a>
                <a id="rel-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matt De Vries" target="_blank">Matt De Vries</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Reed Naidoo" target="_blank">Reed Naidoo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Olga Fourkioti" target="_blank">Olga Fourkioti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lucas G. Dent" target="_blank">Lucas G. Dent</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Curry" target="_blank">Nathan Curry</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chris Dunsby" target="_blank">Chris Dunsby</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chris Bakal" target="_blank">Chris Bakal</a>
            </p>
            <p id="summary-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="summary">Understanding 3D cell shape is crucial in biomedical research, where morphology serves as a key indicator of disease, cellular state, and drug response. However, many existing 3D point cloud classification models lack interpretability, limiting their utility for extracting biologically meaningful insights. In this work, we unify standard point cloud backbones and feature aggregation strategies within a Multiple Instance Learning (MIL) framework to enable inherently interpretable classification. Our approach, PointMIL, improves classification performance while providing fine-grained point-level explanations without relying on post hoc analysis. We demonstrate state-of-the-art mACC (97.3%) and F1 (97.5%) in the IntrA biomedical dataset and evaluate the interpretability using quantitative and qualitative metrics. Additionally, we introduce ATLAS-1, a novel dataset of drug-treated 3D cancer cells, and use it to show how PointMIL captures fine-grained morphological effects of chemical treatments. Beyond biomedical applications, PointMIL generalises to standard benchmarks such as ModelNet40 and ScanObjectNN, offering interpretable 3D object recognition across domains</p>
            <p id="subjects-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" onclick="foldPdfKimi('De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="panel paper" keywords="blim,candidate,video,retrieval,cpn,text,query,mlvlab,likelihood,mllms">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video_ICCV_2025_paper.html" target="_blank" title="130/263"><span class="index notranslate">#130</span></a>
                <a id="title-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="title-link" href="/venue/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" target="_blank">Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval</a>
                <a id="pdf-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dohwan Ko" target="_blank">Dohwan Ko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ji Soo Lee" target="_blank">Ji Soo Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minhyuk Choi" target="_blank">Minhyuk Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihang Meng" target="_blank">Zihang Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunwoo J. Kim" target="_blank">Hyunwoo J. Kim</a>
            </p>
            <p id="summary-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="summary">Text-Video Retrieval aims to find the most relevant text (or video) candidate given a video (or text) query from large-scale online databases. Recent work leverages multi-modal large language models (MLLMs) to improve retrieval, especially for long or complex query-candidate pairs. However, we observe that the naive application of MLLMs, i.e., retrieval based on candidate likelihood, introduces candidate prior bias, favoring candidates with inherently higher priors over those more relevant to the query. To this end, we propose a novel retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM), which leverages both query and candidate likelihoods by training the model to generate text from a given video as well as video features from a given text. Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet effective training-free score calibration module designed to mitigate candidate prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks, our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average, effectively alleviating candidate prior bias and emphasizing query-candidate relevance. Our in-depth analysis across various multi-modal tasks beyond retrieval highlights the broad applicability of CPN which enhances visual understanding by reducing reliance on textual priors. Code is available at \href https://github.com/mlvlab/BLiM https://github.com/mlvlab/BLiM .</p>
            <p id="subjects-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" onclick="foldPdfKimi('Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="panel paper" keywords="voldemort108x,segmentation,energy,pttea,test,seg,progressive,adaptation,medical,distribution">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation_ICCV_2025_paper.html" target="_blank" title="131/263"><span class="index notranslate">#131</span></a>
                <a id="title-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" target="_blank">Progressive Test Time Energy Adaptation for Medical Image Segmentation</a>
                <a id="pdf-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoran Zhang" target="_blank">Xiaoran Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Byung-Woo Hong" target="_blank">Byung-Woo Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyoungseob Park" target="_blank">Hyoungseob Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel H. Pak" target="_blank">Daniel H. Pak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anne-Marie Rickmann" target="_blank">Anne-Marie Rickmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lawrence H. Staib" target="_blank">Lawrence H. Staib</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James S. Duncan" target="_blank">James S. Duncan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Wong" target="_blank">Alex Wong</a>
            </p>
            <p id="summary-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="summary">We propose a model-agnostic, progressive test-time energy adaptation approach for medical image segmentation. Maintaining model performance across diverse medical datasets is challenging, as distribution shifts arise from inconsistent imaging protocols and patient variations. Unlike domain adaptation methods that require multiple passes through target data--impractical in clinical settings--our approach adapts pretrained models progressively as they process test data. Our method leverages a shape energy model trained on source data, which assigns an energy score at the patch level to segmentation maps: low energy represents in-distribution (accurate) shapes, while high energy signals out-of-distribution (erroneous) predictions. By minimizing this energy score at test time, we refine the segmentation model to align with the target distribution. To validate effectiveness and adaptability, we evaluated our framework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets spanning cardiac, spinal cord, and lung segmentation. We consistently outperform baselines both quantitatively and qualitatively. Project page is available at: \href https://voldemort108x.github.io/pttea_seg/ https://voldemort108x.github.io/pttea_seg/</p>
            <p id="subjects-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="panel paper" keywords="winograd,wins,pruning,convolution,structured,gpus,inference,25x,alongside,inefficient">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution_ICCV_2025_paper.html" target="_blank" title="132/263"><span class="index notranslate">#132</span></a>
                <a id="title-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="title-link" href="/venue/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" target="_blank">WINS: Winograd Structured Pruning for Fast Winograd Convolution</a>
                <a id="pdf-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cheonjun Park" target="_blank">Cheonjun Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyun Jae Oh" target="_blank">Hyun Jae Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mincheol Park" target="_blank">Mincheol Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunchan Moon" target="_blank">Hyunchan Moon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minsik Kim" target="_blank">Minsik Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suhyun Kim" target="_blank">Suhyun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Myung Kuk Yoon" target="_blank">Myung Kuk Yoon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Won Woo Ro" target="_blank">Won Woo Ro</a>
            </p>
            <p id="summary-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="summary">Recent GPUs leverage Winograd convolution and structured pruning to significantly accelerate inference. First, Winograd convolution is theoretically 2.25x faster than standard convolution. Second, structured pruning reduces inference time without additional overhead as the pruning ratio increases. However, applying conventional structured pruning alongside Winograd convolution is inefficient. Existing structured pruning methods, which do not account for how GPUs process Winograd convolution, require large pruning unit sizes, leading to significant information loss. In this paper, we propose Winograd Structured Pruning (WINS), the first approach to employ optimized structured pruning for Winograd convolution. WINS is designed based on an in-depth analysis of Winograd convolution's computational characteristics on GPUs. Additionally, we introduce two variants, WINS-B and WINS-AB, which further enhance performance. Experimental results show that WINS-AB achieves up to 2.8x practical speedup in baseline inference on GPUs while preserving the accuracy of ResNet-18 on ImageNet.</p>
            <p id="subjects-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" onclick="foldPdfKimi('Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="panel paper" keywords="cell,novo,lines,profiling,microscopy,biological,specific,perturbation,rxrx,rxrx19a">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De_ICCV_2025_paper.html" target="_blank" title="133/263"><span class="index notranslate">#133</span></a>
                <a id="title-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="title-link" href="/venue/Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" target="_blank">Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines</a>
                <a id="pdf-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayuan Chen" target="_blank">Jiayuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thai-Hoang Pham" target="_blank">Thai-Hoang Pham</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanlong Wang" target="_blank">Yuanlong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Zhang" target="_blank">Ping Zhang</a>
            </p>
            <p id="summary-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="summary">High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for de novo cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to de novo cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for de novo cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications.</p>
            <p id="subjects-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="panel paper" keywords="lvbench,comprehension,video,long,understanding,multimodal,videos,benchmark,commentary,underperform">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark_ICCV_2025_paper.html" target="_blank" title="134/263"><span class="index notranslate">#134</span></a>
                <a id="title-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="title-link" href="/venue/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" target="_blank">LVBench: An Extreme Long Video Understanding Benchmark</a>
                <a id="pdf-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weihan Wang" target="_blank">Weihan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zehai He" target="_blank">Zehai He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenyi Hong" target="_blank">Wenyi Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yean Cheng" target="_blank">Yean Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohan Zhang" target="_blank">Xiaohan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ji Qi" target="_blank">Ji Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Ding" target="_blank">Ming Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaotao Gu" target="_blank">Xiaotao Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyu Huang" target="_blank">Shiyu Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Xu" target="_blank">Bin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxiao Dong" target="_blank">Yuxiao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Tang" target="_blank">Jie Tang</a>
            </p>
            <p id="summary-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="summary">Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension.</p>
            <p id="subjects-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" onclick="foldPdfKimi('Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="panel paper" keywords="mpnet,similarity,dmw,memory,medical,segmentation,prior,sim,grandmother,macaques">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image_ICCV_2025_paper.html" target="_blank" title="135/263"><span class="index notranslate">#135</span></a>
                <a id="title-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="title-link" href="/venue/Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" target="_blank">Similarity Memory Prior is All You Need for Medical Image Segmentation</a>
                <a id="pdf-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF">7</sup>]</a>
                <a id="copy-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tang" target="_blank">Hao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqing Guo" target="_blank">Zhiqing Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liejun Wang" target="_blank">Liejun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Liu" target="_blank">Chao Liu</a>
            </p>
            <p id="summary-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="summary">In recent years, it has been found that "grandmother cells" in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.</p>
            <p id="subjects-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" onclick="foldPdfKimi('Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="panel paper" keywords="architecture,distillation,ofa,suppression,cross,rsd,redundancy,redundant,simple,knowledge">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression_ICCV_2025_paper.html" target="_blank" title="136/263"><span class="index notranslate">#136</span></a>
                <a id="title-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" target="_blank">Cross-Architecture Distillation Made Simple with Redundancy Suppression</a>
                <a id="pdf-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weijia Zhang" target="_blank">Weijia Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuehao Liu" target="_blank">Yuehao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wu Ran" target="_blank">Wu Ran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Ma" target="_blank">Chao Ma</a>
            </p>
            <p id="summary-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="summary">We describe a simple method for cross-architecture knowledge distillation, where the knowledge transfer is cast into a redundant information suppression formulation. Existing methods introduce sophisticated modules, architecture-tailored designs, and excessive parameters, which impair their efficiency and applicability. We propose to extract the architecture-agnostic knowledge in heterogeneous representations by reducing the redundant architecture-exclusive information. To this end, we present a simple redundancy suppression distillation (RSD) loss, which comprises cross-architecture invariance maximisation and feature decorrelation objectives. To prevent the student from entirely losing its architecture-specific capabilities, we further design a lightweight module that decouples the RSD objective from the student's internal representations. Our method is devoid of the architecture-specific designs and complex operations in the pioneering method of OFA. It outperforms OFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their parameter overhead, which highlights its potential as a simple and strong baseline to the cross-architecture distillation community.</p>
            <p id="subjects-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="panel paper" keywords="p2f,segmentation,panoptic,oodis,prior2former,evidential,mask,ood,classes,transformers">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.html" target="_blank" title="137/263"><span class="index notranslate">#137</span></a>
                <a id="title-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="title-link" href="/venue/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" target="_blank">Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation</a>
                <a id="pdf-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Schmidt" target="_blank">Sebastian Schmidt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julius Koerner" target="_blank">Julius Koerner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dominik Fuchsgruber" target="_blank">Dominik Fuchsgruber</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Gasperini" target="_blank">Stefano Gasperini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Tombari" target="_blank">Federico Tombari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Gnnemann" target="_blank">Stephan Gnnemann</a>
            </p>
            <p id="summary-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="summary">In panoptic segmentation, individual instances must be separated within semantic classes. As state-of-the-art methods rely on a pre-defined set of classes, they struggle with novel categories and out-of-distribution (OOD) data. This is particularly problematic in safety-critical applications, such as autonomous driving, where reliability in unseen scenarios is essential. We address the gap between outstanding benchmark performance and reliability by proposing Prior2Former (P2F), the first approach for segmentation vision transformers rooted in evidential learning. P2F extends the mask vision transformer architecture by incorporating a Beta prior for computing model uncertainty in pixel-wise binary mask assignments. This design enables high-quality uncertainty estimation that effectively detects novel and OOD objects, enabling state-of-the-art anomaly instance segmentation and open-world panoptic segmentation. Unlike most segmentation models addressing unknown classes, P2F operates without access to OOD data samples or contrastive training on void (i.e., unlabeled) classes, making it highly applicable in real-world scenarios where such prior information is unavailable. Additionally, P2F can be flexibly applied to anomaly instance and panoptic segmentation. Through comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan, and OoDIS datasets, P2F demonstrates state-of-the-art performance. Especially in OoDIS, P2F ranks first in its category.</p>
            <p id="subjects-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" onclick="foldPdfKimi('Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="panel paper" keywords="sam2,omnisam,fov,circ,panoramic,segmentation,anything,pinhole,semantic,uparrow">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.html" target="_blank" title="138/263"><span class="index notranslate">#138</span></a>
                <a id="title-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="title-link" href="/venue/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" target="_blank">OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic Semantic Segmentation</a>
                <a id="pdf-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ding Zhong" target="_blank">Ding Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Zheng" target="_blank">Xu Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenfei Liao" target="_blank">Chenfei Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanhuiyi Lyu" target="_blank">Yuanhuiyi Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jialei Chen" target="_blank">Jialei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengyang Wu" target="_blank">Shengyang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Zhang" target="_blank">Linfeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuming Hu" target="_blank">Xuming Hu</a>
            </p>
            <p id="summary-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="summary">Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to 360^\circ domain, the significant field-of-view (FoV) gap between pinhole (70^\circ x70^\circ) and panoramic images (180^\circ x360^\circ) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2's memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods by large margins, e.g., 79.06% (10.22%\uparrow) on SPin8-to-SPan8, 62.46% (6.58%\uparrow) on CS13-to-DP13.</p>
            <p id="subjects-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" onclick="foldPdfKimi('Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="panel paper" keywords="token,teva,vlm,patch,vision,details,dynamic,efficient,language,visual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal_ICCV_2025_paper.html" target="_blank" title="139/263"><span class="index notranslate">#139</span></a>
                <a id="title-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="title-link" href="/venue/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" target="_blank">Token-Efficient VLM: High-Resolution Image Understanding via Dynamic Region Proposal</a>
                <a id="pdf-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yitong Jiang" target="_blank">Yitong Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwei Gu" target="_blank">Jinwei Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianfan Xue" target="_blank">Tianfan Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ka Chun Cheung" target="_blank">Ka Chun Cheung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pavlo Molchanov" target="_blank">Pavlo Molchanov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongxu Yin" target="_blank">Hongxu Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sifei Liu" target="_blank">Sifei Liu</a>
            </p>
            <p id="summary-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="summary">Vision-Language Models (VLMs) excel at visual understanding by leveraging pretrained image encoders to generate visual tokens. However, they struggle with high-resolution images and zoomed-in regions due to the computational burden and token redundancy of uniform patch-based processing, often leading to the loss of critical details. To address these challenges, we propose Token-Efficient Vision Language Model (TEVA), a novel framework that detects key regions and applies dynamic patch sampling to efficiently capture fine-grained details while preserving global context. Our approach first identifies subject-oriented regions using an adaptive detection strategy. Then, a dynamic patch sampling mechanism selects and arranges patches at varying scales, ensuring efficient processing without increasing token count. Extensive experiments demonstrate that Token-Efficient Vision Language Model (TEVA) significantly enhances VLM performance in handling visual details, seamlessly integrating with various decoders and LLMs.</p>
            <p id="subjects-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" onclick="foldPdfKimi('Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="panel paper" keywords="grounding,queries,video,enriched,temporal,multimodal,grounded,llms,vtg,language">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs_ICCV_2025_paper.html" target="_blank" title="140/263"><span class="index notranslate">#140</span></a>
                <a id="title-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="title-link" href="/venue/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" target="_blank">Enrich and Detect: Video Temporal Grounding with Multimodal LLMs</a>
                <a id="pdf-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shraman Pramanick" target="_blank">Shraman Pramanick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Effrosyni Mavroudi" target="_blank">Effrosyni Mavroudi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yale Song" target="_blank">Yale Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rama Chellappa" target="_blank">Rama Chellappa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Torresani" target="_blank">Lorenzo Torresani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Triantafyllos Afouras" target="_blank">Triantafyllos Afouras</a>
            </p>
            <p id="summary-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="summary">We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.</p>
            <p id="subjects-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" onclick="foldPdfKimi('Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="panel paper" keywords="attention,self,convattn,emulating,module,transformers,esc,memory,representational,psnr">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution_ICCV_2025_paper.html" target="_blank" title="141/263"><span class="index notranslate">#141</span></a>
                <a id="title-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="title-link" href="/venue/Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" target="_blank">Emulating Self-attention with Convolution for Efficient Image Super-Resolution</a>
                <a id="pdf-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongheon Lee" target="_blank">Dongheon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seokju Yun" target="_blank">Seokju Yun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youngmin Ro" target="_blank">Youngmin Ro</a>
            </p>
            <p id="summary-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="summary">In this paper, we tackle the high computational overhead of Transformers for efficient image super-resolution (SR). Motivated by the observations of self-attention's inter-layer repetition, we introduce a convolutionized self-attention module named Convolutional Attention (ConvAttn) that emulates self-attention's long-range modeling capability and instance-dependent weighting with a single shared large kernel and dynamic kernels. By utilizing the ConvAttn module, we significantly reduce the reliance on self-attention and its involved memory-bound operations while maintaining the representational capability of Transformers. Furthermore, we overcome the challenge of integrating flash attention into the lightweight SR regime, effectively mitigating self-attention's inherent memory bottleneck. We scale up the window size to 32x32 with flash attention rather than proposing an intricate self-attention module, significantly improving PSNR by 0.31dB on Urban100x2 while reducing latency and memory usage by 16xand 12.2x. Building on these approaches, our proposed network, termed Emulating Self-attention with Convolution (ESC), notably improves PSNR by 0.27 dB on Urban100x4 compared to HiT-SRF, reducing the latency and memory usage by 3.7xand 6.2x, respectively. Extensive experiments demonstrate that our ESC maintains the ability for long-range modeling, data scalability, and the representational power of Transformers despite most self-attention being replaced by the ConvAttn module.</p>
            <p id="subjects-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" onclick="foldPdfKimi('Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="panel paper" keywords="rendering,light,image,view,inverse,generation,images,contents,novel,specialized">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images_ICCV_2025_paper.html" target="_blank" title="142/263"><span class="index notranslate">#142</span></a>
                <a id="title-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="title-link" href="/venue/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" target="_blank">Inverse Image-Based Rendering for Light Field Generation from Single Images</a>
                <a id="pdf-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunjun Jung" target="_blank">Hyunjun Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hae-Gon Jeon" target="_blank">Hae-Gon Jeon</a>
            </p>
            <p id="summary-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="summary">A concept of light-fields computed from multiple view images on regular grids has proven its benefit for scene representations, and supported realistic renderings of novel views and photographic effects such as refocusing and shallow depth of field. In spite of its effectiveness of light flow computations, obtaining light fields requires either computational costs or specialized devices like a bulky camera setup and a specialized microlens array. In an effort to broaden its benefit and applicability, in this paper, we propose a novel view synthesis method for light field generation from only single images, named inverse image-based rendering. Unlike previous attempts to implicitly rebuild 3D geometry or to explicitly represent objective scenes, our method reconstructs light flows in a space from image pixels, which behaves in the opposite way to image-based rendering. To accomplish this, we design a neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our neural renderer first stores the light flow of source rays from the input image, then computes the relationships among them through cross-attention, and finally predicts the color of the target ray based on these relationships. After the rendering pipeline generates the first novel view from a single input image, the generated out-of-view contents are updated to the set of source rays, and this procedure is iteratively performed while ensuring the consistent generation of occluded contents. We demonstrate that our inverse image-based rendering works well with various challenging datasets without any retraining or finetuning after once trained on synthetic dataset. In addition, our method outperforms relevant state-of-the-art novel view synthesis methods.</p>
            <p id="subjects-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" onclick="foldPdfKimi('Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="panel paper" keywords="evacuation,crowd,sdm,rescue,simulation,terrain,motor,likun,personalized,tju">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters_ICCV_2025_paper.html" target="_blank" title="143/263"><span class="index notranslate">#143</span></a>
                <a id="title-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="title-link" href="/venue/Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" target="_blank">RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters</a>
                <a id="pdf-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaolin Liu" target="_blank">Xiaolin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Zhou" target="_blank">Tianyi Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongbo Kang" target="_blank">Hongbo Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Ma" target="_blank">Jian Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwen Wang" target="_blank">Ziwen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Huang" target="_blank">Jing Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenguo Weng" target="_blank">Wenguo Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Kun Lai" target="_blank">Yu-Kun Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Li" target="_blank">Kun Li</a>
            </p>
            <p id="summary-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="summary">Crowd evacuation simulation is critical for enhancing public safety, and demanded for realistic virtual environments. Current mainstream evacuation models overlook the complex human behaviors that occur during evacuation, such as pedestrian collisions, interpersonal interactions, and variations in behavior influenced by terrain types or individual body shapes. This results in the failure to accurately simulate the escape of people in the real world. In this paper, aligned with the sensory-decision-motor (SDM) flow of the human brain, we propose a real-time 3D crowd evacuation simulation framework that integrates a 3D-adaptive SFM (Social Force Model) Decision Mechanism and a Personalized Gait Control Motor. This framework allows multiple agents to move in parallel and is suitable for various scenarios, with dynamic crowd awareness. Additionally, we introduce Part-level Force Visualization to assist in evacuation analysis. Experimental results demonstrate that our framework supports dynamic trajectory planning and personalized behavior for each agent throughout the evacuation process, and is compatible with uneven terrain. Visually, our method generates evacuation results that are more realistic and plausible, providing enhanced insights for crowd simulation. The code is available at http://cic.tju.edu.cn/faculty/likun/projects/RESCUE.</p>
            <p id="subjects-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" onclick="foldPdfKimi('Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="panel paper" keywords="prm,reconstruction,photometric,rendering,images,pbr,stereo,specular,lighting,cues">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model_ICCV_2025_paper.html" target="_blank" title="144/263"><span class="index notranslate">#144</span></a>
                <a id="title-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="title-link" href="/venue/Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" target="_blank">PRM: Photometric Stereo based Large Reconstruction Model</a>
                <a id="pdf-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhang Ge" target="_blank">Wenhang Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiantao Lin" target="_blank">Jiantao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guibao Shen" target="_blank">Guibao Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Feng" target="_blank">Jiawei Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Hu" target="_blank">Tao Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinli Xu" target="_blank">Xinli Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying-Cong Chen" target="_blank">Ying-Cong Chen</a>
            </p>
            <p id="summary-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="summary">We propose PRM, a novel photometric stereo based large reconstruction model to reconstruct high-quality meshes with fine-grained details. Previous large reconstruction models typically prepare training images under fixed and simple lighting, offering minimal photometric cues for precise reconstruction. Furthermore, images containing specular surfaces are treated as out-of-distribution samples, resulting in degraded reconstruction quality. To handle these challenges, PRM renders images by varying materials and lighting, which not only improves the local details by providing rich photometric cues but also increases the model's robustness to variations in the appearance of input images. To offer enhanced flexibility, we incorporate a real-time physically-based rendering (PBR) method and mesh rasterization for ground-truth rendering. By using an explicit mesh as 3D representation, PRM ensures the application of differentiable PBR for predicted rendering. This approach models specular color more accurately for images with varying materials and illumination than previous neural rendering methods and supports multiple supervisions for geometry optimization. Extensive experiments demonstrate that PRM significantly outperforms other models.</p>
            <p id="subjects-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" onclick="foldPdfKimi('Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="panel paper" keywords="slam,egocentric,city,inertial,visual,challenges,ethz,sensors,trajectories,lamaria">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale_ICCV_2025_paper.html" target="_blank" title="145/263"><span class="index notranslate">#145</span></a>
                <a id="title-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="title-link" href="/venue/Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" target="_blank">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a>
                <a id="pdf-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anusha Krishnan" target="_blank">Anusha Krishnan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohui Liu" target="_blank">Shaohui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul-Edouard Sarlin" target="_blank">Paul-Edouard Sarlin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oscar Gentilhomme" target="_blank">Oscar Gentilhomme</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Caruso" target="_blank">David Caruso</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maurizio Monge" target="_blank">Maurizio Monge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Newcombe" target="_blank">Richard Newcombe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Engel" target="_blank">Jakob Engel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>
            </p>
            <p id="summary-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="summary">Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration. While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses. In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data. We record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors. We leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale. This makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle. We show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this. In addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches. The dataset and benchmark are available at www.lamaria.ethz.ch.</p>
            <p id="subjects-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" onclick="foldPdfKimi('Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="panel paper" keywords="raylet,gaussians,rayletdf,surface,generalizable,reconstruction,clouds,distance,point,3dgs">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from_ICCV_2025_paper.html" target="_blank" title="146/263"><span class="index notranslate">#146</span></a>
                <a id="title-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="title-link" href="/venue/Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" target="_blank">RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians</a>
                <a id="pdf-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shenxing Wei" target="_blank">Shenxing Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinxi Li" target="_blank">Jinxi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yafei Yang" target="_blank">Yafei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Zhou" target="_blank">Siyuan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Yang" target="_blank">Bo Yang</a>
            </p>
            <p id="summary-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="summary">In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named **RayletDF**, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing.</p>
            <p id="subjects-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" onclick="foldPdfKimi('Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="panel paper" keywords="hineus,eikonal,surface,radiance,fidelity,reflective,constraints,reflection,rendering,urban">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity_ICCV_2025_paper.html" target="_blank" title="147/263"><span class="index notranslate">#147</span></a>
                <a id="title-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="title-link" href="/venue/Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" target="_blank">HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity</a>
                <a id="pdf-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yida Wang" target="_blank">Yida Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xueyang Zhang" target="_blank">Xueyang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhan" target="_blank">Kun Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Jia" target="_blank">Peng Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianpeng Lang" target="_blank">Xianpeng Lang</a>
            </p>
            <p id="summary-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="summary">Neural surface reconstruction faces persistent challenges in reconciling geometric fidelity with photometric consistency under complex scene conditions. We present HiNeuS, a unified framework that holistically addresses three core limitations in existing approaches: multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from over-enforced Eikonal constraints during joint optimization. To resolve these issues through a unified pipeline, we introduce: 1) Differential visibility verification through SDF-guided ray tracing, resolving reflection ambiguities via continuous occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry patches that enforce local surface coherence while preserving sharp edges through adaptive appearance weighting; and 3) Physically-grounded Eikonal relaxation that dynamically modulates geometric constraints based on local radiance gradients, enabling detail preservation without sacrificing global regularity. Unlike prior methods that handle these aspects through sequential optimizations or isolated modules, our approach achieves cohesive integration where appearance-geometry constraints evolve synergistically throughout training. Comprehensive evaluations across synthetic and real-world datasets demonstrate SotA performance, including a 21.4% reduction in Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement against neural rendering counterparts. Qualitative analyses reveal superior capability in recovering specular instruments, urban layouts with centimeter-scale infrastructure, and low-textured surfaces without local patch collapse. The method's generalizability is further validated through successful application to inverse rendering tasks, including material decomposition and view-consistent relighting. Project hosted at https://wangyida.github.io/posts/hineus, where the urban and vehicle reconstruction related modules are excluded from open-sourced codes due to legal concerns.</p>
            <p id="subjects-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" onclick="foldPdfKimi('Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="panel paper" keywords="degradation,metalens,photography,tunable,multipath,fidelity,svda,modeled,optical,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography_ICCV_2025_paper.html" target="_blank" title="148/263"><span class="index notranslate">#148</span></a>
                <a id="title-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" target="_blank">Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography</a>
                <a id="pdf-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianing Zhang" target="_blank">Jianing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Zhu" target="_blank">Jiayi Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feiyu Ji" target="_blank">Feiyu Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaokang Yang" target="_blank">Xiaokang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyun Yuan" target="_blank">Xiaoyun Yuan</a>
            </p>
            <p id="summary-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="summary">Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/.</p>
            <p id="subjects-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="panel paper" keywords="metascope,metalens,endoscopy,optics,micro,informed,optical,driven,chromatic,ultra">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy_ICCV_2025_paper.html" target="_blank" title="149/263"><span class="index notranslate">#149</span></a>
                <a id="title-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="title-link" href="/venue/Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" target="_blank">MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy</a>
                <a id="pdf-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wuyang Li" target="_blank">Wuyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Pan" target="_blank">Wentao Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyuan Liu" target="_blank">Xiaoyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhendong Luo" target="_blank">Zhendong Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxin Li" target="_blank">Chenxin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengyu Liu" target="_blank">Hengyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Din Ping Tsai" target="_blank">Din Ping Tsai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mu Ku Chen" target="_blank">Mu Ku Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixuan Yuan" target="_blank">Yixuan Yuan</a>
            </p>
            <p id="summary-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="summary">Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes.</p>
            <p id="subjects-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" onclick="foldPdfKimi('Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="panel paper" keywords="integration,normal,normals,generic,surface,central,discontinuities,camera,photometric,cameras">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models_ICCV_2025_paper.html" target="_blank" title="150/263"><span class="index notranslate">#150</span></a>
                <a id="title-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="title-link" href="/venue/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" target="_blank">Discontinuity-aware Normal Integration for Generic Central Camera Models</a>
                <a id="pdf-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Francesco Milano" target="_blank">Francesco Milano</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manuel Lpez-Antequera" target="_blank">Manuel Lpez-Antequera</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naina Dhingra" target="_blank">Naina Dhingra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roland Siegwart" target="_blank">Roland Siegwart</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Thiel" target="_blank">Robert Thiel</a>
            </p>
            <p id="summary-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="summary">Recovering a 3D surface from its surface normal map, a problem known as normal integration, is a key component for photometric shape reconstruction techniques such as shape-from-shading and photometric stereo. The vast majority of existing approaches for normal integration handle only implicitly the presence of depth discontinuities and are limited to orthographic or ideal pinhole cameras. In this paper, we propose a novel formulation that allows modeling discontinuities explicitly and handling generic central cameras. Our key idea is based on a local planarity assumption, that we model through constraints between surface normals and ray directions. Compared to existing methods, our approach more accurately approximates the relation between depth and surface normals, achieves state-of-the-art results on the standard normal integration benchmark, and is the first to directly handle generic central camera models.</p>
            <p id="subjects-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" onclick="foldPdfKimi('Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="panel paper" keywords="edm,matching,feature,efficiency,accuracy,efficient,deep,subpixel,chicleee,level">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_EDM_Efficient_Deep_Feature_Matching_ICCV_2025_paper.html" target="_blank" title="151/263"><span class="index notranslate">#151</span></a>
                <a id="title-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="title-link" href="/venue/Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" target="_blank">EDM: Efficient Deep Feature Matching</a>
                <a id="pdf-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_EDM_Efficient_Deep_Feature_Matching_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Li" target="_blank">Xi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Rao" target="_blank">Tong Rao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cihui Pan" target="_blank">Cihui Pan</a>
            </p>
            <p id="summary-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="summary">Recent feature matching methods have achieved remarkable performance but lack efficiency consideration. In this paper, we revisit the mainstream detector-free matching pipeline and improve all its stages considering both accuracy and efficiency. We propose an Efficient Deep feature Matching network, EDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level features. Then we present a Correlation Injection Module that conducts feature transformation on high-level deep features, and progressively injects feature correlations from global to local for efficient multi-scale feature aggregation, improving both speed and performance. In the refinement stage, a novel lightweight bidirectional axis-based regression head is designed to directly predict subpixel-level correspondences from latent features, avoiding the significant computational cost of explicitly locating keypoints on high-resolution local feature heatmaps. Moreover, effective selection strategies are introduced to enhance matching accuracy. Extensive experiments show that our EDM achieves competitive matching accuracy on various benchmarks and exhibits excellent efficiency, offering valuable best practices for real-world applications. The code is available at https://github.com/chicleee/EDM.</p>
            <p id="subjects-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" onclick="foldPdfKimi('Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="panel paper" keywords="pbr,material,materialmvp,illumination,textures,albedo,generation,lighting,multi,invariant">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion_ICCV_2025_paper.html" target="_blank" title="152/263"><span class="index notranslate">#152</span></a>
                <a id="title-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="title-link" href="/venue/He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" target="_blank">MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion</a>
                <a id="pdf-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF"></sup>]</a>
                <a id="rel-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zebin He" target="_blank">Zebin He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingxin Yang" target="_blank">Mingxin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhui Yang" target="_blank">Shuhui Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixuan Tang" target="_blank">Yixuan Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Wang" target="_blank">Tao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaihao Zhang" target="_blank">Kaihao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guanying Chen" target="_blank">Guanying Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhong Liu" target="_blank">Yuhong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Jiang" target="_blank">Jie Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunchao Guo" target="_blank">Chunchao Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhan Luo" target="_blank">Wenhan Luo</a>
            </p>
            <p id="summary-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="summary">Physically-based rendering (PBR) has become a cornerstone in modern computer graphics, enabling realistic material representation and lighting interactions in 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model for generating PBR textures from 3D meshes and image prompts, addressing key challenges in multi-view material synthesis. Our approach leverages Reference Attention to extract and encode informative latent from the input reference images, enabling intuitive and controllable texture generation. We also introduce a Consistency-Regularized Training strategy to enforce stability across varying viewpoints and illumination conditions, ensuring illumination-invariant and geometrically consistent results. Additionally, we propose Dual-Channel Material Generation, which separately optimizes albedo and metallic-roughness (MR) textures while maintaining precise spatial alignment with the input images through Multi-Channel Aligned Attention. Learnable material embeddings are further integrated to capture the distinct properties of albedo and MR. Experimental results demonstrate that our model generates PBR textures with realistic behavior across diverse lighting scenarios, outperforming existing methods in both consistency and quality for scalable 3D asset creation.</p>
            <p id="subjects-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" onclick="foldPdfKimi('He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="panel paper" keywords="polarization,polaranything,images,photorealistic,mitsuba,diffusion,synthesizing,polarimetric,image,pbr">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis_ICCV_2025_paper.html" target="_blank" title="153/263"><span class="index notranslate">#153</span></a>
                <a id="title-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="title-link" href="/venue/Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" target="_blank">PolarAnything: Diffusion-based Polarimetric Image Synthesis</a>
                <a id="pdf-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kailong Zhang" target="_blank">Kailong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youwei Lyu" target="_blank">Youwei Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heng Guo" target="_blank">Heng Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Si Li" target="_blank">Si Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanyu Ma" target="_blank">Zhanyu Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>
            </p>
            <p id="summary-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="summary">Polarization images facilitate image enhancement and 3D reconstruction tasks, but the limited accessibility of polarization cameras hinders their broader application. This gap drives the need for synthesizing photorealistic polarization images. The existing polarization simulator Mitsuba relies on a parametric polarization image formation model and requires extensive 3D assets covering shape and PBR materials, preventing it from generating large-scale photorealistic images. To address this problem, we propose PolarAnything, capable of synthesizing polarization images from a single RGB input with both photorealism and physical accuracy, eliminating the dependency on 3D asset collections. Drawing inspiration from the zero-shot performance of pretrained diffusion models, we introduce a diffusion-based generative framework with an effective representation strategy that preserves the fidelity of polarization properties. Experiments show that our model generates high-quality polarization images and supports downstream tasks like shape from polarization.</p>
            <p id="subjects-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="panel paper" keywords="hyperspectral,illumination,indirect,separation,direct,pattern,spatio,single,spectral,appearance">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a_ICCV_2025_paper.html" target="_blank" title="154/263"><span class="index notranslate">#154</span></a>
                <a id="title-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="title-link" href="/venue/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" target="_blank">Spatio-Spectral Pattern Illumination for Direct and Indirect Separation from a Single Hyperspectral Image</a>
                <a id="pdf-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shin Ishihara" target="_blank">Shin Ishihara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Imari Sato" target="_blank">Imari Sato</a>
            </p>
            <p id="summary-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="summary">Hyperspectral imaging has proven effective for appearance inspection because it can identify material compositions and reveal hidden features. Similarly, direct/indirect separation provides essential information about surface appearance and internal conditions, including layer structures and scattering behaviors. This paper presents a novel illumination system incorporating dispersive optics to unify both advantages for scene analysis. In general, achieving distinct direct/indirect separation requires multiple images with varying patterns. In a hyperspectral scenario, using a hyperspectral camera or tunable filters extends exposure and measurement times, hindering practical application. Our proposed system enables the illumination of a wavelength-dependent, spatially shifted pattern. With proper consideration of reflectance differences, we demonstrate that robust separation of direct and indirect components for each wavelength can be achieved using a single hyperspectral image acquired under our single spatio-spectral pattern illumination.</p>
            <p id="subjects-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" onclick="foldPdfKimi('Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="panel paper" keywords="metrics,preferences,structured,metric,expert,wireframe,human,uttered,lord,modelers">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction_ICCV_2025_paper.html" target="_blank" title="155/263"><span class="index notranslate">#155</span></a>
                <a id="title-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="title-link" href="/venue/Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" target="_blank">Explaining Human Preferences via Metrics for Structured 3D Reconstruction</a>
                <a id="pdf-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jack Langerman" target="_blank">Jack Langerman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Denys Rozumnyi" target="_blank">Denys Rozumnyi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhong Huang" target="_blank">Yuzhong Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dmytro Mishkin" target="_blank">Dmytro Mishkin</a>
            </p>
            <p id="summary-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="summary">"What cannot be measured cannot be improved" while likely never uttered by Lord Kelvin, summarizes effectively the driving force behind this work. This paper presents a detailed discussion of automated metrics for evaluating structured 3D reconstructions. Pitfalls of each metric are discussed, and an analysis through the lens of expert 3D modelers' preferences is presented. A set of systematic "unit tests" are proposed to empirically verify desirable properties, and context aware recommendations regarding which metric to use depending on application are provided. Finally, a learned metric distilled from human expert judgments is proposed and analyzed. The source code is available at https://github.com/s23dr/ wireframe-metrics-iccv2025.</p>
            <p id="subjects-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" onclick="foldPdfKimi('Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="panel paper" keywords="cooptrack,cooperative,perception,end,instance,association,v2x,seq,sequential,amota">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception_ICCV_2025_paper.html" target="_blank" title="156/263"><span class="index notranslate">#156</span></a>
                <a id="title-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="title-link" href="/venue/Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" target="_blank">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a>
                <a id="pdf-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaru Zhong" target="_blank">Jiaru Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Wang" target="_blank">Jiahao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahui Xu" target="_blank">Jiahui Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofan Li" target="_blank">Xiaofan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaiqing Nie" target="_blank">Zaiqing Nie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haibao Yu" target="_blank">Haibao Yu</a>
            </p>
            <p id="summary-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="summary">Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0% mAP and 32.8% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.</p>
            <p id="subjects-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" onclick="foldPdfKimi('Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="panel paper" keywords="deltamic,microscopy,shape,contour,inverse,mesh,inference,active,cell,differentiable">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active_ICCV_2025_paper.html" target="_blank" title="157/263"><span class="index notranslate">#157</span></a>
                <a id="title-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="title-link" href="/venue/Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" target="_blank">Inverse 3D Microscopy Rendering for Cell Shape Inference with Active Mesh</a>
                <a id="pdf-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sacha Ichbiah" target="_blank">Sacha Ichbiah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anshuman Sinha" target="_blank">Anshuman Sinha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabrice Delbary" target="_blank">Fabrice Delbary</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Herv Turlier" target="_blank">Herv Turlier</a>
            </p>
            <p id="summary-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="summary">Traditional methods for biological shape inference, such as deep learning (DL) and active contour models, face important limitations in 3D. DL approaches require large annotated datasets, which are often impractical to obtain, while active contour methods depend on carefully tuned heuristics for intensity attraction and shape regularization. We introduce deltaMic, a novel differentiable 3D renderer for fluorescence microscopy that formulates shape inference as an inverse problem. By leveraging differentiable convolutions, deltaMic simulates the image formation process, integrating a parameterized point spread function (PSF) with a triangle mesh-based representation of biological structures. Unlike DL- or contour-based segmentation, deltaMic directly optimizes both shape and optical parameters to align synthetic and real microscopy images, removing the need for large datasets or sample-specific fine-tuning. To ensure scalability, we implement a GPU-accelerated Fourier transform for triangle meshes along with narrow-band spectral filtering. We show that deltaMic accurately reconstructs cell geometries from both synthetic and diverse experimental 3D microscopy data, while remaining robust to noise and initialization. This establishes a new physics-informed framework for biophysical image analysis and inverse modeling.</p>
            <p id="subjects-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" onclick="foldPdfKimi('Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="panel paper" keywords="sgad,matching,82s,area,descriptor,containment,a2pm,51s,aware,accuracy">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching_ICCV_2025_paper.html" target="_blank" title="158/263"><span class="index notranslate">#158</span></a>
                <a id="title-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="title-link" href="/venue/Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" target="_blank">SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching</a>
                <a id="pdf-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangzeng Liu" target="_blank">Xiangzeng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chi Wang" target="_blank">Chi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guanglu Shi" target="_blank">Guanglu Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaodong Zhang" target="_blank">Xiaodong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiguang Miao" target="_blank">Qiguang Miao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miao Fan" target="_blank">Miao Fan</a>
            </p>
            <p id="summary-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="summary">Local feature matching remains a fundamental challenge in computer vision. Recent Area to Point Matching (A2PM) methods have improved matching accuracy. However, existing research based on this framework relies on inefficient pixel-level comparisons and complex graph matching that limit scalability. In this work, we introduce the Semantic and Geometric-aware Descriptor Network (SGAD), which fundamentally rethinks area-based matching by generating highly discriminative area descriptors that enable direct matching without complex graph optimization. This approach significantly improves both accuracy and efficiency of area matching. We further improve the performance of area matching through a novel supervision strategy that decomposes the area matching task into classification and ranking subtasks. Finally, we introduce the Hierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping areas by analyzing containment graphs. SGAD demonstrates remarkable performance gains, reducing runtime by 60x(0.82s vs. 60.23s) compared to MESA. Extensive evaluations show consistent improvements across multiple point matchers: SGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy (0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA delivers +7.39% AUC@5^\circ in indoor pose estimation, establishing a new state-of-the-art.</p>
            <p id="subjects-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" onclick="foldPdfKimi('Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="panel paper" keywords="rectification,orientations,local,orientation,constraint,affine,scales,facades,projective,linear">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation_ICCV_2025_paper.html" target="_blank" title="159/263"><span class="index notranslate">#159</span></a>
                <a id="title-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="title-link" href="/venue/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" target="_blank">Planar Affine Rectification from Local Change of Scale and Orientation</a>
                <a id="pdf-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuval Nissan" target="_blank">Yuval Nissan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Barath" target="_blank">Daniel Barath</a>
            </p>
            <p id="summary-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="summary">We propose a method for affine rectification of an image plane by leveraging changes in local scales and orientations under projective distortion. Specifically, we derive a novel linear constraint that directly relates pairs of points with orientations to the parameters of a projective transformation. This constraint is combined with an existing linear constraint on local scales, leading to highly robust rectification. The method reduces to solving a system of linear equations, enabling an efficient algebraic least-squares solution. It requires only two local scales and two local orientations, which can be extracted from, e.g., SIFT features. Unlike prior approaches, our method does not impose restrictions on individual features, does not require class segmentation, and makes no assumptions about feature interrelations. It is compatible with any feature detector that provides local scale or orientation. Furthermore, combining scaled and oriented points with line segments yields a highly robust algorithm that outperforms baselines. Extensive experiments show the effectiveness of our approach on real-world images, including repetitive patterns, building facades, and text-based content.</p>
            <p id="subjects-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" onclick="foldPdfKimi('Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="panel paper" keywords="polarimetric,polarization,thermal,lwir,stereo,view,translucent,detailed,ambiguities,reconstructs">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kushida_Thermal_Polarimetric_Multi-view_Stereo_ICCV_2025_paper.html" target="_blank" title="160/263"><span class="index notranslate">#160</span></a>
                <a id="title-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="title-link" href="/venue/Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" target="_blank">Thermal Polarimetric Multi-view Stereo</a>
                <a id="pdf-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kushida_Thermal_Polarimetric_Multi-view_Stereo_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Takahiro Kushida" target="_blank">Takahiro Kushida</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kenichiro Tanaka" target="_blank">Kenichiro Tanaka</a>
            </p>
            <p id="summary-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="summary">This paper introduces a novel method for detailed 3D shape reconstruction utilizing thermal polarization cues. Unlike state-of-the-art methods, the proposed approach is independent of illumination and material properties. In this paper, we formulate a general theory of polarization observation and show that long-wave infrared (LWIR) polarimetric imaging is free from the ambiguities that affect visible polarization analyses. Subsequently, we propose a method for recovering detailed 3D shapes using multi-view thermal polarimetric images. Experimental results demonstrate that our approach effectively reconstructs fine details in transparent, translucent, and heterogeneous objects, outperforming existing techniques.</p>
            <p id="subjects-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" onclick="foldPdfKimi('Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="panel paper" keywords="gaussians,aliasing,popping,rendering,artifact,3dgs,rasterization,artifacts,aaa,aliased">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering_ICCV_2025_paper.html" target="_blank" title="161/263"><span class="index notranslate">#161</span></a>
                <a id="title-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="title-link" href="/venue/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" target="_blank">AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering</a>
                <a id="pdf-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Steiner" target="_blank">Michael Steiner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Khler" target="_blank">Thomas Khler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lukas Radl" target="_blank">Lukas Radl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Windisch" target="_blank">Felix Windisch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dieter Schmalstieg" target="_blank">Dieter Schmalstieg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Markus Steinberger" target="_blank">Markus Steinberger</a>
            </p>
            <p id="summary-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="summary">Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction, it still faces challenges such as aliasing, projection artifacts, and view inconsistencies, primarily due to the simplification of treating splats as 2D entities. We argue that incorporating full 3D evaluation of Gaussians throughout the 3DGS pipeline can effectively address these issues while preserving rasterization efficiency. Specifically, we introduce an adaptive 3D smoothing filter to mitigate aliasing and present a stable view-space bounding method that eliminates popping artifacts when Gaussians extend beyond the view frustum. Furthermore, we promote tile-based culling to 3D with screen-space planes, accelerating rendering and reducing sorting costs for hierarchical rasterization. Our method achieves state-of-the-art quality on in-distribution evaluation sets and significantly outperforms other approaches for out-of-distribution views. Our qualitative evaluations further demonstrate the effective removal of aliasing, distortions, and popping artifacts, ensuring real-time, artifact-free rendering.</p>
            <p id="subjects-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" onclick="foldPdfKimi('Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="panel paper" keywords="monocular,fabrics,reconstruction,video,appearance,saft,differentiable,rendering,rgb,template">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable_ICCV_2025_paper.html" target="_blank" title="162/263"><span class="index notranslate">#162</span></a>
                <a id="title-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="title-link" href="/venue/Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" target="_blank">SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video</a>
                <a id="pdf-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=David Stotko" target="_blank">David Stotko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Reinhard Klein" target="_blank">Reinhard Klein</a>
            </p>
            <p id="summary-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="summary">The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.</p>
            <p id="subjects-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" onclick="foldPdfKimi('Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="panel paper" keywords="omnidepth,monocular,stereo,alignment,ambiguities,reflective,bridgedepth,surfaces,aeolusguan,reasoning">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment_ICCV_2025_paper.html" target="_blank" title="163/263"><span class="index notranslate">#163</span></a>
                <a id="title-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="title-link" href="/venue/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" target="_blank">BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment</a>
                <a id="pdf-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tongfan Guan" target="_blank">Tongfan Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxin Guo" target="_blank">Jiaxin Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Wang" target="_blank">Chen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun-Hui Liu" target="_blank">Yun-Hui Liu</a>
            </p>
            <p id="summary-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="summary">Monocular and stereo depth estimation offer complementary strengths: monocular methods capture rich contextual priors but lack geometric precision, while stereo approaches leverage epipolar geometry yet struggle with ambiguities such as reflective or textureless surfaces. Despite post-hoc synergies, these paradigms remain largely disjoint in practice. We introduce OmniDepth, a unified framework that bridges both through iterative bidirectional alignment of their latent representations. At its core, a novel cross-attentive alignment mechanism dynamically synchronizes monocular contextual cues with stereo hypothesis representations during stereo reasoning. This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by injecting monocular structure priors while refining monocular depth with stereo geometry within a single network.Extensive experiments demonstrate state-of-the-art results: OmniDepth reduces zero-shot generalization error by \!&gt;\!40% on Middlebury and ETH3D, while addressing longstanding failures on transparent and reflective surfaces. By harmonizing multi-view geometry with monocular context, OmniDepth enables robust 3D perception that transcends modality-specific limitations. Codes available at https://github.com/aeolusguan/OmniDepth.</p>
            <p id="subjects-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" onclick="foldPdfKimi('Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="panel paper" keywords="reconstructions,nvs,views,dense,flowr,quality,captures,renderings,sparse,view">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions_ICCV_2025_paper.html" target="_blank" title="164/263"><span class="index notranslate">#164</span></a>
                <a id="title-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="title-link" href="/venue/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" target="_blank">FlowR: Flowing from Sparse to Dense 3D Reconstructions</a>
                <a id="pdf-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tobias Fischer" target="_blank">Tobias Fischer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Rota Bul" target="_blank">Samuel Rota Bul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yung-Hsu Yang" target="_blank">Yung-Hsu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikhil Keetha" target="_blank">Nikhil Keetha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Porzi" target="_blank">Lorenzo Porzi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Norman Mller" target="_blank">Norman Mller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katja Schwarz" target="_blank">Katja Schwarz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathon Luiten" target="_blank">Jonathon Luiten</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Kontschieder" target="_blank">Peter Kontschieder</a>
            </p>
            <p id="summary-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="summary">3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of applications like Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These models typically rely on a noise-to-data generative process conditioned only on a handful of reference input views, leading to hallucinations, inconsistent generation results, and subsequent reconstruction artifacts. Instead, we propose a multi-view, flow matching model that learns a flow to directly connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with consistent, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks.</p>
            <p id="subjects-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" onclick="foldPdfKimi('Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="panel paper" keywords="quadrangulation,neuframeq,quad,meshes,frame,mesh,generalizable,scalable,quality,field">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation_ICCV_2025_paper.html" target="_blank" title="165/263"><span class="index notranslate">#165</span></a>
                <a id="title-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="title-link" href="/venue/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" target="_blank">NeuFrameQ: Neural Frame Fields for Scalable and Generalizable Anisotropic Quadrangulation</a>
                <a id="pdf-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ying-Tian Liu" target="_blank">Ying-Tian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Li" target="_blank">Jiajun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Tao Liu" target="_blank">Yu-Tao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Yu" target="_blank">Xin Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan-Chen Guo" target="_blank">Yuan-Chen Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan-Pei Cao" target="_blank">Yan-Pei Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ding Liang" target="_blank">Ding Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ariel Shamir" target="_blank">Ariel Shamir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song-Hai Zhang" target="_blank">Song-Hai Zhang</a>
            </p>
            <p id="summary-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="summary">Quad meshes play a crucial role in computer graphics applications, yet automatically generating high-quality quad meshes remains challenging. Traditional quadrangulation approaches rely on local geometric features and manual constraints, often producing suboptimal mesh layouts that fail to capture global shape semantics. We introduce NeuFrameQ, a novel learning-based framework for scalable and generalizable mesh quadrangulation via frame field prediction. We first create a large-scale dataset of high-quality quad meshes with various shapes to serve as priors of domain knowledge. Empowered by this dataset, we employ a connectivity-agnostic learning approach that operates on point clouds with normals, enabling robust processing of complex geometries. By decomposing frame field prediction into direction regression and magnitude estimation tasks, we effectively handle the ill-posed nature in frame field estimation. We also employ the polyvector representation and attention mechanism in both tasks to handle the inherent ambiguities in frame field representation. Extensive experiments demonstrate that NeuFrameQ produces high-quality quad meshes with superior semantic alignment, also for geometries derived from neural fields. Our method significantly advances the state of the art in automatic quad mesh generation, bridging the gap between neural content creation and production-ready geometric assets.</p>
            <p id="subjects-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" onclick="foldPdfKimi('Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="panel paper" keywords="leaf,neuraleaf,shapes,leaves,parametric,deformation,skinning,plant,modeling,deformations">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement_ICCV_2025_paper.html" target="_blank" title="166/263"><span class="index notranslate">#166</span></a>
                <a id="title-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="title-link" href="/venue/Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" target="_blank">NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement</a>
                <a id="pdf-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Yang" target="_blank">Yang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongni Mao" target="_blank">Dongni Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hiroaki Santo" target="_blank">Hiroaki Santo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yasuyuki Matsushita" target="_blank">Yasuyuki Matsushita</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fumio Okura" target="_blank">Fumio Okura</a>
            </p>
            <p id="summary-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="summary">We develop a neural parametric model for 3D plant leaves for modeling and reconstruction of plants that are essential for agriculture and computer graphics. While parametric modeling has been actively studied for human and animal shapes, plant leaves present unique challenges due to their diverse shapes and flexible deformation, making common approaches inapplicable. To this problem, we introduce a learning-based parametric model, NeuraLeaf, disentangling the leaves' geometry into their 2D base shapes and 3D deformations. Since the base shapes represent flattened 2D leaves, it allows learning from rich sources of 2D leaf image datasets, and also has the advantage of simultaneously learning texture aligned with the geometry. To model the 3D deformation, we propose a novel skeleton-free skinning model and a newly captured 3D leaf dataset called DeformLeaf. We establish a parametric deformation space by converting the sample-wise skinning parameters into a compact latent representation, allowing for flexible and efficient modeling of leaf deformations. We show that NeuraLeaf successfully generates a wide range of leaf shapes with deformation, resulting in accurate model fitting to 3D observations like depth maps and point clouds. Our implementation and datasets will be released upon acceptance.</p>
            <p id="subjects-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" onclick="foldPdfKimi('Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="panel paper" keywords="rendering,differentials,gradient,higher,order,hessians,rasterization,differentiable,optimizers,hessian">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering_ICCV_2025_paper.html" target="_blank" title="167/263"><span class="index notranslate">#167</span></a>
                <a id="title-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="title-link" href="/venue/Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" target="_blank">Stochastic Gradient Estimation for Higher-Order Differentiable Rendering</a>
                <a id="pdf-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zican Wang" target="_blank">Zican Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Fischer" target="_blank">Michael Fischer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tobias Ritschel" target="_blank">Tobias Ritschel</a>
            </p>
            <p id="summary-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="summary">We derive methods to compute higher order differentials (Hessians and Hessian-vector products) of the rendering operator. Our approach is based on importance sampling of a convolution that represents the differentials of rendering parameters and shows to be applicable to both rasterization and path tracing. We demonstrate that this information improves convergence when used in higher-order optimizers such as Newton or Conjugate Gradient relative to a gradient descent baseline in several inverse rendering tasks.</p>
            <p id="subjects-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="panel paper" keywords="housecrafter,floorplan,images,diffusion,locations,floorplans,house,scenes,scene,rgb">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models_ICCV_2025_paper.html" target="_blank" title="168/263"><span class="index notranslate">#168</span></a>
                <a id="title-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="title-link" href="/venue/Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" target="_blank">HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Models</a>
                <a id="pdf-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiwen Chen" target="_blank">Yiwen Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hieu T. Nguyen" target="_blank">Hieu T. Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vikram Voleti" target="_blank">Vikram Voleti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Varun Jampani" target="_blank">Varun Jampani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaizu Jiang" target="_blank">Huaizu Jiang</a>
            </p>
            <p id="summary-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="summary">We introduce HouseCrafter, a novel approach that can lift a 2D floorplan into a complete large 3D indoor scene (e.g., a house). Our key insight is to adapt a 2D diffusion model, which is trained on web-scale images, to generate consistent multi-view color (RGB) and depth (D) images across different locations of the scene. Specifically, the RGB-D images are generated autoregressively in batches along sampled locations derived from the floorplan. At each step, the diffusion model conditions on previously generated images to produce new images at nearby locations. The global floorplan and attention design in the diffusion model ensures the consistency of the generated images, from which a 3D scene can be reconstructed. Through extensive evaluation on the 3D-FRONT dataset, we demonstrate that HouseCrafter can generate high-quality house-scale 3D scenes. Ablation studies also validate the effectiveness of different design choices. We will release our code and model weights.</p>
            <p id="subjects-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" onclick="foldPdfKimi('Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="panel paper" keywords="planning,immp,domain,merged,motion,diverse,datasets,target,interactions,merging">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust_ICCV_2025_paper.html" target="_blank" title="169/263"><span class="index notranslate">#169</span></a>
                <a id="title-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="title-link" href="/venue/Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" target="_blank">Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning</a>
                <a id="pdf-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Giwon Lee" target="_blank">Giwon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wooseong Jeong" target="_blank">Wooseong Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daehee Park" target="_blank">Daehee Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaewoo Jeong" target="_blank">Jaewoo Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuk-Jin Yoon" target="_blank">Kuk-Jin Yoon</a>
            </p>
            <p id="summary-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="summary">Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.</p>
            <p id="subjects-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" onclick="foldPdfKimi('Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="panel paper" keywords="lidar,waveforms,dsp,automotive,fog,40x128x33,clouds,waveform,32cm,conventional">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words_ICCV_2025_paper.html" target="_blank" title="170/263"><span class="index notranslate">#170</span></a>
                <a id="title-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="title-link" href="/venue/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" target="_blank">Lidar Waveforms are Worth 40x128x33 Words</a>
                <a id="pdf-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dominik Scheuble" target="_blank">Dominik Scheuble</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanno Holzhter" target="_blank">Hanno Holzhter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Steven Peters" target="_blank">Steven Peters</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mario Bijelic" target="_blank">Mario Bijelic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Heide" target="_blank">Felix Heide</a>
            </p>
            <p id="summary-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="summary">Lidar has become crucial for autonomous driving, providing high-resolution 3D scans that are key for accurate scene understanding. To this end, lidar sensors measure the time-resolved full waveforms from the returning laser light, which a subsequent digital signal processor (DSP) converts to point clouds by identifying peaks in the waveform. Conventional automotive lidar DSPs process each waveform individually, ignoring potentially valuable context from neighboring waveforms. As a result, lidar point clouds are prone to artifacts from low signal-to-noise ratio (SNR) regions, highly reflective objects, and environmental conditions like fog. While leveraging neighboring waveforms is investigated extensively in transient imaging, applications remain limited to scientific or experimental hardware. In this work, we propose a learned DSP that directly processes full waveforms using a transformer architecture, leveraging features from adjacent waveforms to generate high-fidelity multi-echo point clouds. To assess our method, we capture data in real-world driving scenarios and a weather chamber with a conventional automotive lidar. Trained on synthetic and real data, the method improves Chamfer distance by 32cm and 20cm compared to conventional peak finding and existing transient imaging approaches, respectively. This translates to maximum range improvements of up to 17m in fog and 14m in nominal real-world conditions.</p>
            <p id="subjects-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" onclick="foldPdfKimi('Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="panel paper" keywords="lbm,image,bridge,relighting,latent,matching,translation,tasks,fast,object">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation_ICCV_2025_paper.html" target="_blank" title="171/263"><span class="index notranslate">#171</span></a>
                <a id="title-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="title-link" href="/venue/Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" target="_blank">LBM: Latent Bridge Matching for Fast Image-to-Image Translation</a>
                <a id="pdf-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Clment Chadebec" target="_blank">Clment Chadebec</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Onur Tasar" target="_blank">Onur Tasar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjeev Sreetharan" target="_blank">Sanjeev Sreetharan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Aubin" target="_blank">Benjamin Aubin</a>
            </p>
            <p id="summary-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="summary">In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation. We show that the method can reach state-of-the-art results for various image-to-image tasks using only a single inference step. In addition to its efficiency, we also demonstrate the versatility of the method across different image translation tasks such as object removal, normal and depth estimation, and object relighting. We also derive a conditional framework of LBM and demonstrate its effectiveness by tackling the tasks of controllable image relighting and shadow generation.</p>
            <p id="subjects-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" onclick="foldPdfKimi('Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="panel paper" keywords="distortions,optics,wavefront,telescope,super,fov,tradeoff,resolved,telescopes,mirror">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Swanson_Super_Resolved_Imaging_with_Adaptive_Optics_ICCV_2025_paper.html" target="_blank" title="172/263"><span class="index notranslate">#172</span></a>
                <a id="title-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="title-link" href="/venue/Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" target="_blank">Super Resolved Imaging with Adaptive Optics</a>
                <a id="pdf-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Swanson_Super_Resolved_Imaging_with_Adaptive_Optics_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Robin Swanson" target="_blank">Robin Swanson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Esther Y. H. Lin" target="_blank">Esther Y. H. Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Masen Lamb" target="_blank">Masen Lamb</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suresh Sivanandam" target="_blank">Suresh Sivanandam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kiriakos N. Kutulakos" target="_blank">Kiriakos N. Kutulakos</a>
            </p>
            <p id="summary-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="summary">Astronomical telescopes suffer from a tradeoff between field-of-view (FoV) and image resolution: increasing the FoV leads to an optical field that is under-sampled by the science camera. This work presents a novel computational imaging approach to overcome this tradeoff by leveraging the existing adaptive optics (AO) systems in modern ground-based telescopes. Our key idea is to use the AO system's deformable mirror to apply a series of learned, precisely controlled distortions to the optical wavefront, producing a sequence of images that exhibit distinct, high-frequency, sub-pixel shifts. These images can then be jointly upsampled to yield the final super-resolved image. Crucially, we show this can be done while simultaneously maintaining the core AO operation --- correcting for the unknown and rapidly changing wavefront distortions caused by Earth's atmosphere. To achieve this, we incorporate end-to-end optimization of both the induced mirror distortions and the upsampling algorithm, such that telescope-specific optics and temporal statistics of atmospheric wavefront distortions are accounted for. Our experimental results with a hardware prototype, as well as simulations, demonstrate significant SNR improvements of up to 12 dB over non-AO super-resolution baselines, using only existing telescope optics and no hardware modifications. Moreover, by using a precise bench-top replica of a complete telescope and AO system, we show that our methodology can be readily transferred to an operational telescope.</p>
            <p id="subjects-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" onclick="foldPdfKimi('Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="panel paper" keywords="m2sformer,forgery,difficulty,localization,multi,subtle,attention,forgeries,guidance,tampering">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for_ICCV_2025_paper.html" target="_blank" title="173/263"><span class="index notranslate">#173</span></a>
                <a id="title-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="title-link" href="/venue/Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" target="_blank">M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization</a>
                <a id="pdf-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ju-Hyeon Nam" target="_blank">Ju-Hyeon Nam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong-Hyun Moon" target="_blank">Dong-Hyun Moon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sang-Chul Lee" target="_blank">Sang-Chul Lee</a>
            </p>
            <p id="summary-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="summary">Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map--a curvature metric indicating the difficulty of forgery localization--which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains. Our M2SFormer code is available in Github Link.</p>
            <p id="subjects-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" onclick="foldPdfKimi('Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="panel paper" keywords="ego,objectrelator,exo,object,centric,psalm,view,cross,segmentation,mcfuse">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric_ICCV_2025_paper.html" target="_blank" title="174/263"><span class="index notranslate">#174</span></a>
                <a id="title-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="title-link" href="/venue/Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" target="_blank">ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives</a>
                <a id="pdf-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqian Fu" target="_blank">Yuqian Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runze Wang" target="_blank">Runze Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Ren" target="_blank">Bin Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guolei Sun" target="_blank">Guolei Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Biao Gong" target="_blank">Biao Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Fu" target="_blank">Yanwei Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danda Pani Paudel" target="_blank">Danda Pani Paudel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanjing Huang" target="_blank">Xuanjing Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luc Van Gool" target="_blank">Luc Van Gool</a>
            </p>
            <p id="summary-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="summary">Bridging the gap between ego-centric and exo-centric views has been a long-standing question in computer vision. In this paper, we focus on the emerging Ego-Exo object correspondence task, which aims to understand object relations across ego-exo perspectives through segmentation. While numerous segmentation models have been proposed, most operate on a single image (view), making them impractical for cross-view scenarios. PSALM, a recently proposed segmentation method, stands out as a notable exception with its demonstrated zero-shot ability on this task. However, due to the drastic viewpoint change between ego and exo, PSALM fails to accurately locate and segment objects, especially in complex backgrounds or when object appearances change significantly. To address these issues, we propose ObjectRelator, a novel approach featuring two key modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse introduces language as an additional cue, integrating both visual masks and textual descriptions to improve object localization and prevent incorrect associations. XObjAlign enforces cross-view consistency through self-supervised alignment, enhancing robustness to object appearance variations. Extensive experiments demonstrate ObjectRelator's effectiveness on the large-scale Ego-Exo4D benchmark and HANDAL-X (an adapted dataset for cross-view segmentation) with state-of-the-art performance. Code is available at: http://yuqianfu.com/ObjectRelator.</p>
            <p id="subjects-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" onclick="foldPdfKimi('Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="panel paper" keywords="uncertainty,m3od,duo,tta,shifts,semantic,dual,monocular,optimization,spatial">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under_ICCV_2025_paper.html" target="_blank" title="175/263"><span class="index notranslate">#175</span></a>
                <a id="title-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="title-link" href="/venue/Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" target="_blank">Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts</a>
                <a id="pdf-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Hu" target="_blank">Zixuan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongxiao Li" target="_blank">Dongxiao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinzhu Ma" target="_blank">Xinzhu Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shixiang Tang" target="_blank">Shixiang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaotong Li" target="_blank">Xiaotong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhan Yang" target="_blank">Wenhan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling-Yu Duan" target="_blank">Ling-Yu Duan</a>
            </p>
            <p id="summary-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="summary">Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel conjugate loss, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types. The source code is available at https://github.com/hzcar/DUO.</p>
            <p id="subjects-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" onclick="foldPdfKimi('Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="panel paper" keywords="sana,sprint,distillation,t2i,scm,ladd,geneval,step,h100,controlnet">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.html" target="_blank" title="176/263"><span class="index notranslate">#176</span></a>
                <a id="title-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="title-link" href="/venue/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" target="_blank">SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation</a>
                <a id="pdf-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junsong Chen" target="_blank">Junsong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuchen Xue" target="_blank">Shuchen Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuyang Zhao" target="_blank">Yuyang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jincheng Yu" target="_blank">Jincheng Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sayak Paul" target="_blank">Sayak Paul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyu Chen" target="_blank">Junyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Cai" target="_blank">Han Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Han" target="_blank">Song Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enze Xie" target="_blank">Enze Xie</a>
            </p>
            <p id="summary-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="summary">This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4.We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in just 1 step -- outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024x1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.</p>
            <p id="subjects-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" onclick="foldPdfKimi('Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="panel paper" keywords="mtl,task,gradient,conflicts,weight,generalization,reweighting,empowering,harmonizes,perturbation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective_ICCV_2025_paper.html" target="_blank" title="177/263"><span class="index notranslate">#177</span></a>
                <a id="title-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="title-link" href="/venue/Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" target="_blank">Beyond Losses Reweighting: Empowering Multi-Task Learning via the Generalization Perspective</a>
                <a id="pdf-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hoang Phan" target="_blank">Hoang Phan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lam Tran" target="_blank">Lam Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quyen Tran" target="_blank">Quyen Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ngoc Tran" target="_blank">Ngoc Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuan Truong" target="_blank">Tuan Truong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Lei" target="_blank">Qi Lei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nhat Ho" target="_blank">Nhat Ho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dinh Phung" target="_blank">Dinh Phung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Trung Le" target="_blank">Trung Le</a>
            </p>
            <p id="summary-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="summary">Multi-task learning (MTL) trains deep neural networks to optimize several objectives simultaneously using a shared backbone, which leads to reduced computational costs, improved data efficiency, and enhanced performance through cross-task knowledge sharing. Although recent gradient manipulation techniques seek a common descent direction to benefit all tasks, conventional empirical loss minimization still leaves models prone to overfitting and gradient conflicts. To address this, we introduce a novel MTL framework that leverages weight perturbation to regulate gradient norms. thus improve generalization. By carefully modulating weight perturbations, our approach harmonizes task-specific gradients, reducing conflicts and encouraging more robust learning across tasks. Theoretical insights reveal that controlling the gradient norm through weight perturbation directly contributes to better generalization. Extensive experiments across diverse applications demonstrate that our method significantly outperforms existing gradient-based MTL techniques in terms of task performance and overall model robustness.</p>
            <p id="subjects-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" onclick="foldPdfKimi('Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="panel paper" keywords="coda,selection,consensus,model,active,candidate,justinkay,best,driven,data">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kay_Consensus-Driven_Active_Model_Selection_ICCV_2025_paper.html" target="_blank" title="178/263"><span class="index notranslate">#178</span></a>
                <a id="title-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="title-link" href="/venue/Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" target="_blank">Consensus-Driven Active Model Selection</a>
                <a id="pdf-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kay_Consensus-Driven_Active_Model_Selection_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Justin Kay" target="_blank">Justin Kay</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Grant Van Horn" target="_blank">Grant Van Horn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Subhransu Maji" target="_blank">Subhransu Maji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Sheldon" target="_blank">Daniel Sheldon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sara Beery" target="_blank">Sara Beery</a>
            </p>
            <p id="summary-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="summary">The widespread availability of off-the-shelf machine learning models poses a challenge: which model, of the many available candidates, should be chosen for a given data analysis task? This question of model selection is traditionally answered by collecting and annotating a validation dataset---a costly and time-intensive process. We propose a method for active model selection, using predictions from candidate models to prioritize the labeling of test data points that efficiently differentiate the best candidate. Our method, CODA, performs consensus-driven active model selection by modeling relationships between classifiers, categories, and data points within a probabilistic framework. The framework uses the consensus and disagreement between models in the candidate pool to guide the label acquisition process, and Bayesian inference to update beliefs about which model is best as more information is collected. We validate our approach by curating a collection of 26 benchmark tasks capturing a range of model selection scenarios.CODA outperforms existing methods for active model selection significantly, reducing the annotation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art. We will make our code and data public. Code and data are available at https://github.com/justinkay/coda.</p>
            <p id="subjects-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" onclick="foldPdfKimi('Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="panel paper" keywords="causalnet,frame,key,mer,indexes,accurate,micro,recognition,expression,errors">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against_ICCV_2025_paper.html" target="_blank" title="179/263"><span class="index notranslate">#179</span></a>
                <a id="title-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" target="_blank">Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors</a>
                <a id="pdf-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheyuan Zhang" target="_blank">Zheyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weihao Tang" target="_blank">Weihao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong Chen" target="_blank">Hong Chen</a>
            </p>
            <p id="summary-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="summary">Micro-expression recognition (MER) is a highly challenging task in affective computing. With the reduced-sized micro-expression (ME) input that contains key information based on key-frame indexes, key-frame-based methods have significantly improved the performance of MER. However, most of these methods focus on improving the performance with relatively accurate key-frame indexes, while ignoring the difficulty of obtaining accurate key-frame indexes and the objective existence of key-frame index errors, which impedes them from moving towards practical applications. In this paper, we propose CausalNet, a novel framework to achieve robust MER facing key-frame index errors while maintaining accurate recognition. To enhance robustness, CausalNet takes the representation of the entire ME sequence as the input. To address the information redundancy brought by the complete ME range input and maintain accurate recognition, first, the Causal Motion Position Learning Module (CMPLM) is proposed to help the model locate the muscle movement areas related to Action Units (AUs), thereby reducing the attention to other redundant areas. Second, the Causal Attention Block (CAB) is proposed to deeply learn the causal relationships between the muscle contraction and relaxation movements in MEs. Empirical experiments have demonstrated that on popular ME benchmarks, the CausalNet has achieved robust MER under different levels of key-frame index noise. Meanwhile, it has surpassed state-of-the-art (SOTA) methods on several standard MER benchmarks when using the provided annotated key-frames. Code is available at https://github.com/tony19980810/CausalNet.</p>
            <p id="subjects-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="panel paper" keywords="dso,generator,feedback,objects,dpo,dro,aligning,optimization,generators,simulation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness_ICCV_2025_paper.html" target="_blank" title="180/263"><span class="index notranslate">#180</span></a>
                <a id="title-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="title-link" href="/venue/Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" target="_blank">DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</a>
                <a id="pdf-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruining Li" target="_blank">Ruining Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuanxia Zheng" target="_blank">Chuanxia Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Rupprecht" target="_blank">Christian Rupprecht</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>
            </p>
            <p id="summary-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="summary">Most 3D object generators prioritize aesthetic quality, often neglecting the physical constraints necessary for practical applications. One such constraint is that a 3D object should be self-supporting, i.e., remain balanced under gravity. Previous approaches to generating stable 3D objects relied on differentiable physics simulators to optimize geometry at test time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models with external feedback, we propose Direct Simulation Optimization (DSO). This framework leverages feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator directly outputs stable 3D objects. We construct a dataset of 3D objects labeled with stability scores obtained from the physics simulator. This dataset enables fine-tuning of the 3D generator using the stability score as an alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO)---a novel objective we introduce to align diffusion models without requiring pairwise preferences. Our experiments demonstrate that the fine-tuned feed-forward generator, using either the DPO or DRO objective, is significantly faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework functions even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.</p>
            <p id="subjects-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" onclick="foldPdfKimi('Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="panel paper" keywords="grounding,embodied,mtu3d,navigation,exploration,understand,bridging,language,move,vision">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and_ICCV_2025_paper.html" target="_blank" title="181/263"><span class="index notranslate">#181</span></a>
                <a id="title-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="title-link" href="/venue/Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" target="_blank">Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</a>
                <a id="pdf-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyu Zhu" target="_blank">Ziyu Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xilin Wang" target="_blank">Xilin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixuan Li" target="_blank">Yixuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuofan Zhang" target="_blank">Zhuofan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojian Ma" target="_blank">Xiaojian Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixin Chen" target="_blank">Yixin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoxiong Jia" target="_blank">Baoxiong Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Liang" target="_blank">Wei Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Yu" target="_blank">Qian Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhidong Deng" target="_blank">Zhidong Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Huang" target="_blank">Siyuan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Li" target="_blank">Qing Li</a>
            </p>
            <p id="summary-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="summary">Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce Move to Understand (MTU3D), a unified framework that integrates active perception with 3D vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploration that represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines Vision-Language-Exploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14%, 23%, 9%, and 2% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. MTU3D's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. Additionally, we deploy it on a real robot to demonstrate its effectiveness in handling real-world data. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.</p>
            <p id="subjects-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" onclick="foldPdfKimi('Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Video_Motion_Graphs@ICCV2025@CVF" class="panel paper" keywords="video,motion,interpolation,hminterp,frame,graphs,human,videos,frames,vfi">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Video_Motion_Graphs_ICCV_2025_paper.html" target="_blank" title="182/263"><span class="index notranslate">#182</span></a>
                <a id="title-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="title-link" href="/venue/Liu_Video_Motion_Graphs@ICCV2025@CVF" target="_blank">Video Motion Graphs</a>
                <a id="pdf-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Video_Motion_Graphs@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Video_Motion_Graphs_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Video_Motion_Graphs@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Video_Motion_Graphs@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Video_Motion_Graphs@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Video_Motion_Graphs@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Video_Motion_Graphs@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyang Liu" target="_blank">Haiyang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhan Xu" target="_blank">Zhan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fa-Ting Hong" target="_blank">Fa-Ting Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hsin-Ping Huang" target="_blank">Hsin-Ping Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhou" target="_blank">Yi Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Zhou" target="_blank">Yang Zhou</a>
            </p>
            <p id="summary-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="summary">We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Our Video Motion Graphs outperforms existing generative- and retrieval-based methods for human motion video generation. Our codes and pretrained models are public available.</p>
            <p id="subjects-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Video_Motion_Graphs@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Video_Motion_Graphs@ICCV2025@CVF" onclick="foldPdfKimi('Liu_Video_Motion_Graphs@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="panel paper" keywords="gaze,gazegaussian,3dgs,redirection,eye,splatting,nerf,ucwxb,fidelity,generalization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting_ICCV_2025_paper.html" target="_blank" title="183/263"><span class="index notranslate">#183</span></a>
                <a id="title-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="title-link" href="/venue/Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" target="_blank">GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting</a>
                <a id="pdf-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaobao Wei" target="_blank">Xiaobao Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Chen" target="_blank">Peng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangyu Li" target="_blank">Guangyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Lu" target="_blank">Ming Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Chen" target="_blank">Hui Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Tian" target="_blank">Feng Tian</a>
            </p>
            <p id="summary-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="summary">Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, the first high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. Leveraging the unstructured nature of 3DGS, we develop a novel representation of the eye for rigid eye rotation based on the target gaze direction. To enable synthesis generalization across various subjects, we integrate an expression-guided module to inject subject-specific information into the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. The code is available at: https://ucwxb.github.io/GazeGaussian</p>
            <p id="subjects-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" onclick="foldPdfKimi('Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="panel paper" keywords="dynamic,static,features,ds4d,regions,frame,decoupling,video,liyingcv,dsfd">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling_ICCV_2025_paper.html" target="_blank" title="184/263"><span class="index notranslate">#184</span></a>
                <a id="title-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="title-link" href="/venue/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" target="_blank">Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features</a>
                <a id="pdf-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liying Yang" target="_blank">Liying Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Liu" target="_blank">Chen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenwei Zhu" target="_blank">Zhenwei Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ajian Liu" target="_blank">Ajian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Ma" target="_blank">Hui Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Nong" target="_blank">Jian Nong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanyan Liang" target="_blank">Yanyan Liang</a>
            </p>
            <p id="summary-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="summary">Recently, the generation of dynamic 3D objects from a video has shown impressive results. Existing methods directly optimize Gaussians using whole information in frames. However, when dynamic regions are interwoven with static regions within frames, particularly if the static regions account for a large proportion, existing methods often overlook information in dynamic regions and are prone to overfitting on static regions. This leads to producing results with blurry textures. We consider that decoupling dynamic-static features to enhance dynamic representations can alleviate this issue. Thus, we propose a dynamic-static feature decoupling module (DSFD). Along temporal axes, it regards the regions of current frame features that possess significant differences relative to reference frame features as dynamic features. Conversely, the remaining parts are the static features. Then, we acquire decoupled features driven by dynamic features and current frame features. Moreover, to further enhance the dynamic representation of decoupled features from different viewpoints and ensure accurate motion prediction, we design a temporal-spatial similarity fusion module (TSSF). Along spatial axes, it adaptively selects similar information of dynamic regions. Hinging on the above, we construct a novel approach, DS4D. Experimental results verify our method achieves state-of-the-art (SOTA) results in video-to-4D. In addition, the experiments on a real-world scenario dataset demonstrate its effectiveness on the 4D scene. Project page: https://github.com/LiyingCV/DS4D.</p>
            <p id="subjects-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" onclick="foldPdfKimi('Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="panel paper" keywords="spfsplat,pose,splatting,gaussian,poses,primitives,feed,sparse,view,unposed">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from_ICCV_2025_paper.html" target="_blank" title="185/263"><span class="index notranslate">#185</span></a>
                <a id="title-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="title-link" href="/venue/Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" target="_blank">No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</a>
                <a id="pdf-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ranran Huang" target="_blank">Ranran Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Krystian Mikolajczyk" target="_blank">Krystian Mikolajczyk</a>
            </p>
            <p id="summary-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="summary">We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.</p>
            <p id="subjects-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" onclick="foldPdfKimi('Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="panel paper" keywords="ood,interactions,samples,detection,training,dnns,distribution,unified,methods,time">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection_ICCV_2025_paper.html" target="_blank" title="186/263"><span class="index notranslate">#186</span></a>
                <a id="title-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-link" href="/venue/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" target="_blank">A Unified Interpretation of Training-Time Out-of-Distribution Detection</a>
                <a id="pdf-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Cheng" target="_blank">Xu Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Jiang" target="_blank">Xin Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zechao Li" target="_blank">Zechao Li</a>
            </p>
            <p id="summary-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="summary">This paper explains training-time out-of-distribution (OOD) detection from a novel view, i.e., interactions between different input variables of deep neural networks (DNNs). Specifically, we provide a unified understanding of the effectiveness of current training-time OOD detection methods, i.e., DNNs trained with these methods all encode more complex interactions for inference than those trained without training-time methods, which contributes to their superior OOD detection performance. We further conduct thorough empirical analyses and verify that complex interactions play a primary role in OOD detection, by developing a simple-yet-efficient method to force the DNN to learn interactions of specific complexities and evaluate the change of OOD detection performances. Besides, we also use interactions to investigate why near-OOD samples are more difficult to distinguish from in-distribution (ID) samples than far-OOD samples, mainly because compared to far-OOD samples, the distribution of interactions in near-OOD samples is more similar to that of ID samples. Moreover, we discover that training-time OOD detection methods can effectively decrease such similarities.</p>
            <p id="subjects-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" onclick="foldPdfKimi('Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="panel paper" keywords="perception,collaborative,fusion,unified,temporal,transmission,spatio,feature,agents,simultanesouly">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective_ICCV_2025_paper.html" target="_blank" title="187/263"><span class="index notranslate">#187</span></a>
                <a id="title-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="title-link" href="/venue/Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" target="_blank">CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective</a>
                <a id="pdf-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zongheng Tang" target="_blank">Zongheng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Liu" target="_blank">Yi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Sun" target="_blank">Yifan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulu Gao" target="_blank">Yulu Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinyu Chen" target="_blank">Jinyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runsheng Xu" target="_blank">Runsheng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Si Liu" target="_blank">Si Liu</a>
            </p>
            <p id="summary-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="summary">Collaborative perception shares information among different agents and helps solving problems that individual agents may face, e.g., occlusions and small sensing range. Prior methods usually separate the multi-agent fusion and multi-time fusion into two consecutive steps. In contrast, this paper proposes an efficient collaborative perception that aggregates the observations from different agents (space) and different times into a unified spatio-temporal space simultanesouly. The unified spatio-temporal space brings two benefits, i.e., efficient feature transmission and superior feature fusion. 1) Efficient feature transmission: each static object yields a single observation in the spatial temporal space, and thus only requires transmission only once (whereas prior methods re-transmit all the object features multiple times). 2) superior feature fusion: merging the multi-agent and multi-time fusion into a unified spatial-temporal aggregation enables a more holistic perspective, thereby enhancing perception performance in challenging scenarios. Consequently, our Collaborative perception with Spatio-temporal Transformer (CoST) gains improvement in both efficiency and accuracy. Notably, CoST is not tied to any specific method and is compatible with a majority of previous methods, enhancing their accuracy while reducing the transmission bandwidth.</p>
            <p id="subjects-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" onclick="foldPdfKimi('Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="panel paper" keywords="event,unauthorized,unlearnable,uevs,asynchronous,safeguarding,noise,prevent,streams,minimizing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset_ICCV_2025_paper.html" target="_blank" title="188/263"><span class="index notranslate">#188</span></a>
                <a id="title-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="title-link" href="/venue/Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" target="_blank">Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset</a>
                <a id="pdf-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruofei Wang" target="_blank">Ruofei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiqi Duan" target="_blank">Peiqi Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renjie Wan" target="_blank">Renjie Wan</a>
            </p>
            <p id="summary-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="summary">With more event datasets being released online, safeguarding the event dataset against unauthorized usage has become a serious concern for data owners. Unlearnable Examples are proposed to prevent the unauthorized exploitation of image datasets. However, it's unclear how to create unlearnable asynchronous event streams to prevent event misuse. In this work, we propose the first unlearnable event stream generation method to prevent unauthorized training from event datasets. A new form of asynchronous event error-minimizing noise is proposed to perturb event streams, tricking the unauthorized model into learning embedded noise instead of realistic features. To be compatible with the sparse event, a projection strategy is presented to sparsify the noise to render our unlearnable event streams (UEvs). Extensive experiments demonstrate that our method effectively protects event data from unauthorized exploitation, while preserving their utility for legitimate use. We hope our UEvs contribute to the advancement of secure and trustworthy event dataset sharing. Code is available at: https://github.com/rfww/uevs.</p>
            <p id="subjects-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="panel paper" keywords="t2i,backdoor,navit2i,activation,defense,neuron,backdoors,variation,input,text">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation_ICCV_2025_paper.html" target="_blank" title="189/263"><span class="index notranslate">#189</span></a>
                <a id="title-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="title-link" href="/venue/Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" target="_blank">Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via Neuron Activation Variation</a>
                <a id="pdf-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shengfang Zhai" target="_blank">Shengfang Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Li" target="_blank">Jiajun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Liu" target="_blank">Yue Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huanran Chen" target="_blank">Huanran Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihua Tian" target="_blank">Zhihua Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjie Qu" target="_blank">Wenjie Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingni Shen" target="_blank">Qingni Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoxi Jia" target="_blank">Ruoxi Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinpeng Dong" target="_blank">Yinpeng Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaheng Zhang" target="_blank">Jiaheng Zhang</a>
            </p>
            <p id="summary-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="summary">In recent years, text-to-image (T2I) diffusion models have gained significant attention for their ability to generate high-quality images reflecting text prompts. However, their growing popularity has also led to the emergence of backdoor threats, posing substantial risks. Currently, effective defense strategies against such threats are lacking due to the diversity of backdoor targets in T2I synthesis. In this paper, we propose NaviT2I, an efficient input-level backdoor defense framework against diverse T2I backdoors. Our approach is based on the new observation that trigger tokens tend to induce significant neuron activation variation in the early stage of the diffusion generation process, a phenomenon we term Early-step Activation Variation. Leveraging this insight, NaviT2I navigates T2I models to prevent malicious inputs by analyzing Neuron activation variations caused by input tokens. Extensive experiments show that NaviT2I significantly outperforms the baselines in both effectiveness and efficiency across diverse datasets, various T2I backdoors, and different model architectures including UNet and DiT. Furthermore, we show that our method remains effective under potential adaptive attacks.</p>
            <p id="subjects-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" onclick="foldPdfKimi('Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="panel paper" keywords="vmem,views,scene,surfel,memory,video,indexed,past,generators,surfels">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory_ICCV_2025_paper.html" target="_blank" title="190/263"><span class="index notranslate">#190</span></a>
                <a id="title-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="title-link" href="/venue/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" target="_blank">VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory</a>
                <a id="pdf-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Runjia Li" target="_blank">Runjia Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Torr" target="_blank">Philip Torr</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomas Jakab" target="_blank">Tomas Jakab</a>
            </p>
            <p id="summary-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="summary">We propose a novel memory module for building video generators capable of interactively exploring environments. Previous approaches have achieved similar results either by out-painting 2D views of a scene while incrementally reconstructing its 3D geometry--which quickly accumulates errors--or by using video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a memory module that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost required to use all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control.</p>
            <p id="subjects-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" onclick="foldPdfKimi('Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="panel paper" keywords="omnihuman,human,animation,end,driven,audio,supports,talking,generation,portrait">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models_ICCV_2025_paper.html" target="_blank" title="191/263"><span class="index notranslate">#191</span></a>
                <a id="title-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="title-link" href="/venue/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" target="_blank">OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models</a>
                <a id="pdf-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gaojie Lin" target="_blank">Gaojie Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianwen Jiang" target="_blank">Jianwen Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Yang" target="_blank">Jiaqi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zerong Zheng" target="_blank">Zerong Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Liang" target="_blank">Chao Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Zhang" target="_blank">Yuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingtuo Liu" target="_blank">Jingtuo Liu</a>
            </p>
            <p id="summary-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="summary">End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals).</p>
            <p id="subjects-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" onclick="foldPdfKimi('Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="panel paper" keywords="ood,samples,synood,ind,boundary,mllms,clip,models,foundation,fpr95">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection_ICCV_2025_paper.html" target="_blank" title="192/263"><span class="index notranslate">#192</span></a>
                <a id="title-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-link" href="/venue/Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" target="_blank">Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection</a>
                <a id="pdf-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinglun Li" target="_blank">Jinglun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaixun Jiang" target="_blank">Kaixun Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyu Chen" target="_blank">Zhaoyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Lin" target="_blank">Bo Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Tang" target="_blank">Yao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weifeng Ge" target="_blank">Weifeng Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqiang Zhang" target="_blank">Wenqiang Zhang</a>
            </p>
            <p id="summary-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="summary">Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD/OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by 11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.</p>
            <p id="subjects-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" onclick="foldPdfKimi('Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="panel paper" keywords="countse,exemplar,counting,guided,exemplars,shot,text,object,zero,methods">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting_ICCV_2025_paper.html" target="_blank" title="193/263"><span class="index notranslate">#193</span></a>
                <a id="title-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="title-link" href="/venue/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" target="_blank">CountSE: Soft Exemplar Open-set Object Counting</a>
                <a id="pdf-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Liu" target="_blank">Shuai Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Zhang" target="_blank">Peng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiwei Zhang" target="_blank">Shiwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Ke" target="_blank">Wei Ke</a>
            </p>
            <p id="summary-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="summary">Open-set counting is garnering increasing attention due to its capability to enumerate objects of arbitrary category. It can be generally categorized into two methodologies: text-guided zero-shot counting methods and exemplar-guided few-shot counting methods. Previous text-guided zero-shot methods only provide limited object information through text, resulting in poor performance. Besides, though exemplar-guided few-shot approaches gain better results, they rely heavily on manually annotated visual exemplars, resulting in low efficiency and high labor intensity. Therefore, we propose CountSE, which simultaneously achieves high efficiency and high performance. CountSE is a new text-guided zero-shot object counting algorithm that generates multiple precise soft exemplars at different scales to enhance counting models driven solely by semantics. Specifically, to obtain richer object information and address the diversity in object scales, we introduce Semantic-guided Exemplar Selection, a module that generates candidate soft exemplars at various scales and selects those with high similarity scores. Then, to ensure accuracy and representativeness, Clustering-based Exemplar Filtering is introduced to refine the candidate exemplars by effectively eliminating inaccurate exemplars through clustering analysis. In the text-guided zero-shot setting, CountSE outperforms all state-of-the-art methods on the FSC-147 benchmark by at least 15%. Additionally, experiments on two other widely used datasets demonstrate that CountSE significantly outperforms all previous text-guided zero-shot counting methods and is competitive with the most advanced exemplar-guided few-shot methods. Codes will be available. Code is available at https://github.com/pppppz22/CountSE.</p>
            <p id="subjects-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" onclick="foldPdfKimi('Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="panel paper" keywords="sparse,unposed,2dgs,reconstruction,splatting,sparfels,radiance,splatted,view,shape">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery_ICCV_2025_paper.html" target="_blank" title="194/263"><span class="index notranslate">#194</span></a>
                <a id="title-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="title-link" href="/venue/Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" target="_blank">Sparfels: Fast Reconstruction from Sparse Unposed Imagery</a>
                <a id="pdf-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shubhendu Jena" target="_blank">Shubhendu Jena</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amine Ouasfi" target="_blank">Amine Ouasfi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mae Younes" target="_blank">Mae Younes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adnane Boukhayma" target="_blank">Adnane Boukhayma</a>
            </p>
            <p id="summary-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="summary">We present a method for Sparse view reconstruction with surface element splatting that runs within 2 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate stat-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view Benchmarks based on established multi-view datasets.</p>
            <p id="subjects-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" onclick="foldPdfKimi('Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="panel paper" keywords="slam,underwater,medium,duv,visual,uncertainty,depth,geometric,modeling,mapping">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.html" target="_blank" title="195/263"><span class="index notranslate">#195</span></a>
                <a id="title-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="title-link" href="/venue/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" target="_blank">Underwater Visual SLAM with Depth Uncertainty and Medium Modeling</a>
                <a id="pdf-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Liu" target="_blank">Rui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Fan" target="_blank">Sheng Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenguan Wang" target="_blank">Wenguan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>
            </p>
            <p id="summary-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="summary">Underwater visual simultaneous localization and mapping (SLAM) faces critical challenges in light attenuation and degraded geometric consistency. Despite recent advances of visual SLAM in indoor and urban scenes, these approaches typically assume a clear medium and neglect medium-light interactions, leading to performance degradation in underwater environments. To overcome these limitations, we propose DUV-SLAM, a dense underwater visual SLAM framework that integrates uncertainty-aware geometry estimation with physics-inspired neural scattering modeling. Our method introduces two core innovations: i) depth uncertainty quantification derived from differentiable bundle adjustment, which propagates geometric confidence to guide mapping optimization; and ii) a neural-Gaussian hybrid representation that combines adaptive 3D Gaussians for underwater reconstruction with a neural field capturing wavelength-dependent medium properties, optimized using a combination of photometric, geometric, and distribution losses. Experiments on synthetic and real-world datasets demonstrate that DUV-SLAM achieves high-quality monocular reconstruction while maintaining real-time efficiency and robust tracking accuracy.</p>
            <p id="subjects-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" onclick="foldPdfKimi('Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="panel paper" keywords="vae,tvt,unet,fine,resolution,trained,joyies,transfer,image,super">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training_ICCV_2025_paper.html" target="_blank" title="196/263"><span class="index notranslate">#196</span></a>
                <a id="title-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="title-link" href="/venue/Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" target="_blank">Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training</a>
                <a id="pdf-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF">8</sup>]</a>
                <a id="copy-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qiaosi Yi" target="_blank">Qiaosi Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Li" target="_blank">Shuai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rongyuan Wu" target="_blank">Rongyuan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingchen Sun" target="_blank">Lingchen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhui Wu" target="_blank">Yuhui Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Zhang" target="_blank">Lei Zhang</a>
            </p>
            <p id="summary-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="summary">Impressive results on real-world image super-resolution (Real-ISR) have been achieved by employing pre-trained stable diffusion (SD) models. However, one critical issue of such methods lies in their poor reconstruction of image fine structures, such as small characters and textures, due to the aggressive resolution reduction of the VAE (e.g., 8xdownsampling) in the SD model. One solution is to employ a VAE with a lower downsampling rate for diffusion; however, adapting its latent features with the pre-trained UNet while mitigating the increased computational cost poses new challenges. To address these issues, we propose a Transfer VAE Training (TVT) strategy to transfer the 8xdownsampled VAE into a 4xone while adapting to the pre-trained UNet. Specifically, we first train a 4xdecoder based on the output features of the original VAE encoder, then train a 4xencoder while keeping the newly trained decoder fixed. Such a TVT strategy aligns the new encoder-decoder pair with the original VAE latent space while enhancing image fine details. Additionally, we introduce a compact VAE and compute-efficient UNet by optimizing their network architectures, reducing the computational cost while capturing high-resolution fine-scale features. Experimental results demonstrate that our TVT method significantly improves fine-structure preservation, which is often compromised by other SD-based methods, while requiring fewer FLOPs than state-of-the-art one-step diffusion models. The official code can be found at https://github.com/Joyies/TVT.</p>
            <p id="subjects-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" onclick="foldPdfKimi('Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="panel paper" keywords="attention,mala,softmax,linear,magnitude,exhibits,query,score,rectifying,complexity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention_ICCV_2025_paper.html" target="_blank" title="197/263"><span class="index notranslate">#197</span></a>
                <a id="title-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="title-link" href="/venue/Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" target="_blank">Rectifying Magnitude Neglect in Linear Attention</a>
                <a id="pdf-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qihang Fan" target="_blank">Qihang Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaibo Huang" target="_blank">Huaibo Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuang Ai" target="_blank">Yuang Ai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran He" target="_blank">Ran He</a>
            </p>
            <p id="summary-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="summary">As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query(Q or \phi(Q)). The absence of magnitude information prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose **Magnitude-Aware Linear Attention** (MALA), which modifies the computation of Linear Attention to fully incorporate the Query's magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. As a result, MALA surpasses Softmax Attention in performance while maintaining only linear complexity. We build Magnitude-Aware Vision Transformer (MAViT) based on MALA, achieving **84.7%** accuracy on ImageNet-1K with only **27M** parameters and **4.6G** flops, without using any additional data or labels. It also exhibits excellent inference efficiency. This result highlights the strong potential of MALA.</p>
            <p id="subjects-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" onclick="foldPdfKimi('Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="panel paper" keywords="adapter,att,attributes,t2i,control,autoencoder,multiple,domain,diffusion,precise">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter_ICCV_2025_paper.html" target="_blank" title="198/263"><span class="index notranslate">#198</span></a>
                <a id="title-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="title-link" href="/venue/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" target="_blank">Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder</a>
                <a id="pdf-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wonwoong Cho" target="_blank">Wonwoong Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan-Ying Chen" target="_blank">Yan-Ying Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Klenk" target="_blank">Matthew Klenk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David I. Inouye" target="_blank">David I. Inouye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanxia Zhang" target="_blank">Yanxia Zhang</a>
            </p>
            <p id="summary-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="summary">Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the **Attribute (Att) Adapter**, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning.We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world.Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model.</p>
            <p id="subjects-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" onclick="foldPdfKimi('Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="panel paper" keywords="dap,mae,cloud,point,domain,masked,autoencoder,domains,pre,training">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.html" target="_blank" title="199/263"><span class="index notranslate">#199</span></a>
                <a id="title-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="title-link" href="/venue/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" target="_blank">DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning</a>
                <a id="pdf-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqi Gao" target="_blank">Ziqi Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiufu Li" target="_blank">Qiufu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linlin Shen" target="_blank">Linlin Shen</a>
            </p>
            <p id="summary-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="summary">Compared to 2D data, the scale of point cloud data in different domains available for training, is quite limited. Researchers have been trying to combine these data of different domains for masked autoencoder (MAE) pre-training to leverage such a data scarcity issue. However, the prior knowledge learned from mixed domains may not align well with the downstream 3D point cloud analysis tasks, leading to degraded performance. To address such an issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE), an MAE pre-training method, to adaptively integrate the knowledge of cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a heterogeneous domain adapter that utilizes an adaptation mode during pre-training, enabling the model to comprehensively learn information from point clouds across different domains, while employing a fusion mode in the fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks. With only one pre-training, DAP-MAE achieves excellent performance across four different point cloud analysis tasks, reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus. The code will be released at https://github.com/CVI-SZU/DAP-MAE</p>
            <p id="subjects-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" onclick="foldPdfKimi('Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="panel paper" keywords="pig,panollama,endless,token,coherent,panoramic,crop,var,panoramas,paradigm">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs_ICCV_2025_paper.html" target="_blank" title="200/263"><span class="index notranslate">#200</span></a>
                <a id="title-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="title-link" href="/venue/Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" target="_blank">PanoLlama: Generating Endless and Coherent Panoramas with Next-Token-Prediction LLMs</a>
                <a id="pdf-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Teng Zhou" target="_blank">Teng Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Zhang" target="_blank">Xiaoyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongchuan Tang" target="_blank">Yongchuan Tang</a>
            </p>
            <p id="summary-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="summary">Panoramic Image Generation (PIG) aims to create coherent images of arbitrary lengths. Most existing methods fall in the joint diffusion paradigm, but their complex and heuristic crop connection designs often limit their ability to achieve multilevel coherence. By deconstructing this challenge into its core components, we find it naturally aligns with next-token prediction, leading us to adopt an autoregressive (AR) paradigm for PIG modeling. However, existing visual AR (VAR) models are limited to fixed-size generation, lacking the capability to produce panoramic images. In this paper, we propose PanoLlama, a novel framework that achieves endless and coherent panorama generation with the autoregressive paradigm. Our approach develops a training-free strategy that utilizes token redirection to overcome the size limitations of existing VAR models, enabling next-crop prediction in both horizontal and vertical directions. This refreshes the PIG pipeline while achieving SOTA performance in coherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally, PanoLlama supports applications other PIG methods cannot achieve, including mask-free layout control, multi-scale and multi-guidance synthesis. To facilitate standardized evaluation, we also establish a dataset with 1,000 prompts spanning 100+ themes, providing a new testing benchmark for PIG research.</p>
            <p id="subjects-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" onclick="foldPdfKimi('Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="panel paper" keywords="backdoor,adversarial,diffusion,attacks,dadet,anomaly,deviate,coco,safeguarding,detection">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.html" target="_blank" title="201/263"><span class="index notranslate">#201</span></a>
                <a id="title-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="title-link" href="/venue/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" target="_blank">DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection</a>
                <a id="pdf-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongwei Yu" target="_blank">Hongwei Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlong Ding" target="_blank">Xinlong Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Li" target="_blank">Jiawei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinlong Wang" target="_blank">Jinlong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yudong Zhang" target="_blank">Yudong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rongquan Wang" target="_blank">Rongquan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huimin Ma" target="_blank">Huimin Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiansheng Chen" target="_blank">Jiansheng Chen</a>
            </p>
            <p id="summary-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="summary">While image conditional diffusion models demonstrate impressive generation capabilities, they exhibit high vulnerability when facing backdoor and adversarial attacks. In this paper, we define a scenario named diffusion anomaly where the generated results of a reverse process under attack deviate significantly from the normal ones. By analyzing the underlying formation mechanism of the diffusion anomaly, we reveal how perturbations are amplified during the reverse process and accumulated in the results. Based on the analysis, we reveal the phenomena of divergence and homogeneity, which cause the diffusion process to deviate significantly from the normal process and to decline in diversity. Leveraging these two phenomena, we propose a method named Diffusion Anomaly Detection (DADet) to effectively detect both backdoor and adversarial attacks. Extensive experiments demonstrate that our proposal achieves excellent defense performance against backdoor and adversarial attacks. Specifically, for the backdoor attack detection, our method achieves an F1 score of 99% on different datasets, including MS COCO and CIFAR-10. For the detection of adversarial samples, the F1 score exceeds 84% across three adversarial attacks and two different tasks, evaluated on the MS COCO and Places365 datasets, respectively.</p>
            <p id="subjects-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" onclick="foldPdfKimi('Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="panel paper" keywords="ovs,reme,vocabulary,training,quality,data,centric,reference,free,segmentation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation_ICCV_2025_paper.html" target="_blank" title="202/263"><span class="index notranslate">#202</span></a>
                <a id="title-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" target="_blank">ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation</a>
                <a id="pdf-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiwei Xuan" target="_blank">Xiwei Xuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziquan Deng" target="_blank">Ziquan Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwan-Liu Ma" target="_blank">Kwan-Liu Ma</a>
            </p>
            <p id="summary-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="summary">Training-free open-vocabulary semantic segmentation (OVS) aims to segment images given a set of arbitrary textual categories without costly model fine-tuning. Existing solutions often explore attention mechanisms of pre-trained models, such as CLIP, or generate synthetic data and design complex retrieval processes to perform OVS. However, their performance is limited by the capability of reliant models or the suboptimal quality of reference sets. In this work, we investigate the largely overlooked data quality problem for this challenging dense scene understanding task, and identify that a high-quality reference set can significantly benefit training-free OVS. With this observation, we introduce a data-quality-oriented framework, comprising a data pipeline to construct a reference set with well-paired segment-text embeddings and a simple similarity-based retrieval to unveil the essential effect of data. Remarkably, extensive evaluations on ten benchmark datasets demonstrate that our method outperforms all existing training-free OVS approaches, highlighting the importance of data-centric design for advancing OVS without training.</p>
            <p id="subjects-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="panel paper" keywords="casp,pipeline,feature,matching,cascaded,priors,guidance,dense,correspondence,matches">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors_ICCV_2025_paper.html" target="_blank" title="203/263"><span class="index notranslate">#203</span></a>
                <a id="title-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="title-link" href="/venue/Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" target="_blank">CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance</a>
                <a id="pdf-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peiqi Chen" target="_blank">Peiqi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Yu" target="_blank">Lei Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Wan" target="_blank">Yi Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingying Pei" target="_blank">Yingying Pei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyi Liu" target="_blank">Xinyi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxiang Yao" target="_blank">Yongxiang Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingying Zhang" target="_blank">Yingying Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lixiang Ru" target="_blank">Lixiang Ru</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liheng Zhong" target="_blank">Liheng Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingdong Chen" target="_blank">Jingdong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Yang" target="_blank">Ming Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongjun Zhang" target="_blank">Yongjun Zhang</a>
            </p>
            <p id="summary-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="summary">Semi-dense feature matching methods have shown strong performance in challenging scenarios. However, the existing pipeline relies on a global search across the entire feature map to establish coarse matches, limiting further improvements in accuracy and efficiency. Motivated by this limitation, we propose a novel pipeline, CasP, which leverages cascaded correspondence priors for guidance. Specifically, the matching stage is decomposed into two progressive phases, bridged by a region-based selective cross-attention mechanism designed to enhance feature discriminability. In the second phase, one-to-one matches are determined by restricting the search range to the one-to-many prior areas identified in the first phase. Additionally, this pipeline benefits from incorporating high-level features, which helps reduce the computational costs of low-level feature extraction. The acceleration gains of CasP increase with higher resolution, and our lite model achieves a speedup of ~2.2xat a resolution of 1152 compared to the most efficient method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority in geometric estimation, particularly with impressive cross-domain generalization. These advantages highlight its potential for latency-sensitive and high-robustness applications, such as SLAM and UAV systems.</p>
            <p id="subjects-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" onclick="foldPdfKimi('Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="panel paper" keywords="minimal,problems,plmp,sfm,cameras,subarrangements,underconstrained,lines,pinhole,291">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM_ICCV_2025_paper.html" target="_blank" title="204/263"><span class="index notranslate">#204</span></a>
                <a id="title-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="title-link" href="/venue/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" target="_blank">PLMP - Point-Line Minimal Problems for Projective SfM</a>
                <a id="pdf-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kim Kiehn" target="_blank">Kim Kiehn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albin Ahlbck" target="_blank">Albin Ahlbck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kathln Kohn" target="_blank">Kathln Kohn</a>
            </p>
            <p id="summary-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="summary">We completely classify all minimal problems for Structure-from-Motion (SfM) where arrangements of points and lines are fully observed by multiple uncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have unique solutions and can thus be solved linearly.Two of the linear problems allow an arbitrary number of views, while all other minimal problems have at most 9 cameras. All minimal problems have at most 7 points and at most 12 lines. We compute the number of solutions of each minimal problem, as this gives a measurement of the problem's intrinsic difficulty, and find that these number are relatively low (e.g., when comparing with minimal problems for calibrated cameras). Finally, by exploring stabilizer subgroups of subarrangements, we develop a geometric and systematic way to 1) factorize minimal problems into smaller problems, 2) identify minimal problems in underconstrained problems, and 3) formally prove non-minimality.</p>
            <p id="subjects-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" onclick="foldPdfKimi('Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="panel paper" keywords="reconstruction,implicit,tpg,target,sparse,prior,view,voxel,inr,nerp">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view_ICCV_2025_paper.html" target="_blank" title="205/263"><span class="index notranslate">#205</span></a>
                <a id="title-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="title-link" href="/venue/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" target="_blank">TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging</a>
                <a id="pdf-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qinglei Cao" target="_blank">Qinglei Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyao Tang" target="_blank">Ziyao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoqin Tang" target="_blank">Xiaoqin Tang</a>
            </p>
            <p id="summary-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="summary">X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available upon request.</p>
            <p id="subjects-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" onclick="foldPdfKimi('Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="panel paper" keywords="cross,modal,semantic,unidxmd,uda,unified,adaptation,segmentation,domain,representation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in_ICCV_2025_paper.html" target="_blank" title="206/263"><span class="index notranslate">#206</span></a>
                <a id="title-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="title-link" href="/venue/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" target="_blank">UniDxMD: Towards Unified Representation for Cross-Modal Unsupervised Domain Adaptation in 3D Semantic Segmentation</a>
                <a id="pdf-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyin Liang" target="_blank">Zhengyin Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Yin" target="_blank">Hui Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Liang" target="_blank">Min Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianqian Du" target="_blank">Qianqian Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Yang" target="_blank">Ying Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hua Huang" target="_blank">Hua Huang</a>
            </p>
            <p id="summary-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="summary">Modality or domain distribution shifts pose formidable challenges in 3D semantic segmentation. Existing methods predominantly address either cross-modal or cross-domain adaptation in isolation, leading to insufficient exploration of semantic associations and complementary features in heterogeneous data. To bridge this gap, we present UniDxMD, a unified representation method for cross-modal unsupervised domain adaptation (UDA) in 3D semantic segmentation that simultaneously tackles both cross-modal and cross-domain adaptation objectives. Our core insight is deriving a unified discrete representation from heterogeneous data to mitigate distribution shifts, inspired by vector quantization. Specifically, we propose a differentiable, cluster-based soft quantization mechanism (CSQM) that maps heterogeneous data (spanning modalities and domains) into a shared discrete latent space. Then, we introduce latent space regularization (LSR), leveraging joint prototypes that satisfy semantic relation consistency as learnable anchors to enhance the compactness and semantic discriminability of the discrete latent space. Our method paves the way for advancing cross-modal UDA in 3D semantic segmentation towards the unified representation. Extensive results across four challenging cross-modal UDA scenarios demonstrate the superiority of our method.</p>
            <p id="subjects-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" onclick="foldPdfKimi('Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="panel paper" keywords="zim,matte,matting,shot,zero,masks,anything,segmentation,sam,precise">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ZIM_Zero-Shot_Image_Matting_for_Anything_ICCV_2025_paper.html" target="_blank" title="207/263"><span class="index notranslate">#207</span></a>
                <a id="title-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="title-link" href="/venue/Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" target="_blank">ZIM: Zero-Shot Image Matting for Anything</a>
                <a id="pdf-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_ZIM_Zero-Shot_Image_Matting_for_Anything_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Beomyoung Kim" target="_blank">Beomyoung Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chanyong Shin" target="_blank">Chanyong Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joonhyun Jeong" target="_blank">Joonhyun Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyungsik Jung" target="_blank">Hyungsik Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Se-Yun Lee" target="_blank">Se-Yun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sewhan Chun" target="_blank">Sewhan Chun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong-Hyun Hwang" target="_blank">Dong-Hyun Hwang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joonsang Yu" target="_blank">Joonsang Yu</a>
            </p>
            <p id="summary-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="summary">The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D segmentation. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at https://naver-ai.github.io/ZIM.</p>
            <p id="subjects-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" onclick="foldPdfKimi('Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="panel paper" keywords="backdoor,detoxification,defense,poisoned,backdoors,attacks,detoxify,sota,distance,driven">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification_ICCV_2025_paper.html" target="_blank" title="208/263"><span class="index notranslate">#208</span></a>
                <a id="title-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="title-link" href="/venue/Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" target="_blank">Backdoor Mitigation by Distance-Driven Detoxification</a>
                <a id="pdf-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaokui Wei" target="_blank">Shaokui Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayin Liu" target="_blank">Jiayin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyuan Zha" target="_blank">Hongyuan Zha</a>
            </p>
            <p id="summary-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="summary">Backdoor attacks undermine the integrity of machine learning models by allowing attackers to manipulate predictions using poisoned training data. Such attacks lead to targeted misclassification when specific triggers are present, while the model behaves normally under other conditions. This paper considers a post-training backdoor defense task, aiming to detoxify the backdoors in pre-trained models. We begin by analyzing the underlying issues of vanilla fine-tuning and observe that it is often trapped in regions with low loss for both clean and poisoned samples. Motivated by such observations, we propose Distance-Driven Detoxification (D3), an innovative approach that reformulates backdoor defense as a constrained optimization problem. Specifically, D3 promotes the model's departure from the vicinity of its initial weights, effectively reducing the influence of backdoors. Extensive experiments on state-of-the-art (SOTA) backdoor attacks across various model architectures and datasets demonstrate that D3 not only matches but often surpasses the performance of existing SOTA post-training defense techniques.</p>
            <p id="subjects-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" onclick="foldPdfKimi('Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="panel paper" keywords="unrealzoo,embodied,worlds,photo,unrealcv,realistic,virtual,enriching,world,navigation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI_ICCV_2025_paper.html" target="_blank" title="209/263"><span class="index notranslate">#209</span></a>
                <a id="title-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="title-link" href="/venue/Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" target="_blank">UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI</a>
                <a id="pdf-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fangwei Zhong" target="_blank">Fangwei Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kui Wu" target="_blank">Kui Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Churan Wang" target="_blank">Churan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chen" target="_blank">Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hai Ci" target="_blank">Hai Ci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhoujun Li" target="_blank">Zhoujun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhou Wang" target="_blank">Yizhou Wang</a>
            </p>
            <p id="summary-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="summary">We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects.UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment.</p>
            <p id="subjects-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" onclick="foldPdfKimi('Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="panel paper" keywords="height,lidar,mamba,fusion,modal,mambafusion,global,autolab,fidelity,dense">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection_ICCV_2025_paper.html" target="_blank" title="210/263"><span class="index notranslate">#210</span></a>
                <a id="title-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="title-link" href="/venue/Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" target="_blank">Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection</a>
                <a id="pdf-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanshi Wang" target="_blank">Hanshi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Gao" target="_blank">Jin Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiming Hu" target="_blank">Weiming Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhipeng Zhang" target="_blank">Zhipeng Zhang</a>
            </p>
            <p id="summary-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="summary">We present the first work demonstrating that a pure Mamba block can achieve efficient Dense Global Fusion, meanwhile guaranteeing top performance for camera-LiDAR multi-modal 3D object detection. Our motivation stems from the observation that existing fusion strategies are constrained by their inability to simultaneously achieve efficiency, long-range modeling, and retaining complete scene information. Inspired by recent advances in state-space models (SSMs) and linear attention, we leverage their linear complexity and long-range modeling capabilities to address these challenges. However, this is non-trivial since our experiments reveal that simply adopting efficient linear-complexity methods does not necessarily yield improvements and may even degrade performance. We attribute this degradation to the loss of height information during multi-modal alignment, leading to deviations in sequence order. To resolve this, we propose height-fidelity LiDAR encoding that preserves precise height information through voxel compression in continuous space, thereby enhancing camera-LiDAR alignment. Subsequently, we introduce the Hybrid Mamba Block, which leverages the enriched height-informed features to conduct local and global contextual learning. By integrating these components, our method achieves state-of-the-art performance with the top-tire NDS score of 75.0 on the nuScenes validation benchmark, even surpassing methods that utilize high-resolution inputs. Meanwhile, our method maintains efficiency, achieving faster inference speed than most recent state-of-the-art methods. Code is available at https://github.com/AutoLab-SAI-SJTU/MambaFusion</p>
            <p id="subjects-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="panel paper" keywords="smstracker,modal,tri,rgb,fusion,mask,path,score,sigma,tracking">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking_ICCV_2025_paper.html" target="_blank" title="211/263"><span class="index notranslate">#211</span></a>
                <a id="title-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="title-link" href="/venue/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" target="_blank">SMSTracker: Tri-path Score Mask Sigma Fusion for Multi-Modal Tracking</a>
                <a id="pdf-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sixian Chan" target="_blank">Sixian Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zedong Li" target="_blank">Zedong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Li" target="_blank">Wenhao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijian Lu" target="_blank">Shijian Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunhua Shen" target="_blank">Chunhua Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoqin Zhang" target="_blank">Xiaoqin Zhang</a>
            </p>
            <p id="summary-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="summary">Multi-modal object tracking has emerged as a significant research focus in computer vision due to its robustness in complex environments, such as exposure variations, blur, and occlusions. Despite existing studies integrating supplementary modal information into pre-trained RGB trackers through visual prompt mechanisms, this approach exhibits a critical limitation: it inherently prioritizes RGB information as the dominant modality, thereby underutilizing the complementary information of alternative modalities. To address this fundamental limitation, we present SMSTracker, an innovative tri-path score mask sigma fusion framework for multi-modal tracking, including three key modules. Firstly, we design a tri-path Score Mask Fusion (SMF) module to evaluate and quantify the reliability of each modality, allowing optimal exploitation of complementary features between modalities. Secondly, we introduce a pioneering Sigma Interaction (SGI) module to facilitate a sophisticated fusion of modal features across tri-branches. Furthermore, we advance a Drop Key Fine-tuning (DKF) strategy to address the inherent challenge of unequal data contribution in multi-modal learning scenarios, thereby enhancing the model's capacity for comprehensive multi-modal information processing. Finally, extensive experiments on RGB+Thermal, RGB+Depth, and RGB+Event datasets demonstrate the significant performance improvements achieved by SMSTracker over existing state-of-the-art methods. Code and model are available at https://github.com/Leezed525/SMSTracker.</p>
            <p id="subjects-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" onclick="foldPdfKimi('Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="panel paper" keywords="pos,supervised,objectives,conflict,segmentation,gradients,semi,descent,gradient,meo">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic_ICCV_2025_paper.html" target="_blank" title="212/263"><span class="index notranslate">#212</span></a>
                <a id="title-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="title-link" href="/venue/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" target="_blank">Two Losses, One Goal: Balancing Conflict Gradients for Semi-supervised Semantic Segmentation</a>
                <a id="pdf-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Sun" target="_blank">Rui Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huayu Mai" target="_blank">Huayu Mai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wangkai Li" target="_blank">Wangkai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujia Chen" target="_blank">Yujia Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Wang" target="_blank">Yuan Wang</a>
            </p>
            <p id="summary-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="summary">Semi-supervised semantic segmentation has attracted considerable attention as it alleviates the need for extensive pixel-level annotations. However, existing methods often overlook the potential optimization conflict between supervised and unsupervised learning objectives, leading to suboptimal performance. In this paper, we identify this under-explored issue and propose a novel Pareto Optimization Strategy (POS) to tackle it. POS aims to find a descent gradient direction that benefits both learning objectives, thereby facilitating model training. By dynamically assigning weights to the gradients at each iteration based on the model's learning status, POS effectively reconciles the intrinsic tension between the two objectives. Furthermore, we analyze POS from the perspective of gradient descent in random batch sampling and propose the Magnitude Enhancement Operation (MEO) to further unleash its potential by considering both direction and magnitude during gradient integration. Extensive experiments on challenging benchmarks demonstrate that integrating POS into existing semi-supervised segmentation methods yields consistent improvements across different data splits and architectures (CNN, Transformer), showcasing its effectiveness.</p>
            <p id="subjects-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" onclick="foldPdfKimi('Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="panel paper" keywords="region,ocr,discrimination,visual,rice,cluster,siglip,deepglint,tasks,language">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning_ICCV_2025_paper.html" target="_blank" title="213/263"><span class="index notranslate">#213</span></a>
                <a id="title-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="title-link" href="/venue/Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" target="_blank">Region-based Cluster Discrimination for Visual Representation Learning</a>
                <a id="pdf-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yin Xie" target="_blank">Yin Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaicheng Yang" target="_blank">Kaicheng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang An" target="_blank">Xiang An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Wu" target="_blank">Kun Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongle Zhao" target="_blank">Yongle Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weimo Deng" target="_blank">Weimo Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zimin Ran" target="_blank">Zimin Ran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yumeng Wang" target="_blank">Yumeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyong Feng" target="_blank">Ziyong Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roy Miles" target="_blank">Roy Miles</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ismail Elezi" target="_blank">Ismail Elezi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiankang Deng" target="_blank">Jiankang Deng</a>
            </p>
            <p id="summary-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="summary">Learning visual representations is foundational for a broad spectrum of downstream tasks. Although recent vision-language contrastive models, such as CLIP and SigLIP, have achieved impressive zero-shot performance via large-scale vision-language alignment, their reliance on global representations constrains their effectiveness for dense prediction tasks, such as grounding, OCR, and segmentation. To address this gap, we introduce Region-Aware Cluster Discrimination (RICE), a novel method that enhances region-level visual and OCR capabilities. We first construct a billion-scale candidate region dataset and propose a Region Transformer layer to extract rich regional semantics. We further design a unified region cluster discrimination loss that jointly supports object and OCR learning within a single classification framework, enabling efficient and scalable distributed training on large-scale data. Extensive experiments show that RICE consistently outperforms previous methods on tasks, including segmentation, dense detection, and visual perception for Multimodal Large Language Models (MLLMs). The pre-trained models have been released at https://github.com/deepglint/MVT.</p>
            <p id="subjects-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" onclick="foldPdfKimi('Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="panel paper" keywords="motion,monocular,scenes,dynamic,scene,reconstruction,bases,casually,long,rigidly">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video_ICCV_2025_paper.html" target="_blank" title="214/263"><span class="index notranslate">#214</span></a>
                <a id="title-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="title-link" href="/venue/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" target="_blank">Shape of Motion: 4D Reconstruction from a Single Video</a>
                <a id="pdf-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qianqian Wang" target="_blank">Qianqian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vickie Ye" target="_blank">Vickie Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Gao" target="_blank">Hang Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijia Zeng" target="_blank">Weijia Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jake Austin" target="_blank">Jake Austin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengqi Li" target="_blank">Zhengqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angjoo Kanazawa" target="_blank">Angjoo Kanazawa</a>
            </p>
            <p id="summary-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="summary">Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. We introduce a method for reconstructing generic dynamic scenes, featuring explicit, persistent 3D motion trajectories in the world coordinate frame, from casually captured monocular videos.We tackle the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE(3) motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we take advantage of off-the-shelf data-driven priors such as monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes.</p>
            <p id="subjects-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="panel paper" keywords="fap,fpem,attractiveness,facial,live,livebeauty,retouching,face,dataset,videos">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos_ICCV_2025_paper.html" target="_blank" title="215/263"><span class="index notranslate">#215</span></a>
                <a id="title-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="title-link" href="/venue/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" target="_blank">FPEM: Face Prior Enhanced Facial Attractiveness Prediction for Live Videos with Face Retouching</a>
                <a id="pdf-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Li" target="_blank">Hui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Ren" target="_blank">Xiaoyu Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongjiu Yu" target="_blank">Hongjiu Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Chen" target="_blank">Ying Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Li" target="_blank">Kai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=L Wang" target="_blank">L Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiongkuo Min" target="_blank">Xiongkuo Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huiyu Duan" target="_blank">Huiyu Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangtao Zhai" target="_blank">Guangtao Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Liu" target="_blank">Xu Liu</a>
            </p>
            <p id="summary-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="summary">Facial attractiveness prediction (FAP) has long been an important computer vision task, which could be widely applied in live videos with facial retouching. However, previous FAP datasets are either small or closed-source. Moreover, the corresponding FAP models exhibit limited generalization and adaptation ability.To overcome these limitations, we introduce the first large-scale FAP dataset LiveBeauty specifically designed for live video scenarios wherein face images may be real-time processed for aesthetics purposes.10,000 face images are collected directly from a live streaming platform, with 200,000 corresponding attractiveness annotations obtained from a well-devised subjective experiment, making LiveBeauty the largest open-access FAP dataset. Based on the built dataset, a novel FAP method named Facial Prior Enhanced Multi-modal model (FPEM) is proposed to measure the attractiveness of facial images.Extensive experiments conducted on both LiveBeauty and other open-source FAP datasets demonstrate that our proposed method achieves state-of-the-art performance. The dataset will be released at https://github.com/Estella-LH/FPEM.</p>
            <p id="subjects-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" onclick="foldPdfKimi('Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="panel paper" keywords="distillation,dataset,vision,language,prototypes,text,prototype,category,datasets,information">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype_ICCV_2025_paper.html" target="_blank" title="216/263"><span class="index notranslate">#216</span></a>
                <a id="title-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="title-link" href="/venue/Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" target="_blank">Dataset Distillation via Vision-Language Category Prototype</a>
                <a id="pdf-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yawen Zou" target="_blank">Yawen Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guang Li" target="_blank">Guang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Duo Su" target="_blank">Duo Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zi Wang" target="_blank">Zi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Yu" target="_blank">Jun Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Zhang" target="_blank">Chao Zhang</a>
            </p>
            <p id="summary-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="summary">Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source vision-language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/.</p>
            <p id="subjects-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" onclick="foldPdfKimi('Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="panel paper" keywords="motion,motions,wild,difficulty,video,flawed,plug,capture,module,physical">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions_ICCV_2025_paper.html" target="_blank" title="217/263"><span class="index notranslate">#217</span></a>
                <a id="title-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="title-link" href="/venue/Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" target="_blank">A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions</a>
                <a id="pdf-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Youliang Zhang" target="_blank">Youliang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronghui Li" target="_blank">Ronghui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yachao Zhang" target="_blank">Yachao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Pan" target="_blank">Liang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingbo Wang" target="_blank">Jingbo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yebin Liu" target="_blank">Yebin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiu Li" target="_blank">Xiu Li</a>
            </p>
            <p id="summary-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="summary">Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions; and propose a physics-based motion transfer module (PTM), which employs a prior injected pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture, which also excels in motion generation tasks. Finally, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public datasets. Our project page is : https://physicalmotionrestoration.github.io/</p>
            <p id="subjects-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="panel paper" keywords="stereo,video,matching,temporally,temporal,consistent,disparities,upsampling,monocular,architectural">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching_ICCV_2025_paper.html" target="_blank" title="218/263"><span class="index notranslate">#218</span></a>
                <a id="title-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="title-link" href="/venue/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" target="_blank">Stereo Any Video: Temporally Consistent Stereo Matching</a>
                <a id="pdf-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junpeng Jing" target="_blank">Junpeng Jing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weixun Luo" target="_blank">Weixun Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Mao" target="_blank">Ye Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Krystian Mikolajczyk" target="_blank">Krystian Mikolajczyk</a>
            </p>
            <p id="summary-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="summary">This paper introduces Stereo Any Video, a powerful framework for video stereo matching. It can estimate spatially accurate and temporally consistent disparities without relying on auxiliary information such as camera poses or optical flow. The strong capability is driven by rich priors from monocular video depth models, which are integrated with convolutional features to produce stable representations. To further enhance performance, key architectural innovations are introduced: all-to-all-pairs correlation, which constructs smooth and robust matching cost volumes, and temporal convex upsampling, which improves temporal coherence. These components collectively ensure robustness, accuracy, and temporal consistency, setting a new standard in video stereo matching. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple datasets both qualitatively and quantitatively in zero-shot settings, as well as strong generalization to real-world indoor and outdoor scenarios. Code and models will be publicly released.</p>
            <p id="subjects-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" onclick="foldPdfKimi('Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="panel paper" keywords="restoration,video,codebook,facial,variational,face,dirichlet,constrained,pretrained,quality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration_ICCV_2025_paper.html" target="_blank" title="219/263"><span class="index notranslate">#219</span></a>
                <a id="title-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="title-link" href="/venue/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" target="_blank">Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration</a>
                <a id="pdf-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Baoyou Chen" target="_blank">Baoyou Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ce Liu" target="_blank">Ce Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weihao Yuan" target="_blank">Weihao Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zilong Dong" target="_blank">Zilong Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Zhu" target="_blank">Siyu Zhu</a>
            </p>
            <p id="summary-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="summary">Video face restoration faces a critical challenge in maintaining temporal consistency while recovering fine facial details from degraded inputs. This paper presents a novel approach that extends Vector-Quantized Variational Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a video restoration framework through variational latent space modeling. Our key innovation lies in reformulating discrete codebook representations as Dirichlet-distributed continuous variables, enabling probabilistic transitions between facial features across frames. A spatio-temporal Transformer architecture jointly models inter-frame dependencies and predicts latent distributions, while a Laplacian-constrained reconstruction loss combined with perceptual (LPIPS) regularization enhances both pixel accuracy and visual quality. Comprehensive evaluations on blind face restoration, video inpainting, and facial colorization tasks demonstrate state-of-the-art performance. This work establishes an effective paradigm for adapting intensive image priors, pretrained on high-quality images, to video restoration while addressing the critical challenge of flicker artifacts. The source code has been open-sourced and is available at https://github.com/fudan-generative-vision/DicFace.</p>
            <p id="subjects-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="panel paper" keywords="csl,confidence,pseudo,label,predictions,discarding,selection,semantic,segmentation,revisiting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation_ICCV_2025_paper.html" target="_blank" title="220/263"><span class="index notranslate">#220</span></a>
                <a id="title-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" target="_blank">When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation</a>
                <a id="pdf-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pan Liu" target="_blank">Pan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinshi Liu" target="_blank">Jinshi Liu</a>
            </p>
            <p id="summary-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="summary">While significant advances exist in pseudo-label generation for semi-supervised semantic segmentation, pseudo-label selection remains understudied. Existing methods typically use fixed confidence thresholds to retain high-confidence predictions as pseudo-labels. However, these methods cannot cope with network overconfidence tendency, where correct and incorrect predictions overlap significantly in high-confidence regions, making separation challenging and amplifying model cognitive bias. Meanwhile, the direct discarding of low-confidence predictions disrupts spatial-semantic continuity, causing critical context loss. We propose Confidence Separable Learning (CSL) to address these limitations. CSL formulates pseudo-label selection as a convex optimization problem within the confidence distribution feature space, establishing sample-specific decision boundaries to distinguish reliable from unreliable predictions. Additionally, CSL introduces random masking of reliable pixels to guide the network in learning contextual relationships from low-reliability regions, thereby mitigating the adverse effects of discarding uncertain predictions. Extensive experimental results on the Pascal, Cityscapes, and COCO benchmarks show that CSL performs favorably against state-of-the-art methods. Code and model weights are available at: https://github.com/PanLiuCSU/CSL.</p>
            <p id="subjects-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="panel paper" keywords="scendi,text,prompt,clip,diversity,schur,score,complement,image,embeddings">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP_ICCV_2025_paper.html" target="_blank" title="221/263"><span class="index notranslate">#221</span></a>
                <a id="title-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="title-link" href="/venue/Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" target="_blank">Scendi Score: Prompt-Aware Diversity Evaluation via Schur Complement of CLIP Embeddings</a>
                <a id="pdf-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Azim Ospanov" target="_blank">Azim Ospanov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Jalali" target="_blank">Mohammad Jalali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Farzan Farnia" target="_blank">Farzan Farnia</a>
            </p>
            <p id="summary-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="summary">The use of CLIP embeddings to assess the fidelity of samples produced by text-to-image generative models has been extensively explored in the literature. While the widely adopted CLIPScore, derived from the cosine similarity of text and image embeddings, effectively measures the alignment of a generated image, it does not quantify the diversity of images generated by a text-to-image model. In this work, we extend the application of CLIP embeddings to quantify and interpret the intrinsic diversity of text-to-image models, which are responsible for generating diverse images from similar text prompts, which we refer to as prompt-aware diversity. To achieve this, we propose a decomposition of the CLIP-based kernel covariance matrix of image data into text-based and non-text-based components. Using the Schur complement of the joint image-text kernel covariance matrix, we perform this decomposition and define the matrix-based entropy of the decomposed component as the Schur Complement ENtopy DIversity (Scendi) score, as a measure of the prompt-aware diversity for prompt-guided generative models. Additionally, we discuss the application of the Schur complement-based decomposition to nullify the influence of a given prompt on the CLIP embedding of an image, enabling focus or defocus of the embedded vectors on specific objects. We present several numerical results that apply our proposed Scendi score to evaluate text-to-image and LLM (text-to-text) models. Our numerical results indicate the success of the Scendi score in capturing the intrinsic diversity of prompt-guided generative models. The codebase is available at https://github.com/aziksh-ospanov/scendi-score.</p>
            <p id="subjects-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" onclick="foldPdfKimi('Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="panel paper" keywords="disease,grounding,tokens,dap,regions,visual,medical,seeing,rethinking,textual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual_ICCV_2025_paper.html" target="_blank" title="222/263"><span class="index notranslate">#222</span></a>
                <a id="title-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="title-link" href="/venue/Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" target="_blank">Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</a>
                <a id="pdf-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ta Duc Huy" target="_blank">Ta Duc Huy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Duy Anh Huynh" target="_blank">Duy Anh Huynh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yutong Xie" target="_blank">Yutong Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuankai Qi" target="_blank">Yuankai Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Chen" target="_blank">Qi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Phi Le Nguyen" target="_blank">Phi Le Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sen Kim Tran" target="_blank">Sen Kim Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Son Lam Phung" target="_blank">Son Lam Phung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anton van den Hengel" target="_blank">Anton van den Hengel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhibin Liao" target="_blank">Zhibin Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minh-Son To" target="_blank">Minh-Son To</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johan W. Verjans" target="_blank">Johan W. Verjans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vu Minh Hieu Phan" target="_blank">Vu Minh Hieu Phan</a>
            </p>
            <p id="summary-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="summary">Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.</p>
            <p id="subjects-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" onclick="foldPdfKimi('Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="panel paper" keywords="moe,stereo,vfms,matching,experts,smoestereo,lora,robustness,mixture,domain">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts_ICCV_2025_paper.html" target="_blank" title="223/263"><span class="index notranslate">#223</span></a>
                <a id="title-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="title-link" href="/venue/Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" target="_blank">Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts</a>
                <a id="pdf-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Wang" target="_blank">Yun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longguang Wang" target="_blank">Longguang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenghao Zhang" target="_blank">Chenghao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongjian Zhang" target="_blank">Yongjian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanjie Zhang" target="_blank">Zhanjie Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ao Ma" target="_blank">Ao Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenyou Fan" target="_blank">Chenyou Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tin Lun Lam" target="_blank">Tin Lun Lam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Hu" target="_blank">Junjie Hu</a>
            </p>
            <p id="summary-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="summary">Recently, learning-based stereo matching networks have advanced significantly.However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets.Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge.To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules.SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction.Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at https://github.com/cocowy1/SMoE-Stereo.</p>
            <p id="subjects-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="panel paper" keywords="dimo,motion,motions,diverse,latent,embed,generation,arbitrary,objects,shared">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects_ICCV_2025_paper.html" target="_blank" title="224/263"><span class="index notranslate">#224</span></a>
                <a id="title-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="title-link" href="/venue/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" target="_blank">DIMO: Diverse 3D Motion Generation for Arbitrary Objects</a>
                <a id="pdf-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linzhan Mou" target="_blank">Linzhan Mou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahui Lei" target="_blank">Jiahui Lei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Wang" target="_blank">Chen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingjie Liu" target="_blank">Lingjie Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kostas Daniilidis" target="_blank">Kostas Daniilidis</a>
            </p>
            <p id="summary-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="summary">We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation.</p>
            <p id="subjects-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" onclick="foldPdfKimi('Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="panel paper" keywords="object,video,segmentation,refinement,oasis,davis,boundary,structure,svos,youtubevos">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation_ICCV_2025_paper.html" target="_blank" title="225/263"><span class="index notranslate">#225</span></a>
                <a id="title-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" target="_blank">Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation</a>
                <a id="pdf-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guanyi Qin" target="_blank">Guanyi Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyue Wang" target="_blank">Ziyue Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daiyun Shen" target="_blank">Daiyun Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haofeng Liu" target="_blank">Haofeng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hantao Zhou" target="_blank">Hantao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junde Wu" target="_blank">Junde Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runze Hu" target="_blank">Runze Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueming Jin" target="_blank">Yueming Jin</a>
            </p>
            <p id="summary-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="summary">Given an object mask, Semi-supervised Video Object Segmentation (SVOS) technique aims to track and segment the object across video frames, serving as a fundamental task in computer vision. Although recent memory-based methods demonstrate potential, they often struggle with scenes involving occlusion, particularly in handling object interactions and high feature similarity. To address these issues and meet the real-time processing requirements of downstream applications, in this paper, we propose a novel bOundary Amendment video object Segmentation method with Inherent Structure refinement, hereby named OASIS. Specifically, a lightweight structure refinement module is proposed to enhance segmentation accuracy. With the fusion of rough edge priors captured by the Canny filter and stored object features, the module can generate an object-level structure map and refine the representations by highlighting boundary features. Evidential learning for uncertainty estimation is introduced to further address challenges in occluded regions. The proposed method, OASIS, maintains an efficient design, yet extensive experiments on challenging benchmarks demonstrate its superior performance and competitive inference speed compared to other state-of-the-art methods, i.e., achieving the F values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6 (vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive speed of 48 FPS on DAVIS.</p>
            <p id="subjects-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="panel paper" keywords="aim,amending,interpretability,masking,genuine,spurious,features,self,supervised,inherent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking_ICCV_2025_paper.html" target="_blank" title="226/263"><span class="index notranslate">#226</span></a>
                <a id="title-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="title-link" href="/venue/Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" target="_blank">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a>
                <a id="pdf-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Eyad Alshami" target="_blank">Eyad Alshami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shashank Agnihotri" target="_blank">Shashank Agnihotri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernt Schiele" target="_blank">Bernt Schiele</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Margret Keuper" target="_blank">Margret Keuper</a>
            </p>
            <p id="summary-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="summary">It has been observed that deep neural networks (DNNs) often use both genuine as well as spurious features.In this work, we propose "Amending Inherent Interpretability via Self-Supervised Masking" (AIM), a simple yet surprisingly effective method that promotes the network's utilization of genuine features over spurious alternatives without requiring additional annotations.In particular, AIM uses features at multiple encoding stages to guide a self-supervised, sample-specific feature-masking process. As a result, AIM allows training well-performing and inherently interpretable models that faithfully summarize the decision process.When tested on challenging datasets designed to assess reliance on spurious features and out-of-domain generalization, AIM networks demonstrate significant dual benefits: Evaluations show that AIM improves interpretability, as measured by the Energy Pointing Game (EPG) score, by ~6-37%, while simultaneously enhancing accuracy by ~10-40%. These impressive performance gains are further validated on the standard in-domain CUB-200 dataset for fine-grained classification. The results provide compelling evidence supporting our hypothesis that AIM finds genuine and meaningful features that directly contribute to its improved human interpretability.</p>
            <p id="subjects-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" onclick="foldPdfKimi('Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="panel paper" keywords="talking,ggtalker,priors,head,generalizable,adaptation,systhesis,identity,heads,audio">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific_ICCV_2025_paper.html" target="_blank" title="227/263"><span class="index notranslate">#227</span></a>
                <a id="title-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="title-link" href="/venue/Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" target="_blank">GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation</a>
                <a id="pdf-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Hu" target="_blank">Wentao Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shunkai Li" target="_blank">Shunkai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqiao Peng" target="_blank">Ziqiao Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoxian Zhang" target="_blank">Haoxian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Shi" target="_blank">Fan Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoqiang Liu" target="_blank">Xiaoqiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Wan" target="_blank">Pengfei Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Zhang" target="_blank">Di Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Tian" target="_blank">Hui Tian</a>
            </p>
            <p id="summary-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="summary">Creating high-quality, generalizable speech-driven 3D talking heads remains a persistent challenge. Previous methods achieve satisfactory results for fixed viewpoints and small-scale audio variations, but they struggle with large head rotations and out-of-distribution (OOD) audio. Moreover, they are constrained by the need for time-consuming, identity-specific training. We believe the core issue lies in the lack of sufficient 3D priors, which limits the extrapolation capabilities of synthesized talking heads. To address this, we propose GGTalker, which synthesizes talking heads through a combination of generalizable priors and identity-specific adaptation. We introduce a two-stage Prior-Adaptation training strategy to learn Gaussian head priors and adapt to individual characteristics. We train Audio-Expression and Expression-Visual priors to capture the universal patterns of lip movements and the general distribution of head textures. During the Customized Adaptation, individual speaking styles and texture details are precisely modeled. Additionally, we introduce a color MLP to generate fine-grained, motion-aligned textures and a Body Inpainter to blend rendered results with the background, producing indistinguishable, photorealistic video frames. Comprehensive experiments show that GGTalker achieves state-of-the-art performance in rendering quality, 3D consistency, lip-sync accuracy, and training efficiency.</p>
            <p id="subjects-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" onclick="foldPdfKimi('Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="panel paper" keywords="3dgs,sfm,splatting,reconstruction,camera,clouds,optimization,initialization,restricts,coarsely">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images_ICCV_2025_paper.html" target="_blank" title="228/263"><span class="index notranslate">#228</span></a>
                <a id="title-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="title-link" href="/venue/Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" target="_blank">A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds</a>
                <a id="pdf-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jizong Peng" target="_blank">Jizong Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tze Ho Elden Tse" target="_blank">Tze Ho Elden Tse</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Xu" target="_blank">Kai Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenchao Gao" target="_blank">Wenchao Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angela Yao" target="_blank">Angela Yao</a>
            </p>
            <p id="summary-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="summary">3D Gaussian Splatting (3DGS) is a powerful reconstruction technique; however, it requires initialization from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks.</p>
            <p id="subjects-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" onclick="foldPdfKimi('Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="panel paper" keywords="spatial,intelligence,imagery,scalable,scale,objects365,data,lifting,pipeline,camera">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting_ICCV_2025_paper.html" target="_blank" title="229/263"><span class="index notranslate">#229</span></a>
                <a id="title-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="title-link" href="/venue/Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" target="_blank">Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting</a>
                <a id="pdf-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyu Miao" target="_blank">Xingyu Miao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoran Duan" target="_blank">Haoran Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanhao Qian" target="_blank">Quanhao Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiuniu Wang" target="_blank">Jiuniu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Long" target="_blank">Yang Long</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling Shao" target="_blank">Ling Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deli Zhao" target="_blank">Deli Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Xu" target="_blank">Ran Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gongjie Zhang" target="_blank">Gongjie Zhang</a>
            </p>
            <p id="summary-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="summary">Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation. In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations -- including point clouds, camera poses, depth maps, and pseudo-RGBD -- via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence. We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments.</p>
            <p id="subjects-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" onclick="foldPdfKimi('Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="panel paper" keywords="hoi,amodal,regional,inpainting,human,contact,occlusions,completions,completion,object">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting_ICCV_2025_paper.html" target="_blank" title="230/263"><span class="index notranslate">#230</span></a>
                <a id="title-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="title-link" href="/venue/Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" target="_blank">Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting</a>
                <a id="pdf-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seunggeun Chi" target="_blank">Seunggeun Chi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enna Sachdeva" target="_blank">Enna Sachdeva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pin-Hao Huang" target="_blank">Pin-Hao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwonjoon Lee" target="_blank">Kwonjoon Lee</a>
            </p>
            <p id="summary-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="summary">Amodal completion, the task of inferring the complete appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, including pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios due to their limited understanding of HOI. To address this challenge, we propose a novel approach that leverages physical prior knowledge alongside a specialized multi-regional inpainting technique tailored for HOI. By incorporating physical constraints derived from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to reside, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method employs customized denoising strategies across these regions within a diffusion model, thereby enhancing the accuracy and realism of generated completions in both shape and visual detail. Experimental results demonstrate that our approach substantially outperforms existing methods in HOI scenarios, advancing machine perception toward a more human-like understanding of dynamic environments. Furthermore, we show that our pipeline remains robust even without ground-truth contact annotations, broadening its applicability to tasks such as 3D reconstruction and novel view/pose synthesis.</p>
            <p id="subjects-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" onclick="foldPdfKimi('Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="panel paper" keywords="sega,layout,stepwise,planning,coarse,content,aware,reasoning,brucew91,generation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with_ICCV_2025_paper.html" target="_blank" title="231/263"><span class="index notranslate">#231</span></a>
                <a id="title-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="title-link" href="/venue/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" target="_blank">SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior</a>
                <a id="pdf-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoran Wang" target="_blank">Haoran Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zhao" target="_blank">Bo Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinghui Wang" target="_blank">Jinghui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanzhang Wang" target="_blank">Hanzhang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Yang" target="_blank">Huan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Ji" target="_blank">Wei Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Liu" target="_blank">Hao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyan Xiao" target="_blank">Xinyan Xiao</a>
            </p>
            <p id="summary-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="summary">In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io</p>
            <p id="subjects-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" onclick="foldPdfKimi('Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="panel paper" keywords="derm1m,clinical,dermatology,skin,dermlip,dataset,vision,ontology,medical,shot">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.html" target="_blank" title="232/263"><span class="index notranslate">#232</span></a>
                <a id="title-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="title-link" href="/venue/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" target="_blank">Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology</a>
                <a id="pdf-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Yan" target="_blank">Siyuan Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Hu" target="_blank">Ming Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiwen Jiang" target="_blank">Yiwen Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xieji Li" target="_blank">Xieji Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Fei" target="_blank">Hao Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Tschandl" target="_blank">Philipp Tschandl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harald Kittler" target="_blank">Harald Kittler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongyuan Ge" target="_blank">Zongyuan Ge</a>
            </p>
            <p id="summary-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="summary">The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M's potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code are available at https://github.com/SiyuanYan1/Derm1M.</p>
            <p id="subjects-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" onclick="foldPdfKimi('Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="panel paper" keywords="turbovsr,1080p,video,super,resolution,upscalers,vsr,fantastic,frame,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them_ICCV_2025_paper.html" target="_blank" title="233/263"><span class="index notranslate">#233</span></a>
                <a id="title-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="title-link" href="/venue/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" target="_blank">TurboVSR: Fantastic Video Upscalers and Where to Find Them</a>
                <a id="pdf-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongdao Wang" target="_blank">Zhongdao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guodongfang Zhao" target="_blank">Guodongfang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingjing Ren" target="_blank">Jingjing Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bailan Feng" target="_blank">Bailan Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shifeng Zhang" target="_blank">Shifeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbo Li" target="_blank">Wenbo Li</a>
            </p>
            <p id="summary-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="summary">Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32x32x8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648x2048) image SR show surprising fine details.</p>
            <p id="subjects-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" onclick="foldPdfKimi('Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="panel paper" keywords="kaleidoscopic,pose,attack,estimation,segments,textures,fold,background,viewpoints,kba">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry_ICCV_2025_paper.html" target="_blank" title="234/263"><span class="index notranslate">#234</span></a>
                <a id="title-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="title-link" href="/venue/Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" target="_blank">Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures</a>
                <a id="pdf-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlong Ding" target="_blank">Xinlong Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongwei Yu" target="_blank">Hongwei Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Li" target="_blank">Jiawei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feifan Li" target="_blank">Feifan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Shang" target="_blank">Yu Shang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bochao Zou" target="_blank">Bochao Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huimin Ma" target="_blank">Huimin Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiansheng Chen" target="_blank">Jiansheng Chen</a>
            </p>
            <p id="summary-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="summary">Camera pose estimation is a fundamental computer vision task that is essential for applications like visual localization and multi-view stereo reconstruction. In the object-centric scenarios with sparse inputs, the accuracy of pose estimation can be significantly influenced by background textures that occupy major portions of the images across different viewpoints. In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which uses identical segments to form discs with multi-fold radial symmetry. These discs maintain high similarity across different viewpoints, enabling effective attacks on pose estimation models even with natural texture segments. Additionally, a projected orientation consistency loss is proposed to optimize the kaleidoscopic segments, leading to a significant enhancement in the attack effectiveness. Experimental results show that optimized adversarial kaleidoscopic backgrounds can effectively attack various camera pose estimation models.</p>
            <p id="subjects-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" onclick="foldPdfKimi('Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="panel paper" keywords="dpo,rdpo,preference,dispreferred,step,diffusion,aligning,wise,alignment,rethinking">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks_ICCV_2025_paper.html" target="_blank" title="235/263"><span class="index notranslate">#235</span></a>
                <a id="title-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="title-link" href="/venue/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" target="_blank">Rethinking DPO-style Diffusion Aligning Frameworks</a>
                <a id="pdf-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xun Wu" target="_blank">Xun Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohan Huang" target="_blank">Shaohan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingjie Jiang" target="_blank">Lingjie Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Furu Wei" target="_blank">Furu Wei</a>
            </p>
            <p id="summary-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="summary">Direct preference optimization (DPO) has shown success in aligning diffusion models with human preference. However, We identify two potential risks for existing DPO algorithms: First, current DPO methods for estimating the rewards of step-wise intermediate samples are biased, leading to inaccurate preference ordering for step-wise optimization. Second, existing DPO methods may inadvertently increase the sampling probabilities of dispreferred samples, potentially introducing application risks. To address these issues, we propose Revised Direct Preference Optimization (RDPO), a simple but effective step-wise DPO-based text-to-image diffusion model alignment method. By designing a more theoretically grounded and efficient intermediate-step reward estimation and introducing an additional regularization terms to constrain the sampling probability of dispreferred samples, RDPO can achieve more effective and stable text-to-image alignment performance. Our experiments on two datasets, with base models including Stable Diffusion v1.5 and SDXL, demonstrate that RDPO can effectively learn and construct reward signals for each step of the model, improving alignment performance while ensuring better generalization.</p>
            <p id="subjects-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" onclick="foldPdfKimi('Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="panel paper" keywords="foreground,uod,unioncut,discovery,unionseg,union,objects,object,ensemble,unsupervised">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery_ICCV_2025_paper.html" target="_blank" title="236/263"><span class="index notranslate">#236</span></a>
                <a id="title-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="title-link" href="/venue/Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" target="_blank">Ensemble Foreground Management for Unsupervised Object Discovery</a>
                <a id="pdf-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziling Wu" target="_blank">Ziling Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Armaghan Moemeni" target="_blank">Armaghan Moemeni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Praminda Caleb-Solly" target="_blank">Praminda Caleb-Solly</a>
            </p>
            <p id="summary-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="summary">Unsupervised object discovery (UOD) aims to detect and segment objects in 2D images without handcrafted annotations. Recent progress in self-supervised representation learning has led to some success in UOD algorithms. However, the absence of ground truth provides existing UOD methods with two challenges: 1) determining if a discovered region is foreground or background, and 2) knowing how many objects remain undiscovered. To address these two problems, previous solutions rely on foreground priors to distinguish if the discovered region is foreground, and conduct one or fixed iterations of discovery. However, the existing foreground priors are heuristic and not always robust, and a fixed number of discoveries leads to under or over-segmentation, since the number of objects in images varies. This paper introduces UnionCut, a robust and well-grounded foreground prior based on min-cut and ensemble methods that detects the union of foreground areas of an image, allowing UOD algorithms to identify foreground objects and stop discovery once the majority of the foreground union in the image is segmented. In addition, we propose UnionSeg, a distilled transformer of UnionCut that outputs the foreground union more efficiently and accurately. Our experiments show that by combining with UnionCut or UnionSeg, previous state-of-the-art UOD methods witness an increase in the performance of single object discovery, saliency detection and self-supervised instance segmentation on various benchmarks. The code is available at https://github.com/YFaris/UnionCut.</p>
            <p id="subjects-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" onclick="foldPdfKimi('Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="panel paper" keywords="infu,identity,infiniteyou,dits,recrafting,flexible,image,generation,spms,ameliorates">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity_ICCV_2025_paper.html" target="_blank" title="237/263"><span class="index notranslate">#237</span></a>
                <a id="title-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="title-link" href="/venue/Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" target="_blank">InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity</a>
                <a id="pdf-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liming Jiang" target="_blank">Liming Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Yan" target="_blank">Qing Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yumin Jia" target="_blank">Yumin Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zichuan Liu" target="_blank">Zichuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Kang" target="_blank">Hao Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Lu" target="_blank">Xin Lu</a>
            </p>
            <p id="summary-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="summary">Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.</p>
            <p id="subjects-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" onclick="foldPdfKimi('Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="panel paper" keywords="sail,mllms,vision,scalability,vit,transformer,modular,language,multimodal,visual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with_ICCV_2025_paper.html" target="_blank" title="238/263"><span class="index notranslate">#238</span></a>
                <a id="title-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="title-link" href="/venue/Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" target="_blank">The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer</a>
                <a id="pdf-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weixian Lei" target="_blank">Weixian Lei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacong Wang" target="_blank">Jiacong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haochen Wang" target="_blank">Haochen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangtai Li" target="_blank">Xiangtai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Hao Liew" target="_blank">Jun Hao Liew</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiashi Feng" target="_blank">Jiashi Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zilong Huang" target="_blank">Zilong Huang</a>
            </p>
            <p id="summary-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="summary">This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal rotary position embedding to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.</p>
            <p id="subjects-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" onclick="foldPdfKimi('Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="panel paper" keywords="counterfactual,domain,counterpc,realignment,adaptation,feature,category,clouds,alignment,graspnetpc">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.html" target="_blank" title="239/263"><span class="index notranslate">#239</span></a>
                <a id="title-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="title-link" href="/venue/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" target="_blank">CounterPC: Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds</a>
                <a id="pdf-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Yang" target="_blank">Feng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yichao Cao" target="_blank">Yichao Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiu Su" target="_blank">Xiu Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Niu" target="_blank">Dan Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanpeng Li" target="_blank">Xuanpeng Li</a>
            </p>
            <p id="summary-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="summary">Understanding real-world 3D point clouds is challenging due to domain shifts, causing geometric variations like density changes, noise, and occlusions. The key challenge is disentangling domain-invariant semantics from domain-specific geometric variations, as point clouds exhibit local inconsistency and global redundancy, making direct alignment ineffective. To address this, we propose CounterPC, a counterfactual intervention-based domain adaptation framework, which formulates domain adaptation within a causal latent space, identifying category-discriminative features entangled with intra-class geometric variation confounders. Through counterfactual interventions, we generate counterfactual target samples that retain domain-specific characteristics while improving class separation, mitigating domain bias for optimal feature transfer. To achieve this, we introduce two key modules: i) Joint Distribution Alignment, which leverages 3D foundation models (3D-FMs) and a self-supervised autoregressive generative prediction task to unify feature alignment, and ii) Counterfactual Feature Realignment, which employs Optimal Transport (OT) to align category-relevant and category-irrelevant feature distributions, ensuring robust sample-level adaptation while preserving domain and category properties. CounterPC outperforms state-of-the-art methods on PointDA and GraspNetPC-10, achieving accuracy improvements of 4.7 and 3.6, respectively. Code and pre-trained weights will be publicly released.</p>
            <p id="subjects-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" onclick="foldPdfKimi('Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="panel paper" keywords="geospatial,geobench,vlms,vlm,challenges,object,tasks,onevision,vision,specific">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks_ICCV_2025_paper.html" target="_blank" title="240/263"><span class="index notranslate">#240</span></a>
                <a id="title-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="title-link" href="/venue/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" target="_blank">GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks</a>
                <a id="pdf-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Danish" target="_blank">Muhammad Danish</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Akhtar Munir" target="_blank">Muhammad Akhtar Munir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Syed Roshaan Ali Shah" target="_blank">Syed Roshaan Ali Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kartik Kuckreja" target="_blank">Kartik Kuckreja</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fahad Shahbaz Khan" target="_blank">Fahad Shahbaz Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paolo Fraccaro" target="_blank">Paolo Fraccaro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre Lacoste" target="_blank">Alexandre Lacoste</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Salman Khan" target="_blank">Salman Khan</a>
            </p>
            <p id="summary-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="summary">While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications.Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management.Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery.To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales.We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark will be publicly available.</p>
            <p id="subjects-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" onclick="foldPdfKimi('Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="panel paper" keywords="captioning,caption,pseudo,embodied,captions,fine,image,coherent,tune,consensus">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image_ICCV_2025_paper.html" target="_blank" title="241/263"><span class="index notranslate">#241</span></a>
                <a id="title-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="title-link" href="/venue/Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" target="_blank">Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</a>
                <a id="pdf-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tommaso Galliena" target="_blank">Tommaso Galliena</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommaso Apicella" target="_blank">Tommaso Apicella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Rosa" target="_blank">Stefano Rosa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pietro Morerio" target="_blank">Pietro Morerio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessio Del Bue" target="_blank">Alessio Del Bue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Natale" target="_blank">Lorenzo Natale</a>
            </p>
            <p id="summary-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="summary">We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning.</p>
            <p id="subjects-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" onclick="foldPdfKimi('Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="panel paper" keywords="legion,artifact,synthetic,image,synthscars,annotations,detection,explain,textual,images">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection_ICCV_2025_paper.html" target="_blank" title="242/263"><span class="index notranslate">#242</span></a>
                <a id="title-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="title-link" href="/venue/Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" target="_blank">LEGION: Learning to Ground and Explain for Synthetic Image Detection</a>
                <a id="pdf-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hengrui Kang" target="_blank">Hengrui Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Wen" target="_blank">Siwei Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Wen" target="_blank">Zichen Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyan Ye" target="_blank">Junyan Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijia Li" target="_blank">Weijia Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peilin Feng" target="_blank">Peilin Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baichuan Zhou" target="_blank">Baichuan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Wang" target="_blank">Bin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dahua Lin" target="_blank">Dahua Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Zhang" target="_blank">Linfeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Conghui He" target="_blank">Conghui He</a>
            </p>
            <p id="summary-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="summary">The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. More information about LEGION can be found at https://opendatalab.github.io/LEGION.</p>
            <p id="subjects-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" onclick="foldPdfKimi('Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="panel paper" keywords="text,pab,person,walking,anomaly,pose,benchmark,search,image,anomalies">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly_ICCV_2025_paper.html" target="_blank" title="243/263"><span class="index notranslate">#243</span></a>
                <a id="title-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="title-link" href="/venue/Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" target="_blank">Beyond Walking: A Large-Scale Image-Text Benchmark for Text-based Person Anomaly Search</a>
                <a id="pdf-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuyu Yang" target="_blank">Shuyu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaxiong Wang" target="_blank">Yaxiong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Zhu" target="_blank">Li Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhedong Zheng" target="_blank">Zhedong Zheng</a>
            </p>
            <p id="summary-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="summary">Text-based person search aims to retrieve specific individuals across camera networks using natural language descriptions. However, current benchmarks often exhibit biases towards common actions like walking or standing, neglecting the critical need for identifying abnormal behaviors in real-world scenarios. To meet such demands, we propose a new task, text-based person anomaly search, locating pedestrians engaged in both routine or anomalous activities via text. To enable the training and evaluation of this new task, we construct a large-scale image-text Pedestrian Anomaly Behavior (PAB) benchmark, featuring a broad spectrum of actions, e.g., running, performing, playing soccer, and the corresponding anomalies, e.g., lying, being hit, and falling of the same identity. The training set of PAB comprises 1,013,605 synthesized image-text pairs of both normalities and anomalies, while the test set includes 1,978 real-world image-text pairs. To validate the potential of PAB, we introduce a cross-modal pose-aware framework, which integrates human pose patterns with identity-based hard negative pair sampling. Extensive experiments on the proposed benchmark show that synthetic training data facilitates the fine-grained behavior retrieval, and the proposed pose-aware method arrives at 84.93% recall@1 accuracy, surpassing other competitive methods.</p>
            <p id="subjects-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" onclick="foldPdfKimi('Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="panel paper" keywords="scene,clip,diffusion,vocabulary,proxy,dedos,denoising,queries,segmentation,images">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion_ICCV_2025_paper.html" target="_blank" title="244/263"><span class="index notranslate">#244</span></a>
                <a id="title-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="title-link" href="/venue/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" target="_blank">Images as Noisy Labels: Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation</a>
                <a id="pdf-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Li" target="_blank">Fan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanbin Wang" target="_blank">Xuanbin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Wang" target="_blank">Xuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxiang Zhang" target="_blank">Zhaoxiang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuelei Xu" target="_blank">Yuelei Xu</a>
            </p>
            <p id="summary-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="summary">Recently, open-vocabulary semantic segmentation has garnered growing attention. Most current methods leverage vision-language models like CLIP to recognize unseen categories through their zero-shot capabilities. However, CLIP struggles to establish potential spatial dependencies among scene objects due to its holistic pre-training objective, causing sub-optimal results. In this paper, we propose a DEnoising learning framework based on the Diffusion model for Open-vocabulary semantic Segmentation, called DEDOS, which is aimed at constructing the scene skeleton. Motivation stems from the fact that diffusion models incorporate not only the visual appearance of objects but also embed rich scene spatial priors. Our core idea is to view images as labels embedded with "noise"--non-essential details for perceptual tasks--and to disentangle the intrinsic scene prior from the diffusion feature during the denoising process of the images. Specifically, to fully harness the scene prior knowledge of the diffusion model, we introduce learnable proxy queries during the denoising process. Meanwhile, we leverage the robustness of CLIP features to texture shifts as supervision, guiding proxy queries to focus on constructing the scene skeleton and avoiding interference from texture information in the diffusion feature space. Finally, we enhance spatial understanding within CLIP features using proxy queries, which also serve as an interface for multi-level interaction between text and visual modalities. Extensive experiments validate the effectiveness of our method, experimental results on five standard benchmarks have shown that DEDOS achieves state-of-the-art performance. We will make the code publicly available.</p>
            <p id="subjects-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" onclick="foldPdfKimi('Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="panel paper" keywords="temporally,residualvit,temporal,dense,foundation,video,tasks,frame,frames,features">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding_ICCV_2025_paper.html" target="_blank" title="245/263"><span class="index notranslate">#245</span></a>
                <a id="title-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="title-link" href="/venue/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" target="_blank">ResidualViT for Efficient Temporally Dense Video Encoding</a>
                <a id="pdf-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mattia Soldan" target="_blank">Mattia Soldan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Caba Heilbron" target="_blank">Fabian Caba Heilbron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernard Ghanem" target="_blank">Bernard Ghanem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Josef Sivic" target="_blank">Josef Sivic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bryan Russell" target="_blank">Bryan Russell</a>
            </p>
            <p id="summary-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="summary">Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require "temporally dense" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.</p>
            <p id="subjects-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" onclick="foldPdfKimi('Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="panel paper" keywords="4dgs,gaussian,gaussians,mega,splatting,memory,scenes,storage,color,dynamic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes_ICCV_2025_paper.html" target="_blank" title="246/263"><span class="index notranslate">#246</span></a>
                <a id="title-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="title-link" href="/venue/Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" target="_blank">MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes</a>
                <a id="pdf-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinjie Zhang" target="_blank">Xinjie Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhening Liu" target="_blank">Zhening Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Zhang" target="_blank">Yifan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingtong Ge" target="_blank">Xingtong Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dailan He" target="_blank">Dailan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tongda Xu" target="_blank">Tongda Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Wang" target="_blank">Yan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zehong Lin" target="_blank">Zehong Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuicheng Yan" target="_blank">Shuicheng Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Zhang" target="_blank">Jun Zhang</a>
            </p>
            <p id="summary-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="summary">4D Gaussian Splatting (4DGS) has recently emerged as a promising technique for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering speeds. Despite its advantages, 4DGS faces significant challenges, notably the requirement of millions of 4D Gaussians, each with extensive associated attributes, leading to substantial memory and storage cost. This paper introduces a memory-efficient framework for 4DGS. We streamline the color attribute by decomposing it into a per-Gaussian direct color component with only 3 parameters and a shared lightweight alternating current color predictor. This approach eliminates the need for spherical harmonics coefficients, which typically involve up to 144 parameters in classic 4DGS, thereby creating a memory-efficient 4D Gaussian representation. Furthermore, we introduce an entropy-constrained Gaussian deformation technique that uses a deformation field to expand the action range of each Gaussian and integrates an opacity-based entropy loss to limit the number of Gaussians, thus forcing our model to use as few Gaussians as possible to fit a dynamic scene well. With simple half-precision storage and zip compression, our framework achieves a storage reduction by approximately 190xand 125xon the Technicolor and Neural 3D Video datasets, respectively, compared to the original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene representation quality, setting a new standard in the field. Code is available at https://github.com/Xinjie-Q/MEGA.</p>
            <p id="subjects-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="panel paper" keywords="dynfacerestore,guidance,fidelity,restoration,blind,blurry,face,diffusion,timesteps,detail">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration_ICCV_2025_paper.html" target="_blank" title="247/263"><span class="index notranslate">#247</span></a>
                <a id="title-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="title-link" href="/venue/Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" target="_blank">DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance</a>
                <a id="pdf-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huu-Phu Do" target="_blank">Huu-Phu Do</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Wei Chen" target="_blank">Yu-Wei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi-Cheng Liao" target="_blank">Yi-Cheng Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chi-Wei Hsiao" target="_blank">Chi-Wei Hsiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han-Yang Wang" target="_blank">Han-Yang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Chen Chiu" target="_blank">Wei-Chen Chiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ching-Chun Huang" target="_blank">Ching-Chun Huang</a>
            </p>
            <p id="summary-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="summary">Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration.</p>
            <p id="subjects-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" onclick="foldPdfKimi('Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="panel paper" keywords="video,subject,generation,text,consistent,phantom,modal,image,alignment,cross">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment_ICCV_2025_paper.html" target="_blank" title="248/263"><span class="index notranslate">#248</span></a>
                <a id="title-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="title-link" href="/venue/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" target="_blank">Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment</a>
                <a id="pdf-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lijie Liu" target="_blank">Lijie Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianxiang Ma" target="_blank">Tianxiang Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingchuan Li" target="_blank">Bingchuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuowei Chen" target="_blank">Zhuowei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Liu" target="_blank">Jiawei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gen Li" target="_blank">Gen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Zhou" target="_blank">Siyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian He" target="_blank">Qian He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinglong Wu" target="_blank">Xinglong Wu</a>
            </p>
            <p id="summary-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="summary">The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent videos following textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single- and multi-subject references.Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. The proposed method achieves perfect subject-consistent video generation while addressing issues of image content leakage and multi-subject confusion.Evaluation results indicate that our method outperforms other state-of-the-art closed-source commercial solutions.In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages.</p>
            <p id="subjects-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" onclick="foldPdfKimi('Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="panel paper" keywords="sliced,wasserstein,vocabulary,open,video,instance,segmentation,alignments,bridging,weighting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2025_paper.html" target="_blank" title="249/263"><span class="index notranslate">#249</span></a>
                <a id="title-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" target="_blank">Sliced Wasserstein Bridge for Open-Vocabulary Video Instance Segmentation</a>
                <a id="pdf-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheyun Qin" target="_blank">Zheyun Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deng Yu" target="_blank">Deng Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuanchen Luo" target="_blank">Chuanchen Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhumin Chen" target="_blank">Zhumin Chen</a>
            </p>
            <p id="summary-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="summary">In recent years, researchers have explored the task of open-vocabulary video instance segmentation, which aims to identify, track, and segment any instance within an open set of categories. The core challenge of Open-Vocabulary VIS lies in solving the cross-domain alignment problem, including spatial-temporal and text-visual domain alignments. Existing methods have made progress but still face shortcomings in addressing these alignments, especially due to data heterogeneity. Inspired by metric learning, we propose an innovative Sliced Wasserstein Bridging Learning Framework. This framework utilizes the Sliced Wasserstein distance as the core tool for metric learning, effectively bridging the four domains involved in the task. Our innovations are threefold: (1) Domain Alignment: By mapping features from different domains into a unified metric space, our method maintains temporal consistency and learns intrinsic consistent features between modalities, improving the fusion of text and visual information. (2) Weighting Mechanism: We introduce an importance weighting mechanism to enhance the discriminative ability of our method when dealing with imbalanced or significantly different data. (3) High Efficiency: Our method inherits the computational efficiency of the Sliced Wasserstein distance, allowing for online processing of large-scale video data while maintaining segmentation accuracy. Through extensive experimental evaluations, we have validated the robustness of our concept and the effectiveness of our framework.</p>
            <p id="subjects-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="panel paper" keywords="splatting,fov,lenses,fisheye,lens,reconstruction,self,gaussian,view,calibrating">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction_ICCV_2025_paper.html" target="_blank" title="250/263"><span class="index notranslate">#250</span></a>
                <a id="title-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="title-link" href="/venue/Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" target="_blank">Self-Calibrating Gaussian Splatting for Large Field-of-View Reconstruction</a>
                <a id="pdf-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Youming Deng" target="_blank">Youming Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Xian" target="_blank">Wenqi Xian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guandao Yang" target="_blank">Guandao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonidas Guibas" target="_blank">Leonidas Guibas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gordon Wetzstein" target="_blank">Gordon Wetzstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Steve Marschner" target="_blank">Steve Marschner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Debevec" target="_blank">Paul Debevec</a>
            </p>
            <p id="summary-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="summary">Large field-of-view (FOV) cameras can simplify and accelerate scene capture because they provide complete coverage with fewer views. However, existing reconstruction pipelines fail to take full advantage of large-FOV input data because they convert input views to perspective images, resulting in stretching that prevents the use of the full image. Additionally, they calibrate lenses using models that do not accurately fit real fisheye lenses in the periphery. We present a new reconstruction pipeline based on Gaussian Splatting that uses a flexible lens model and supports fields of view approaching 180 degrees. We represent lens distortion with a hybrid neural field based on an Invertible ResNet and use a cubemap to render wide-FOV images while retaining the efficiency of the Gaussian Splatting pipeline. Our system jointly optimizes lens distortion, camera intrinsics, camera poses, and scene representations using a loss measured directly against the original input pixels. We present extensive experiments on both synthetic and real-world scenes, demonstrating that our model accurately fits real-world fisheye lenses and that our end-to-end self-calibration approach provides higher-quality reconstructions than existing methods. More details and videos can be found at the project page: https://denghilbert.github.io/self-cali/.</p>
            <p id="subjects-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" onclick="foldPdfKimi('Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="panel paper" keywords="lvface,face,pco,recognition,ncs,progressive,uniface,topofr,cluster,vit">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face_ICCV_2025_paper.html" target="_blank" title="251/263"><span class="index notranslate">#251</span></a>
                <a id="title-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="title-link" href="/venue/You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" target="_blank">LVFace: Progressive Cluster Optimization for Large Vision Models in Face Recognition</a>
                <a id="pdf-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF"></sup>]</a>
                <a id="rel-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinghan You" target="_blank">Jinghan You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shanglin Li" target="_blank">Shanglin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanrui Sun" target="_blank">Yuanrui Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangchuan Wei" target="_blank">Jiangchuan Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyu Guo" target="_blank">Mingyu Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Feng" target="_blank">Chao Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiao Ran" target="_blank">Jiao Ran</a>
            </p>
            <p id="summary-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="summary">Vision Transformers (ViTs) have revolutionized large-scale visual modeling, yet remain underexplored in face recognition (FR) where CNNs still dominate. We identify a critical bottleneck: CNN-inspired training paradigms fail to unlock ViT's potential, leading to suboptimal performance and convergence instability.To address this challenge, we propose LVFace, a ViT-based FR model that integrates Progressive Cluster Optimization (PCO) to achieve superior results. Specifically, PCO sequentially applies negative class sub-sampling (NCS) for robust and fast feature alignment from random initialization, feature expectation penalties for centroid stabilization, performing cluster boundary refinement through full-batch training without NCS constraints. LVFace establishes a new state-of-the-art face recognition baseline, surpassing leading approaches such as UniFace and TopoFR across multiple benchmarks. Extensive experiments demonstrate that LVFace delivers consistent performance gains, while exhibiting scalability to large-scale datasets and compatibility with mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV 2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its efficacy in real-world scenarios. Project is available at https://github.com/bytedance/LVFace.</p>
            <p id="subjects-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" onclick="foldPdfKimi('You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="panel paper" keywords="braid,srefiner,refinement,soft,trajectories,trajectory,agent,prediction,topological,attention">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement_ICCV_2025_paper.html" target="_blank" title="252/263"><span class="index notranslate">#252</span></a>
                <a id="title-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="title-link" href="/venue/Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" target="_blank">SRefiner: Soft-Braid Attention for Multi-Agent Trajectory Refinement</a>
                <a id="pdf-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liwen Xiao" target="_blank">Liwen Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyu Pan" target="_blank">Zhiyu Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhicheng Wang" target="_blank">Zhicheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiguo Cao" target="_blank">Zhiguo Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Li" target="_blank">Wei Li</a>
            </p>
            <p id="summary-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="summary">Accurate prediction of multi-agent future trajectories is crucial for autonomous driving systems to make safe and efficient decisions. Trajectory refinement has emerged as a key strategy to enhance prediction accuracy. However, existing refinement methods often overlook the topological relationships between trajectories, which are vital for improving prediction precision. Inspired by braid theory, we propose a novel trajectory refinement approach, Soft-Braid Refiner (SRefiner), guided by the soft-braid topological structure of trajectories using Soft-Braid Attention. Soft-Braid Attention captures spatio-temporal topological relationships between trajectories by considering both spatial proximity and vehicle motion states at "soft intersection points". Additionally, we extend this approach to model interactions between trajectories and lanes, further improving the prediction accuracy. SRefiner is a multi-iteration, multi-agent framework that iteratively refines trajectories, incorporating topological information to enhance interactions within traffic scenarios. SRefiner achieves significant performance improvements over four baseline methods across two datasets, establishing a new state-of-the-art in trajectory refinement.</p>
            <p id="subjects-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" onclick="foldPdfKimi('Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="panel paper" keywords="lmm4lmm,50k,evalmi,multimodal,t2i,image,lmms,generation,evaluating,text">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs_ICCV_2025_paper.html" target="_blank" title="253/263"><span class="index notranslate">#253</span></a>
                <a id="title-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="title-link" href="/venue/Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" target="_blank">LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs</a>
                <a id="pdf-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiarui Wang" target="_blank">Jiarui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huiyu Duan" target="_blank">Huiyu Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Zhao" target="_blank">Yu Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juntong Wang" target="_blank">Juntong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangtao Zhai" target="_blank">Guangtao Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiongkuo Min" target="_blank">Xiongkuo Min</a>
            </p>
            <p id="summary-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="summary">Recent breakthroughs in large multimodal models (LMMs) have significantly advanced both text-to-image (T2I) generation and image-to-text (I2T) interpretation. However, many generated images still suffer from issues related to perceptual quality and text-image alignment. Given the high cost and inefficiency of manual evaluation, an automatic metric that aligns with human preferences is desirable. To this end, we present EvalMi-50K, a comprehensive dataset and benchmark for evaluating large-multimodal image generation,which features (i) comprehensive tasks, encompassing 2,100 extensive prompts across 20 fine-grained task dimensions, and (ii) large-scale human-preference annotations, including 100K mean-opinion scores (MOSs) and 50K question-answering (QA) pairs annotated on 50,400 images generated from 24 T2I models.Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for evaluating large multimodal T2I generation from multiple dimensions including perceptual quality, text-image correspondence, and task-specific accuracy.Extensive experimental results show that LMM4LMM achieves state-of-the-art performance on EvalMi-50K, and exhibits strong generalization ability on other AI-generated image evaluation benchmark datasets, manifesting the generality of both the EvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be released at https://github.com/IntMeGroup/LMM4LMM.</p>
            <p id="subjects-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" onclick="foldPdfKimi('Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Geometry_Distributions@ICCV2025@CVF" class="panel paper" keywords="representation,geometric,distributions,surface,watertight,geometry,across,object,textured,neural">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Geometry_Distributions_ICCV_2025_paper.html" target="_blank" title="254/263"><span class="index notranslate">#254</span></a>
                <a id="title-Zhang_Geometry_Distributions@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Geometry_Distributions@ICCV2025@CVF" target="_blank">Geometry Distributions</a>
                <a id="pdf-Zhang_Geometry_Distributions@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Geometry_Distributions@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Geometry_Distributions_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Geometry_Distributions@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Zhang_Geometry_Distributions@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Geometry_Distributions@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Geometry_Distributions@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Geometry_Distributions@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Geometry_Distributions@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_Geometry_Distributions@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Geometry_Distributions@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Geometry_Distributions@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Biao Zhang" target="_blank">Biao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Ren" target="_blank">Jing Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Wonka" target="_blank">Peter Wonka</a>
            </p>
            <p id="summary-Zhang_Geometry_Distributions@ICCV2025@CVF" class="summary">Neural representations of 3D data have been widely adopted across various applications, particularly in recent work leveraging coordinate-based networks to model scalar or vector fields. However, these approaches face inherent challenges, such as handling thin structures and non-watertight geometries, which limit their flexibility and accuracy. In contrast, we propose a novel geometric data representation that models geometry as distributions-a powerful representation that makes no assumptions about surface genus, connectivity, or boundary conditions. Our approach uses diffusion models with a novel network architecture to learn surface point distributions, capturing fine-grained geometric details. We evaluate our representation qualitatively and quantitatively across various object types, demonstrating its effectiveness in achieving high geometric fidelity. Additionally, we explore applications using our representation, such as textured mesh representation, neural surface compression, dynamic object modeling, and rendering, highlighting its potential to advance 3D geometric learning.</p>
            <p id="subjects-Zhang_Geometry_Distributions@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Geometry_Distributions@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Geometry_Distributions@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Geometry_Distributions@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Geometry_Distributions@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="panel paper" keywords="keypoint,superevent,event,slam,keypoints,cameras,streams,based,ethz,mrl">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM_ICCV_2025_paper.html" target="_blank" title="255/263"><span class="index notranslate">#255</span></a>
                <a id="title-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="title-link" href="/venue/Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" target="_blank">SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM</a>
                <a id="pdf-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yannick Burkhardt" target="_blank">Yannick Burkhardt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Schaefer" target="_blank">Simon Schaefer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Leutenegger" target="_blank">Stefan Leutenegger</a>
            </p>
            <p id="summary-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="summary">Event-based keypoint detection and matching holds significant potential, enabling the integration of event sensors into highly optimized Visual SLAM systems developed for frame cameras over decades of research. Unfortunately, existing approaches struggle with the motion-dependent appearance of keypoints and the complex noise prevalent in event streams, resulting in severely limited feature matching capabilities and poor performance on downstream tasks. To mitigate this problem, we propose SuperEvent, a data-driven approach to predict stable keypoints with expressive descriptors. Due to the absence of event datasets with ground truth keypoint labels, we leverage existing frame-based keypoint detectors on readily available event-aligned and synchronized gray-scale frames for self-supervision: we generate temporally sparse keypoint pseudo-labels considering that events are a product of both scene appearance and camera motion. Combined with our novel, information-rich event representation, we enable SuperEvent to effectively learn robust keypoint detection and description in event streams. Finally, we demonstrate the usefulness of SuperEvent by its integration into a modern sparse keypoint and descriptor-based SLAM framework originally developed for traditional cameras, surpassing the state-of-the-art in event-based SLAM by a wide margin. Source code is available at https://ethz-mrl.github.io/SuperEvent/.</p>
            <p id="subjects-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" onclick="foldPdfKimi('Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="panel paper" keywords="avatar,moga,avatars,generative,gaussian,unseen,views,appearance,prior,ensuring">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction_ICCV_2025_paper.html" target="_blank" title="256/263"><span class="index notranslate">#256</span></a>
                <a id="title-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="title-link" href="/venue/Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" target="_blank">MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction</a>
                <a id="pdf-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zijian Dong" target="_blank">Zijian Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longteng Duan" target="_blank">Longteng Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Song" target="_blank">Jie Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael J. Black" target="_blank">Michael J. Black</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Geiger" target="_blank">Andreas Geiger</a>
            </p>
            <p id="summary-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="summary">We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see https:// zj-dong.github.io/ MoGA/</p>
            <p id="subjects-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" onclick="foldPdfKimi('Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="panel paper" keywords="target,cues,atctrack,context,vlts,textual,tracking,words,language,vision">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust_ICCV_2025_paper.html" target="_blank" title="257/263"><span class="index notranslate">#257</span></a>
                <a id="title-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="title-link" href="/venue/Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" target="_blank">ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking</a>
                <a id="pdf-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaokun Feng" target="_blank">Xiaokun Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyu Hu" target="_blank">Shiyu Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuchen Li" target="_blank">Xuchen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dailing Zhang" target="_blank">Dailing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meiqi Wu" target="_blank">Meiqi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Zhang" target="_blank">Jing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaotang Chen" target="_blank">Xiaotang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiqi Huang" target="_blank">Kaiqi Huang</a>
            </p>
            <p id="summary-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="summary">Vision-language tracking aims to locate the target object in the video sequence using a template patch and a language description provided in the initial frame. To achieve robust tracking, especially in complex long-term scenarios that reflect real-world conditions as recently highlighted by MGIT, it is essential not only to characterize the target features but also to utilize the context features related to the target. However, the visual and textual target-context cues derived from the initial prompts generally align only with the initial target state. Due to their dynamic nature, target states are constantly changing, particularly in complex long-term sequences. It is intractable for these cues to continuously guide Vision-Language Trackers (VLTs). Furthermore, for the text prompts with diverse expressions, our experiments reveal that existing VLTs struggle to discern which words pertain to the target or the context, complicating the utilization of textual cues. In this work, we present a novel tracker named ATCTrack, which can obtain multimodal cues Aligned with the dynamic target states through comprehensive Target-Context feature modeling, thereby achieving robust tracking. Specifically, (1) for the visual modality, we propose an effective temporal visual target-context modeling approach that provides the tracker with timely visual cues. (2) For the textual modality, we achieve precise target words identification solely based on textual content, and design an innovative context words calibration method to adaptively utilize auxiliary context words. (3) We conduct extensive experiments on mainstream benchmarks and ATCTrack achieves a new SOTA performance. The code and models will be released at: https://github.com/XiaokunFeng/ATCTrack.</p>
            <p id="subjects-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" onclick="foldPdfKimi('Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="panel paper" keywords="remote,sensing,context,segmentation,vocabulary,instance,score,scene,matters,diverse">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation_ICCV_2025_paper.html" target="_blank" title="258/263"><span class="index notranslate">#258</span></a>
                <a id="title-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" target="_blank">SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation</a>
                <a id="pdf-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shiqi Huang" target="_blank">Shiqi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuting He" target="_blank">Shuting He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaiyuan Qin" target="_blank">Huaiyuan Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bihan Wen" target="_blank">Bihan Wen</a>
            </p>
            <p id="summary-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="summary">Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose SCORE (Scene Context matters in Open-vocabulary REmote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at https://github.com/HuangShiqi128/SCORE.</p>
            <p id="subjects-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="panel paper" keywords="style,stylized,wsdt,features,wasserstein,blocks,attention,image,generation,transform">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation_ICCV_2025_paper.html" target="_blank" title="259/263"><span class="index notranslate">#259</span></a>
                <a id="title-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="title-link" href="/venue/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" target="_blank">Wasserstein Style Distribution Analysis and Transform for Stylized Image Generation</a>
                <a id="pdf-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Yu" target="_blank">Xi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Gu" target="_blank">Xiang Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Shi" target="_blank">Zhihao Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Sun" target="_blank">Jian Sun</a>
            </p>
            <p id="summary-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="summary">Large-scale text-to-image diffusion models have achieved remarkable success in image generation, thereby driving the development of stylized image generation technologies. Recent studies introduce style information by empirically replacing specific features in attention blocks with style features. However, the relationship between features and style remains unclear. In this paper, we systematically analyze the relationship between features in attention blocks and style. By quantifying the distribution discrepancy induced by style variations using the Wasserstein distance, we find that features in self-attention blocks exhibit high sensitivity to style compared to features in cross-attention blocks. Our analysis provides valuable insights into the contribution of different features to style. Based on our findings, we propose a novel Wasserstein Style Distribution Transform (WSDT) method, which generates stylized images by transforming the distribution of style-sensitive features to align with that of style features. WSDT applies channel adaptive distribution transform to ensure that information not related to the style is not introduced. Our approach is simple yet efficient, optimization-free, and can be seamlessly integrated into attention-based text-to-image diffusion models. Extensive experiments demonstrate the effectiveness of our approach in stylized image generation tasks.</p>
            <p id="subjects-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" onclick="foldPdfKimi('Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="panel paper" keywords="mede,open,vocabulary,clip,static,video,context,meta,learners,optimization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition_ICCV_2025_paper.html" target="_blank" title="260/263"><span class="index notranslate">#260</span></a>
                <a id="title-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="title-link" href="/venue/Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" target="_blank">Learning to Generalize without Bias for Open-Vocabulary Action Recognition</a>
                <a id="pdf-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF">2</sup>]</a>
                <a id="copy-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yating Yu" target="_blank">Yating Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Congqi Cao" target="_blank">Congqi Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Zhang" target="_blank">Yifan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanning Zhang" target="_blank">Yanning Zhang</a>
            </p>
            <p id="summary-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="summary">Leveraging the effective visual-text alignment and static generalizability from CLIP, recent video learners adopt CLIP initialization with further regularization or recombination for generalization in open-vocabulary action recognition in-context. However, due to the static bias of CLIP, such video learners tend to overfit on shortcut static features, thereby compromising their generalizability, especially to novel out-of-context actions. To address this issue, we introduce Open-MeDe, a novel Meta-optimization framework with static Debiasing for Open-vocabulary action recognition. From a fresh perspective of generalization, Open-MeDe adopts a meta-learning approach to improve known-to-open generalizing and image-to-video debiasing in a cost-effective manner. Specifically, Open-MeDe introduces a cross-batch meta-optimization scheme that explicitly encourages video learners to quickly generalize to arbitrary subsequent data via virtual evaluation, steering a smoother optimization landscape. In effect, the free of CLIP regularization during optimization implicitly mitigates the inherent static bias of the video meta-learner. We further apply self-ensemble over the optimization trajectory to obtain generic optimal parameters that can achieve robust generalization to both in-context and out-of-context novel data. Extensive evaluations show that Open-MeDe not only surpasses state-of-the-art regularization methods tailored for in-context open-vocabulary action recognition but also substantially excels in out-of-context scenarios.</p>
            <p id="subjects-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" onclick="foldPdfKimi('Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="panel paper" keywords="graphics,programs,tikzero,captioned,text,aligned,program,figures,captions,caption">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis_ICCV_2025_paper.html" target="_blank" title="261/263"><span class="index notranslate">#261</span></a>
                <a id="title-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="title-link" href="/venue/Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" target="_blank">TikZero: Zero-Shot Text-Guided Graphics Program Synthesis</a>
                <a id="pdf-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Belouadi" target="_blank">Jonas Belouadi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eddy Ilg" target="_blank">Eddy Ilg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Margret Keuper" target="_blank">Margret Keuper</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hideki Tanaka" target="_blank">Hideki Tanaka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Masao Utiyama" target="_blank">Masao Utiyama</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Raj Dabre" target="_blank">Raj Dabre</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Steffen Eger" target="_blank">Steffen Eger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simone Ponzetto" target="_blank">Simone Ponzetto</a>
            </p>
            <p id="summary-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="summary">Automatically synthesizing figures from text captions is a compelling capability. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.</p>
            <p id="subjects-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" onclick="foldPdfKimi('Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="panel paper" keywords="mvqa,usds,mamba,vqa,resampling,sampling,videos,distortion,unified,semantic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment_ICCV_2025_paper.html" target="_blank" title="262/263"><span class="index notranslate">#262</span></a>
                <a id="title-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="title-link" href="/venue/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" target="_blank">MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment</a>
                <a id="pdf-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF">1</sup>]</a>
                <a id="copy-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF"></sup>]</a>
                <a id="rel-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yachun Mi" target="_blank">Yachun Mi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Li" target="_blank">Yu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weicheng Meng" target="_blank">Weicheng Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaofeng Chen" target="_blank">Chaofeng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Hui" target="_blank">Chen Hui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohui Liu" target="_blank">Shaohui Liu</a>
            </p>
            <p id="summary-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="summary">The rapid growth of long-duration, high-definition videos has made efficient video quality assessment (VQA) a critical challenge. Existing research typically tackles this problem through two main strategies: reducing model parameters and resampling inputs. However, light-weight Convolution Neural Networks (CNN) and Transformers often struggle to balance efficiency with high performance due to the requirement of long-range modeling capabilities. Recently, the state-space model, particularly Mamba, has emerged as a promising alternative, offering linear complexity with respect to sequence length. Meanwhile, efficient VQA heavily depends on resampling long sequences to minimize computational costs, yet current resampling methods are often weak in preserving essential semantic information. In this work, we present MVQA, a Mamba-based model designed for efficient VQA along with a novel Unified Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch sampling from low-resolution videos and distortion patch sampling from original-resolution videos. The former captures semantically dense regions, while the latter retains critical distortion details. To prevent computation increase from dual inputs, we propose a fusion mechanism using pre-defined masks, enabling a unified sampling strategy that captures both semantic and quality information without additional computational burden. Experiments show that the proposed MVQA, equipped with USDS, achieve comparable performance to state-of-the-art methods while being 2xas fast and requiring only 1/5 GPU memory. Code is available at https://github.com/xiao-mi-d/MVQA</p>
            <p id="subjects-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" onclick="foldPdfKimi('Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="panel paper" keywords="flow,panoramic,optical,primitive,distortion,orthogonal,prior,view,distortions,oddc">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.html" target="_blank" title="263/263"><span class="index notranslate">#263</span></a>
                <a id="title-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="title-link" href="/venue/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" target="_blank">PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View</a>
                <a id="pdf-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF"></sup>]</a>
                <a id="copy-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Longliang Liu" target="_blank">Longliang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miaojie Feng" target="_blank">Miaojie Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junda Cheng" target="_blank">Junda Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jijun Xiang" target="_blank">Jijun Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Zhu" target="_blank">Xuan Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Yang" target="_blank">Xin Yang</a>
            </p>
            <p id="summary-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="summary">Panoramic optical flow enables a comprehensive understanding of temporal dynamics across wide fields of view. However, severe distortions caused by sphere-to-plane projections, such as the equirectangular projection (ERP), significantly degrade the performance of conventional perspective-based optical flow methods, especially in polar regions. To address this challenge, we propose PriOr-Flow, a novel dual-branch framework that leverages the low-distortion nature of the orthogonal view to enhance optical flow estimation in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup (DCCL) operator, which jointly retrieves correlation information from both the primitive and orthogonal cost volumes, effectively mitigating distortion noise during cost volume construction. Furthermore, our Ortho-Driven Distortion Compensation (ODDC) module iteratively refines motion features of the primitive branch, further suppressing polar distortions. Extensive experiments demonstrate that PriOr-Flow is compatible with various perspective-based iterative optical flow methods and consistently achieves state-of-the-art performance on publicly available panoramic optical flow datasets, setting a new benchmark for wide-field motion estimation.</p>
            <p id="subjects-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Highlight" target="_blank">ICCV.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" onclick="foldPdfKimi('Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF">Unleashing Vecset Diffusion Model for Fast Shape Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF">Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF">UDC-VIT: A Real-World Video Dataset for Under-Display Cameras</a>
                    <a class="i-star" onclick="toggleAppStar('Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF">Where, What, Why: Towards Explainable Driver Attention Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF">VRM: Knowledge Distillation via Virtual Relation Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF">ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF">UnZipLoRA: Separating Content and Style from a Single Image</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF">One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory</a>
                    <a class="i-star" onclick="toggleAppStar('Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF">Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints</a>
                    <a class="i-star" onclick="toggleAppStar('Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF">RALoc: Enhancing Outdoor LiDAR Localization via Rotation Awareness</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF">DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF">AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF">Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF">Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold</a>
                    <a class="i-star" onclick="toggleAppStar('Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF">Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF">BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes</a>
                    <a class="i-star" onclick="toggleAppStar('Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF">Noise-Modeled Diffusion Models for Low-Light Spike Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF">Radiant Foam: Real-Time Differentiable Ray Tracing</a>
                    <a class="i-star" onclick="toggleAppStar('Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF">ReTracker: Exploring Image Matching for Robust Online Any Point Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF">A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks</a>
                    <a class="i-star" onclick="toggleAppStar('Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF">Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats</a>
                    <a class="i-star" onclick="toggleAppStar('Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF">2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF">Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF">Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF">Test-Time Prompt Tuning for Zero-Shot Depth Completion</a>
                    <a class="i-star" onclick="toggleAppStar('Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF">StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF">Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF">Scaling Language-Free Visual Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF">DiffRefine: Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF">Confound from All Sides, Distill with Resilience: Multi-Objective Adversarial Paths to Zero-Shot Robustness</a>
                    <a class="i-star" onclick="toggleAppStar('Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF">VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions</a>
                    <a class="i-star" onclick="toggleAppStar('Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF">Fast Globally Optimal and Geometrically Consistent 3D Shape Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF">Evading Data Provenance in Deep Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF">Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability</a>
                    <a class="i-star" onclick="toggleAppStar('He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF">What to Distill? Fast Knowledge Distillation with Adaptive Sampling</a>
                    <a class="i-star" onclick="toggleAppStar('Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF">Multispectral Demosaicing via Dual Cameras</a>
                    <a class="i-star" onclick="toggleAppStar('Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF">Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling</a>
                    <a class="i-star" onclick="toggleAppStar('Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF">SAC-GNC: SAmple Consensus for adaptive Graduated Non-Convexity</a>
                    <a class="i-star" onclick="toggleAppStar('Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF">Is Tracking Really More Challenging in First Person Egocentric Vision?</a>
                    <a class="i-star" onclick="toggleAppStar('Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF">Less is More: Empowering GUI Agent with Context-Aware Simplification</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF">CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF">SceneMI: Motion In-betweening for Modeling Human-Scene Interaction</a>
                    <a class="i-star" onclick="toggleAppStar('Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF">Learning Large Motion Estimation from Intermediate Representations with a High-Resolution Optical Flow Dataset Featuring Long-Range Dynamic Motion</a>
                    <a class="i-star" onclick="toggleAppStar('Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF">Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF">Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning</a>
                    <a class="i-star" onclick="toggleAppStar('Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF">StableDepth: Scene-Consistent and Scale-Invariant Monocular Depth</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF">HccePose(BF): Predicting Front &amp; Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF">Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF">Exploring View Consistency for Scene-Adaptive Low-Light Light Field Image Enhancement</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF">EventUPS: Uncalibrated Photometric Stereo Using an Event Camera</a>
                    <a class="i-star" onclick="toggleAppStar('Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ma_Find_Any_Part_in_3D@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#Ma_Find_Any_Part_in_3D@ICCV2025@CVF">Find Any Part in 3D</a>
                    <a class="i-star" onclick="toggleAppStar('Ma_Find_Any_Part_in_3D@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ma_Find_Any_Part_in_3D@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF">AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?</a>
                    <a class="i-star" onclick="toggleAppStar('Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF">Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras</a>
                    <a class="i-star" onclick="toggleAppStar('Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF">CObL: Toward Zero-Shot Ordinal Layering without User Prompting</a>
                    <a class="i-star" onclick="toggleAppStar('Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF">Hierarchical Material Recognition from Local Appearance</a>
                    <a class="i-star" onclick="toggleAppStar('Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF">MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation</a>
                    <a class="i-star" onclick="toggleAppStar('Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a>
                    <a class="i-star" onclick="toggleAppStar('Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF">MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF">FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF">Diorama: Unleashing Zero-shot Single-view 3D Indoor Scene Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF">Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures</a>
                    <a class="i-star" onclick="toggleAppStar('Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF">ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users</a>
                    <a class="i-star" onclick="toggleAppStar('Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF">WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions</a>
                    <a class="i-star" onclick="toggleAppStar('Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF">CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image</a>
                    <a class="i-star" onclick="toggleAppStar('Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF">Combinative Matching for Geometric Shape Assembly</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF">FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF">Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF">X-Dancer: Expressive Music to Human Dance Video Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF">F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF">Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF">Disentangled Clothed Avatar Generation with Layered Representation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF">Riemannian-Geometric Fingerprints of Generative Models</a>
                    <a class="i-star" onclick="toggleAppStar('Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF">ISP2HRNet: Learning to Reconstruct High Resolution Image from Irregularly Sampled Pixels via Hierarchical Gradient Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF">Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection</a>
                    <a class="i-star" onclick="toggleAppStar('Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF">GameFactory: Creating New Games with Generative Interactive Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF">GENMO: A GENeralist Model for Human MOtion</a>
                    <a class="i-star" onclick="toggleAppStar('Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF">Video Individual Counting for Moving Drones</a>
                    <a class="i-star" onclick="toggleAppStar('Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF">Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</a>
                    <a class="i-star" onclick="toggleAppStar('Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF">Unified Multimodal Understanding via Byte-Pair Visual Encoding</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF">Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images</a>
                    <a class="i-star" onclick="toggleAppStar('Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF">ChartCap: Mitigating Hallucination of Dense Chart Captioning</a>
                    <a class="i-star" onclick="toggleAppStar('Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF">UniPhys: Unified Planner and Controller with Diffusion for Flexible Physics-Based Character Control</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF">Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</a>
                    <a class="i-star" onclick="toggleAppStar('Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF">DisenQ: Disentangling Q-Former for Activity-Biometrics</a>
                    <a class="i-star" onclick="toggleAppStar('Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF">MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh</a>
                    <a class="i-star" onclick="toggleAppStar('Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF">DexVLG: Dexterous Vision-Language-Grasp Model at Scale</a>
                    <a class="i-star" onclick="toggleAppStar('He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF">DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding</a>
                    <a class="i-star" onclick="toggleAppStar('Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF">AnimalClue: Recognizing Animals by their Traces</a>
                    <a class="i-star" onclick="toggleAppStar('Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF">OminiControl: Minimal and Universal Control for Diffusion Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF">Straighten Viscous Rectified Flow via Noise Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF">Scalable Dual Fingerprinting for Hierarchical Attribution of Text-to-Image Models</a>
                    <a class="i-star" onclick="toggleAppStar('Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF">SummDiff: Generative Modeling of Video Summarization with Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF">IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models</a>
                    <a class="i-star" onclick="toggleAppStar('Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF">DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#96</span>
                    <a class="i-title" href="#Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF">ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#97</span>
                    <a class="i-title" href="#Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF">Magic Insert: Style-Aware Drag-and-Drop</a>
                    <a class="i-star" onclick="toggleAppStar('Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#98</span>
                    <a class="i-title" href="#Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF">Outlier-Aware Post-Training Quantization for Image Super-Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#99</span>
                    <a class="i-title" href="#Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF">Edit360: 2D Image Edits to 3D Assets from Any Angle</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#100</span>
                    <a class="i-title" href="#Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF">Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#101</span>
                    <a class="i-title" href="#Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF">From Image to Video: An Empirical Study of Diffusion Representations</a>
                    <a class="i-star" onclick="toggleAppStar('Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#102</span>
                    <a class="i-title" href="#Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF">Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</a>
                    <a class="i-star" onclick="toggleAppStar('Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#103</span>
                    <a class="i-title" href="#Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF">RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation</a>
                    <a class="i-star" onclick="toggleAppStar('Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#104</span>
                    <a class="i-title" href="#Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF">ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#105</span>
                    <a class="i-title" href="#Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF">The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#106</span>
                    <a class="i-title" href="#Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF">GFPack++: Attention-Driven Gradient Fields for Optimizing 2D Irregular Packing</a>
                    <a class="i-star" onclick="toggleAppStar('Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#107</span>
                    <a class="i-title" href="#Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF">Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#108</span>
                    <a class="i-title" href="#Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF">CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#109</span>
                    <a class="i-title" href="#Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF">DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#110</span>
                    <a class="i-title" href="#Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF">Bi-Level Optimization for Self-Supervised AI-Generated Face Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#111</span>
                    <a class="i-title" href="#Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF">Blended Point Cloud Diffusion for Localized Text-guided Shape Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#112</span>
                    <a class="i-title" href="#Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF">DLF: Extreme Image Compression with Dual-generative Latent Fusion</a>
                    <a class="i-star" onclick="toggleAppStar('Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#113</span>
                    <a class="i-title" href="#Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF">Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#114</span>
                    <a class="i-title" href="#Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF">On the Provable Importance of Gradients for Autonomous Language-Assisted Image Clustering</a>
                    <a class="i-star" onclick="toggleAppStar('Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#115</span>
                    <a class="i-title" href="#Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF">CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning</a>
                    <a class="i-star" onclick="toggleAppStar('Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#116</span>
                    <a class="i-title" href="#Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF">Visual Test-time Scaling for GUI Agent Grounding</a>
                    <a class="i-star" onclick="toggleAppStar('Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#117</span>
                    <a class="i-title" href="#Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF">GECKO: Gigapixel Vision-Concept Contrastive Pretraining in Histopathology</a>
                    <a class="i-star" onclick="toggleAppStar('Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#118</span>
                    <a class="i-title" href="#Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF">Test-time Adaptation for Foundation Medical Segmentation Model Without Parametric Updates</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#119</span>
                    <a class="i-title" href="#Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF">Feature Purification Matters: Suppressing Outlier Propagation for Training-Free Open-Vocabulary Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#120</span>
                    <a class="i-title" href="#Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF">Leveraging Prior Knowledge of Diffusion Model for Person Search</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#121</span>
                    <a class="i-title" href="#Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF">Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#122</span>
                    <a class="i-title" href="#Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF">RhythmGuassian: Repurposing Generalizable Gaussian Model For Remote Physiological Measurement</a>
                    <a class="i-star" onclick="toggleAppStar('Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#123</span>
                    <a class="i-title" href="#Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF">On the Recovery of Cameras from Fundamental Matrices</a>
                    <a class="i-star" onclick="toggleAppStar('Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#124</span>
                    <a class="i-title" href="#Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF">EVEv2: Improved Baselines for Encoder-Free Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#125</span>
                    <a class="i-title" href="#Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF">Few-Shot Pattern Detection via Template Matching and Regression</a>
                    <a class="i-star" onclick="toggleAppStar('Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#126</span>
                    <a class="i-title" href="#Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF">CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#127</span>
                    <a class="i-title" href="#Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF">WeaveSeg: Iterative Contrast-weaving and Spectral Feature-refining for Nuclei Instance Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#128</span>
                    <a class="i-title" href="#Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF">Modeling Saliency Dataset Bias</a>
                    <a class="i-star" onclick="toggleAppStar('Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#129</span>
                    <a class="i-title" href="#De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF">Interpretable point cloud classification using multiple instance learning</a>
                    <a class="i-star" onclick="toggleAppStar('De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#130</span>
                    <a class="i-title" href="#Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF">Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#131</span>
                    <a class="i-title" href="#Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF">Progressive Test Time Energy Adaptation for Medical Image Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#132</span>
                    <a class="i-title" href="#Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF">WINS: Winograd Structured Pruning for Fast Winograd Convolution</a>
                    <a class="i-star" onclick="toggleAppStar('Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#133</span>
                    <a class="i-title" href="#Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF">Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#134</span>
                    <a class="i-title" href="#Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF">LVBench: An Extreme Long Video Understanding Benchmark</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#135</span>
                    <a class="i-title" href="#Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF">Similarity Memory Prior is All You Need for Medical Image Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#136</span>
                    <a class="i-title" href="#Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF">Cross-Architecture Distillation Made Simple with Redundancy Suppression</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#137</span>
                    <a class="i-title" href="#Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF">Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#138</span>
                    <a class="i-title" href="#Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF">OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#139</span>
                    <a class="i-title" href="#Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF">Token-Efficient VLM: High-Resolution Image Understanding via Dynamic Region Proposal</a>
                    <a class="i-star" onclick="toggleAppStar('Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#140</span>
                    <a class="i-title" href="#Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF">Enrich and Detect: Video Temporal Grounding with Multimodal LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#141</span>
                    <a class="i-title" href="#Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF">Emulating Self-attention with Convolution for Efficient Image Super-Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#142</span>
                    <a class="i-title" href="#Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF">Inverse Image-Based Rendering for Light Field Generation from Single Images</a>
                    <a class="i-star" onclick="toggleAppStar('Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#143</span>
                    <a class="i-title" href="#Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF">RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#144</span>
                    <a class="i-title" href="#Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF">PRM: Photometric Stereo based Large Reconstruction Model</a>
                    <a class="i-star" onclick="toggleAppStar('Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#145</span>
                    <a class="i-title" href="#Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a>
                    <a class="i-star" onclick="toggleAppStar('Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#146</span>
                    <a class="i-title" href="#Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF">RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians</a>
                    <a class="i-star" onclick="toggleAppStar('Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#147</span>
                    <a class="i-title" href="#Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF">HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#148</span>
                    <a class="i-title" href="#Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF">Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#149</span>
                    <a class="i-title" href="#Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF">MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy</a>
                    <a class="i-star" onclick="toggleAppStar('Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#150</span>
                    <a class="i-title" href="#Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF">Discontinuity-aware Normal Integration for Generic Central Camera Models</a>
                    <a class="i-star" onclick="toggleAppStar('Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#151</span>
                    <a class="i-title" href="#Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF">EDM: Efficient Deep Feature Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#152</span>
                    <a class="i-title" href="#He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF">MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#153</span>
                    <a class="i-title" href="#Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF">PolarAnything: Diffusion-based Polarimetric Image Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#154</span>
                    <a class="i-title" href="#Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF">Spatio-Spectral Pattern Illumination for Direct and Indirect Separation from a Single Hyperspectral Image</a>
                    <a class="i-star" onclick="toggleAppStar('Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#155</span>
                    <a class="i-title" href="#Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF">Explaining Human Preferences via Metrics for Structured 3D Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#156</span>
                    <a class="i-title" href="#Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a>
                    <a class="i-star" onclick="toggleAppStar('Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#157</span>
                    <a class="i-title" href="#Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF">Inverse 3D Microscopy Rendering for Cell Shape Inference with Active Mesh</a>
                    <a class="i-star" onclick="toggleAppStar('Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#158</span>
                    <a class="i-title" href="#Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF">SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#159</span>
                    <a class="i-title" href="#Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF">Planar Affine Rectification from Local Change of Scale and Orientation</a>
                    <a class="i-star" onclick="toggleAppStar('Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#160</span>
                    <a class="i-title" href="#Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF">Thermal Polarimetric Multi-view Stereo</a>
                    <a class="i-star" onclick="toggleAppStar('Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#161</span>
                    <a class="i-title" href="#Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF">AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering</a>
                    <a class="i-star" onclick="toggleAppStar('Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#162</span>
                    <a class="i-title" href="#Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF">SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video</a>
                    <a class="i-star" onclick="toggleAppStar('Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#163</span>
                    <a class="i-title" href="#Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF">BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#164</span>
                    <a class="i-title" href="#Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF">FlowR: Flowing from Sparse to Dense 3D Reconstructions</a>
                    <a class="i-star" onclick="toggleAppStar('Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#165</span>
                    <a class="i-title" href="#Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF">NeuFrameQ: Neural Frame Fields for Scalable and Generalizable Anisotropic Quadrangulation</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#166</span>
                    <a class="i-title" href="#Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF">NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#167</span>
                    <a class="i-title" href="#Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF">Stochastic Gradient Estimation for Higher-Order Differentiable Rendering</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#168</span>
                    <a class="i-title" href="#Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF">HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#169</span>
                    <a class="i-title" href="#Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF">Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#170</span>
                    <a class="i-title" href="#Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF">Lidar Waveforms are Worth 40x128x33 Words</a>
                    <a class="i-star" onclick="toggleAppStar('Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#171</span>
                    <a class="i-title" href="#Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF">LBM: Latent Bridge Matching for Fast Image-to-Image Translation</a>
                    <a class="i-star" onclick="toggleAppStar('Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#172</span>
                    <a class="i-title" href="#Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF">Super Resolved Imaging with Adaptive Optics</a>
                    <a class="i-star" onclick="toggleAppStar('Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#173</span>
                    <a class="i-title" href="#Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF">M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization</a>
                    <a class="i-star" onclick="toggleAppStar('Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#174</span>
                    <a class="i-title" href="#Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF">ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives</a>
                    <a class="i-star" onclick="toggleAppStar('Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#175</span>
                    <a class="i-title" href="#Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF">Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts</a>
                    <a class="i-star" onclick="toggleAppStar('Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#176</span>
                    <a class="i-title" href="#Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF">SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#177</span>
                    <a class="i-title" href="#Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF">Beyond Losses Reweighting: Empowering Multi-Task Learning via the Generalization Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#178</span>
                    <a class="i-title" href="#Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF">Consensus-Driven Active Model Selection</a>
                    <a class="i-star" onclick="toggleAppStar('Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#179</span>
                    <a class="i-title" href="#Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF">Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#180</span>
                    <a class="i-title" href="#Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF">DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</a>
                    <a class="i-star" onclick="toggleAppStar('Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#181</span>
                    <a class="i-title" href="#Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF">Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Video_Motion_Graphs@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#182</span>
                    <a class="i-title" href="#Liu_Video_Motion_Graphs@ICCV2025@CVF">Video Motion Graphs</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Video_Motion_Graphs@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Video_Motion_Graphs@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#183</span>
                    <a class="i-title" href="#Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF">GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#184</span>
                    <a class="i-title" href="#Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF">Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#185</span>
                    <a class="i-title" href="#Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF">No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#186</span>
                    <a class="i-title" href="#Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF">A Unified Interpretation of Training-Time Out-of-Distribution Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#187</span>
                    <a class="i-title" href="#Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF">CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#188</span>
                    <a class="i-title" href="#Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF">Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#189</span>
                    <a class="i-title" href="#Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF">Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via Neuron Activation Variation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#190</span>
                    <a class="i-title" href="#Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF">VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory</a>
                    <a class="i-star" onclick="toggleAppStar('Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#191</span>
                    <a class="i-title" href="#Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF">OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models</a>
                    <a class="i-star" onclick="toggleAppStar('Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#192</span>
                    <a class="i-title" href="#Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF">Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#193</span>
                    <a class="i-title" href="#Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF">CountSE: Soft Exemplar Open-set Object Counting</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#194</span>
                    <a class="i-title" href="#Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF">Sparfels: Fast Reconstruction from Sparse Unposed Imagery</a>
                    <a class="i-star" onclick="toggleAppStar('Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#195</span>
                    <a class="i-title" href="#Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF">Underwater Visual SLAM with Depth Uncertainty and Medium Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#196</span>
                    <a class="i-title" href="#Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF">Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training</a>
                    <a class="i-star" onclick="toggleAppStar('Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#197</span>
                    <a class="i-title" href="#Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF">Rectifying Magnitude Neglect in Linear Attention</a>
                    <a class="i-star" onclick="toggleAppStar('Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#198</span>
                    <a class="i-title" href="#Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF">Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder</a>
                    <a class="i-star" onclick="toggleAppStar('Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#199</span>
                    <a class="i-title" href="#Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF">DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#200</span>
                    <a class="i-title" href="#Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF">PanoLlama: Generating Endless and Coherent Panoramas with Next-Token-Prediction LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#201</span>
                    <a class="i-title" href="#Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF">DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#202</span>
                    <a class="i-title" href="#Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF">ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#203</span>
                    <a class="i-title" href="#Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF">CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#204</span>
                    <a class="i-title" href="#Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF">PLMP - Point-Line Minimal Problems for Projective SfM</a>
                    <a class="i-star" onclick="toggleAppStar('Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#205</span>
                    <a class="i-title" href="#Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF">TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging</a>
                    <a class="i-star" onclick="toggleAppStar('Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#206</span>
                    <a class="i-title" href="#Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF">UniDxMD: Towards Unified Representation for Cross-Modal Unsupervised Domain Adaptation in 3D Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#207</span>
                    <a class="i-title" href="#Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF">ZIM: Zero-Shot Image Matting for Anything</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#208</span>
                    <a class="i-title" href="#Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF">Backdoor Mitigation by Distance-Driven Detoxification</a>
                    <a class="i-star" onclick="toggleAppStar('Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#209</span>
                    <a class="i-title" href="#Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF">UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI</a>
                    <a class="i-star" onclick="toggleAppStar('Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#210</span>
                    <a class="i-title" href="#Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF">Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#211</span>
                    <a class="i-title" href="#Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF">SMSTracker: Tri-path Score Mask Sigma Fusion for Multi-Modal Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#212</span>
                    <a class="i-title" href="#Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF">Two Losses, One Goal: Balancing Conflict Gradients for Semi-supervised Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#213</span>
                    <a class="i-title" href="#Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF">Region-based Cluster Discrimination for Visual Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#214</span>
                    <a class="i-title" href="#Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF">Shape of Motion: 4D Reconstruction from a Single Video</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#215</span>
                    <a class="i-title" href="#Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF">FPEM: Face Prior Enhanced Facial Attractiveness Prediction for Live Videos with Face Retouching</a>
                    <a class="i-star" onclick="toggleAppStar('Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#216</span>
                    <a class="i-title" href="#Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF">Dataset Distillation via Vision-Language Category Prototype</a>
                    <a class="i-star" onclick="toggleAppStar('Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#217</span>
                    <a class="i-title" href="#Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF">A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#218</span>
                    <a class="i-title" href="#Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF">Stereo Any Video: Temporally Consistent Stereo Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#219</span>
                    <a class="i-title" href="#Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF">Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#220</span>
                    <a class="i-title" href="#Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF">When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#221</span>
                    <a class="i-title" href="#Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF">Scendi Score: Prompt-Aware Diversity Evaluation via Schur Complement of CLIP Embeddings</a>
                    <a class="i-star" onclick="toggleAppStar('Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#222</span>
                    <a class="i-title" href="#Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF">Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</a>
                    <a class="i-star" onclick="toggleAppStar('Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#223</span>
                    <a class="i-title" href="#Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF">Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#224</span>
                    <a class="i-title" href="#Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF">DIMO: Diverse 3D Motion Generation for Arbitrary Objects</a>
                    <a class="i-star" onclick="toggleAppStar('Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#225</span>
                    <a class="i-title" href="#Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF">Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#226</span>
                    <a class="i-title" href="#Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a>
                    <a class="i-star" onclick="toggleAppStar('Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#227</span>
                    <a class="i-title" href="#Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF">GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#228</span>
                    <a class="i-title" href="#Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF">A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds</a>
                    <a class="i-star" onclick="toggleAppStar('Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#229</span>
                    <a class="i-title" href="#Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF">Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting</a>
                    <a class="i-star" onclick="toggleAppStar('Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#230</span>
                    <a class="i-title" href="#Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF">Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting</a>
                    <a class="i-star" onclick="toggleAppStar('Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#231</span>
                    <a class="i-title" href="#Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF">SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#232</span>
                    <a class="i-title" href="#Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF">Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology</a>
                    <a class="i-star" onclick="toggleAppStar('Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#233</span>
                    <a class="i-title" href="#Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF">TurboVSR: Fantastic Video Upscalers and Where to Find Them</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#234</span>
                    <a class="i-title" href="#Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF">Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures</a>
                    <a class="i-star" onclick="toggleAppStar('Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#235</span>
                    <a class="i-title" href="#Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF">Rethinking DPO-style Diffusion Aligning Frameworks</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#236</span>
                    <a class="i-title" href="#Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF">Ensemble Foreground Management for Unsupervised Object Discovery</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#237</span>
                    <a class="i-title" href="#Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF">InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity</a>
                    <a class="i-star" onclick="toggleAppStar('Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#238</span>
                    <a class="i-title" href="#Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF">The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#239</span>
                    <a class="i-title" href="#Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF">CounterPC: Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#240</span>
                    <a class="i-title" href="#Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF">GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#241</span>
                    <a class="i-title" href="#Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF">Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</a>
                    <a class="i-star" onclick="toggleAppStar('Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#242</span>
                    <a class="i-title" href="#Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF">LEGION: Learning to Ground and Explain for Synthetic Image Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#243</span>
                    <a class="i-title" href="#Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF">Beyond Walking: A Large-Scale Image-Text Benchmark for Text-based Person Anomaly Search</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#244</span>
                    <a class="i-title" href="#Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF">Images as Noisy Labels: Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#245</span>
                    <a class="i-title" href="#Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF">ResidualViT for Efficient Temporally Dense Video Encoding</a>
                    <a class="i-star" onclick="toggleAppStar('Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#246</span>
                    <a class="i-title" href="#Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF">MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#247</span>
                    <a class="i-title" href="#Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF">DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#248</span>
                    <a class="i-title" href="#Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF">Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#249</span>
                    <a class="i-title" href="#Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF">Sliced Wasserstein Bridge for Open-Vocabulary Video Instance Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#250</span>
                    <a class="i-title" href="#Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF">Self-Calibrating Gaussian Splatting for Large Field-of-View Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#251</span>
                    <a class="i-title" href="#You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF">LVFace: Progressive Cluster Optimization for Large Vision Models in Face Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#252</span>
                    <a class="i-title" href="#Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF">SRefiner: Soft-Braid Attention for Multi-Agent Trajectory Refinement</a>
                    <a class="i-star" onclick="toggleAppStar('Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#253</span>
                    <a class="i-title" href="#Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF">LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Geometry_Distributions@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#254</span>
                    <a class="i-title" href="#Zhang_Geometry_Distributions@ICCV2025@CVF">Geometry Distributions</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Geometry_Distributions@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Geometry_Distributions@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#255</span>
                    <a class="i-title" href="#Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF">SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM</a>
                    <a class="i-star" onclick="toggleAppStar('Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#256</span>
                    <a class="i-title" href="#Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF">MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#257</span>
                    <a class="i-title" href="#Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF">ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#258</span>
                    <a class="i-title" href="#Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF">SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#259</span>
                    <a class="i-title" href="#Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF">Wasserstein Style Distribution Analysis and Transform for Stylized Image Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#260</span>
                    <a class="i-title" href="#Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF">Learning to Generalize without Bias for Open-Vocabulary Action Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#261</span>
                    <a class="i-title" href="#Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF">TikZero: Zero-Shot Text-Guided Graphics Program Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#262</span>
                    <a class="i-title" href="#Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF">MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment</a>
                    <a class="i-star" onclick="toggleAppStar('Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#263</span>
                    <a class="i-title" href="#Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF">PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



</body></html>