{
  "source_html": "html\\neurlips-spotlight.html",
  "paper_count": 687,
  "conference": "neurlips",
  "year": 2025,
  "status": "spotlight",
  "papers": [
    {
      "paper_id": "lClK4uBxSG@OpenReview",
      "index": 1,
      "title": "ModHiFi: Identifying High Fidelity predictive components for Model Modification",
      "authors": [
        "Dhruva Kashyap",
        "Chaitanya Murti",
        "Pranav K Nayak",
        "Tanay Narshana",
        "Chiranjib Bhattacharyya"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "modhifi",
        "unlearning",
        "modification",
        "access",
        "fidelity",
        "components",
        "identifying",
        "predictive",
        "pruning",
        "loss"
      ],
      "summary": "Modifying well-trained models for purposes such as pruning or unlearning, without access to training data or the original loss function, is a challenging problem. While techniques exist for such modification, they often require training data, are computationally expensive, or are architecture-specific. To address this, we investigate the fundamental question of identifying components that are critical to the model’s predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term *Subset Fidelity*. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose **ModHiFi**, an algorithm for model modification that requires no training data or loss function access. **ModHiFi-P**, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. **ModHiFi-U**, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lClK4uBxSG"
        ],
        "venue": [
          "/venue/lClK4uBxSG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lClK4uBxSG"
        ],
        "detail": [
          "https://openreview.net/forum?id=lClK4uBxSG"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 16
      },
      "raw_excerpt": "ModHiFi: Identifying High Fidelity predictive components for Model Modification [PDF 15 ] [Copy] [Kimi 16 ] [REL] Authors : Dhruva Kashyap , Chaitanya Murti , Pranav K Nayak , Tanay Narshana , Chiranjib Bhattacharyya Modifying well-trained models for purposes such as pruning or unlearning, without access to training data or the original loss function, is a challenging problem. While techniques exist for such modification, they often require training data, are computationally expensive, or are architecture-specific. To address this, we investigate the fundamental question of identifying components that are critical to the model’s predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term *Subset Fidelity*. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose **ModHiFi**, an algorithm for model modification that requires no training data or loss function access. **ModHiFi-P**, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. **ModHiFi-U**, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XsBzmJzJ2l@OpenReview",
      "index": 2,
      "title": "The Structure of Relation Decoding Linear Operators in Large Language Models",
      "authors": [
        "Miranda Anna Christ",
        "Adrián Csiszárik",
        "Gergely Becsó",
        "Dániel Varga"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "relation",
        "decoding",
        "country",
        "linear",
        "operators",
        "language",
        "relational",
        "relations",
        "property",
        "structure"
      ],
      "summary": "This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XsBzmJzJ2l"
        ],
        "venue": [
          "/venue/XsBzmJzJ2l@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XsBzmJzJ2l"
        ],
        "detail": [
          "https://openreview.net/forum?id=XsBzmJzJ2l"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 13
      },
      "raw_excerpt": "The Structure of Relation Decoding Linear Operators in Large Language Models [PDF 5 ] [Copy] [Kimi 13 ] [REL] Authors : Miranda Anna Christ , Adrián Csiszárik , Gergely Becsó , Dániel Varga This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gOG9Zoyn4R@OpenReview",
      "index": 3,
      "title": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
      "authors": [
        "Seo Hyun Kim",
        "Sunwoo Hong",
        "Hojung Jung",
        "Youngrok Park",
        "Se-Young Yun"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "klass",
        "masked",
        "diffusion",
        "generation",
        "sampling",
        "fast",
        "inference",
        "bottlenecked",
        "unmasking",
        "speedups"
      ],
      "summary": "Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to 2.78 × 2.78 × wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gOG9Zoyn4R"
        ],
        "venue": [
          "/venue/gOG9Zoyn4R@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gOG9Zoyn4R"
        ],
        "detail": [
          "https://openreview.net/forum?id=gOG9Zoyn4R"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 8
      },
      "raw_excerpt": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models [PDF 14 ] [Copy] [Kimi 8 ] [REL] Authors : Seo Hyun Kim , Sunwoo Hong , Hojung Jung , Youngrok Park , Se-Young Yun Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to 2.78 × 2.78 × wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "JeP0lpusYw@OpenReview",
      "index": 4,
      "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
      "authors": [
        "Yu Zhou",
        "Xingyu Wu",
        "Jibin Wu",
        "Liang Feng",
        "KC Tan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hm3",
        "merging",
        "architecture",
        "pretrained",
        "space",
        "model",
        "hierarchical",
        "objective",
        "parameter",
        "models"
      ],
      "summary": "Model merging is a technique that combines multiple large pretrained models into a single model, enhancing performance and broadening task adaptability without original data or additional training. However, most existing model merging methods focus primarily on exploring the parameter space, merging models with identical architectures. Despite its potential, merging in the architecture space remains in its early stages due to the vast search space and challenges related to layer compatibility. This paper designs a hierarchical model merging framework named HM3, formulating a bilevel multi-objective model merging problem across both parameter and architecture spaces. At the parameter level, HM3 integrates existing merging methods to quickly identify optimal parameters. Based on these, an actor-critic strategy with efficient policy discretization is employed at the architecture level to explore inference paths with Markov property in the layer-granularity search space for reconstructing these optimal models. By training reusable policy and value networks, HM3 learns Pareto optimal models to provide customized solutions for various tasks. Experimental results on language and vision tasks demonstrate that HM3 outperforms methods focusing solely on the parameter or architecture space.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JeP0lpusYw"
        ],
        "venue": [
          "/venue/JeP0lpusYw@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JeP0lpusYw"
        ],
        "detail": [
          "https://openreview.net/forum?id=JeP0lpusYw"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 7
      },
      "raw_excerpt": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models [PDF 7 ] [Copy] [Kimi 7 ] [REL] Authors : Yu Zhou , Xingyu Wu , Jibin Wu , Liang Feng , KC Tan Model merging is a technique that combines multiple large pretrained models into a single model, enhancing performance and broadening task adaptability without original data or additional training. However, most existing model merging methods focus primarily on exploring the parameter space, merging models with identical architectures. Despite its potential, merging in the architecture space remains in its early stages due to the vast search space and challenges related to layer compatibility. This paper designs a hierarchical model merging framework named HM3, formulating a bilevel multi-objective model merging problem across both parameter and architecture spaces. At the parameter level, HM3 integrates existing merging methods to quickly identify optimal parameters. Based on these, an actor-critic strategy with efficient policy discretization is employed at the architecture level to explore inference paths with Markov property in the layer-granularity search space for reconstructing these optimal models. By training reusable policy and value networks, HM3 learns Pareto optimal models to provide customized solutions for various tasks. Experimental results on language and vision tasks demonstrate that HM3 outperforms methods focusing solely on the parameter or architecture space. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RDbuSCWhad@OpenReview",
      "index": 5,
      "title": "Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models",
      "authors": [
        "Aleksandar Terzic",
        "Nicolas Menet",
        "Michael Hersche",
        "Thomas Hofmann",
        "Abbas Rahimi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fsa",
        "ssm",
        "ssms",
        "state",
        "structured",
        "matrices",
        "transition",
        "sparse",
        "tracking",
        "expressivity"
      ],
      "summary": "Modern state-space models (SSMs) often utilize structured transition matrices which enable efficient computation but pose restrictions on the model’s expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost, even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with provably optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, \\emph{PD-SSM}, parametrizes the transition matrix as the product of a column one-hot matrix ( P P ) and a complex-valued diagonal matrix ( D D ). As a result, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any N N -state FSA with one layer of dimension N N and a linear readout of size N × N N × N , significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multivariate time-series classification, it outperforms neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded into sets of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RDbuSCWhad"
        ],
        "venue": [
          "/venue/RDbuSCWhad@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RDbuSCWhad"
        ],
        "detail": [
          "https://openreview.net/forum?id=RDbuSCWhad"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 5
      },
      "raw_excerpt": "Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models [PDF 2 ] [Copy] [Kimi 5 ] [REL] Authors : Aleksandar Terzic , Nicolas Menet , Michael Hersche , Thomas Hofmann , Abbas Rahimi Modern state-space models (SSMs) often utilize structured transition matrices which enable efficient computation but pose restrictions on the model’s expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost, even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with provably optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, \\emph{PD-SSM}, parametrizes the transition matrix as the product of a column one-hot matrix ( P P ) and a complex-valued diagonal matrix ( D D ). As a result, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any N N -state FSA with one layer of dimension N N and a linear readout of size N × N N × N , significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multivariate time-series classification, it outperforms neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded into sets of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Ve693NkzcU@OpenReview",
      "index": 6,
      "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top- p p Pruning",
      "authors": [
        "Chaofan Lin",
        "Jiaming Tang",
        "Shuo Yang",
        "Hanshuo Wang",
        "Tian Tang",
        "Boyu Tian",
        "Ion Stoica",
        "Song Han",
        "Mingyu Gao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "twilight",
        "attention",
        "sparse",
        "budget",
        "sparsity",
        "adaptive",
        "top",
        "tokens",
        "accuracy",
        "leveraging"
      ],
      "summary": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been of great importance recently. However, most existing sparse attention algorithms use a fixed budget of how many tokens to use in their computations. This simple static decision raises critical issues in real-world deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we reveal a key insight that leveraging the idea of top- p p sampling (a.k.a., nucleus sampling) in sparse attention could enable efficient and adaptive budget decisions. Based on this, we propose Twilight, a framework that enhances any existing sparse attention algorithm with adaptive budget decision capabilities without sacrificing accuracy. Empirical results show that Twilight can adaptively prune up to 98% tokens with nearly no accuracy loss in both mid- and long-context scenarios, leading to a 1.4 × 1.4 × speedup over state-of-the-art sparse attention mechanisms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Ve693NkzcU"
        ],
        "venue": [
          "/venue/Ve693NkzcU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Ve693NkzcU"
        ],
        "detail": [
          "https://openreview.net/forum?id=Ve693NkzcU"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 11
      },
      "raw_excerpt": "Twilight: Adaptive Attention Sparsity with Hierarchical Top- p p Pruning [PDF 13 ] [Copy] [Kimi 11 ] [REL] Authors : Chaofan Lin , Jiaming Tang , Shuo Yang , Hanshuo Wang , Tian Tang , Boyu Tian , Ion Stoica , Song Han , Mingyu Gao Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been of great importance recently. However, most existing sparse attention algorithms use a fixed budget of how many tokens to use in their computations. This simple static decision raises critical issues in real-world deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we reveal a key insight that leveraging the idea of top- p p sampling (a.k.a., nucleus sampling) in sparse attention could enable efficient and adaptive budget decisions. Based on this, we propose Twilight, a framework that enhances any existing sparse attention algorithm with adaptive budget decision capabilities without sacrificing accuracy. Empirical results show that Twilight can adaptively prune up to 98% tokens with nearly no accuracy loss in both mid- and long-context scenarios, leading to a 1.4 × 1.4 × speedup over state-of-the-art sparse attention mechanisms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "C1LVIInfZO@OpenReview",
      "index": 7,
      "title": "An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation",
      "authors": [
        "UZAIR AKBAR",
        "Niki Kilbertus",
        "Hao Shen",
        "Krikamol Muandet",
        "Bo Dai"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ivl",
        "causal",
        "outcome",
        "interventions",
        "ivs",
        "confounding",
        "augmentation",
        "case",
        "estimation",
        "invariant"
      ],
      "summary": "The technique of data augmentation (DA) is often used in machine learning for regularization purposes to better generalize under i.i.d. settings. In this work, we present a unifying framework with topics in causal inference to make a case for the use of DA beyond just the i.i.d. setting, but for generalization across interventions as well. Specifically, we argue that when the outcome generating mechanism is invariant to our choice of DA, then such augmentations can effectively be thought of as interventions on the treatment generating mechanism itself. This can potentially help to reduce bias in causal effect estimation arising from hidden confounders. In the presence of such unobserved confounding we typically make use of instrumental variables (IVs)—sources of treatment randomization that are conditionally independent of the outcome. However, IVs may not be as readily available as DA for many applications, which is the main motivation behind this work. By appropriately regularizing IV based estimators, we introduce the concept of *IV-like (IVL)* regression for mitigating confounding bias and improving predictive performance across interventions even when certain IV properties are relaxed. Finally, we cast parameterized DA as an IVL regression problem and show that when used in composition can simulate a worst-case application of such DA, further improving performance on causal estimation and generalization tasks beyond what simple DA may offer. This is shown both theoretically for the population case and via simulation experiments for the finite sample case using a simple linear example. We also present real data experiments to support our case.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=C1LVIInfZO"
        ],
        "venue": [
          "/venue/C1LVIInfZO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=C1LVIInfZO"
        ],
        "detail": [
          "https://openreview.net/forum?id=C1LVIInfZO"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 8
      },
      "raw_excerpt": "An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation [PDF 4 ] [Copy] [Kimi 8 ] [REL] Authors : UZAIR AKBAR , Niki Kilbertus , Hao Shen , Krikamol Muandet , Bo Dai The technique of data augmentation (DA) is often used in machine learning for regularization purposes to better generalize under i.i.d. settings. In this work, we present a unifying framework with topics in causal inference to make a case for the use of DA beyond just the i.i.d. setting, but for generalization across interventions as well. Specifically, we argue that when the outcome generating mechanism is invariant to our choice of DA, then such augmentations can effectively be thought of as interventions on the treatment generating mechanism itself. This can potentially help to reduce bias in causal effect estimation arising from hidden confounders. In the presence of such unobserved confounding we typically make use of instrumental variables (IVs)—sources of treatment randomization that are conditionally independent of the outcome. However, IVs may not be as readily available as DA for many applications, which is the main motivation behind this work. By appropriately regularizing IV based estimators, we introduce the concept of *IV-like (IVL)* regression for mitigating confounding bias and improving predictive performance across interventions even when certain IV properties are relaxed. Finally, we cast parameterized DA as an IVL regression problem and show that when used in composition can simulate a worst-case application of such DA, further improving performance on causal estimation and generalization tasks beyond what simple DA may offer. This is shown both theoretically for the population case and via simulation experiments for the finite sample case using a simple linear example. We also present real data experiments to support our case. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "E16vULI6AF@OpenReview",
      "index": 8,
      "title": "Deciphering the Extremes: A Novel Approach for Pathological Long-tailed Recognition in Scientific Discovery",
      "authors": [
        "Zhe Zhao",
        "HaiBin Wen",
        "Xianfu Liu",
        "Rui Mao",
        "Pengkun Wang",
        "Liheng Yu",
        "Linjiang Chen",
        "Bo An",
        "Qingfu Zhang",
        "Yang Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "scientific",
        "tailed",
        "tail",
        "pathological",
        "extremes",
        "zincfluor",
        "discovery",
        "recognition",
        "imbalance",
        "cifar"
      ],
      "summary": "Scientific discovery across diverse fields increasingly grapples with datasets exhibiting pathological long-tailed distributions: a few common phenomena overshadow a multitude of rare yet scientifically critical instances. Unlike standard benchmarks, these scientific datasets often feature extreme imbalance coupled with a modest number of classes and limited overall sample volume, rendering existing long-tailed recognition (LTR) techniques ineffective. Such methods, biased by majority classes or prone to overfitting on scarce tail data, frequently fail to identify the very instances—novel materials, rare disease biomarkers, faint astronomical signals—that drive scientific breakthroughs. This paper introduces a novel, end-to-end framework explicitly designed to address pathological long-tailed recognition in scientific contexts. Our approach synergizes a Balanced Supervised Contrastive Learning (B-SCL) mechanism, which enhances the representation of tail classes by dynamically re-weighting their contributions, with a Smooth Objective Regularization (SOR) strategy that manages the inherent tension between tail-class focus and overall classification performance. We introduce and analyze the real-world ZincFluor chemical dataset ( T = 137.54 T = 137.54 ) and synthetic benchmarks with controllable extreme imbalances (CIFAR-LT variants). Extensive evaluations demonstrate our method's superior ability to decipher these extremes. Notably, on ZincFluor, our approach achieves a Tail Top-2 accuracy of 66.84 % 66.84 % , significantly outperforming existing techniques. On CIFAR-10-LT with an imbalance ratio of 1000 1000 ( T = 100 T = 100 ), our method achieves a tail-class accuracy of 38.99 % 38.99 % , substantially leading the next best. These results underscore our framework's potential to unlock novel insights from complex, imbalanced scientific datasets, thereby accelerating discovery.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=E16vULI6AF"
        ],
        "venue": [
          "/venue/E16vULI6AF@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=E16vULI6AF"
        ],
        "detail": [
          "https://openreview.net/forum?id=E16vULI6AF"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 5
      },
      "raw_excerpt": "Deciphering the Extremes: A Novel Approach for Pathological Long-tailed Recognition in Scientific Discovery [PDF 7 ] [Copy] [Kimi 5 ] [REL] Authors : Zhe Zhao , HaiBin Wen , Xianfu Liu , Rui Mao , Pengkun Wang , Liheng Yu , Linjiang Chen , Bo An , Qingfu Zhang , Yang Wang Scientific discovery across diverse fields increasingly grapples with datasets exhibiting pathological long-tailed distributions: a few common phenomena overshadow a multitude of rare yet scientifically critical instances. Unlike standard benchmarks, these scientific datasets often feature extreme imbalance coupled with a modest number of classes and limited overall sample volume, rendering existing long-tailed recognition (LTR) techniques ineffective. Such methods, biased by majority classes or prone to overfitting on scarce tail data, frequently fail to identify the very instances—novel materials, rare disease biomarkers, faint astronomical signals—that drive scientific breakthroughs. This paper introduces a novel, end-to-end framework explicitly designed to address pathological long-tailed recognition in scientific contexts. Our approach synergizes a Balanced Supervised Contrastive Learning (B-SCL) mechanism, which enhances the representation of tail classes by dynamically re-weighting their contributions, with a Smooth Objective Regularization (SOR) strategy that manages the inherent tension between tail-class focus and overall classification performance. We introduce and analyze the real-world ZincFluor chemical dataset ( T = 137.54 T = 137.54 ) and synthetic benchmarks with controllable extreme imbalances (CIFAR-LT variants). Extensive evaluations demonstrate our method's superior ability to decipher these extremes. Notably, on ZincFluor, our approach achieves a Tail Top-2 accuracy of 66.84 % 66.84 % , significantly outperforming existing techniques. On CIFAR-10-LT with an imbalance ratio of 1000 1000 ( T = 100 T = 100 ), our method achieves a tail-class accuracy of 38.99 % 38.99 % , substantially leading the next best. These results underscore our framework's potential to unlock novel insights from complex, imbalanced scientific datasets, thereby accelerating discovery. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6iRZvJiC9Q@OpenReview",
      "index": 9,
      "title": "OpenCUA: Open Foundations for Computer-Use Agents",
      "authors": [
        "Xinyuan Wang",
        "Bowen Wang",
        "Dunjie Lu",
        "Junlin Yang",
        "Tianbao Xie",
        "Junli Wang",
        "Jiaqi Deng",
        "Xiaole Guo",
        "Yiheng Xu",
        "Chen Henry Wu",
        "Zhennan Shen",
        "Zhuokai Li",
        "Ryan Li",
        "Xiaochuan Li",
        "Junda Chen",
        "Zheng Boyuan",
        "LI PEIHANG",
        "Fangyu Lei",
        "Ruisheng Cao",
        "Yeqiao Fu",
        "Dongchan Shin",
        "Martin Shin",
        "Hu Jiarui",
        "Yuyan Wang",
        "Jixuan Chen",
        "Yuxiao Ye",
        "Danyang Zhang",
        "Yipu Wang",
        "Heng Wang",
        "Diyi Yang",
        "Victor Zhong",
        "Y.Charles",
        "Zhilin Yang",
        "Tao Yu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cua",
        "opencua",
        "computer",
        "open",
        "foundations",
        "agents",
        "cuas",
        "demonstrations",
        "agentnet",
        "osworld"
      ],
      "summary": "Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state–action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-72B achieves an average success rate of 45.0% on OSWorld‑Verified, establishing a new state-of-the-art (SOTA) among open-source models. Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6iRZvJiC9Q"
        ],
        "venue": [
          "/venue/6iRZvJiC9Q@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6iRZvJiC9Q"
        ],
        "detail": [
          "https://openreview.net/forum?id=6iRZvJiC9Q"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 12
      },
      "raw_excerpt": "OpenCUA: Open Foundations for Computer-Use Agents [PDF 12 ] [Copy] [Kimi 12 ] [REL] Authors : Xinyuan Wang , Bowen Wang , Dunjie Lu , Junlin Yang , Tianbao Xie , Junli Wang , Jiaqi Deng , Xiaole Guo , Yiheng Xu , Chen Henry Wu , Zhennan Shen , Zhuokai Li , Ryan Li , Xiaochuan Li , Junda Chen , Zheng Boyuan , LI PEIHANG , Fangyu Lei , Ruisheng Cao , Yeqiao Fu , Dongchan Shin , Martin Shin , Hu Jiarui , Yuyan Wang , Jixuan Chen , Yuxiao Ye , Danyang Zhang , Yipu Wang , Heng Wang , Diyi Yang , Victor Zhong , Y.Charles , Zhilin Yang , Tao Yu Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state–action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-72B achieves an average success rate of 45.0% on OSWorld‑Verified, establishing a new state-of-the-art (SOTA) among open-source models. Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "opAU0pYlcP@OpenReview",
      "index": 10,
      "title": "Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models",
      "authors": [
        "Ehsan Sharifian",
        "Saber Salehkaleybar",
        "Negar Kiyavash"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "equivalence",
        "causal",
        "reward",
        "class",
        "interventions",
        "matchings",
        "intervention",
        "bipartite",
        "graphs",
        "optimal"
      ],
      "summary": "We study the problem of causal structure learning from a combination of observational and interventional data generated by a linear non-Gaussian structural equation model that might contain cycles. Recent results show that using mere observational data identifies the causal graph only up to a permutation-equivalence class. We obtain a combinatorial characterization of this class by showing that each equivalence class corresponds to a perfect matching in a bipartite graph. This bipartite representation allows us to analyze how interventions modify or constrain the matchings. Specifically, we show that each atomic intervention reveals one edge of the true matching and eliminates all incompatible causal graphs. Consequently, we formalize the optimal experiment design task as an adaptive stochastic optimization problem over the set of equivalence classes with a natural reward function that quantifies how many graphs are eliminated from the equivalence class by an intervention. We show that this reward function is adaptive submodular and provide a greedy policy with a provable near-optimal performance guarantee. A key technical challenge is to efficiently estimate the reward function without having to explicitly enumerate all the graphs in the equivalence class. We propose a sampling-based estimator using random matchings and analyze its bias and concentration behavior. Our simulation results show that performing a small number of interventions guided by our stochastic optimization framework recovers the true underlying causal structure.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=opAU0pYlcP"
        ],
        "venue": [
          "/venue/opAU0pYlcP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=opAU0pYlcP"
        ],
        "detail": [
          "https://openreview.net/forum?id=opAU0pYlcP"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Ehsan Sharifian , Saber Salehkaleybar , Negar Kiyavash We study the problem of causal structure learning from a combination of observational and interventional data generated by a linear non-Gaussian structural equation model that might contain cycles. Recent results show that using mere observational data identifies the causal graph only up to a permutation-equivalence class. We obtain a combinatorial characterization of this class by showing that each equivalence class corresponds to a perfect matching in a bipartite graph. This bipartite representation allows us to analyze how interventions modify or constrain the matchings. Specifically, we show that each atomic intervention reveals one edge of the true matching and eliminates all incompatible causal graphs. Consequently, we formalize the optimal experiment design task as an adaptive stochastic optimization problem over the set of equivalence classes with a natural reward function that quantifies how many graphs are eliminated from the equivalence class by an intervention. We show that this reward function is adaptive submodular and provide a greedy policy with a provable near-optimal performance guarantee. A key technical challenge is to efficiently estimate the reward function without having to explicitly enumerate all the graphs in the equivalence class. We propose a sampling-based estimator using random matchings and analyze its bias and concentration behavior. Our simulation results show that performing a small number of interventions guided by our stochastic optimization framework recovers the true underlying causal structure. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7qrhHzZpTA@OpenReview",
      "index": 11,
      "title": "Escaping saddle points without Lipschitz smoothness: the power of nonlinear preconditioning",
      "authors": [
        "Alexander Bodard",
        "Panagiotis Patrinos"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "smoothness",
        "preconditioning",
        "saddle",
        "gradient",
        "lipschitz",
        "escaping",
        "classical",
        "nonlinear",
        "nonlinearly",
        "preconditioned"
      ],
      "summary": "We study generalized smoothness in nonconvex optimization, focusing on ( L 0 , L 1 ) ( L 0 , L 1 ) -smoothness and anisotropic smoothness. The former was empirically derived from practical neural network training examples, while the latter arises naturally in the analysis of nonlinearly preconditioned gradient methods. We introduce a new sufficient condition that encompasses both notions, reveals their close connection, and holds in key applications such as phase retrieval and matrix factorization. Leveraging tools from dynamical systems theory, we then show that nonlinear preconditioning—including gradient clipping—preserves the saddle point avoidance property of classical gradient descent. Crucially, the assumptions required for this analysis are actually satisfied in these applications, unlike in classical results that rely on restrictive Lipschitz smoothness conditions. We further analyze a perturbed variant that efficiently attains second-order stationarity with only logarithmic dependence on dimension, matching similar guarantees of classical gradient methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7qrhHzZpTA"
        ],
        "venue": [
          "/venue/7qrhHzZpTA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7qrhHzZpTA"
        ],
        "detail": [
          "https://openreview.net/forum?id=7qrhHzZpTA"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Escaping saddle points without Lipschitz smoothness: the power of nonlinear preconditioning [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Alexander Bodard , Panagiotis Patrinos We study generalized smoothness in nonconvex optimization, focusing on ( L 0 , L 1 ) ( L 0 , L 1 ) -smoothness and anisotropic smoothness. The former was empirically derived from practical neural network training examples, while the latter arises naturally in the analysis of nonlinearly preconditioned gradient methods. We introduce a new sufficient condition that encompasses both notions, reveals their close connection, and holds in key applications such as phase retrieval and matrix factorization. Leveraging tools from dynamical systems theory, we then show that nonlinear preconditioning—including gradient clipping—preserves the saddle point avoidance property of classical gradient descent. Crucially, the assumptions required for this analysis are actually satisfied in these applications, unlike in classical results that rely on restrictive Lipschitz smoothness conditions. We further analyze a perturbed variant that efficiently attains second-order stationarity with only logarithmic dependence on dimension, matching similar guarantees of classical gradient methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XNo4yS9n1k@OpenReview",
      "index": 12,
      "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models",
      "authors": [
        "Zekai Zhao",
        "Qi Liu",
        "Kun Zhou",
        "Zihan Liu",
        "Yifei Shao",
        "Zhiting Hu",
        "Biwei Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "eliciting",
        "activation",
        "cot",
        "activations",
        "long",
        "ability",
        "lora",
        "thought",
        "reasoning",
        "efficiently"
      ],
      "summary": "Despite the remarkable reasoning performance, eliciting the long chain-of-thought(CoT) ability in large language models(LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers, greatly govern the long-form reasoning attributes, e.g. output length and self-reflection. Through simply amplifying these activations and adding ``wait'' tokens, the long CoT ability can be invoked without training, leading to significantly increased self-reflection rate and accuracy. In addition, we also find that the activation changes follow predictable trajectories, i.e. a sharp rise after special tokens and a subsequent exponential decay. Based on these insights, we introduce a general training-free activation control technique. It utilizes a few contrastive examples to identify the relevant activations, and then incorporates simple analytic functions to adjust their values at inference time to elicit long CoTs. Extensive experiments have verified the effectiveness of our methods in efficiently eliciting the long CoT ability of LLMs and improving the performance. Besides, we further propose a parameter-efficient fine-tuning method that trains only the last-layer activation amplification module and a few LoRA layers, outperforming LoRA on reasoning benchmarks with much fewer parameters. Our code and data will be fully public released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XNo4yS9n1k"
        ],
        "venue": [
          "/venue/XNo4yS9n1k@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XNo4yS9n1k"
        ],
        "detail": [
          "https://openreview.net/forum?id=XNo4yS9n1k"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 17
      },
      "raw_excerpt": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models [PDF 15 ] [Copy] [Kimi 17 ] [REL] Authors : Zekai Zhao , Qi Liu , Kun Zhou , Zihan Liu , Yifei Shao , Zhiting Hu , Biwei Huang Despite the remarkable reasoning performance, eliciting the long chain-of-thought(CoT) ability in large language models(LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers, greatly govern the long-form reasoning attributes, e.g. output length and self-reflection. Through simply amplifying these activations and adding ``wait'' tokens, the long CoT ability can be invoked without training, leading to significantly increased self-reflection rate and accuracy. In addition, we also find that the activation changes follow predictable trajectories, i.e. a sharp rise after special tokens and a subsequent exponential decay. Based on these insights, we introduce a general training-free activation control technique. It utilizes a few contrastive examples to identify the relevant activations, and then incorporates simple analytic functions to adjust their values at inference time to elicit long CoTs. Extensive experiments have verified the effectiveness of our methods in efficiently eliciting the long CoT ability of LLMs and improving the performance. Besides, we further propose a parameter-efficient fine-tuning method that trains only the last-layer activation amplification module and a few LoRA layers, outperforming LoRA on reasoning benchmarks with much fewer parameters. Our code and data will be fully public released. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2h8bFmEQwh@OpenReview",
      "index": 13,
      "title": "Direct Fisher Score Estimation for Likelihood Maximization",
      "authors": [
        "Sherman Khoo",
        "Yakun Wang",
        "Song Liu",
        "Mark Beaumont"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "score",
        "likelihood",
        "fisher",
        "maximization",
        "smoothing",
        "technique",
        "iterate",
        "intractable",
        "parameterization",
        "landscapes"
      ],
      "summary": "We study the problem of likelihood maximization when the likelihood function is intractable but model simulations are readily available. We propose a sequential, gradient-based optimization method that directly models the Fisher score based on a local score matching technique which uses simulations from a localized region around each parameter iterate. By employing a linear parameterization for the surrogate score model, our technique admits a closed-form, least-squares solution. This approach yields a fast, flexible, and efficient approximation to the Fisher score, effectively smoothing the likelihood objective and mitigating the challenges posed by complex likelihood landscapes. We provide theoretical guarantees for our score estimator, including bounds on the bias introduced by the smoothing. Empirical results on a range of synthetic and real-world problems demonstrate the superior performance of our method compared to existing benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2h8bFmEQwh"
        ],
        "venue": [
          "/venue/2h8bFmEQwh@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2h8bFmEQwh"
        ],
        "detail": [
          "https://openreview.net/forum?id=2h8bFmEQwh"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Direct Fisher Score Estimation for Likelihood Maximization [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Sherman Khoo , Yakun Wang , Song Liu , Mark Beaumont We study the problem of likelihood maximization when the likelihood function is intractable but model simulations are readily available. We propose a sequential, gradient-based optimization method that directly models the Fisher score based on a local score matching technique which uses simulations from a localized region around each parameter iterate. By employing a linear parameterization for the surrogate score model, our technique admits a closed-form, least-squares solution. This approach yields a fast, flexible, and efficient approximation to the Fisher score, effectively smoothing the likelihood objective and mitigating the challenges posed by complex likelihood landscapes. We provide theoretical guarantees for our score estimator, including bounds on the bias introduced by the smoothing. Empirical results on a range of synthetic and real-world problems demonstrate the superior performance of our method compared to existing benchmarks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xKmlBQhgI4@OpenReview",
      "index": 14,
      "title": "Path-Enhanced Contrastive Learning for Recommendation",
      "authors": [
        "Haoran Sun",
        "Fei Xiong",
        "Yuanzhe Hu",
        "Liang Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "path",
        "contrastive",
        "node",
        "graph",
        "loss",
        "recommendation",
        "nodes",
        "enhanced",
        "pecl",
        "methods"
      ],
      "summary": "Collaborative filtering (CF) methods are now facing the challenge of data sparsity in recommender systems. In order to reduce the effect of data sparsity, researchers proposed contrastive learning methods to extract self-supervised signals from raw data. Contrastive learning methods address this problem by graph augmentation and maximizing the consistency of node representations between different augmented graphs. However, these methods tends to unintentionally distance the target node from its path nodes on the interaction path, thus limiting its effectiveness. In this regard, we propose a solution that uses paths as samples in the contrastive loss function. In order to obtain the path samples, we design a path sampling method. In addition to the contrast of the relationship between the target node and the nodes within the path (intra-path contrast), we also designed a method of contrasting the relationship between the paths (inter-path contrast) to better pull the target node and its path nodes closer to each other. We use Simplifying and Powering Graph Convolution Network (LightGCN) as the basis and combine with a new path-enhanced graph approach proposed for graph augmentation. It effectively improves the performance of recommendation models. Our proposed Path Enhanced Contrastive Loss (PECL) model replaces the common contrastive loss function with our novel loss function, showing significant performance improvement. Experiments on three real-world datasets demonstrate the effectiveness of our model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xKmlBQhgI4"
        ],
        "venue": [
          "/venue/xKmlBQhgI4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xKmlBQhgI4"
        ],
        "detail": [
          "https://openreview.net/forum?id=xKmlBQhgI4"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 7
      },
      "raw_excerpt": "Path-Enhanced Contrastive Learning for Recommendation [PDF 9 ] [Copy] [Kimi 7 ] [REL] Authors : Haoran Sun , Fei Xiong , Yuanzhe Hu , Liang Wang Collaborative filtering (CF) methods are now facing the challenge of data sparsity in recommender systems. In order to reduce the effect of data sparsity, researchers proposed contrastive learning methods to extract self-supervised signals from raw data. Contrastive learning methods address this problem by graph augmentation and maximizing the consistency of node representations between different augmented graphs. However, these methods tends to unintentionally distance the target node from its path nodes on the interaction path, thus limiting its effectiveness. In this regard, we propose a solution that uses paths as samples in the contrastive loss function. In order to obtain the path samples, we design a path sampling method. In addition to the contrast of the relationship between the target node and the nodes within the path (intra-path contrast), we also designed a method of contrasting the relationship between the paths (inter-path contrast) to better pull the target node and its path nodes closer to each other. We use Simplifying and Powering Graph Convolution Network (LightGCN) as the basis and combine with a new path-enhanced graph approach proposed for graph augmentation. It effectively improves the performance of recommendation models. Our proposed Path Enhanced Contrastive Loss (PECL) model replaces the common contrastive loss function with our novel loss function, showing significant performance improvement. Experiments on three real-world datasets demonstrate the effectiveness of our model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "jvObbvshjE@OpenReview",
      "index": 15,
      "title": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning",
      "authors": [
        "Julie Mordacq",
        "David Loiseaux",
        "Vicky Kalogeiton",
        "Steve Oudot"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "regs",
        "ssl",
        "spanning",
        "uniformity",
        "minimum",
        "tree",
        "regularization",
        "self",
        "supervised",
        "collapse"
      ],
      "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data, often by enforcing invariance to input transformations such as rotations or blurring. Recent studies have highlighted two pivotal properties for effective representations: (i) avoiding dimensional collapse-where the learned features occupy only a low-dimensional subspace, and (ii) enhancing uniformity of the induced distribution. In this work, we introduce T-REGS, a simple regularization framework for SSL based on the length of the Minimum Spanning Tree (MST) over the learned representation. We provide theoretical analysis demonstrating that T-REGS simultaneously mitigates dimensional collapse and promotes distribution uniformity on arbitrary compact Riemannian manifolds. Several experiments on synthetic data and on classical SSL benchmarks validate the effectiveness of our approach at enhancing representation quality.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jvObbvshjE"
        ],
        "venue": [
          "/venue/jvObbvshjE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jvObbvshjE"
        ],
        "detail": [
          "https://openreview.net/forum?id=jvObbvshjE"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Julie Mordacq , David Loiseaux , Vicky Kalogeiton , Steve Oudot Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data, often by enforcing invariance to input transformations such as rotations or blurring. Recent studies have highlighted two pivotal properties for effective representations: (i) avoiding dimensional collapse-where the learned features occupy only a low-dimensional subspace, and (ii) enhancing uniformity of the induced distribution. In this work, we introduce T-REGS, a simple regularization framework for SSL based on the length of the Minimum Spanning Tree (MST) over the learned representation. We provide theoretical analysis demonstrating that T-REGS simultaneously mitigates dimensional collapse and promotes distribution uniformity on arbitrary compact Riemannian manifolds. Several experiments on synthetic data and on classical SSL benchmarks validate the effectiveness of our approach at enhancing representation quality. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kfB5Ciz2XZ@OpenReview",
      "index": 16,
      "title": "Generating Informative Samples for Risk-Averse Fine-Tuning of Downstream Tasks",
      "authors": [
        "Heasung Kim",
        "Taekyun Lee",
        "Hyeji Kim",
        "Gustavo De Veciana"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "risk",
        "cvar",
        "loss",
        "averse",
        "informative",
        "samples",
        "minimization",
        "generative",
        "reweighted",
        "high"
      ],
      "summary": "Risk-averse modeling is critical in safety-sensitive and high-stakes applications. Conditional Value-at-Risk (CVaR) quantifies such risk by measuring the expected loss in the tail of the loss distribution, and minimizing it provides a principled framework for training robust models. However, direct CVaR minimization remains challenging due to the difficulty of accurately estimating rare, high-loss events—particularly at extreme quantiles. In this work, we propose a novel training framework that synthesizes informative samples for CVaR optimization using score-based generative models. Specifically, we guide a diffusion-based generative model to sample from a reweighted distribution that emphasizes inputs likely to incur high loss under a pretrained reference model. These samples are then incorporated via a loss-weighted importance sampling scheme to reduce noise in stochastic optimization. We establish convergence guarantees and show that the synthesized, high-loss-emphasized dataset substantially contributes to the noise reduction. Empirically, we validate the effectiveness of our approach across multiple settings, including a real-world wireless channel compression task, where our method achieves significant improvements over standard risk minimization strategies.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kfB5Ciz2XZ"
        ],
        "venue": [
          "/venue/kfB5Ciz2XZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kfB5Ciz2XZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=kfB5Ciz2XZ"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Generating Informative Samples for Risk-Averse Fine-Tuning of Downstream Tasks [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Heasung Kim , Taekyun Lee , Hyeji Kim , Gustavo De Veciana Risk-averse modeling is critical in safety-sensitive and high-stakes applications. Conditional Value-at-Risk (CVaR) quantifies such risk by measuring the expected loss in the tail of the loss distribution, and minimizing it provides a principled framework for training robust models. However, direct CVaR minimization remains challenging due to the difficulty of accurately estimating rare, high-loss events—particularly at extreme quantiles. In this work, we propose a novel training framework that synthesizes informative samples for CVaR optimization using score-based generative models. Specifically, we guide a diffusion-based generative model to sample from a reweighted distribution that emphasizes inputs likely to incur high loss under a pretrained reference model. These samples are then incorporated via a loss-weighted importance sampling scheme to reduce noise in stochastic optimization. We establish convergence guarantees and show that the synthesized, high-loss-emphasized dataset substantially contributes to the noise reduction. Empirically, we validate the effectiveness of our approach across multiple settings, including a real-world wireless channel compression task, where our method achieves significant improvements over standard risk minimization strategies. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "jSgCM0uZn3@OpenReview",
      "index": 17,
      "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
      "authors": [
        "Ran Xu",
        "Yuchen Zhuang",
        "Zihan Dong",
        "Ruiyu Wang",
        "Yue Yu",
        "Joyce C. Ho",
        "Linjun Zhang",
        "Haoyu Wang",
        "Wenqi Shi",
        "Carl Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "acesearcher",
        "reasoning",
        "tasks",
        "search",
        "llms",
        "iits",
        "play",
        "bootstrapping",
        "reinforced",
        "32b"
      ],
      "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the giant DeepSeek-V3 model using less than 5% of iits parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9× more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jSgCM0uZn3"
        ],
        "venue": [
          "/venue/jSgCM0uZn3@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jSgCM0uZn3"
        ],
        "detail": [
          "https://openreview.net/forum?id=jSgCM0uZn3"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 10
      },
      "raw_excerpt": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play [PDF 8 ] [Copy] [Kimi 10 ] [REL] Authors : Ran Xu , Yuchen Zhuang , Zihan Dong , Ruiyu Wang , Yue Yu , Joyce C. Ho , Linjun Zhang , Haoyu Wang , Wenqi Shi , Carl Yang Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the giant DeepSeek-V3 model using less than 5% of iits parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9× more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "T9qNDtvAJX@OpenReview",
      "index": 18,
      "title": "DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method",
      "authors": [
        "Qingwen Zhang",
        "Xiaomeng Zhu",
        "Yushan Zhang",
        "Yixi Cai",
        "Olov Andersson",
        "Patric Jensfelt"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "deltaflow",
        "flow",
        "frames",
        "frame",
        "scene",
        "delta",
        "estimation",
        "temporal",
        "motion",
        "multi"
      ],
      "summary": "Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ( Δ Δ Flow), a lightweight 3D framework that captures motion cues via a Δ Δ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that Δ Δ Flow achieves state-of-the-art performance with up to 22\\% lower error and 2 × 2 × faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=T9qNDtvAJX"
        ],
        "venue": [
          "/venue/T9qNDtvAJX@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=T9qNDtvAJX"
        ],
        "detail": [
          "https://openreview.net/forum?id=T9qNDtvAJX"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Qingwen Zhang , Xiaomeng Zhu , Yushan Zhang , Yixi Cai , Olov Andersson , Patric Jensfelt Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ( Δ Δ Flow), a lightweight 3D framework that captures motion cues via a Δ Δ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that Δ Δ Flow achieves state-of-the-art performance with up to 22\\% lower error and 2 × 2 × faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "G2kMroO9UV@OpenReview",
      "index": 19,
      "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
      "authors": [
        "Hyungjoo Chae",
        "Sunghwan Kim",
        "Junhee Cho",
        "Seungone Kim",
        "Seungjun Moon",
        "Gyeom Hwangbo",
        "Dongha Lim",
        "Minjin Kim",
        "Yeonjun Hwang",
        "Minju Gwak",
        "Dongwook Choi",
        "Minseok Kang",
        "Gwanhoon Im",
        "ByeongUng Cho",
        "Hyojun Kim",
        "Jun Hee Han",
        "Taeyoon Kwon",
        "Minju Kim",
        "Beong-woo Kwak",
        "Dongjin Kang",
        "Jinyoung Yeo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shepherd",
        "web",
        "prms",
        "webrewardbench",
        "navigation",
        "reward",
        "gpt",
        "verifier",
        "mini",
        "reinforcing"
      ],
      "summary": "Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10x less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at https://github.com/kyle8581/Web-Shepherd.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=G2kMroO9UV"
        ],
        "venue": [
          "/venue/G2kMroO9UV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=G2kMroO9UV"
        ],
        "detail": [
          "https://openreview.net/forum?id=G2kMroO9UV"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 2
      },
      "raw_excerpt": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents [PDF 7 ] [Copy] [Kimi 2 ] [REL] Authors : Hyungjoo Chae , Sunghwan Kim , Junhee Cho , Seungone Kim , Seungjun Moon , Gyeom Hwangbo , Dongha Lim , Minjin Kim , Yeonjun Hwang , Minju Gwak , Dongwook Choi , Minseok Kang , Gwanhoon Im , ByeongUng Cho , Hyojun Kim , Jun Hee Han , Taeyoon Kwon , Minju Kim , Beong-woo Kwak , Dongjin Kang , Jinyoung Yeo Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10x less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at https://github.com/kyle8581/Web-Shepherd. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "19ygs48nOa@OpenReview",
      "index": 20,
      "title": "How do Transformers Learn Implicit Reasoning?",
      "authors": [
        "Jiaran Ye",
        "Zijun Yao",
        "Zhidian Huang",
        "Liangming Pan",
        "Jinxin Liu",
        "Yushi Bai",
        "Amy Xin",
        "Liu Weichuan",
        "Xiaoyin Che",
        "Lei Hou",
        "Juanzi Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "hop",
        "implicit",
        "representational",
        "transformers",
        "cosine",
        "verbalizing",
        "generalization",
        "llms",
        "intermediate"
      ],
      "summary": "Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly---producing correct answers without explicitly verbalizing intermediate steps---but the underlying mechanisms remain poorly understood. In this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment. Our analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization. We find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures. To interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space. This clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. These findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=19ygs48nOa"
        ],
        "venue": [
          "/venue/19ygs48nOa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=19ygs48nOa"
        ],
        "detail": [
          "https://openreview.net/forum?id=19ygs48nOa"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 11
      },
      "raw_excerpt": "How do Transformers Learn Implicit Reasoning? [PDF 17 ] [Copy] [Kimi 11 ] [REL] Authors : Jiaran Ye , Zijun Yao , Zhidian Huang , Liangming Pan , Jinxin Liu , Yushi Bai , Amy Xin , Liu Weichuan , Xiaoyin Che , Lei Hou , Juanzi Li Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly---producing correct answers without explicitly verbalizing intermediate steps---but the underlying mechanisms remain poorly understood. In this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment. Our analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization. We find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures. To interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space. This clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. These findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IrgQe6YjKm@OpenReview",
      "index": 21,
      "title": "On the sample complexity of semi-supervised multi-objective learning",
      "authors": [
        "Tobias Wegel",
        "Geelon So",
        "Junhyung Park",
        "Fanny Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "complexity",
        "mol",
        "mathcal",
        "objective",
        "tasks",
        "losses",
        "unlabeled",
        "sample",
        "semi",
        "supervised"
      ],
      "summary": "In multi-objective learning (MOL), several possibly competing prediction tasks must be solved jointly by a single model. Achieving good trade-offs may require a model class G G with larger capacity than what is necessary for solving the individual tasks. This, in turn, increases the statistical cost, as reflected in known MOL bounds that depend on the complexity of G G . We show that this cost is unavoidable for some losses, even in an idealized semi-supervised setting, where the learner has access to the Bayes-optimal solutions for the individual tasks as well as the marginal distributions over the covariates. On the other hand, for objectives defined with Bregman losses, we prove that the complexity of G G may come into play only in terms of unlabeled data. Concretely, we establish sample complexity upper bounds, showing precisely when and how unlabeled data can significantly alleviate the need for labeled data. This is achieved by a simple pseudo-labeling algorithm.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IrgQe6YjKm"
        ],
        "venue": [
          "/venue/IrgQe6YjKm@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IrgQe6YjKm"
        ],
        "detail": [
          "https://openreview.net/forum?id=IrgQe6YjKm"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "On the sample complexity of semi-supervised multi-objective learning [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Tobias Wegel , Geelon So , Junhyung Park , Fanny Yang In multi-objective learning (MOL), several possibly competing prediction tasks must be solved jointly by a single model. Achieving good trade-offs may require a model class G G with larger capacity than what is necessary for solving the individual tasks. This, in turn, increases the statistical cost, as reflected in known MOL bounds that depend on the complexity of G G . We show that this cost is unavoidable for some losses, even in an idealized semi-supervised setting, where the learner has access to the Bayes-optimal solutions for the individual tasks as well as the marginal distributions over the covariates. On the other hand, for objectives defined with Bregman losses, we prove that the complexity of G G may come into play only in terms of unlabeled data. Concretely, we establish sample complexity upper bounds, showing precisely when and how unlabeled data can significantly alleviate the need for labeled data. This is achieved by a simple pseudo-labeling algorithm. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5eZ0iykpDU@OpenReview",
      "index": 22,
      "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
      "authors": [
        "Jian Yao",
        "Ran Cheng",
        "Xingyu Wu",
        "Jibin Wu",
        "KC Tan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "diversity",
        "reasoning",
        "llm",
        "aware",
        "policy",
        "deepseek",
        "optimization",
        "language",
        "llms",
        "training"
      ],
      "summary": "The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek-R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and potential@k (a novel metric quantifying an LLM’s reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically, we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5\\% average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5eZ0iykpDU"
        ],
        "venue": [
          "/venue/5eZ0iykpDU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5eZ0iykpDU"
        ],
        "detail": [
          "https://openreview.net/forum?id=5eZ0iykpDU"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 5
      },
      "raw_excerpt": "Diversity-Aware Policy Optimization for Large Language Model Reasoning [PDF 11 ] [Copy] [Kimi 5 ] [REL] Authors : Jian Yao , Ran Cheng , Xingyu Wu , Jibin Wu , KC Tan The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek-R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and potential@k (a novel metric quantifying an LLM’s reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically, we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5\\% average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "KT8y9pFgJE@OpenReview",
      "index": 23,
      "title": "Fixed-Point RNNs: Interpolating from Diagonal to Dense",
      "authors": [
        "Sajad Movahedi",
        "Felix Sarnthein",
        "Nicola Muca Cirone",
        "Antonio Orvieto"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rnns",
        "diagonal",
        "expressivity",
        "fixed",
        "dense",
        "interpolating",
        "mixing",
        "tracking",
        "linear",
        "ssms"
      ],
      "summary": "Linear recurrent neural networks (RNNs) and state-space models (SSMs) such as Mamba have become promising alternatives to softmax-attention as sequence mixing layers in Transformer architectures. Current models, however, do not exhibit the full state-tracking expressivity of RNNs because they rely on channel-wise (i.e. diagonal) sequence mixing. In this paper, we investigate parameterizations of a large class of dense linear RNNs as fixed-points of parallelizable diagonal linear RNNs. The resulting models can naturally trade expressivity for efficiency at a fixed number of parameters and achieve state-of-the-art results on the state-tracking benchmarks A 5 A 5 and S 5 S 5 , while matching performance on copying and other tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KT8y9pFgJE"
        ],
        "venue": [
          "/venue/KT8y9pFgJE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KT8y9pFgJE"
        ],
        "detail": [
          "https://openreview.net/forum?id=KT8y9pFgJE"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Fixed-Point RNNs: Interpolating from Diagonal to Dense [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Sajad Movahedi , Felix Sarnthein , Nicola Muca Cirone , Antonio Orvieto Linear recurrent neural networks (RNNs) and state-space models (SSMs) such as Mamba have become promising alternatives to softmax-attention as sequence mixing layers in Transformer architectures. Current models, however, do not exhibit the full state-tracking expressivity of RNNs because they rely on channel-wise (i.e. diagonal) sequence mixing. In this paper, we investigate parameterizations of a large class of dense linear RNNs as fixed-points of parallelizable diagonal linear RNNs. The resulting models can naturally trade expressivity for efficiency at a fixed number of parameters and achieve state-of-the-art results on the state-tracking benchmarks A 5 A 5 and S 5 S 5 , while matching performance on copying and other tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WYnvP3DePZ@OpenReview",
      "index": 24,
      "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks",
      "authors": [
        "Veronica Lachi",
        "Francesco Ferrini",
        "Antonio Longa",
        "Bruno Lepri",
        "Andrea Passerini",
        "Manfred Jaeger"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "expressiveness",
        "link",
        "graph",
        "practice",
        "expressive",
        "links",
        "bridging",
        "representation",
        "framework",
        "subsumes"
      ],
      "summary": "Graph Neural Networks (GNNs) are widely used to compute representations of node pairs for downstream tasks such as link prediction. Yet, theoretical understanding of their expressive power has focused almost entirely on graph-level representations. In this work, we shift the focus to links and provide the first comprehensive study of GNN expressiveness in link representation. We introduce a unifying framework, the k ϕ k ϕ - k ρ k ρ - m m framework, that subsumes existing message-passing link models and enables formal expressiveness comparisons. Using this framework, we derive a hierarchy of state-of-the-art methods and offer theoretical tools to analyze future architectures. To complement our analysis, we propose a synthetic evaluation protocol comprising the first benchmark specifically designed to assess link-level expressiveness. Finally, we ask: does expressiveness matter in practice? We use a graph symmetry metric that quantifies the difficulty of distinguishing links and show that while expressive models may underperform on standard benchmarks, they significantly outperform simpler ones as symmetry increases, highlighting the need for dataset-aware model selection.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WYnvP3DePZ"
        ],
        "venue": [
          "/venue/WYnvP3DePZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WYnvP3DePZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=WYnvP3DePZ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Bridging Theory and Practice in Link Representation with Graph Neural Networks [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Veronica Lachi , Francesco Ferrini , Antonio Longa , Bruno Lepri , Andrea Passerini , Manfred Jaeger Graph Neural Networks (GNNs) are widely used to compute representations of node pairs for downstream tasks such as link prediction. Yet, theoretical understanding of their expressive power has focused almost entirely on graph-level representations. In this work, we shift the focus to links and provide the first comprehensive study of GNN expressiveness in link representation. We introduce a unifying framework, the k ϕ k ϕ - k ρ k ρ - m m framework, that subsumes existing message-passing link models and enables formal expressiveness comparisons. Using this framework, we derive a hierarchy of state-of-the-art methods and offer theoretical tools to analyze future architectures. To complement our analysis, we propose a synthetic evaluation protocol comprising the first benchmark specifically designed to assess link-level expressiveness. Finally, we ask: does expressiveness matter in practice? We use a graph symmetry metric that quantifies the difficulty of distinguishing links and show that while expressive models may underperform on standard benchmarks, they significantly outperform simpler ones as symmetry increases, highlighting the need for dataset-aware model selection. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3P3PL7aCXM@OpenReview",
      "index": 25,
      "title": "ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space",
      "authors": [
        "Chuanchao Zang",
        "Xiangtao Meng",
        "Wenyu Chen",
        "Tianshuo Cong",
        "Zha Yaxing",
        "Dong Qi",
        "Zheng Li",
        "Shanqing Guo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "errortrace",
        "traceability",
        "8518",
        "error",
        "2593",
        "box",
        "protection",
        "black",
        "mechanism",
        "attacks"
      ],
      "summary": "The open-source release of large language models (LLMs) enables malicious users to create unauthorized derivative models at low cost, posing significant threats to intellectual property (IP) and market stability. Existing IP protection methods either require access to model parameters or are vulnerable to fine-tuning attacks. To fill this gap, we propose ErrorTrace, a robust and black-box traceability mechanism for protecting LLM IP. Specifically, ErrorTrace leverages the unique error patterns of model families by mapping and analyzing their distinct error spaces, enabling robust and efficient IP protection without relying on internal parameters or specific query responses. Experimental results show that ErrorTrace achieves a traceability accuracy of 0.8518 for 27 base models when the suspect model is not included in ErrorTrace's training set, outperforming the baseline by 0.2593. Additionally,ErrorTrace successfully tracks 34 fine-tuned, pruned and merged models across various scenarios, demonstrating its broad applicability and robustness. In addition, ErrorTrace shows a certain level of resilience when subjected to adversarial attacks. Our code is available at: https://github.com/csdatazcc/ErrorTrace.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3P3PL7aCXM"
        ],
        "venue": [
          "/venue/3P3PL7aCXM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3P3PL7aCXM"
        ],
        "detail": [
          "https://openreview.net/forum?id=3P3PL7aCXM"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 4
      },
      "raw_excerpt": "ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space [PDF 2 ] [Copy] [Kimi 4 ] [REL] Authors : Chuanchao Zang , Xiangtao Meng , Wenyu Chen , Tianshuo Cong , Zha Yaxing , Dong Qi , Zheng Li , Shanqing Guo The open-source release of large language models (LLMs) enables malicious users to create unauthorized derivative models at low cost, posing significant threats to intellectual property (IP) and market stability. Existing IP protection methods either require access to model parameters or are vulnerable to fine-tuning attacks. To fill this gap, we propose ErrorTrace, a robust and black-box traceability mechanism for protecting LLM IP. Specifically, ErrorTrace leverages the unique error patterns of model families by mapping and analyzing their distinct error spaces, enabling robust and efficient IP protection without relying on internal parameters or specific query responses. Experimental results show that ErrorTrace achieves a traceability accuracy of 0.8518 for 27 base models when the suspect model is not included in ErrorTrace's training set, outperforming the baseline by 0.2593. Additionally,ErrorTrace successfully tracks 34 fine-tuned, pruned and merged models across various scenarios, demonstrating its broad applicability and robustness. In addition, ErrorTrace shows a certain level of resilience when subjected to adversarial attacks. Our code is available at: https://github.com/csdatazcc/ErrorTrace. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MpJkAzwUtl@OpenReview",
      "index": 26,
      "title": "Protein Design with Dynamic Protein Vocabulary",
      "authors": [
        "Nuowei Liu",
        "Jiahao Kuang",
        "Yanting Liu",
        "Tao Ji",
        "Changzhi Sun",
        "Man Lan",
        "Yuanbin Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "protein",
        "proteins",
        "foldability",
        "fragments",
        "prodva",
        "design",
        "descriptions",
        "plddt",
        "textual",
        "pae"
      ],
      "summary": "Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.62%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MpJkAzwUtl"
        ],
        "venue": [
          "/venue/MpJkAzwUtl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MpJkAzwUtl"
        ],
        "detail": [
          "https://openreview.net/forum?id=MpJkAzwUtl"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "Protein Design with Dynamic Protein Vocabulary [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Nuowei Liu , Jiahao Kuang , Yanting Liu , Tao Ji , Changzhi Sun , Man Lan , Yuanbin Wu Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.62%. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VaC4sa96EI@OpenReview",
      "index": 27,
      "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning",
      "authors": [
        "Sanghyun Ahn",
        "Wonje Choi",
        "Junyong Lee",
        "Jinwoo Park",
        "Honguk Woo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "task",
        "code",
        "embodied",
        "planning",
        "policies",
        "symbolic",
        "neuro",
        "framework",
        "success",
        "grounding"
      ],
      "summary": "Recent advances in large language models (LLMs) have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots, demonstrating the potential of LLM-based embodied intelligence. However, these LLM-based code-as-policies approaches often suffer from limited environmental grounding, particularly in dynamic or partially observable settings, leading to suboptimal task success rates due to incorrect or incomplete code generation. In this work, we propose a neuro-symbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation. In the validation phase, the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving task-relevant states. This integrated process enhances the grounding of generated code, resulting in improved task reliability and success rates in complex environments. We evaluate our framework on RLBench and in real-world settings across dynamic, partially observable scenarios. Experimental results demonstrate that our framework improves task success rates by 46.2\\% over Code as Policies baselines and attains over 86.8\\% executability of task-relevant actions, thereby enhancing the reliability of task planning in dynamic environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VaC4sa96EI"
        ],
        "venue": [
          "/venue/VaC4sa96EI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VaC4sa96EI"
        ],
        "detail": [
          "https://openreview.net/forum?id=VaC4sa96EI"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": null
      },
      "raw_excerpt": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning [PDF 5 ] [Copy] [Kimi ] [REL] Authors : Sanghyun Ahn , Wonje Choi , Junyong Lee , Jinwoo Park , Honguk Woo Recent advances in large language models (LLMs) have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots, demonstrating the potential of LLM-based embodied intelligence. However, these LLM-based code-as-policies approaches often suffer from limited environmental grounding, particularly in dynamic or partially observable settings, leading to suboptimal task success rates due to incorrect or incomplete code generation. In this work, we propose a neuro-symbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation. In the validation phase, the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving task-relevant states. This integrated process enhances the grounding of generated code, resulting in improved task reliability and success rates in complex environments. We evaluate our framework on RLBench and in real-world settings across dynamic, partially observable scenarios. Experimental results demonstrate that our framework improves task success rates by 46.2\\% over Code as Policies baselines and attains over 86.8\\% executability of task-relevant actions, thereby enhancing the reliability of task planning in dynamic environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kQWyOYUAC4@OpenReview",
      "index": 28,
      "title": "AI-Researcher: Autonomous Scientific Innovation",
      "authors": [
        "Jiabin Tang",
        "Lianghao Xia",
        "Zhonghang Li",
        "Chao Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "innovation",
        "researcher",
        "autonomous",
        "scientific",
        "research",
        "human",
        "papers",
        "capabilities",
        "agentic",
        "orchestrates"
      ],
      "summary": "The powerful reasoning capabilities of Large Language Models (LLMs) in mathematics and coding, combined with their ability to automate complex tasks through agentic frameworks, present unprecedented opportunities for accelerating scientific innovation. In this paper, we introduce AI-Researcher, a fully autonomous research system that transforms how AI-driven scientific discovery is conducted and evaluated. Our framework seamlessly orchestrates the complete research pipeline--from literature review and hypothesis generation to algorithm implementation and publication-ready manuscript preparation--with minimal human intervention. To rigorously assess autonomous research capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising state-of-the-art papers across diverse AI research domains, featuring both guided innovation and open-ended exploration tasks. Through extensive experiments, we demonstrate that AI-Researcher achieves remarkable implementation success rates and produces research papers that approach human-level quality. This work establishes new foundations for autonomous scientific innovation that can complement human researchers by systematically exploring solution spaces beyond cognitive limitations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kQWyOYUAC4"
        ],
        "venue": [
          "/venue/kQWyOYUAC4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kQWyOYUAC4"
        ],
        "detail": [
          "https://openreview.net/forum?id=kQWyOYUAC4"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 14
      },
      "raw_excerpt": "AI-Researcher: Autonomous Scientific Innovation [PDF 8 ] [Copy] [Kimi 14 ] [REL] Authors : Jiabin Tang , Lianghao Xia , Zhonghang Li , Chao Huang The powerful reasoning capabilities of Large Language Models (LLMs) in mathematics and coding, combined with their ability to automate complex tasks through agentic frameworks, present unprecedented opportunities for accelerating scientific innovation. In this paper, we introduce AI-Researcher, a fully autonomous research system that transforms how AI-driven scientific discovery is conducted and evaluated. Our framework seamlessly orchestrates the complete research pipeline--from literature review and hypothesis generation to algorithm implementation and publication-ready manuscript preparation--with minimal human intervention. To rigorously assess autonomous research capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising state-of-the-art papers across diverse AI research domains, featuring both guided innovation and open-ended exploration tasks. Through extensive experiments, we demonstrate that AI-Researcher achieves remarkable implementation success rates and produces research papers that approach human-level quality. This work establishes new foundations for autonomous scientific innovation that can complement human researchers by systematically exploring solution spaces beyond cognitive limitations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "KrglRiOKYT@OpenReview",
      "index": 29,
      "title": "Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency",
      "authors": [
        "Renzhao Liang",
        "Sizhe Xu",
        "Chenggang Xie",
        "Jingru Chen",
        "Feiyang Ren",
        "Shu Yang",
        "Takahiro Yabe"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "amrc",
        "masking",
        "consistency",
        "abstain",
        "loss",
        "representation",
        "redundant",
        "forecasting",
        "retain",
        "core"
      ],
      "summary": "Time series forecasting plays a pivotal role in critical domains such as energy management and financial markets. Although deep learning-based approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the prevailing \"long-sequence information gain hypothesis\" exhibits inherent limitations. Through systematic experimentation, this study reveals a counterintuitive phenomenon: appropriately truncating historical data can paradoxically enhance prediction accuracy, indicating that existing models learn substantial redundant features (e.g., noise or irrelevant fluctuations) during training, thereby compromising effective signal extraction. Building upon information bottleneck theory, we propose an innovative solution termed Adaptive Masking Loss with Representation Consistency (AMRC), which features two core components: 1) Dynamic masking loss, which adaptively identified highly discriminative temporal segments to guide gradient descent during model training; 2) Representation consistency constraint, which stabilized the mapping relationships among inputs, labels, and predictions. Experimental results demonstrate that AMRC effectively suppresses redundant feature learning while significantly improving model performance. This work not only challenges conventional assumptions in temporal modeling but also provides novel theoretical insights and methodological breakthroughs for developing efficient and robust forecasting models. We have made our code available at \\url{https://github.com/MazelTovy/AMRC}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KrglRiOKYT"
        ],
        "venue": [
          "/venue/KrglRiOKYT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KrglRiOKYT"
        ],
        "detail": [
          "https://openreview.net/forum?id=KrglRiOKYT"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Renzhao Liang , Sizhe Xu , Chenggang Xie , Jingru Chen , Feiyang Ren , Shu Yang , Takahiro Yabe Time series forecasting plays a pivotal role in critical domains such as energy management and financial markets. Although deep learning-based approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the prevailing \"long-sequence information gain hypothesis\" exhibits inherent limitations. Through systematic experimentation, this study reveals a counterintuitive phenomenon: appropriately truncating historical data can paradoxically enhance prediction accuracy, indicating that existing models learn substantial redundant features (e.g., noise or irrelevant fluctuations) during training, thereby compromising effective signal extraction. Building upon information bottleneck theory, we propose an innovative solution termed Adaptive Masking Loss with Representation Consistency (AMRC), which features two core components: 1) Dynamic masking loss, which adaptively identified highly discriminative temporal segments to guide gradient descent during model training; 2) Representation consistency constraint, which stabilized the mapping relationships among inputs, labels, and predictions. Experimental results demonstrate that AMRC effectively suppresses redundant feature learning while significantly improving model performance. This work not only challenges conventional assumptions in temporal modeling but also provides novel theoretical insights and methodological breakthroughs for developing efficient and robust forecasting models. We have made our code available at \\url{https://github.com/MazelTovy/AMRC}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "a49F7EAm6l@OpenReview",
      "index": 30,
      "title": "DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation",
      "authors": [
        "Kefei Zhu",
        "Fengshuo Bai",
        "YuanHao Xiang",
        "Yishuai Cai",
        "Xinglin Chen",
        "Ruochong Li",
        "Xingtao Wang",
        "Hao Dong",
        "Yaodong Yang",
        "Xiaopeng Fan",
        "Yuanpei Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dexflywheel",
        "dexterous",
        "demonstrations",
        "diverse",
        "improving",
        "data",
        "manipulation",
        "cycle",
        "policy",
        "self"
      ],
      "summary": "Dexterous manipulation is critical for advancing robot capabilities in real-world applications, yet diverse and high-quality datasets remain scarce. Existing data collection methods either rely on human teleoperation or require significant human engineering, or generate data with limited diversity, which restricts their scalability and generalization. In this paper, we introduce DexFlyWheel, a scalable data generation framework that employs a self-improving cycle to continuously enrich data diversity. Starting from efficient seed demonstrations warmup, DexFlyWheel expands the dataset through iterative cycles. Each cycle follows a closed-loop pipeline that integrates Imitation Learning (IL), residual Reinforcement Learning (RL), rollout trajectory collection, and data augmentation. Specifically, IL extracts human-like behaviors from demonstrations, and residual RL enhances policy generalization. The learned policy is then used to generate trajectories in simulation, which are further augmented across diverse environments and spatial configurations before being fed back into the next cycle. Over successive iterations, a self-improving data flywheel effect emerges, producing datasets that cover diverse scenarios and thereby scaling policy performance. Experimental results demonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across four challenging tasks. Policies trained on our dataset achieve an average success rate of 81.9\\% on the challenge test sets and successfully transfer to the real world through digital twin, achieving a 78.3\\% success rate on dual-arm lift tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=a49F7EAm6l"
        ],
        "venue": [
          "/venue/a49F7EAm6l@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=a49F7EAm6l"
        ],
        "detail": [
          "https://openreview.net/forum?id=a49F7EAm6l"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Kefei Zhu , Fengshuo Bai , YuanHao Xiang , Yishuai Cai , Xinglin Chen , Ruochong Li , Xingtao Wang , Hao Dong , Yaodong Yang , Xiaopeng Fan , Yuanpei Chen Dexterous manipulation is critical for advancing robot capabilities in real-world applications, yet diverse and high-quality datasets remain scarce. Existing data collection methods either rely on human teleoperation or require significant human engineering, or generate data with limited diversity, which restricts their scalability and generalization. In this paper, we introduce DexFlyWheel, a scalable data generation framework that employs a self-improving cycle to continuously enrich data diversity. Starting from efficient seed demonstrations warmup, DexFlyWheel expands the dataset through iterative cycles. Each cycle follows a closed-loop pipeline that integrates Imitation Learning (IL), residual Reinforcement Learning (RL), rollout trajectory collection, and data augmentation. Specifically, IL extracts human-like behaviors from demonstrations, and residual RL enhances policy generalization. The learned policy is then used to generate trajectories in simulation, which are further augmented across diverse environments and spatial configurations before being fed back into the next cycle. Over successive iterations, a self-improving data flywheel effect emerges, producing datasets that cover diverse scenarios and thereby scaling policy performance. Experimental results demonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across four challenging tasks. Policies trained on our dataset achieve an average success rate of 81.9\\% on the challenge test sets and successfully transfer to the real world through digital twin, achieving a 78.3\\% success rate on dual-arm lift tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7G9YKty2UZ@OpenReview",
      "index": 31,
      "title": "Learning Robust Vision-Language Models from Natural Latent Spaces",
      "authors": [
        "Zhangyun Wang",
        "Ni Ding",
        "Aniket Mahanti"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "vlms",
        "adversarial",
        "coapt",
        "natural",
        "vision",
        "latent",
        "pre",
        "trained",
        "prompt",
        "generalization"
      ],
      "summary": "Pre-trained vision-language models (VLMs) exhibit significant vulnerability to imperceptible adversarial perturbations. Current advanced defense strategies typically employ adversarial prompt tuning to improve the adversarial robustness of VLMs, which struggle to simultaneously maintain generalization across both natural and adversarial examples under different benchmarks and downstream tasks. We propose a collaborative adversarial prompt tuning (CoAPT) approach from pre-trained VLMs to target robust VLMs. Inspired by the image mask modeling, we adopt an improved real-time total variation algorithm to suppress and eliminate high-frequency details from images while preserving edge structures, thereby disrupting the adversarial perturbation space. Subsequently, guided by the high-level image and text representations in the latent space of the pre-trained VLMs, the corrupted natural features are restored while inheriting the superior generalization capability. Experiments on four benchmarks demonstrate that CoAPT achieves an excellent trade-off among natural generalization, adversarial robustness, and task-specific adaptation compared to state-of-the-art methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7G9YKty2UZ"
        ],
        "venue": [
          "/venue/7G9YKty2UZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7G9YKty2UZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=7G9YKty2UZ"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 6
      },
      "raw_excerpt": "Learning Robust Vision-Language Models from Natural Latent Spaces [PDF 10 ] [Copy] [Kimi 6 ] [REL] Authors : Zhangyun Wang , Ni Ding , Aniket Mahanti Pre-trained vision-language models (VLMs) exhibit significant vulnerability to imperceptible adversarial perturbations. Current advanced defense strategies typically employ adversarial prompt tuning to improve the adversarial robustness of VLMs, which struggle to simultaneously maintain generalization across both natural and adversarial examples under different benchmarks and downstream tasks. We propose a collaborative adversarial prompt tuning (CoAPT) approach from pre-trained VLMs to target robust VLMs. Inspired by the image mask modeling, we adopt an improved real-time total variation algorithm to suppress and eliminate high-frequency details from images while preserving edge structures, thereby disrupting the adversarial perturbation space. Subsequently, guided by the high-level image and text representations in the latent space of the pre-trained VLMs, the corrupted natural features are restored while inheriting the superior generalization capability. Experiments on four benchmarks demonstrate that CoAPT achieves an excellent trade-off among natural generalization, adversarial robustness, and task-specific adaptation compared to state-of-the-art methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xwqTt26NJf@OpenReview",
      "index": 32,
      "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
      "authors": [
        "Daniel Mingyi Israel",
        "Guy Van den Broeck",
        "Aditya Grover"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "apd",
        "decoding",
        "autoregressive",
        "parallel",
        "dllm",
        "llms",
        "tokens",
        "quality",
        "throughput",
        "diffusion"
      ],
      "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xwqTt26NJf"
        ],
        "venue": [
          "/venue/xwqTt26NJf@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xwqTt26NJf"
        ],
        "detail": [
          "https://openreview.net/forum?id=xwqTt26NJf"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 2
      },
      "raw_excerpt": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding [PDF 7 ] [Copy] [Kimi 2 ] [REL] Authors : Daniel Mingyi Israel , Guy Van den Broeck , Aditya Grover The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "AwLRF1lZvI@OpenReview",
      "index": 33,
      "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
      "authors": [
        "Rakshit Trivedi",
        "Kartik Sharma",
        "David C. Parkes"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mimic",
        "human",
        "behavior",
        "inner",
        "speech",
        "imitation",
        "guides",
        "behaviors",
        "coordination",
        "behavioral"
      ],
      "summary": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AwLRF1lZvI"
        ],
        "venue": [
          "/venue/AwLRF1lZvI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AwLRF1lZvI"
        ],
        "detail": [
          "https://openreview.net/forum?id=AwLRF1lZvI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Rakshit Trivedi , Kartik Sharma , David C. Parkes Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "g9XLUU3TaG@OpenReview",
      "index": 34,
      "title": "Graph-Based Attention for Differentiable MaxSAT Solving",
      "authors": [
        "Sota Moriyama",
        "Katsumi Inoue"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "maxsat",
        "sgats",
        "graph",
        "attention",
        "satisfiability",
        "sat",
        "solve",
        "based",
        "differentiably",
        "reasoning"
      ],
      "summary": "The use of deep learning to solve fundamental AI problems such as Boolean Satisfiability (SAT) has been explored recently to develop robust and scalable reasoning systems. This work advances such neural-based reasoning approaches by developing a new Graph Neural Network (GNN) to differentiably solve (weighted) Maximum Satisfiability (MaxSAT). To this end, we propose SAT-based Graph Attention Networks (SGATs) as novel GNNs that are built on t-norm based attention and message passing mechanisms, and structurally designed to approximate greedy distributed local search. To demonstrate the effectiveness of our model, we develop a local search solver that uses SGATs to continuously solve any given MaxSAT problem. Experiments on (weighted) MaxSAT benchmark datasets demonstrate that SGATs significantly outperform existing neural-based architectures, and achieve state-of-the-art performance among continuous approaches, highlighting the strength of the proposed model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=g9XLUU3TaG"
        ],
        "venue": [
          "/venue/g9XLUU3TaG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=g9XLUU3TaG"
        ],
        "detail": [
          "https://openreview.net/forum?id=g9XLUU3TaG"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Graph-Based Attention for Differentiable MaxSAT Solving [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Sota Moriyama , Katsumi Inoue The use of deep learning to solve fundamental AI problems such as Boolean Satisfiability (SAT) has been explored recently to develop robust and scalable reasoning systems. This work advances such neural-based reasoning approaches by developing a new Graph Neural Network (GNN) to differentiably solve (weighted) Maximum Satisfiability (MaxSAT). To this end, we propose SAT-based Graph Attention Networks (SGATs) as novel GNNs that are built on t-norm based attention and message passing mechanisms, and structurally designed to approximate greedy distributed local search. To demonstrate the effectiveness of our model, we develop a local search solver that uses SGATs to continuously solve any given MaxSAT problem. Experiments on (weighted) MaxSAT benchmark datasets demonstrate that SGATs significantly outperform existing neural-based architectures, and achieve state-of-the-art performance among continuous approaches, highlighting the strength of the proposed model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "W6WC6047X2@OpenReview",
      "index": 35,
      "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems",
      "authors": [
        "Christian Walder",
        "Deep Tejas Karkhanis"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pass",
        "harder",
        "optimization",
        "reward",
        "reinforcement",
        "batches",
        "performance",
        "utility",
        "pkpo",
        "passk"
      ],
      "summary": "Reinforcement Learning algorithms commonly sample multiple ( n > 1 n > 1 ) solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes individual sample performance over the diversity and collective utility of a set of samples. Such algorithms under-utilize the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at- k k Policy Optimization (PKPO), a multivariate transformation on batches of rewards which leads to direct optimization of \\passk\\ performance, thus optimizing for sets of samples that feature a large maximum reward when considered jointly. Our primary contribution is to derive novel low variance unbiased estimators for the pass@k and its gradient, in both the binary and continuous reward settings. We show that optimizing with these estimators reduces to reinforcement learning with (batches of) rewards that have been jointly transformed by a function that is stable and efficient to compute. While previous efforts propose transformations for k = n k = n , our transformations are the first to enable robust optimization of the pass@k for any arbitrary k ≤ n k ≤ n . Rather than simply trading off pass@1 performance for pass@k gains, our method allows annealing k k during training, optimizing both metrics and often achieving strong pass@1 performance alongside significant pass@k gains. We validate our transformations on illustrative toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source models Gemma and Llama . We find that our transformation effectively optimizes for the target k k . Furthermore, higher k k values enable solving more and harder problems, while annealing k k boosts both the pass@1 and pass@k. Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely by improving exploration through the prioritization of joint utility over the utility of individual samples",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=W6WC6047X2"
        ],
        "venue": [
          "/venue/W6WC6047X2@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=W6WC6047X2"
        ],
        "detail": [
          "https://openreview.net/forum?id=W6WC6047X2"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Christian Walder , Deep Tejas Karkhanis Reinforcement Learning algorithms commonly sample multiple ( n > 1 n > 1 ) solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes individual sample performance over the diversity and collective utility of a set of samples. Such algorithms under-utilize the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at- k k Policy Optimization (PKPO), a multivariate transformation on batches of rewards which leads to direct optimization of \\passk\\ performance, thus optimizing for sets of samples that feature a large maximum reward when considered jointly. Our primary contribution is to derive novel low variance unbiased estimators for the pass@k and its gradient, in both the binary and continuous reward settings. We show that optimizing with these estimators reduces to reinforcement learning with (batches of) rewards that have been jointly transformed by a function that is stable and efficient to compute. While previous efforts propose transformations for k = n k = n , our transformations are the first to enable robust optimization of the pass@k for any arbitrary k ≤ n k ≤ n . Rather than simply trading off pass@1 performance for pass@k gains, our method allows annealing k k during training, optimizing both metrics and often achieving strong pass@1 performance alongside significant pass@k gains. We validate our transformations on illustrative toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source models Gemma and Llama . We find that our transformation effectively optimizes for the target k k . Furthermore, higher k k values enable solving more and harder problems, while annealing k k boosts both the pass@1 and pass@k. Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely by improving exploration through the prioritization of joint utility over the utility of individual samples Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "z4AMrCOetn@OpenReview",
      "index": 36,
      "title": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data",
      "authors": [
        "Zehao Wang",
        "Lin Yang",
        "Jie Wang",
        "Kehan Wang",
        "Hanzhu Chen",
        "Bin Wang",
        "Jianye HAO",
        "Defu Lian",
        "Bin Li",
        "Enhong Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "logical",
        "logictree",
        "llms",
        "step",
        "logic",
        "synthesizing",
        "templates",
        "complex",
        "instantiated"
      ],
      "summary": "Despite their remarkable performance on various tasks, Large Language Models (LLMs) still struggle with logical reasoning, particularly in complex and multi-step reasoning processes. Among various efforts to enhance LLMs' reasoning capabilities, synthesizing large-scale, high-quality logical reasoning datasets has emerged as a promising direction. However, existing methods often rely on predefined templates for logical reasoning data generation, limiting their adaptability to real-world scenarios. To address the limitation, we propose **LogicTree**, a novel framework for efficiently synthesizing multi-step logical reasoning dataset that excels in both complexity and instantiation. By iteratively searching for applicable logic rules based on structural pattern matching to perform backward deduction, **LogicTree** constructs multi-step logic trees that capture complex reasoning patterns. Furthermore, we employ a two-stage LLM-based approach to instantiate various real-world scenarios for each logic tree, generating consistent real-world reasoning processes that carry contextual significance. This helps LLMs develop generalizable logical reasoning abilities across diverse scenarios rather than merely memorizing templates. Experiments on multiple benchmarks demonstrate that our approach achieves an average improvement of 9.4\\% in accuracy on complex logical reasoning tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=z4AMrCOetn"
        ],
        "venue": [
          "/venue/z4AMrCOetn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=z4AMrCOetn"
        ],
        "detail": [
          "https://openreview.net/forum?id=z4AMrCOetn"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Zehao Wang , Lin Yang , Jie Wang , Kehan Wang , Hanzhu Chen , Bin Wang , Jianye HAO , Defu Lian , Bin Li , Enhong Chen Despite their remarkable performance on various tasks, Large Language Models (LLMs) still struggle with logical reasoning, particularly in complex and multi-step reasoning processes. Among various efforts to enhance LLMs' reasoning capabilities, synthesizing large-scale, high-quality logical reasoning datasets has emerged as a promising direction. However, existing methods often rely on predefined templates for logical reasoning data generation, limiting their adaptability to real-world scenarios. To address the limitation, we propose **LogicTree**, a novel framework for efficiently synthesizing multi-step logical reasoning dataset that excels in both complexity and instantiation. By iteratively searching for applicable logic rules based on structural pattern matching to perform backward deduction, **LogicTree** constructs multi-step logic trees that capture complex reasoning patterns. Furthermore, we employ a two-stage LLM-based approach to instantiate various real-world scenarios for each logic tree, generating consistent real-world reasoning processes that carry contextual significance. This helps LLMs develop generalizable logical reasoning abilities across diverse scenarios rather than merely memorizing templates. Experiments on multiple benchmarks demonstrate that our approach achieves an average improvement of 9.4\\% in accuracy on complex logical reasoning tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "9k9ZsDs9Vc@OpenReview",
      "index": 37,
      "title": "Multitask Learning with Stochastic Interpolants",
      "authors": [
        "Hugo Negrel",
        "Florentin Coeurdoux",
        "Michael Samuel Albergo",
        "Eric Vanden-Eijnden"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "interpolants",
        "multitask",
        "stochastic",
        "generative",
        "probability",
        "distributions",
        "models",
        "fulfilling",
        "task",
        "broadly"
      ],
      "summary": "We propose a framework for learning maps between probability distributions that broadly generalizes the time dynamics of flow and diffusion models. To enable this, we generalize stochastic interpolants by replacing the scalar time variable with vectors, matrices, or linear operators, allowing us to bridge probability distributions across multiple dimensional spaces. This approach enables the construction of versatile generative models capable of fulfilling multiple tasks without task-specific training. Our operator-based interpolants not only provide a unifying theoretical perspective for existing generative models but also extend their capabilities. Through numerical experiments, we demonstrate the zero-shot efficacy of our method on conditional generation and inpainting, fine-tuning and posterior sampling, and multiscale modeling, suggesting its potential as a generic task-agnostic alternative to specialized models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9k9ZsDs9Vc"
        ],
        "venue": [
          "/venue/9k9ZsDs9Vc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9k9ZsDs9Vc"
        ],
        "detail": [
          "https://openreview.net/forum?id=9k9ZsDs9Vc"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Multitask Learning with Stochastic Interpolants [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Hugo Negrel , Florentin Coeurdoux , Michael Samuel Albergo , Eric Vanden-Eijnden We propose a framework for learning maps between probability distributions that broadly generalizes the time dynamics of flow and diffusion models. To enable this, we generalize stochastic interpolants by replacing the scalar time variable with vectors, matrices, or linear operators, allowing us to bridge probability distributions across multiple dimensional spaces. This approach enables the construction of versatile generative models capable of fulfilling multiple tasks without task-specific training. Our operator-based interpolants not only provide a unifying theoretical perspective for existing generative models but also extend their capabilities. Through numerical experiments, we demonstrate the zero-shot efficacy of our method on conditional generation and inpainting, fine-tuning and posterior sampling, and multiscale modeling, suggesting its potential as a generic task-agnostic alternative to specialized models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RSVdHXZN6D@OpenReview",
      "index": 38,
      "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities",
      "authors": [
        "Jin Wang",
        "Yao Lai",
        "Aoxue Li",
        "Shifeng Zhang",
        "Jiacheng Sun",
        "Ning Kang",
        "Chengyue Wu",
        "Zhenguo Li",
        "Ping Luo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fudoki",
        "mllms",
        "generation",
        "multimodal",
        "unified",
        "velocities",
        "discrete",
        "flow",
        "understanding",
        "based"
      ],
      "summary": "The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RSVdHXZN6D"
        ],
        "venue": [
          "/venue/RSVdHXZN6D@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RSVdHXZN6D"
        ],
        "detail": [
          "https://openreview.net/forum?id=RSVdHXZN6D"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Jin Wang , Yao Lai , Aoxue Li , Shifeng Zhang , Jiacheng Sun , Ning Kang , Chengyue Wu , Zhenguo Li , Ping Luo The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ugBmWX3H1R@OpenReview",
      "index": 39,
      "title": "Fast MRI for All: Bridging Access Gaps by Training without Raw Data",
      "authors": [
        "Yasar Utku Alcalar",
        "Merve Gulle",
        "Mehmet Akcakaya"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cupid",
        "mri",
        "access",
        "training",
        "fast",
        "clinical",
        "imaging",
        "rural",
        "compressibility",
        "raw"
      ],
      "summary": "Physics-driven deep learning (PD-DL) approaches have become popular for improved reconstruction of fast magnetic resonance imaging (MRI) scans. Though PD-DL offers higher acceleration rates than existing clinical fast MRI techniques, their use has been limited outside specialized MRI centers. A key challenge is generalization to rare pathologies or different populations, noted in multiple studies, with fine-tuning on target populations suggested for improvement. However, current approaches for PD-DL training require access to raw k-space measurements, which is typically only available at specialized MRI centers that have research agreements for such data access. This is especially an issue for rural and under-resourced areas, where commercial MRI scanners only provide access to a final reconstructed image. To tackle these challenges, we propose Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity (CUPID) for high-quality PD-DL training using only routine clinical reconstructed images exported from an MRI scanner. CUPID evaluates output quality with a compressibility-based approach while ensuring that the output stays consistent with the clinical parallel imaging reconstruction through well-designed perturbations. Our results show CUPID achieves similar quality to established PD-DL training that requires k-space data while outperforming compressed sensing (CS) and diffusion-based generative methods. We further demonstrate its effectiveness in a zero-shot training setup for retrospectively and prospectively sub-sampled acquisitions, attesting to its minimal training burden. As an approach that radically deviates from existing strategies, CUPID presents an opportunity to provide broader access to fast MRI for remote and rural populations in an attempt to reduce the obstacles associated with this expensive imaging modality. Code is available at https://github.com/ualcalar17/CUPID.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ugBmWX3H1R"
        ],
        "venue": [
          "/venue/ugBmWX3H1R@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ugBmWX3H1R"
        ],
        "detail": [
          "https://openreview.net/forum?id=ugBmWX3H1R"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Fast MRI for All: Bridging Access Gaps by Training without Raw Data [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Yasar Utku Alcalar , Merve Gulle , Mehmet Akcakaya Physics-driven deep learning (PD-DL) approaches have become popular for improved reconstruction of fast magnetic resonance imaging (MRI) scans. Though PD-DL offers higher acceleration rates than existing clinical fast MRI techniques, their use has been limited outside specialized MRI centers. A key challenge is generalization to rare pathologies or different populations, noted in multiple studies, with fine-tuning on target populations suggested for improvement. However, current approaches for PD-DL training require access to raw k-space measurements, which is typically only available at specialized MRI centers that have research agreements for such data access. This is especially an issue for rural and under-resourced areas, where commercial MRI scanners only provide access to a final reconstructed image. To tackle these challenges, we propose Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity (CUPID) for high-quality PD-DL training using only routine clinical reconstructed images exported from an MRI scanner. CUPID evaluates output quality with a compressibility-based approach while ensuring that the output stays consistent with the clinical parallel imaging reconstruction through well-designed perturbations. Our results show CUPID achieves similar quality to established PD-DL training that requires k-space data while outperforming compressed sensing (CS) and diffusion-based generative methods. We further demonstrate its effectiveness in a zero-shot training setup for retrospectively and prospectively sub-sampled acquisitions, attesting to its minimal training burden. As an approach that radically deviates from existing strategies, CUPID presents an opportunity to provide broader access to fast MRI for remote and rural populations in an attempt to reduce the obstacles associated with this expensive imaging modality. Code is available at https://github.com/ualcalar17/CUPID. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "cb0xbZ3APM@OpenReview",
      "index": 40,
      "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",
      "authors": [
        "Danny Driess",
        "Jost Tobias Springenberg",
        "brian ichter",
        "LILI YU",
        "Adrian Li-Bell",
        "Karl Pertsch",
        "Allen Z. Ren",
        "Homer Walke",
        "Quan Vuong",
        "Lucy Xiaoyang Shi",
        "Sergey Levine"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "vlm",
        "vla",
        "knowledge",
        "action",
        "vision",
        "control",
        "vlms",
        "continuous",
        "insulating",
        "training"
      ],
      "summary": "Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation and open-source model weights are available at https://github.com/Physical-Intelligence/openpi.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cb0xbZ3APM"
        ],
        "venue": [
          "/venue/cb0xbZ3APM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cb0xbZ3APM"
        ],
        "detail": [
          "https://openreview.net/forum?id=cb0xbZ3APM"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Danny Driess , Jost Tobias Springenberg , brian ichter , LILI YU , Adrian Li-Bell , Karl Pertsch , Allen Z. Ren , Homer Walke , Quan Vuong , Lucy Xiaoyang Shi , Sergey Levine Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation and open-source model weights are available at https://github.com/Physical-Intelligence/openpi. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4f6mEr1DQs@OpenReview",
      "index": 41,
      "title": "Complete Structure Guided Point Cloud Completion via Cluster- and Instance-Level Contrastive Learning",
      "authors": [
        "Yang Chen",
        "Yirun Zhou",
        "WEIZHONG ZHANG",
        "Cheng Jin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "complete",
        "point",
        "clouds",
        "cloud",
        "structure",
        "self",
        "completion",
        "contrastive",
        "guided",
        "instance"
      ],
      "summary": "Point cloud completion, aiming to reconstruct missing part from incomplete point clouds, is a pivotal task in 3D computer vision. Traditional supervised approaches often necessitate complete point clouds for training supervision, which are not readily accessible in real-world applications. Recent studies have attempted to mitigate this dependency by employing self-supervise mechanisms. However, these approaches frequently yield suboptimal results due to the absence of complete structure in the point cloud data during training. To address these issues, in this paper, we propose an effective framework to complete the point cloud under the guidance of self learned complete structure. A key contribution of our work is the development of a novel self-supervised complete structure reconstruction module, which can learn the complete structure explicitly from incomplete point clouds and thus eliminate the reliance on training data from complete point clouds. Additionally, we introduce a contrastive learning approach at both the cluster- and instance-level to extract shape features guided by the complete structure and to capture style features, respectively. This dual-level learning design ensures that the generated point clouds are both shape-completed and detail-preserving. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms state-of-the-art self-supervised methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4f6mEr1DQs"
        ],
        "venue": [
          "/venue/4f6mEr1DQs@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4f6mEr1DQs"
        ],
        "detail": [
          "https://openreview.net/forum?id=4f6mEr1DQs"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Complete Structure Guided Point Cloud Completion via Cluster- and Instance-Level Contrastive Learning [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Yang Chen , Yirun Zhou , WEIZHONG ZHANG , Cheng Jin Point cloud completion, aiming to reconstruct missing part from incomplete point clouds, is a pivotal task in 3D computer vision. Traditional supervised approaches often necessitate complete point clouds for training supervision, which are not readily accessible in real-world applications. Recent studies have attempted to mitigate this dependency by employing self-supervise mechanisms. However, these approaches frequently yield suboptimal results due to the absence of complete structure in the point cloud data during training. To address these issues, in this paper, we propose an effective framework to complete the point cloud under the guidance of self learned complete structure. A key contribution of our work is the development of a novel self-supervised complete structure reconstruction module, which can learn the complete structure explicitly from incomplete point clouds and thus eliminate the reliance on training data from complete point clouds. Additionally, we introduce a contrastive learning approach at both the cluster- and instance-level to extract shape features guided by the complete structure and to capture style features, respectively. This dual-level learning design ensures that the generated point clouds are both shape-completed and detail-preserving. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms state-of-the-art self-supervised methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "P2yIMJP5b1@OpenReview",
      "index": 42,
      "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
      "authors": [
        "Jiatong Shi",
        "Yifan Cheng",
        "Bo-Hao Su",
        "Hye-jin Shim",
        "Jinchuan Tian",
        "Samuele Cornell",
        "Yiwen Zhao",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "arecho",
        "speech",
        "evaluation",
        "chain",
        "autoregressive",
        "metrics",
        "metric",
        "perceptual",
        "hypothesis",
        "quality"
      ],
      "summary": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=P2yIMJP5b1"
        ],
        "venue": [
          "/venue/P2yIMJP5b1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=P2yIMJP5b1"
        ],
        "detail": [
          "https://openreview.net/forum?id=P2yIMJP5b1"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Jiatong Shi , Yifan Cheng , Bo-Hao Su , Hye-jin Shim , Jinchuan Tian , Samuele Cornell , Yiwen Zhao , Siddhant Arora , Shinji Watanabe Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "crczm2smVo@OpenReview",
      "index": 43,
      "title": "Projective Equivariant Networks via Second-order Fundamental Differential Invariants",
      "authors": [
        "Yikang Li",
        "Yeqing Qiu",
        "Yuxuan Chen",
        "Lingshen He",
        "Lexiang Hu",
        "Zhouchen Lin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "projective",
        "equivariant",
        "invariants",
        "pdinet",
        "differential",
        "group",
        "fundamental",
        "convolutions",
        "networks",
        "imagenette"
      ],
      "summary": "Equivariant networks enhance model efficiency and generalization by embedding symmetry priors into their architectures. However, most existing methods, primarily based on group convolutions and steerable convolutions, face significant limitations when dealing with complex transformation groups, particularly the projective group, which plays a crucial role in vision. In this work, we tackle the challenge by constructing projective equivariant networks based on differential invariants. Using the moving frame method with a carefully selected cross section tailored for multi-dimensional functions, we derive a complete and concise set of second-order fundamental differential invariants of the projective group. We provide a rigorous analysis of the properties and transformation relationships of their underlying components, yielding a further simplified and unified set of fundamental differential invariants, which facilitates both theoretical analysis and practical applications. Building on this foundation, we develop the first deep projective equivariant networks, PDINet, which achieve full projective equivariance without discretizing or sampling the group. Empirical results on the projectively transformed STL-10 and Imagenette datasets show that PDINet achieves improvements of 11.39\\% and 5.66\\% in accuracy over the respective standard baselines under out-of-distribution settings, demonstrating its strong generalization to complex geometric transformations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=crczm2smVo"
        ],
        "venue": [
          "/venue/crczm2smVo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=crczm2smVo"
        ],
        "detail": [
          "https://openreview.net/forum?id=crczm2smVo"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Projective Equivariant Networks via Second-order Fundamental Differential Invariants [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Yikang Li , Yeqing Qiu , Yuxuan Chen , Lingshen He , Lexiang Hu , Zhouchen Lin Equivariant networks enhance model efficiency and generalization by embedding symmetry priors into their architectures. However, most existing methods, primarily based on group convolutions and steerable convolutions, face significant limitations when dealing with complex transformation groups, particularly the projective group, which plays a crucial role in vision. In this work, we tackle the challenge by constructing projective equivariant networks based on differential invariants. Using the moving frame method with a carefully selected cross section tailored for multi-dimensional functions, we derive a complete and concise set of second-order fundamental differential invariants of the projective group. We provide a rigorous analysis of the properties and transformation relationships of their underlying components, yielding a further simplified and unified set of fundamental differential invariants, which facilitates both theoretical analysis and practical applications. Building on this foundation, we develop the first deep projective equivariant networks, PDINet, which achieve full projective equivariance without discretizing or sampling the group. Empirical results on the projectively transformed STL-10 and Imagenette datasets show that PDINet achieves improvements of 11.39\\% and 5.66\\% in accuracy over the respective standard baselines under out-of-distribution settings, demonstrating its strong generalization to complex geometric transformations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Ms6IXbfzzX@OpenReview",
      "index": 44,
      "title": "ZeroS: Zero‑Sum Linear Attention for Efficient Transformers",
      "authors": [
        "Jiecheng Lu",
        "Xu Han",
        "Yan Sun",
        "Viresh Pati",
        "Yubin Kim",
        "Siddhartha Somani",
        "Shihao Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "attention",
        "zeros",
        "softmax",
        "zero",
        "transformers",
        "sum",
        "combinations",
        "dilutes",
        "linear",
        "limitations"
      ],
      "summary": "Linear attention methods offer Transformers O ( N ) O ( N ) complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term 1 / t 1 / t and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining O ( N ) O ( N ) complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Ms6IXbfzzX"
        ],
        "venue": [
          "/venue/Ms6IXbfzzX@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Ms6IXbfzzX"
        ],
        "detail": [
          "https://openreview.net/forum?id=Ms6IXbfzzX"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 1
      },
      "raw_excerpt": "ZeroS: Zero‑Sum Linear Attention for Efficient Transformers [PDF 6 ] [Copy] [Kimi 1 ] [REL] Authors : Jiecheng Lu , Xu Han , Yan Sun , Viresh Pati , Yubin Kim , Siddhartha Somani , Shihao Yang Linear attention methods offer Transformers O ( N ) O ( N ) complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term 1 / t 1 / t and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining O ( N ) O ( N ) complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "V8SndhCN0z@OpenReview",
      "index": 45,
      "title": "Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models",
      "authors": [
        "Aloni Cohen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "protection",
        "copyright",
        "copy",
        "blameless",
        "clean",
        "room",
        "naf",
        "copying",
        "defining",
        "provable"
      ],
      "summary": "Are there any conditions under which a generative model’s outputs are guaranteed not to infringe the copyrights of its training data? This is the question of \"provable copyright protection\" first posed by Vyas, Kakade, and Barak [ICML 2023]. They define _near access-freeness (NAF)_ and propose it as sufficient for protection. This paper revisits the question and establishes new foundations for provable copyright protection---foundations that are firmer both technically and legally. First, we show that NAF alone does not prevent infringement. In fact, NAF models can enable verbatim copying, a blatant failure of copy protection that we dub being _tainted_. Then, we introduce our _blameless copy protection framework_ for defining meaningful guarantees, and instantiate it with _clean-room copy protection_. Clean-room copy protection allows a user to control their risk of copying by behaving in a way that is unlikely to copy in a counterfactual \"clean-room setting.\" Finally, we formalize a common intuition about differential privacy and copyright by proving that DP implies clean-room copy protection when the dataset is _golden_, a copyright deduplication requirement.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=V8SndhCN0z"
        ],
        "venue": [
          "/venue/V8SndhCN0z@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=V8SndhCN0z"
        ],
        "detail": [
          "https://openreview.net/forum?id=V8SndhCN0z"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Author : Aloni Cohen Are there any conditions under which a generative model’s outputs are guaranteed not to infringe the copyrights of its training data? This is the question of \"provable copyright protection\" first posed by Vyas, Kakade, and Barak [ICML 2023]. They define _near access-freeness (NAF)_ and propose it as sufficient for protection. This paper revisits the question and establishes new foundations for provable copyright protection---foundations that are firmer both technically and legally. First, we show that NAF alone does not prevent infringement. In fact, NAF models can enable verbatim copying, a blatant failure of copy protection that we dub being _tainted_. Then, we introduce our _blameless copy protection framework_ for defining meaningful guarantees, and instantiate it with _clean-room copy protection_. Clean-room copy protection allows a user to control their risk of copying by behaving in a way that is unlikely to copy in a counterfactual \"clean-room setting.\" Finally, we formalize a common intuition about differential privacy and copyright by proving that DP implies clean-room copy protection when the dataset is _golden_, a copyright deduplication requirement. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IJLqUjtrls@OpenReview",
      "index": 46,
      "title": "Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates",
      "authors": [
        "Seunghyeok Shin",
        "Dabin Kim",
        "Hongki Lim"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reconstruction",
        "likelihood",
        "fcm",
        "diffusion",
        "update",
        "forward",
        "curvature",
        "updates",
        "retraining",
        "matching"
      ],
      "summary": "Reconstructing high-quality point clouds from images remains challenging in computer vision. Existing generative models, particularly diffusion models, based approaches that directly learn the posterior may suffer from inflexibility—they require conditioning signals during training, support only a fixed number of input views, and need complete retraining for different measurements. Recent diffusion-based methods have attempted to address this by combining prior models with likelihood updates, but they rely on heuristic fixed step sizes for the likelihood update that lead to slow convergence and suboptimal reconstruction quality. We advance this line of approach by integrating our novel Forward Curvature-Matching (FCM) update method with diffusion sampling. Our method dynamically determines optimal step sizes using only forward automatic differentiation and finite-difference curvature estimates, enabling precise optimization of the likelihood update. This formulation enables high-fidelity reconstruction from both single-view and multi-view inputs, and supports various input modalities through simple operator substitution—all without retraining. Experiments on ShapeNet and CO3D datasets demonstrate that our method achieves superior reconstruction quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD, validating its efficiency and adaptability for practical applications. Code is available at https://github.com/Seunghyeok0715/FCM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IJLqUjtrls"
        ],
        "venue": [
          "/venue/IJLqUjtrls@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IJLqUjtrls"
        ],
        "detail": [
          "https://openreview.net/forum?id=IJLqUjtrls"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Seunghyeok Shin , Dabin Kim , Hongki Lim Reconstructing high-quality point clouds from images remains challenging in computer vision. Existing generative models, particularly diffusion models, based approaches that directly learn the posterior may suffer from inflexibility—they require conditioning signals during training, support only a fixed number of input views, and need complete retraining for different measurements. Recent diffusion-based methods have attempted to address this by combining prior models with likelihood updates, but they rely on heuristic fixed step sizes for the likelihood update that lead to slow convergence and suboptimal reconstruction quality. We advance this line of approach by integrating our novel Forward Curvature-Matching (FCM) update method with diffusion sampling. Our method dynamically determines optimal step sizes using only forward automatic differentiation and finite-difference curvature estimates, enabling precise optimization of the likelihood update. This formulation enables high-fidelity reconstruction from both single-view and multi-view inputs, and supports various input modalities through simple operator substitution—all without retraining. Experiments on ShapeNet and CO3D datasets demonstrate that our method achieves superior reconstruction quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD, validating its efficiency and adaptability for practical applications. Code is available at https://github.com/Seunghyeok0715/FCM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "n0QvMU2kON@OpenReview",
      "index": 47,
      "title": "From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers",
      "authors": [
        "Ryotaro Kawata",
        "Yujin Song",
        "Alberto Bietti",
        "Naoki Nishikawa",
        "Taiji Suzuki",
        "Samuel Vaiter",
        "Denny Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "induction",
        "trigger",
        "transformers",
        "pretraining",
        "ood",
        "shortcut",
        "head",
        "heads",
        "positional",
        "diversity"
      ],
      "summary": "Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low “max-sum” ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=n0QvMU2kON"
        ],
        "venue": [
          "/venue/n0QvMU2kON@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=n0QvMU2kON"
        ],
        "detail": [
          "https://openreview.net/forum?id=n0QvMU2kON"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Ryotaro Kawata , Yujin Song , Alberto Bietti , Naoki Nishikawa , Taiji Suzuki , Samuel Vaiter , Denny Wu Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low “max-sum” ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RespmwOoCH@OpenReview",
      "index": 48,
      "title": "Learning Interestingness in Automated Mathematical Theory Formation",
      "authors": [
        "George Tsoukalas",
        "Rahul Saha",
        "Amitayush Thakur",
        "Sabrina Reguyal",
        "Swarat Chaudhuri"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "interestingness",
        "fermat",
        "mathematical",
        "discovery",
        "trishullab",
        "theory",
        "evolutionary",
        "automating",
        "open",
        "environment"
      ],
      "summary": "We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce Fermat, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through Fermat: automatically scoring the interestingness of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the \\fermat environment at github.com/trishullab/Fermat.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RespmwOoCH"
        ],
        "venue": [
          "/venue/RespmwOoCH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RespmwOoCH"
        ],
        "detail": [
          "https://openreview.net/forum?id=RespmwOoCH"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Learning Interestingness in Automated Mathematical Theory Formation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : George Tsoukalas , Rahul Saha , Amitayush Thakur , Sabrina Reguyal , Swarat Chaudhuri We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce Fermat, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through Fermat: automatically scoring the interestingness of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the \\fermat environment at github.com/trishullab/Fermat. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yQoHUijSHx@OpenReview",
      "index": 49,
      "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm",
      "authors": [
        "Xiaowei Zhu",
        "Yubing Ren",
        "Fang Fang",
        "Qingfeng Tan",
        "Shi Wang",
        "Yanan Cao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "detectllm",
        "dna",
        "generated",
        "text",
        "repair",
        "written",
        "human",
        "detection",
        "inspired",
        "interpretably"
      ],
      "summary": "The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce **DNA-DetectLLM**, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of **5.55\\%** in AUROC and **2.08\\%** in F1 score across multiple public benchmark datasets. Code and data are available at https://github.com/Xiaoweizhu57/DNA-DetectLLM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yQoHUijSHx"
        ],
        "venue": [
          "/venue/yQoHUijSHx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yQoHUijSHx"
        ],
        "detail": [
          "https://openreview.net/forum?id=yQoHUijSHx"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Xiaowei Zhu , Yubing Ren , Fang Fang , Qingfeng Tan , Shi Wang , Yanan Cao The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce **DNA-DetectLLM**, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of **5.55\\%** in AUROC and **2.08\\%** in F1 score across multiple public benchmark datasets. Code and data are available at https://github.com/Xiaoweizhu57/DNA-DetectLLM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "sAFottNlra@OpenReview",
      "index": 50,
      "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
      "authors": [
        "David Heineman",
        "Valentin Hofmann",
        "Ian Magnusson",
        "Yuling Gu",
        "Noah A. Smith",
        "Hannaneh Hajishirzi",
        "Kyle Lo",
        "Jesse Dodge"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "noise",
        "benchmarks",
        "signal",
        "better",
        "evaluation",
        "decisions",
        "benchmark",
        "reliable",
        "language",
        "interventions"
      ],
      "summary": "Developing large language models is expensive and often involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable and useful for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark’s ability to separate better models from worse models, and noise, a benchmark’s sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce four interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and scaling law error. We also find that filtering noisy benchmarks such that they have better signal-to-noise ratio leads to more reliable evaluations. We also find that averaging the output of a model's checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 465 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 50K evaluation benchmark results, totaling 200M instances.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sAFottNlra"
        ],
        "venue": [
          "/venue/sAFottNlra@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sAFottNlra"
        ],
        "detail": [
          "https://openreview.net/forum?id=sAFottNlra"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation [PDF 3 ] [Copy] [Kimi ] [REL] Authors : David Heineman , Valentin Hofmann , Ian Magnusson , Yuling Gu , Noah A. Smith , Hannaneh Hajishirzi , Kyle Lo , Jesse Dodge Developing large language models is expensive and often involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable and useful for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark’s ability to separate better models from worse models, and noise, a benchmark’s sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce four interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and scaling law error. We also find that filtering noisy benchmarks such that they have better signal-to-noise ratio leads to more reliable evaluations. We also find that averaging the output of a model's checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 465 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 50K evaluation benchmark results, totaling 200M instances. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "u2GzxdWLFW@OpenReview",
      "index": 51,
      "title": "Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning",
      "authors": [
        "Idan Attias",
        "Steve Hanneke",
        "Arvind Ramaswami"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "erm",
        "oracle",
        "online",
        "calls",
        "transductive",
        "littlestone",
        "consistency",
        "mistakes",
        "mathrm",
        "weak"
      ],
      "summary": "We study online and transductive online learning in settings where the learner can interact with the concept class only via Empirical Risk Minimization (ERM) or weak consistency oracles on arbitrary subsets of the instance domain. This contrasts with standard online models, where the learner has full knowledge of the concept class. The ERM oracle returns a hypothesis that minimizes the loss on a given subset, while the weak consistency oracle returns only a binary signal indicating whether the subset is realizable by a concept in the class. The learner's performance is measured by the number of mistakes and oracle calls. In the standard online setting with ERM access, we establish tight lower bounds in both the realizable and agnostic cases: Ω ( 2 d L D ) Ω ( 2 d L D ) mistakes and Ω ( T 2 d L D − − − − − √ ) Ω ( T 2 d L D ) regret, respectively, where T T is the number of timesteps and d L D d L D is the Littlestone dimension of the class. We further show how existing results for online learning with ERM access translate to the setting with a weak consistency oracle, at the cost of increasing the number of oracle calls by O ( T ) O ( T ) . We then consider the transductive online model, where the instance sequence is known in advance but labels are revealed sequentially. For general Littlestone classes, we show that the optimal mistake bound in the realizable case and in the agnostic case can be achieved using O ( T d V C + 1 ) O ( T d V C + 1 ) weak consistency oracle calls, where d V C d V C is the VC dimension of the class. On the negative side, we show that Ω ( T ) Ω ( T ) weak consistency queries are necessary for transductive online learnability, and that Ω ( T ) Ω ( T ) ERM queries are necessary to avoid exponential dependence on the Littlestone dimension. % Finally, for special families of concept classes, we demonstrate how to reduce the number of oracle calls using randomized algorithms while maintaining similar mistake bounds. In particular, for Thresholds on an unknown ordering, O ( log T ) O ( log ⁡ T ) ERM queries suffice, and for k k -Intervals, O ( T 3 2 2 k ) O ( T 3 2 2 k ) weak consistency queries suffice.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=u2GzxdWLFW"
        ],
        "venue": [
          "/venue/u2GzxdWLFW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=u2GzxdWLFW"
        ],
        "detail": [
          "https://openreview.net/forum?id=u2GzxdWLFW"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Idan Attias , Steve Hanneke , Arvind Ramaswami We study online and transductive online learning in settings where the learner can interact with the concept class only via Empirical Risk Minimization (ERM) or weak consistency oracles on arbitrary subsets of the instance domain. This contrasts with standard online models, where the learner has full knowledge of the concept class. The ERM oracle returns a hypothesis that minimizes the loss on a given subset, while the weak consistency oracle returns only a binary signal indicating whether the subset is realizable by a concept in the class. The learner's performance is measured by the number of mistakes and oracle calls. In the standard online setting with ERM access, we establish tight lower bounds in both the realizable and agnostic cases: Ω ( 2 d L D ) Ω ( 2 d L D ) mistakes and Ω ( T 2 d L D − − − − − √ ) Ω ( T 2 d L D ) regret, respectively, where T T is the number of timesteps and d L D d L D is the Littlestone dimension of the class. We further show how existing results for online learning with ERM access translate to the setting with a weak consistency oracle, at the cost of increasing the number of oracle calls by O ( T ) O ( T ) . We then consider the transductive online model, where the instance sequence is known in advance but labels are revealed sequentially. For general Littlestone classes, we show that the optimal mistake bound in the realizable case and in the agnostic case can be achieved using O ( T d V C + 1 ) O ( T d V C + 1 ) weak consistency oracle calls, where d V C d V C is the VC dimension of the class. On the negative side, we show that Ω ( T ) Ω ( T ) weak consistency queries are necessary for transductive online learnability, and that Ω ( T ) Ω ( T ) ERM queries are necessary to avoid exponential dependence on the Littlestone dimension. % Finally, for special families of concept classes, we demonstrate how to reduce the number of oracle calls using randomized algorithms while maintaining similar mistake bounds. In particular, for Thresholds on an unknown ordering, O ( log T ) O ( log ⁡ T ) ERM queries suffice, and for k k -Intervals, O ( T 3 2 2 k ) O ( T 3 2 2 k ) weak consistency queries suffice. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gJclyLFSdU@OpenReview",
      "index": 52,
      "title": "The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis",
      "authors": [
        "Alex Lewandowski",
        "Aditya A. Ramesh",
        "Edan Meyer",
        "Dale Schuurmans",
        "Marlos C. Machado"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "agent",
        "interactivity",
        "continually",
        "embedded",
        "world",
        "capacity",
        "bigger",
        "continual",
        "automaton",
        "computationally"
      ],
      "summary": "Continual learning is often motivated by the idea, known as the big world hypothesis, that the \"world is bigger\" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and limiting to the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is implicitly constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. We prove that such an automaton is implicitly constrained and that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We then propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour and to continually learn new predictions. We develop a reinforcement learning algorithm for maximizing interactivity and a synthetic benchmark to experimentation on continual learning. Our results indicate that deep nonlinear networks struggle to sustain interactivity whereas deep linear networks can achieve higher interactivity as capacity increases.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gJclyLFSdU"
        ],
        "venue": [
          "/venue/gJclyLFSdU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gJclyLFSdU"
        ],
        "detail": [
          "https://openreview.net/forum?id=gJclyLFSdU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Alex Lewandowski , Aditya A. Ramesh , Edan Meyer , Dale Schuurmans , Marlos C. Machado Continual learning is often motivated by the idea, known as the big world hypothesis, that the \"world is bigger\" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and limiting to the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is implicitly constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. We prove that such an automaton is implicitly constrained and that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We then propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour and to continually learn new predictions. We develop a reinforcement learning algorithm for maximizing interactivity and a synthetic benchmark to experimentation on continual learning. Our results indicate that deep nonlinear networks struggle to sustain interactivity whereas deep linear networks can achieve higher interactivity as capacity increases. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2afhRWVb6p@OpenReview",
      "index": 53,
      "title": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment",
      "authors": [
        "Chen Liu",
        "Wenfang Yao",
        "Kejing Yin",
        "William K. Cheung",
        "Jing Qin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "progression",
        "disease",
        "ehr",
        "cxrs",
        "dipro",
        "cxr",
        "disentanglement",
        "texttt",
        "multimodal",
        "static"
      ],
      "summary": "Longitudinal multimodal data, including electronic health records (EHR) and sequential chest X-rays (CXRs), is critical for modeling disease progression, yet remains underutilized due to two key challenges: (1) redundancy in consecutive CXR sequences, where static anatomical regions dominate over clinically-meaningful dynamics, and (2) temporal misalignment between sparse, irregular imaging and continuous EHR data. We introduce DiPro DiPro , a novel framework that addresses these challenges through region-aware disentanglement and multi-timescale alignment. First, we disentangle static (anatomy) and dynamic (pathology progression) features in sequential CXRs, prioritizing disease-relevant changes. Second, we hierarchically align these static and dynamic CXR features with asynchronous EHR data via local (pairwise interval-level) and global (full-sequence) synchronization to model coherent progression pathways. Extensive experiments on the MIMIC dataset demonstrate that DiPro DiPro could effectively extract temporal clinical dynamics and achieve state-of-the-art performance on both disease progression identification and general ICU prediction tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2afhRWVb6p"
        ],
        "venue": [
          "/venue/2afhRWVb6p@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2afhRWVb6p"
        ],
        "detail": [
          "https://openreview.net/forum?id=2afhRWVb6p"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Chen Liu , Wenfang Yao , Kejing Yin , William K. Cheung , Jing Qin Longitudinal multimodal data, including electronic health records (EHR) and sequential chest X-rays (CXRs), is critical for modeling disease progression, yet remains underutilized due to two key challenges: (1) redundancy in consecutive CXR sequences, where static anatomical regions dominate over clinically-meaningful dynamics, and (2) temporal misalignment between sparse, irregular imaging and continuous EHR data. We introduce DiPro DiPro , a novel framework that addresses these challenges through region-aware disentanglement and multi-timescale alignment. First, we disentangle static (anatomy) and dynamic (pathology progression) features in sequential CXRs, prioritizing disease-relevant changes. Second, we hierarchically align these static and dynamic CXR features with asynchronous EHR data via local (pairwise interval-level) and global (full-sequence) synchronization to model coherent progression pathways. Extensive experiments on the MIMIC dataset demonstrate that DiPro DiPro could effectively extract temporal clinical dynamics and achieve state-of-the-art performance on both disease progression identification and general ICU prediction tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Tn1M71PDfF@OpenReview",
      "index": 54,
      "title": "On the Hardness of Conditional Independence Testing In Practice",
      "authors": [
        "Zheng He",
        "Roman Pogodin",
        "Yazhe Li",
        "Namrata Deka",
        "Arthur Gretton",
        "Danica J. Sutherland"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "conditional",
        "independence",
        "tests",
        "kci",
        "test",
        "hardness",
        "shah",
        "peters",
        "kernel",
        "underpin"
      ],
      "summary": "Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on “hiding” dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test – of which we show the Generalized Covariance Measure underlying many recent tests is _nearly_ a special case – and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type I error.",
      "session": null,
      "time": null,
      "links": {
        "pdf": [
          "https://openreview.net/forum?id=Tn1M71PDfF",
          "/venue/Tn1M71PDfF@OpenReview",
          "https://openreview.net/pdf?id=Tn1M71PDfF"
        ],
        "venue": [
          "/venue/NeurIPS.2025?group=Spotlight",
          "/venue/Tn1M71PDfF@OpenReview"
        ],
        "detail": [
          "https://openreview.net/forum?id=Tn1M71PDfF"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "On the Hardness of Conditional Independence Testing In Practice [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zheng He , Roman Pogodin , Yazhe Li , Namrata Deka , Arthur Gretton , Danica J. Sutherland Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on “hiding” dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test – of which we show the Generalized Covariance Measure underlying many recent tests is _nearly_ a special case – and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type I error. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "C7LxkebvVW@OpenReview",
      "index": 55,
      "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models",
      "authors": [
        "Berk Tinaz",
        "Zalan Fabian",
        "Mahdi Soltanolkotabi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "diffusion",
        "saes",
        "composition",
        "stages",
        "concepts",
        "workings",
        "interpretable",
        "image",
        "models",
        "inner"
      ],
      "summary": "Diffusion models have become the go-to method for text-to-image generation, producing high-quality images from pure noise. However, the inner workings of diffusion models is still largely a mystery due to their black-box nature and complex, multi-step generation process. Mechanistic interpretability techniques, such as Sparse Autoencoders (SAEs), have been successful in understanding and steering the behavior of large language models at scale. However, the great potential of SAEs has not yet been applied toward gaining insight into the intricate generative process of diffusion models. In this work, we leverage the SAE framework to probe the inner workings of a popular text-to-image diffusion model, and uncover a variety of human-interpretable concepts in its activations. Interestingly, we find that *even before the first reverse diffusion step* is completed, the final composition of the scene can be predicted surprisingly well by looking at the spatial distribution of activated concepts. Moreover, going beyond correlational analysis, we design intervention techniques aimed at manipulating image composition and style, and demonstrate that (1) in early stages of diffusion image composition can be effectively controlled, (2) in the middle stages image composition is finalized, however stylistic interventions are effective, and (3) in the final stages only minor textural details are subject to change.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=C7LxkebvVW"
        ],
        "venue": [
          "/venue/C7LxkebvVW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=C7LxkebvVW"
        ],
        "detail": [
          "https://openreview.net/forum?id=C7LxkebvVW"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 2
      },
      "raw_excerpt": "Emergence and Evolution of Interpretable Concepts in Diffusion Models [PDF 9 ] [Copy] [Kimi 2 ] [REL] Authors : Berk Tinaz , Zalan Fabian , Mahdi Soltanolkotabi Diffusion models have become the go-to method for text-to-image generation, producing high-quality images from pure noise. However, the inner workings of diffusion models is still largely a mystery due to their black-box nature and complex, multi-step generation process. Mechanistic interpretability techniques, such as Sparse Autoencoders (SAEs), have been successful in understanding and steering the behavior of large language models at scale. However, the great potential of SAEs has not yet been applied toward gaining insight into the intricate generative process of diffusion models. In this work, we leverage the SAE framework to probe the inner workings of a popular text-to-image diffusion model, and uncover a variety of human-interpretable concepts in its activations. Interestingly, we find that *even before the first reverse diffusion step* is completed, the final composition of the scene can be predicted surprisingly well by looking at the spatial distribution of activated concepts. Moreover, going beyond correlational analysis, we design intervention techniques aimed at manipulating image composition and style, and demonstrate that (1) in early stages of diffusion image composition can be effectively controlled, (2) in the middle stages image composition is finalized, however stylistic interventions are effective, and (3) in the final stages only minor textural details are subject to change. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "at87L8EuzR@OpenReview",
      "index": 56,
      "title": "PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning",
      "authors": [
        "Mingqi Wu",
        "Qiang Sun",
        "Archer Y. Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pca",
        "contrastive",
        "uniformity",
        "background",
        "noise",
        "robustness",
        "dimensional",
        "recovering",
        "signal",
        "structured"
      ],
      "summary": "High-dimensional data often conceal low-dimensional signals beneath structured background noise, limiting standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs--paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity’s role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity’s role in contrastive learning—showing that explicit feature dispersion defends against structured noise and enhances robustness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=at87L8EuzR"
        ],
        "venue": [
          "/venue/at87L8EuzR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=at87L8EuzR"
        ],
        "detail": [
          "https://openreview.net/forum?id=at87L8EuzR"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Mingqi Wu , Qiang Sun , Archer Y. Yang High-dimensional data often conceal low-dimensional signals beneath structured background noise, limiting standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs--paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity’s role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity’s role in contrastive learning—showing that explicit feature dispersion defends against structured noise and enhances robustness. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "htN3xhUpjD@OpenReview",
      "index": 57,
      "title": "Optimal Nuisance Function Tuning for Estimating a Doubly Robust Functional under Proportional Asymptotics",
      "authors": [
        "Sean McGrath",
        "Debarghya Mukherjee",
        "Rajarshi Mukherjee",
        "Zixiao Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "nuisance",
        "ecc",
        "estimators",
        "tuning",
        "estimating",
        "proportional",
        "estimator",
        "debiased",
        "sample",
        "asymptotic"
      ],
      "summary": "In this paper, we explore the asymptotically optimal tuning parameter choice in ridge regression for estimating nuisance functions of a statistical functional that has recently gained prominence in conditional independence testing and causal inference. Given a sample of size n n , we study estimators of the Expected Conditional Covariance (ECC) between variables Y Y and A A given a high-dimensional covariate X ∈ R p X ∈ R p . Under linear regression models for Y Y and A A on X X and the proportional asymptotic regime p / n → c ∈ ( 0 , ∞ ) p / n → c ∈ ( 0 , ∞ ) , we evaluate three existing ECC estimators and two sample splitting strategies for estimating the required nuisance functions. Since no consistent estimator of the nuisance functions exists in the proportional asymptotic regime without imposing further structure on the problem, we first derive debiased versions of the ECC estimators that utilize the ridge regression nuisance function estimators. We show that our bias correction strategy yields n − − √ n -consistent estimators of the ECC across different sample splitting strategies and estimator choices. We then derive the asymptotic variances of these debiased estimators to illustrate the nuanced interplay between the sample splitting strategy, estimator choice, and tuning parameters of the nuisance function estimators for optimally estimating the ECC. Our analysis reveals that prediction-optimal tuning parameters (i.e., those that optimally estimate the nuisance functions) may not lead to the lowest asymptotic variance of the ECC estimator -- thereby demonstrating the need to be careful in selecting tuning parameters based on the final goal of inference. Finally, we verify our theoretical results through extensive numerical experiments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=htN3xhUpjD"
        ],
        "venue": [
          "/venue/htN3xhUpjD@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=htN3xhUpjD"
        ],
        "detail": [
          "https://openreview.net/forum?id=htN3xhUpjD"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Optimal Nuisance Function Tuning for Estimating a Doubly Robust Functional under Proportional Asymptotics [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Sean McGrath , Debarghya Mukherjee , Rajarshi Mukherjee , Zixiao Wang In this paper, we explore the asymptotically optimal tuning parameter choice in ridge regression for estimating nuisance functions of a statistical functional that has recently gained prominence in conditional independence testing and causal inference. Given a sample of size n n , we study estimators of the Expected Conditional Covariance (ECC) between variables Y Y and A A given a high-dimensional covariate X ∈ R p X ∈ R p . Under linear regression models for Y Y and A A on X X and the proportional asymptotic regime p / n → c ∈ ( 0 , ∞ ) p / n → c ∈ ( 0 , ∞ ) , we evaluate three existing ECC estimators and two sample splitting strategies for estimating the required nuisance functions. Since no consistent estimator of the nuisance functions exists in the proportional asymptotic regime without imposing further structure on the problem, we first derive debiased versions of the ECC estimators that utilize the ridge regression nuisance function estimators. We show that our bias correction strategy yields n − − √ n -consistent estimators of the ECC across different sample splitting strategies and estimator choices. We then derive the asymptotic variances of these debiased estimators to illustrate the nuanced interplay between the sample splitting strategy, estimator choice, and tuning parameters of the nuisance function estimators for optimally estimating the ECC. Our analysis reveals that prediction-optimal tuning parameters (i.e., those that optimally estimate the nuisance functions) may not lead to the lowest asymptotic variance of the ECC estimator -- thereby demonstrating the need to be careful in selecting tuning parameters based on the final goal of inference. Finally, we verify our theoretical results through extensive numerical experiments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "LTgUInLTbP@OpenReview",
      "index": 58,
      "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
      "authors": [
        "Fengxiang Wang",
        "Mingshuo Chen",
        "Yueying Li",
        "Di Wang",
        "Haotian Wang",
        "Zonghao Guo",
        "Zefan Wang",
        "Shan Boqi",
        "Long Lan",
        "Yulin Wang",
        "Hongzhen Wang",
        "Wenjing Yang",
        "Bo Du",
        "Jing Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "geollava",
        "vqa",
        "uhr",
        "token",
        "multimodal",
        "superrs",
        "avg",
        "376",
        "highrs",
        "resolution"
      ],
      "summary": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce **SuperRS-VQA** (avg. 8,376 × × 8,376) and **HighRS-VQA** (avg. 2,000 × × 1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: *Background Token Pruning* and *Anchored Token Selection*, to reduce the memory footprint while preserving key semantics. Integrating these techniques, we introduce **GeoLLaVA-8K**, the first RS-focused multimodal large language model capable of handling inputs up to 8K × × 8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench. Datasets and code were released at https://github.com/MiliLab/GeoLLaVA-8K.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LTgUInLTbP"
        ],
        "venue": [
          "/venue/LTgUInLTbP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LTgUInLTbP"
        ],
        "detail": [
          "https://openreview.net/forum?id=LTgUInLTbP"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Fengxiang Wang , Mingshuo Chen , Yueying Li , Di Wang , Haotian Wang , Zonghao Guo , Zefan Wang , Shan Boqi , Long Lan , Yulin Wang , Hongzhen Wang , Wenjing Yang , Bo Du , Jing Zhang Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce **SuperRS-VQA** (avg. 8,376 × × 8,376) and **HighRS-VQA** (avg. 2,000 × × 1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: *Background Token Pruning* and *Anchored Token Selection*, to reduce the memory footprint while preserving key semantics. Integrating these techniques, we introduce **GeoLLaVA-8K**, the first RS-focused multimodal large language model capable of handling inputs up to 8K × × 8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench. Datasets and code were released at https://github.com/MiliLab/GeoLLaVA-8K. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2M5dTDdGxl@OpenReview",
      "index": 59,
      "title": "Environment Inference for Learning Generalizable Dynamical System",
      "authors": [
        "Shixuan Liu",
        "Yue He",
        "Haotian Wang",
        "Wenjing Yang",
        "Yunfei Wang",
        "Peng Cui",
        "Zhong Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "environment",
        "dynainfer",
        "labels",
        "dynamical",
        "techniques",
        "environmental",
        "generalizable",
        "analyzing",
        "privacy",
        "data"
      ],
      "summary": "Data-driven methods offer efficient and robust solutions for analyzing complex dynamical systems but rely on the assumption of I.I.D. data, driving the development of generalization techniques for handling environmental differences. These techniques, however, are limited by their dependence on environment labels, which are often unavailable during training due to data acquisition challenges, privacy concerns, and environmental variability, particularly in large public datasets and privacy-sensitive domains. In response, we propose DynaInfer, a novel method that infers environment specifications by analyzing prediction errors from fixed neural networks within each training round, enabling environment assignments directly from data. We prove our algorithm effectively solves the alternating optimization problem in unlabeled scenarios and validate it through extensive experiments across diverse dynamical systems. Results show that DynaInfer outperforms existing environment assignment techniques, converges rapidly to true labels, and even achieves superior performance when environment labels are available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2M5dTDdGxl"
        ],
        "venue": [
          "/venue/2M5dTDdGxl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2M5dTDdGxl"
        ],
        "detail": [
          "https://openreview.net/forum?id=2M5dTDdGxl"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Environment Inference for Learning Generalizable Dynamical System [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Shixuan Liu , Yue He , Haotian Wang , Wenjing Yang , Yunfei Wang , Peng Cui , Zhong Liu Data-driven methods offer efficient and robust solutions for analyzing complex dynamical systems but rely on the assumption of I.I.D. data, driving the development of generalization techniques for handling environmental differences. These techniques, however, are limited by their dependence on environment labels, which are often unavailable during training due to data acquisition challenges, privacy concerns, and environmental variability, particularly in large public datasets and privacy-sensitive domains. In response, we propose DynaInfer, a novel method that infers environment specifications by analyzing prediction errors from fixed neural networks within each training round, enabling environment assignments directly from data. We prove our algorithm effectively solves the alternating optimization problem in unlabeled scenarios and validate it through extensive experiments across diverse dynamical systems. Results show that DynaInfer outperforms existing environment assignment techniques, converges rapidly to true labels, and even achieves superior performance when environment labels are available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "EXIKFM1Q9R@OpenReview",
      "index": 60,
      "title": "Abstract Rendering: Certified Rendering Under 3D Semantic Uncertainty",
      "authors": [
        "Chenxi Ji",
        "Yangge Li",
        "Xiangru Zhong",
        "Huan Zhang",
        "Sayan Mitra"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rendering",
        "abstract",
        "scenes",
        "uncertainty",
        "images",
        "certification",
        "camera",
        "pose",
        "downstream",
        "gatenet"
      ],
      "summary": "Rendering produces 2D images from 3D scene representations, yet how continuous variations in camera pose and scenes influence these images—and, consequently, downstream visual models—remains underexplored. We introduce **abstract rendering**, a framework that computes provable bounds on all images rendered under continuously varying camera poses and scenes. The resulting abstract image, expressed as a set of constraints over the image matrix, enables rigorous uncertainty propagation through downstream neural networks and thereby supports certification of model behavior under realistic 3D semantic perturbations, far beyond traditional pixel-level noise models. Our approach propagates camera pose uncertainty through each rendering step using efficient piecewise linear bounds, including custom abstractions for three rendering-specific operations—matrix inversion, sorting-based aggregation, and cumulative product summation—not supported by standard tools. Our implementation, ABSTRACTRENDER, targets two state-of-the-art photorealistic scene representations—3D Gaussian Splats and Neural Radiance Fields (NeRF)—and scales to complex scenes with up to 1M Gaussians. Our computed abstract images achieve up to 3% over-approximation error compared to sampling results (baseline). Through experiments on classification (ResNet), object detection (YOLO), and pose estimation (GATENet) tasks, we demonstrate that abstract rendering enables formal certification of downstream models under realistic 3D variations—an essential step toward safety-critical vision systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EXIKFM1Q9R"
        ],
        "venue": [
          "/venue/EXIKFM1Q9R@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EXIKFM1Q9R"
        ],
        "detail": [
          "https://openreview.net/forum?id=EXIKFM1Q9R"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Abstract Rendering: Certified Rendering Under 3D Semantic Uncertainty [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Chenxi Ji , Yangge Li , Xiangru Zhong , Huan Zhang , Sayan Mitra Rendering produces 2D images from 3D scene representations, yet how continuous variations in camera pose and scenes influence these images—and, consequently, downstream visual models—remains underexplored. We introduce **abstract rendering**, a framework that computes provable bounds on all images rendered under continuously varying camera poses and scenes. The resulting abstract image, expressed as a set of constraints over the image matrix, enables rigorous uncertainty propagation through downstream neural networks and thereby supports certification of model behavior under realistic 3D semantic perturbations, far beyond traditional pixel-level noise models. Our approach propagates camera pose uncertainty through each rendering step using efficient piecewise linear bounds, including custom abstractions for three rendering-specific operations—matrix inversion, sorting-based aggregation, and cumulative product summation—not supported by standard tools. Our implementation, ABSTRACTRENDER, targets two state-of-the-art photorealistic scene representations—3D Gaussian Splats and Neural Radiance Fields (NeRF)—and scales to complex scenes with up to 1M Gaussians. Our computed abstract images achieve up to 3% over-approximation error compared to sampling results (baseline). Through experiments on classification (ResNet), object detection (YOLO), and pose estimation (GATENet) tasks, we demonstrate that abstract rendering enables formal certification of downstream models under realistic 3D variations—an essential step toward safety-critical vision systems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hSX7Dd8dxy@OpenReview",
      "index": 61,
      "title": "Inference-Time Reward Hacking in Large Language Models",
      "authors": [
        "Hadi Khalaf",
        "Claudio Mayrink Verdun",
        "Alex Oesterling",
        "Himabindu Lakkaraju",
        "Flavio Calmon"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "hacking",
        "bon",
        "inference",
        "bop",
        "hedging",
        "sbon",
        "goals",
        "best",
        "time"
      ],
      "summary": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to an LLM’s output that indicates, for example, how likely it is to align with user preferences or safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of- n n (BoN) and Soft Best-of- n n (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, we introduce HedgeTune HedgeTune , an efficient algorithm to find the optimal inference-time parameter. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reasoning, and human-preference setups.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hSX7Dd8dxy"
        ],
        "venue": [
          "/venue/hSX7Dd8dxy@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hSX7Dd8dxy"
        ],
        "detail": [
          "https://openreview.net/forum?id=hSX7Dd8dxy"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Inference-Time Reward Hacking in Large Language Models [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Hadi Khalaf , Claudio Mayrink Verdun , Alex Oesterling , Himabindu Lakkaraju , Flavio Calmon A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to an LLM’s output that indicates, for example, how likely it is to align with user preferences or safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of- n n (BoN) and Soft Best-of- n n (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, we introduce HedgeTune HedgeTune , an efficient algorithm to find the optimal inference-time parameter. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reasoning, and human-preference setups. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "dtsYpcJr1R@OpenReview",
      "index": 62,
      "title": "High-Performance Arithmetic Circuit Optimization via Differentiable Architecture Search",
      "authors": [
        "Xilin Xia",
        "Jie Wang",
        "Wanbo Zhang",
        "Zhihai Wang",
        "Mingxuan Yuan",
        "Jianye HAO",
        "Feng Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "arith",
        "arithmetic",
        "das",
        "optimization",
        "circuit",
        "l2o",
        "interconnect",
        "circuits",
        "differentiable",
        "proxy"
      ],
      "summary": "Arithmetic circuit optimization remains a fundamental challenge in modern integrated circuit design. Recent advances have cast this problem within the Learning to Optimize (L2O) paradigm, where intelligent agents autonomously explore high-performance design spaces with encouraging results. However, existing approaches predominantly target coarse-grained architectural configurations, while the crucial interconnect optimization stage is often relegated to oversimplified proxy models or a heuristic approach. This disconnect undermines design quality, leading to suboptimal solutions in the circuit topology search space. To bridge this gap, we present **Arith-DAS**, a **D**ifferentiable **A**rchitecture **S**earch framework for **Arith**metic circuits. To the best of our knowledge, **Arith-DAS** is the first to formulate interconnect optimization within arithmetic circuits as a differentiable edge prediction problem over a multi-relational directed acyclic graph, enabling fine-grained, proxy-free optimization at the interconnection level. We evaluate **Arith-DAS** on a suite of representative arithmetic circuits, including multipliers and multiply-accumulate units. Experiments show substantial improvements over state-of-the-art L2O and conventional methods, achieving up to 27.05 27.05 % gain in hypervolume of area-delay Pareto front, a standard metric for evaluating multi-objective optimization performance. Moreover, integrating our optimized arithmetic units into large-scale AI accelerators yields up to 6.59 6.59 % delay reduction, demonstrating both scalability and real-world applicability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dtsYpcJr1R"
        ],
        "venue": [
          "/venue/dtsYpcJr1R@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dtsYpcJr1R"
        ],
        "detail": [
          "https://openreview.net/forum?id=dtsYpcJr1R"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "High-Performance Arithmetic Circuit Optimization via Differentiable Architecture Search [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Xilin Xia , Jie Wang , Wanbo Zhang , Zhihai Wang , Mingxuan Yuan , Jianye HAO , Feng Wu Arithmetic circuit optimization remains a fundamental challenge in modern integrated circuit design. Recent advances have cast this problem within the Learning to Optimize (L2O) paradigm, where intelligent agents autonomously explore high-performance design spaces with encouraging results. However, existing approaches predominantly target coarse-grained architectural configurations, while the crucial interconnect optimization stage is often relegated to oversimplified proxy models or a heuristic approach. This disconnect undermines design quality, leading to suboptimal solutions in the circuit topology search space. To bridge this gap, we present **Arith-DAS**, a **D**ifferentiable **A**rchitecture **S**earch framework for **Arith**metic circuits. To the best of our knowledge, **Arith-DAS** is the first to formulate interconnect optimization within arithmetic circuits as a differentiable edge prediction problem over a multi-relational directed acyclic graph, enabling fine-grained, proxy-free optimization at the interconnection level. We evaluate **Arith-DAS** on a suite of representative arithmetic circuits, including multipliers and multiply-accumulate units. Experiments show substantial improvements over state-of-the-art L2O and conventional methods, achieving up to 27.05 27.05 % gain in hypervolume of area-delay Pareto front, a standard metric for evaluating multi-objective optimization performance. Moreover, integrating our optimized arithmetic units into large-scale AI accelerators yields up to 6.59 6.59 % delay reduction, demonstrating both scalability and real-world applicability. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QFuuxfmqb5@OpenReview",
      "index": 63,
      "title": "ROOT: Rethinking Offline Optimization as Distributional Translation via Probabilistic Bridge",
      "authors": [
        "Manh Cuong Dao",
        "The Hung Tran",
        "Phi Le Nguyen",
        "Thao Nguyen Truong",
        "Trong Nghia Hoang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "offline",
        "bridge",
        "distributional",
        "probabilistic",
        "root",
        "inputs",
        "cuong",
        "optimization",
        "translation",
        "rethinking"
      ],
      "summary": "This paper studies the black-box optimization task which aims to find the maxima of a black-box function using a static set of its observed input-output pairs. This is often achieved via learning and optimizing a surrogate function with that offline data. Alternatively, it can also be framed as an inverse modeling task that maps a desired performance to potential input candidates that achieve it. Both approaches are constrained by the limited amount of offline data. To mitigate this limitation, we introduce a new perspective that casts offline optimization as a distributional translation task. This is formulated as learning a probabilistic bridge transforming an implicit distribution of low-value inputs (i.e., offline data) into another distribution of high-value inputs (i.e., solution candidates). Such probabilistic bridge can be learned using low- and high-value inputs sampled from synthetic functions that resemble the target function. These synthetic functions are constructed as the mean posterior of multiple Gaussian processes fitted with different parameterizations on the offline data, alleviating the data bottleneck. The proposed approach is evaluated on an extensive benchmark comprising most recent methods, demonstrating significant improvement and establishing a new state-of-the-art performance. Our code is publicly available at https://github.com/cuong-dm/ROOT.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QFuuxfmqb5"
        ],
        "venue": [
          "/venue/QFuuxfmqb5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QFuuxfmqb5"
        ],
        "detail": [
          "https://openreview.net/forum?id=QFuuxfmqb5"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "ROOT: Rethinking Offline Optimization as Distributional Translation via Probabilistic Bridge [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Manh Cuong Dao , The Hung Tran , Phi Le Nguyen , Thao Nguyen Truong , Trong Nghia Hoang This paper studies the black-box optimization task which aims to find the maxima of a black-box function using a static set of its observed input-output pairs. This is often achieved via learning and optimizing a surrogate function with that offline data. Alternatively, it can also be framed as an inverse modeling task that maps a desired performance to potential input candidates that achieve it. Both approaches are constrained by the limited amount of offline data. To mitigate this limitation, we introduce a new perspective that casts offline optimization as a distributional translation task. This is formulated as learning a probabilistic bridge transforming an implicit distribution of low-value inputs (i.e., offline data) into another distribution of high-value inputs (i.e., solution candidates). Such probabilistic bridge can be learned using low- and high-value inputs sampled from synthetic functions that resemble the target function. These synthetic functions are constructed as the mean posterior of multiple Gaussian processes fitted with different parameterizations on the offline data, alleviating the data bottleneck. The proposed approach is evaluated on an extensive benchmark comprising most recent methods, demonstrating significant improvement and establishing a new state-of-the-art performance. Our code is publicly available at https://github.com/cuong-dm/ROOT. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bmznY5wYXH@OpenReview",
      "index": 64,
      "title": "Go With the Flow: Fast Diffusion for Gaussian Mixture Models",
      "authors": [
        "George Rapakoulias",
        "Ali Reza Pedram",
        "Fengjiao Liu",
        "Lingjiong Zhu",
        "Panagiotis Tsiotras"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sbs",
        "mixture",
        "marginal",
        "diffusion",
        "gaussian",
        "another",
        "georgerapa",
        "gmmflow",
        "distribution",
        "low"
      ],
      "summary": "Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SBs between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SBs, and various other examples. The implementation is publicly available at https://github.com/georgeRapa/GMMflow.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bmznY5wYXH"
        ],
        "venue": [
          "/venue/bmznY5wYXH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bmznY5wYXH"
        ],
        "detail": [
          "https://openreview.net/forum?id=bmznY5wYXH"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 1
      },
      "raw_excerpt": "Go With the Flow: Fast Diffusion for Gaussian Mixture Models [PDF 7 ] [Copy] [Kimi 1 ] [REL] Authors : George Rapakoulias , Ali Reza Pedram , Fengjiao Liu , Lingjiong Zhu , Panagiotis Tsiotras Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SBs between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SBs, and various other examples. The implementation is publicly available at https://github.com/georgeRapa/GMMflow. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3IXdXBpuLn@OpenReview",
      "index": 65,
      "title": "SHF: Symmetrical Hierarchical Forest with Pretrained Vision Transformer Encoder for High-Resolution Medical Segmentation",
      "authors": [
        "Enzhi Zhang",
        "Peng Chen",
        "Rui Zhong",
        "Du Wu",
        "Jun Igarashi",
        "Isaac Lyngaas",
        "Xiao Wang",
        "Masaharu Munetomo",
        "Mohamed Wahib"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shf",
        "hierarchical",
        "resolution",
        "symmetrical",
        "patches",
        "medical",
        "vits",
        "forest",
        "vit",
        "paip"
      ],
      "summary": "This paper presents a novel approach to addressing the long-sequence problem in high-resolution medical images for Vision Transformers (ViTs). Using smaller patches as tokens can enhance ViT performance, but quadratically increases computation and memory requirements. Therefore, the common practice for applying ViTs to high-resolution images is either to: (a) employ complex sub-quadratic attention schemes or (b) use large to medium-sized patches and rely on additional mechanisms within the model to capture the spatial hierarchy of details. We propose Symmetrical Hierarchical Forest (SHF), a lightweight approach that adaptively patches the input image to increase token information density and encode hierarchical spatial structures into the input embedding. We then apply a reverse depatching scheme to the output embeddings of the transformer encoder, eliminating the need for convolution-based decoders. Unlike previous methods that modify attention mechanisms \\wahib{or use a complex hierarchy of interacting models}, SHF can be retrofitted to any ViT model to allow it to learn the hierarchical structure of details in high-resolution images without requiring architectural changes. Experimental results demonstrate significant gains in computational efficiency and performance: on the PAIP WSI dataset, we achieved a 3 ∼ ∼ 32 × × speedup or a 2.95\\% to 7.03\\% increase in accuracy (measured by Dice score) at a 64 K 2 64 K 2 resolution with the same computational budget, compared to state-of-the-art production models. On the 3D medical datasets BTCV and KiTS, training was 6 × × faster, with accuracy gains of 6.93\\% and 5.9\\%, respectively, compared to models without SHF.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3IXdXBpuLn"
        ],
        "venue": [
          "/venue/3IXdXBpuLn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3IXdXBpuLn"
        ],
        "detail": [
          "https://openreview.net/forum?id=3IXdXBpuLn"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "SHF: Symmetrical Hierarchical Forest with Pretrained Vision Transformer Encoder for High-Resolution Medical Segmentation [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Enzhi Zhang , Peng Chen , Rui Zhong , Du Wu , Jun Igarashi , Isaac Lyngaas , Xiao Wang , Masaharu Munetomo , Mohamed Wahib This paper presents a novel approach to addressing the long-sequence problem in high-resolution medical images for Vision Transformers (ViTs). Using smaller patches as tokens can enhance ViT performance, but quadratically increases computation and memory requirements. Therefore, the common practice for applying ViTs to high-resolution images is either to: (a) employ complex sub-quadratic attention schemes or (b) use large to medium-sized patches and rely on additional mechanisms within the model to capture the spatial hierarchy of details. We propose Symmetrical Hierarchical Forest (SHF), a lightweight approach that adaptively patches the input image to increase token information density and encode hierarchical spatial structures into the input embedding. We then apply a reverse depatching scheme to the output embeddings of the transformer encoder, eliminating the need for convolution-based decoders. Unlike previous methods that modify attention mechanisms \\wahib{or use a complex hierarchy of interacting models}, SHF can be retrofitted to any ViT model to allow it to learn the hierarchical structure of details in high-resolution images without requiring architectural changes. Experimental results demonstrate significant gains in computational efficiency and performance: on the PAIP WSI dataset, we achieved a 3 ∼ ∼ 32 × × speedup or a 2.95\\% to 7.03\\% increase in accuracy (measured by Dice score) at a 64 K 2 64 K 2 resolution with the same computational budget, compared to state-of-the-art production models. On the 3D medical datasets BTCV and KiTS, training was 6 × × faster, with accuracy gains of 6.93\\% and 5.9\\%, respectively, compared to models without SHF. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "KI8qan2EA7@OpenReview",
      "index": 66,
      "title": "Proxy-SPEX: Sample-Efficient Interpretability via Sparse Feature Interactions in LLMs",
      "authors": [
        "Landon Butler",
        "Abhineet Agarwal",
        "Justin Singh Kang",
        "Yigit Efe Erginbas",
        "Bin Yu",
        "Kannan Ramchandran"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "proxyspex",
        "spex",
        "interactions",
        "interpretability",
        "attribution",
        "llm",
        "mmschlk",
        "shapiq",
        "inferences",
        "llms"
      ],
      "summary": "Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs n n . Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to n ≈ 10 3 n ≈ 10 3 features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often *hierarchical*—higher-order interactions are accompanied by their lower-order subsets—which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20\\% over marginal attribution approaches while using * 10 × 10 × fewer inferences* than SPEX. By accounting for interactions, ProxySPEX efficiently identifies the most influential features, providing a scalable approximation of their Shapley values. Further, we apply ProxySPEX to two interpretability tasks. *Data attribution*, where we identify interactions among CIFAR-10 training samples that influence test predictions, and *mechanistic interpretability*, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. The ProxySPEX algorithm is available at <https://github.com/mmschlk/shapiq>.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KI8qan2EA7"
        ],
        "venue": [
          "/venue/KI8qan2EA7@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KI8qan2EA7"
        ],
        "detail": [
          "https://openreview.net/forum?id=KI8qan2EA7"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Proxy-SPEX: Sample-Efficient Interpretability via Sparse Feature Interactions in LLMs [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Landon Butler , Abhineet Agarwal , Justin Singh Kang , Yigit Efe Erginbas , Bin Yu , Kannan Ramchandran Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs n n . Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to n ≈ 10 3 n ≈ 10 3 features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often *hierarchical*—higher-order interactions are accompanied by their lower-order subsets—which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20\\% over marginal attribution approaches while using * 10 × 10 × fewer inferences* than SPEX. By accounting for interactions, ProxySPEX efficiently identifies the most influential features, providing a scalable approximation of their Shapley values. Further, we apply ProxySPEX to two interpretability tasks. *Data attribution*, where we identify interactions among CIFAR-10 training samples that influence test predictions, and *mechanistic interpretability*, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. The ProxySPEX algorithm is available at <https://github.com/mmschlk/shapiq>. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yPsJ1PKiAi@OpenReview",
      "index": 67,
      "title": "Fair Cooperation in Mixed-Motive Games via Conflict-Aware Gradient Adjustment",
      "authors": [
        "Woojun Kim",
        "Katia P. Sycara"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fairness",
        "collective",
        "cooperation",
        "conflict",
        "motive",
        "individual",
        "objectives",
        "adjustment",
        "agents",
        "rewards"
      ],
      "summary": "Multi-agent reinforcement learning in mixed-motive settings presents a fundamental challenge: agents must balance individual interests with collective goals, which are neither fully aligned nor strictly opposed. To address this, reward restructuring methods such as gifting and intrinsic motivation have been proposed. However, these approaches primarily focus on promoting cooperation by managing the trade-off between individual and collective returns, without explicitly addressing fairness with respect to agents’ task-specific rewards. In this paper, we propose an adaptive conflict-aware gradient adjustment method that promotes cooperation while ensuring fairness in individual rewards. The proposed method dynamically balances policy gradients derived from individual and collective objectives in situations where the two objectives are in conflict. By explicitly resolving such conflicts, our method improves collective performance while preserving fairness across agents. We provide theoretical results that guarantee monotonic non-decreasing improvement in both the collective and individual objectives and ensure fairness. Empirical results in sequential social dilemma environments demonstrate that our approach outperforms baselines in terms of social welfare, while maintaining fairness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yPsJ1PKiAi"
        ],
        "venue": [
          "/venue/yPsJ1PKiAi@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yPsJ1PKiAi"
        ],
        "detail": [
          "https://openreview.net/forum?id=yPsJ1PKiAi"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Fair Cooperation in Mixed-Motive Games via Conflict-Aware Gradient Adjustment [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Woojun Kim , Katia P. Sycara Multi-agent reinforcement learning in mixed-motive settings presents a fundamental challenge: agents must balance individual interests with collective goals, which are neither fully aligned nor strictly opposed. To address this, reward restructuring methods such as gifting and intrinsic motivation have been proposed. However, these approaches primarily focus on promoting cooperation by managing the trade-off between individual and collective returns, without explicitly addressing fairness with respect to agents’ task-specific rewards. In this paper, we propose an adaptive conflict-aware gradient adjustment method that promotes cooperation while ensuring fairness in individual rewards. The proposed method dynamically balances policy gradients derived from individual and collective objectives in situations where the two objectives are in conflict. By explicitly resolving such conflicts, our method improves collective performance while preserving fairness across agents. We provide theoretical results that guarantee monotonic non-decreasing improvement in both the collective and individual objectives and ensure fairness. Empirical results in sequential social dilemma environments demonstrate that our approach outperforms baselines in terms of social welfare, while maintaining fairness. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fLx3vQPmDu@OpenReview",
      "index": 68,
      "title": "OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts",
      "authors": [
        "Shiting Xiao",
        "Rishabh Kabra",
        "Yuhang Li",
        "Donghyun Lee",
        "Joao Carreira",
        "Priyadarshini Panda"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "openworldsam",
        "sam2",
        "segmentation",
        "prompts",
        "vocabulary",
        "language",
        "vlm",
        "open",
        "segment",
        "unseen"
      ],
      "summary": "The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the model's spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks. Code is available at https://github.com/GinnyXiao/OpenWorldSAM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fLx3vQPmDu"
        ],
        "venue": [
          "/venue/fLx3vQPmDu@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fLx3vQPmDu"
        ],
        "detail": [
          "https://openreview.net/forum?id=fLx3vQPmDu"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 2
      },
      "raw_excerpt": "OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts [PDF 7 ] [Copy] [Kimi 2 ] [REL] Authors : Shiting Xiao , Rishabh Kabra , Yuhang Li , Donghyun Lee , Joao Carreira , Priyadarshini Panda The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the model's spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks. Code is available at https://github.com/GinnyXiao/OpenWorldSAM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WrYWolqKh3@OpenReview",
      "index": 69,
      "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations",
      "authors": [
        "Brian Siyuan Zheng",
        "Alisa Liu",
        "Orevaoghene Ahia",
        "Jonathan Hayase",
        "Yejin Choi",
        "Noah A. Smith"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tokenizations",
        "tokenization",
        "canonical",
        "committed",
        "secretly",
        "character",
        "robustness",
        "language",
        "encoded",
        "instruction"
      ],
      "summary": "Modern tokenizers employ deterministic algorithms to map text into a single ``canonical\" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the language model vocabulary, including tokenizing by character. In this paper, we investigate the robustness of LMs to input encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4\\% of their original performance when given a randomly sampled tokenization, and 90.8\\% with character-level tokenization. We find that overall stronger models tend to be more robust, and that robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we identify settings where non-canonical tokenization schemes can \\textit{improve} performance, finding that character‑level segmentation improves string manipulation and code understanding tasks by up to 15\\%, and right‑aligned digit grouping enhances large‑number arithmetic by over 33\\%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We provide evidence that both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings). However, base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less committed to their tokenizer than previously believed, and highlight the promise of intervening on tokenization at inference time to boost language model performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WrYWolqKh3"
        ],
        "venue": [
          "/venue/WrYWolqKh3@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WrYWolqKh3"
        ],
        "detail": [
          "https://openreview.net/forum?id=WrYWolqKh3"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Brian Siyuan Zheng , Alisa Liu , Orevaoghene Ahia , Jonathan Hayase , Yejin Choi , Noah A. Smith Modern tokenizers employ deterministic algorithms to map text into a single ``canonical\" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the language model vocabulary, including tokenizing by character. In this paper, we investigate the robustness of LMs to input encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4\\% of their original performance when given a randomly sampled tokenization, and 90.8\\% with character-level tokenization. We find that overall stronger models tend to be more robust, and that robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we identify settings where non-canonical tokenization schemes can \\textit{improve} performance, finding that character‑level segmentation improves string manipulation and code understanding tasks by up to 15\\%, and right‑aligned digit grouping enhances large‑number arithmetic by over 33\\%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We provide evidence that both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings). However, base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less committed to their tokenizer than previously believed, and highlight the promise of intervening on tokenization at inference time to boost language model performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "GxGrGswvND@OpenReview",
      "index": 70,
      "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems",
      "authors": [
        "Ni Zhang",
        "Zhiguang Cao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gflownet",
        "hbg",
        "balance",
        "vrps",
        "cvrp",
        "vehicle",
        "routing",
        "depot",
        "optimization",
        "tsp"
      ],
      "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically employ Trajectory Balance (TB) to achieve global optimization but often neglect important aspects of local optimization. While Detailed Balance (DB) addresses local optimization more effectively, it alone falls short in solving VRPs, which inherently require holistic trajectory optimization. To address these limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which uniquely integrates TB and DB in a principled and adaptive manner by aligning their intrinsically complementary strengths. Additionally, we propose a specialized inference strategy for depot-centric scenarios like the Capacitated Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility in selecting successors. Despite this specialization, HBG maintains broad applicability, extending effectively to problems without explicit depots, such as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate consistent and significant improvements across both CVRP and TSP, underscoring the enhanced solution quality and generalization afforded by our approach.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GxGrGswvND"
        ],
        "venue": [
          "/venue/GxGrGswvND@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GxGrGswvND"
        ],
        "detail": [
          "https://openreview.net/forum?id=GxGrGswvND"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Ni Zhang , Zhiguang Cao Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically employ Trajectory Balance (TB) to achieve global optimization but often neglect important aspects of local optimization. While Detailed Balance (DB) addresses local optimization more effectively, it alone falls short in solving VRPs, which inherently require holistic trajectory optimization. To address these limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which uniquely integrates TB and DB in a principled and adaptive manner by aligning their intrinsically complementary strengths. Additionally, we propose a specialized inference strategy for depot-centric scenarios like the Capacitated Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility in selecting successors. Despite this specialization, HBG maintains broad applicability, extending effectively to problems without explicit depots, such as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate consistent and significant improvements across both CVRP and TSP, underscoring the enhanced solution quality and generalization afforded by our approach. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ustF8MMZDJ@OpenReview",
      "index": 71,
      "title": "Feedback-Aware MCTS for Goal-Oriented Information Seeking",
      "authors": [
        "Harshita Chopra",
        "Chirag Shah"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mcts",
        "seeking",
        "feedback",
        "information",
        "questions",
        "aware",
        "goal",
        "oriented",
        "gain",
        "planning"
      ],
      "summary": "Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12\\% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8\\% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ustF8MMZDJ"
        ],
        "venue": [
          "/venue/ustF8MMZDJ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ustF8MMZDJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=ustF8MMZDJ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Feedback-Aware MCTS for Goal-Oriented Information Seeking [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Harshita Chopra , Chirag Shah Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12\\% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8\\% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fGuTN7huo5@OpenReview",
      "index": 72,
      "title": "Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals",
      "authors": [
        "Stefan Stojanov",
        "David Wendt",
        "Seungwoo Kim",
        "Rahul Mysore Venkatesh",
        "Kevin Feigelis",
        "Klemen Kotar",
        "Khai Loong Aw",
        "Jiajun Wu",
        "Daniel LK Yamins"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cwm",
        "video",
        "opt",
        "supervised",
        "motion",
        "self",
        "probes",
        "occlusion",
        "counterfactual",
        "counterfactuals"
      ],
      "summary": "Estimating motion primitives from video (e.g., optical flow and occlusion) is a critically important computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily supervised on synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. A natural solution to transcend these limitations would be to deploy large-scale, self-supervised video models, which can be trained scalably on unrestricted real-world video datasets. However, despite recent progress, motion-primitive extraction from large pretrained video models remains relatively underexplored. In this work, we describe Opt-CWM, a self-supervised flow and occlusion estimation technique from a pretrained video prediction model. Opt-CWM uses ``counterfactual probes'' to extract motion information from a base video model in a zero-shot fashion. The key problem we solve is optimizing the quality of these probes, using a combination of an efficient parameterization of the space counterfactual probes, together with a novel generic sparse-prediction principle for learning the probe-generation parameters in a self-supervised fashion. Opt-CWM achieves state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fGuTN7huo5"
        ],
        "venue": [
          "/venue/fGuTN7huo5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fGuTN7huo5"
        ],
        "detail": [
          "https://openreview.net/forum?id=fGuTN7huo5"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Stefan Stojanov , David Wendt , Seungwoo Kim , Rahul Mysore Venkatesh , Kevin Feigelis , Klemen Kotar , Khai Loong Aw , Jiajun Wu , Daniel LK Yamins Estimating motion primitives from video (e.g., optical flow and occlusion) is a critically important computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily supervised on synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. A natural solution to transcend these limitations would be to deploy large-scale, self-supervised video models, which can be trained scalably on unrestricted real-world video datasets. However, despite recent progress, motion-primitive extraction from large pretrained video models remains relatively underexplored. In this work, we describe Opt-CWM, a self-supervised flow and occlusion estimation technique from a pretrained video prediction model. Opt-CWM uses ``counterfactual probes'' to extract motion information from a base video model in a zero-shot fashion. The key problem we solve is optimizing the quality of these probes, using a combination of an efficient parameterization of the space counterfactual probes, together with a novel generic sparse-prediction principle for learning the probe-generation parameters in a self-supervised fashion. Opt-CWM achieves state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Mat9FTfiYD@OpenReview",
      "index": 73,
      "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning",
      "authors": [
        "Tianxing Wu",
        "Shutong Zhu",
        "Jingting Wang",
        "Ning Xu",
        "Guilin Qi",
        "Haofen Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ukg",
        "confidences",
        "sscdl",
        "confidence",
        "completion",
        "triple",
        "uncertain",
        "triples",
        "ukgs",
        "knowledge"
      ],
      "summary": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence score to provide more precise knowledge representations. Recently, since real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG) completion attracts more attention, aiming to complete missing triples and confidences. Current studies attempt to learn UKG embeddings to solve this problem, but they neglect the extremely imbalanced distributions of triple confidences. This causes that the learnt embeddings are insufficient to high-quality UKG completion. Thus, in this paper, to address the above issue, we propose a new semi-supervised Confidence Distribution Learning (ssCDL) method for UKG completion, where each triple confidence is transformed into a confidence distribution to introduce more supervision information of different confidences to reinforce the embedding learning process. ssCDL iteratively learns UKG embedding by relational learning on labeled data (i.e., existing triples with confidences) and unlabeled data with pseudo labels (i.e., unseen triples with the generated confidences), which are predicted by meta-learning to augment the training data and rebalance the distribution of triple confidences. Experiments on two UKG datasets demonstrate that ssCDL consistently outperforms the state-of-the-art baselines in different evaluation metrics.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Mat9FTfiYD"
        ],
        "venue": [
          "/venue/Mat9FTfiYD@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Mat9FTfiYD"
        ],
        "detail": [
          "https://openreview.net/forum?id=Mat9FTfiYD"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Tianxing Wu , Shutong Zhu , Jingting Wang , Ning Xu , Guilin Qi , Haofen Wang Uncertain knowledge graphs (UKGs) associate each triple with a confidence score to provide more precise knowledge representations. Recently, since real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG) completion attracts more attention, aiming to complete missing triples and confidences. Current studies attempt to learn UKG embeddings to solve this problem, but they neglect the extremely imbalanced distributions of triple confidences. This causes that the learnt embeddings are insufficient to high-quality UKG completion. Thus, in this paper, to address the above issue, we propose a new semi-supervised Confidence Distribution Learning (ssCDL) method for UKG completion, where each triple confidence is transformed into a confidence distribution to introduce more supervision information of different confidences to reinforce the embedding learning process. ssCDL iteratively learns UKG embedding by relational learning on labeled data (i.e., existing triples with confidences) and unlabeled data with pseudo labels (i.e., unseen triples with the generated confidences), which are predicted by meta-learning to augment the training data and rebalance the distribution of triple confidences. Experiments on two UKG datasets demonstrate that ssCDL consistently outperforms the state-of-the-art baselines in different evaluation metrics. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VN5bMTfSZS@OpenReview",
      "index": 74,
      "title": "OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement",
      "authors": [
        "Ye Tian",
        "Angela McCarthy",
        "Gabriel Gomide",
        "Nancy Liddle",
        "Jedrzej Golebka",
        "Royce Chen",
        "Jeff Liebmann",
        "Kaveri A. Thakoor"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "octdiff",
        "oct",
        "portable",
        "bridged",
        "diffusion",
        "ddpms",
        "clinical",
        "resolution",
        "diagnostics",
        "msca"
      ],
      "summary": "Medical imaging super-resolution is critical for improving diagnostic utility and reducing costs, particularly for low-cost modalities such as portable Optical Coherence Tomography (OCT). We propose OCTDiff, a bridged diffusion model designed to enhance image resolution and quality from portable OCT devices. Our image-to-image diffusion framework addresses key challenges in the conditional generation process of denoising diffusion probabilistic models (DDPMs). We introduce Adaptive Noise Aggregation (ANA), a novel module to improve denoising dynamics within the reverse diffusion process. Additionally, we integrate Multi-Scale Cross-Attention (MSCA) into the U-Net backbone to capture local dependencies across spatial resolutions. To address overfitting on small clinical datasets and to preserve fine structural details essential for retinal diagnostics, we design a customized loss function guided by clinical quality scores. OCTDiff outperforms convolutional baselines and standard DDPMs, achieving state-of-the-art performance on clinical portable OCT datasets. Our model and its downstream applications have the potential to generalize to other medical imaging modalities and revolutionize the current workflow of ophthalmic diagnostics. The code is available at https://github.com/AI4VSLab/OCTDiff.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VN5bMTfSZS"
        ],
        "venue": [
          "/venue/VN5bMTfSZS@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VN5bMTfSZS"
        ],
        "detail": [
          "https://openreview.net/forum?id=VN5bMTfSZS"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Ye Tian , Angela McCarthy , Gabriel Gomide , Nancy Liddle , Jedrzej Golebka , Royce Chen , Jeff Liebmann , Kaveri A. Thakoor Medical imaging super-resolution is critical for improving diagnostic utility and reducing costs, particularly for low-cost modalities such as portable Optical Coherence Tomography (OCT). We propose OCTDiff, a bridged diffusion model designed to enhance image resolution and quality from portable OCT devices. Our image-to-image diffusion framework addresses key challenges in the conditional generation process of denoising diffusion probabilistic models (DDPMs). We introduce Adaptive Noise Aggregation (ANA), a novel module to improve denoising dynamics within the reverse diffusion process. Additionally, we integrate Multi-Scale Cross-Attention (MSCA) into the U-Net backbone to capture local dependencies across spatial resolutions. To address overfitting on small clinical datasets and to preserve fine structural details essential for retinal diagnostics, we design a customized loss function guided by clinical quality scores. OCTDiff outperforms convolutional baselines and standard DDPMs, achieving state-of-the-art performance on clinical portable OCT datasets. Our model and its downstream applications have the potential to generalize to other medical imaging modalities and revolutionize the current workflow of ophthalmic diagnostics. The code is available at https://github.com/AI4VSLab/OCTDiff. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "DwXX8c7xst@OpenReview",
      "index": 75,
      "title": "Bits Leaked per Query: Information-Theoretic Bounds for Adversarial Attacks on LLMs",
      "authors": [
        "Masahiro Kaneko",
        "Timothy Baldwin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "leaked",
        "llms",
        "auditors",
        "information",
        "tokens",
        "logits",
        "adversarial",
        "attacks",
        "bits",
        "thinking"
      ],
      "summary": "Adversarial attacks by malicious users that threaten the safety of large language models (LLMs) can be viewed as attempts to infer a target property T T that is unknown when an instruction is issued, and becomes knowable only after the model's reply is observed. Examples of target properties T T include the binary flag that triggers an LLM's harmful response or rejection, and the degree to which information deleted by unlearning can be restored, both elicited via adversarial instructions. The LLM reveals an \\emph{observable signal} Z Z that potentially leaks hints for attacking through a response containing answer tokens, thinking process tokens, or logits. Yet the scale of information leaked remains anecdotal, leaving auditors without principled guidance and defenders blind to the transparency--risk trade-off. We fill this gap with an information-theoretic framework that computes how much information can be safely disclosed, and enables auditors to gauge how close their methods come to the fundamental limit. Treating the mutual information I ( Z ; T ) I ( Z ; T ) between the observation Z Z and the target property T T as the leaked bits per query, we show that achieving error ε ε requires at least log ( 1 / ε ) / I ( Z ; T ) log ⁡ ( 1 / ε ) / I ( Z ; T ) queries, scaling linearly with the inverse leak rate and only logarithmically with the desired accuracy. Thus, even a modest increase in disclosure collapses the attack cost from quadratic to logarithmic in terms of the desired accuracy. Experiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks corroborate the theory: exposing answer tokens alone requires about a thousand queries; adding logits cuts this to about a hundred; and revealing the full thinking process trims it to a few dozen. Our results provide the first principled yardstick for balancing transparency and security when deploying LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DwXX8c7xst"
        ],
        "venue": [
          "/venue/DwXX8c7xst@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DwXX8c7xst"
        ],
        "detail": [
          "https://openreview.net/forum?id=DwXX8c7xst"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Bits Leaked per Query: Information-Theoretic Bounds for Adversarial Attacks on LLMs [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Masahiro Kaneko , Timothy Baldwin Adversarial attacks by malicious users that threaten the safety of large language models (LLMs) can be viewed as attempts to infer a target property T T that is unknown when an instruction is issued, and becomes knowable only after the model's reply is observed. Examples of target properties T T include the binary flag that triggers an LLM's harmful response or rejection, and the degree to which information deleted by unlearning can be restored, both elicited via adversarial instructions. The LLM reveals an \\emph{observable signal} Z Z that potentially leaks hints for attacking through a response containing answer tokens, thinking process tokens, or logits. Yet the scale of information leaked remains anecdotal, leaving auditors without principled guidance and defenders blind to the transparency--risk trade-off. We fill this gap with an information-theoretic framework that computes how much information can be safely disclosed, and enables auditors to gauge how close their methods come to the fundamental limit. Treating the mutual information I ( Z ; T ) I ( Z ; T ) between the observation Z Z and the target property T T as the leaked bits per query, we show that achieving error ε ε requires at least log ( 1 / ε ) / I ( Z ; T ) log ⁡ ( 1 / ε ) / I ( Z ; T ) queries, scaling linearly with the inverse leak rate and only logarithmically with the desired accuracy. Thus, even a modest increase in disclosure collapses the attack cost from quadratic to logarithmic in terms of the desired accuracy. Experiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks corroborate the theory: exposing answer tokens alone requires about a thousand queries; adding logits cuts this to about a hundred; and revealing the full thinking process trims it to a few dozen. Our results provide the first principled yardstick for balancing transparency and security when deploying LLMs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "I822ZIRtms@OpenReview",
      "index": 76,
      "title": "Characterizing control between interacting subsystems with deep Jacobian estimation",
      "authors": [
        "Adam Joseph Eisen",
        "Mitchell Ostrow",
        "Sarthak Chandra",
        "Leo Kozachkov",
        "Earl K Miller",
        "Ila R Fiete"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "subsystems",
        "control",
        "jacobian",
        "jacobianode",
        "subsystem",
        "rnn",
        "interactions",
        "understanding",
        "systems",
        "contextual"
      ],
      "summary": "Biological function arises through the dynamical interactions of multiple subsystems, including those between brain areas, within gene regulatory networks, and more. A common approach to understanding these systems is to model the dynamics of each subsystem and characterize communication between them. An alternative approach is through the lens of control theory: how the subsystems control one another. This approach involves inferring the directionality, strength, and contextual modulation of control between subsystems. However, methods for understanding subsystem control are typically linear and cannot adequately describe the rich contextual effects enabled by nonlinear complex systems. To bridge this gap, we devise a data-driven nonlinear control-theoretic framework to characterize subsystem interactions via the Jacobian of the dynamics. We address the challenge of learning Jacobians from time-series data by proposing the JacobianODE, a deep learning method that leverages properties of the Jacobian to directly estimate it for arbitrary dynamical systems from data alone. We show that JacobianODE models outperform existing Jacobian estimation methods on challenging systems, including high-dimensional chaos. Applying our approach to a multi-area recurrent neural network (RNN) trained on a working memory selection task, we show that the “sensory” area gains greater control over the “cognitive” area over learning. Furthermore, we leverage the JacobianODE to directly control the trained RNN, enabling precise manipulation of its behavior. Our work lays the foundation for a theoretically grounded and data-driven understanding of interactions among biological subsystems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=I822ZIRtms"
        ],
        "venue": [
          "/venue/I822ZIRtms@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=I822ZIRtms"
        ],
        "detail": [
          "https://openreview.net/forum?id=I822ZIRtms"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Characterizing control between interacting subsystems with deep Jacobian estimation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Adam Joseph Eisen , Mitchell Ostrow , Sarthak Chandra , Leo Kozachkov , Earl K Miller , Ila R Fiete Biological function arises through the dynamical interactions of multiple subsystems, including those between brain areas, within gene regulatory networks, and more. A common approach to understanding these systems is to model the dynamics of each subsystem and characterize communication between them. An alternative approach is through the lens of control theory: how the subsystems control one another. This approach involves inferring the directionality, strength, and contextual modulation of control between subsystems. However, methods for understanding subsystem control are typically linear and cannot adequately describe the rich contextual effects enabled by nonlinear complex systems. To bridge this gap, we devise a data-driven nonlinear control-theoretic framework to characterize subsystem interactions via the Jacobian of the dynamics. We address the challenge of learning Jacobians from time-series data by proposing the JacobianODE, a deep learning method that leverages properties of the Jacobian to directly estimate it for arbitrary dynamical systems from data alone. We show that JacobianODE models outperform existing Jacobian estimation methods on challenging systems, including high-dimensional chaos. Applying our approach to a multi-area recurrent neural network (RNN) trained on a working memory selection task, we show that the “sensory” area gains greater control over the “cognitive” area over learning. Furthermore, we leverage the JacobianODE to directly control the trained RNN, enabling precise manipulation of its behavior. Our work lays the foundation for a theoretically grounded and data-driven understanding of interactions among biological subsystems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZvqbNFWQkh@OpenReview",
      "index": 77,
      "title": "Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings",
      "authors": [
        "Evan Markou",
        "Thalaiyasingam Ajanthan",
        "Stephen Gould"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "optimisation",
        "mappings",
        "reparametrise",
        "convergence",
        "reduction",
        "reductions",
        "sharper",
        "nonconvex",
        "minimisers",
        "objective"
      ],
      "summary": "Many high-dimensional optimisation problems exhibit rich geometric structures in their set of minimisers, often forming smooth manifolds due to over-parametrisation or symmetries. When this structure is known, at least locally, it can be exploited through reduction mappings that reparametrise part of the parameter space to lie on the solution manifold. These reductions naturally arise from inner optimisation problems and effectively remove redundant directions, yielding a lower-dimensional objective. In this work, we introduce a general framework to understand how such reductions influence the optimisation landscape. We show that well-designed reduction mappings improve curvature properties of the objective, leading to better-conditioned problems and theoretically faster convergence for gradient-based methods. Our analysis unifies a range of scenarios where structural information at optimality is leveraged to accelerate convergence, offering a principled explanation for the empirical gains observed in such optimisation algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZvqbNFWQkh"
        ],
        "venue": [
          "/venue/ZvqbNFWQkh@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZvqbNFWQkh"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZvqbNFWQkh"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Evan Markou , Thalaiyasingam Ajanthan , Stephen Gould Many high-dimensional optimisation problems exhibit rich geometric structures in their set of minimisers, often forming smooth manifolds due to over-parametrisation or symmetries. When this structure is known, at least locally, it can be exploited through reduction mappings that reparametrise part of the parameter space to lie on the solution manifold. These reductions naturally arise from inner optimisation problems and effectively remove redundant directions, yielding a lower-dimensional objective. In this work, we introduce a general framework to understand how such reductions influence the optimisation landscape. We show that well-designed reduction mappings improve curvature properties of the objective, leading to better-conditioned problems and theoretically faster convergence for gradient-based methods. Our analysis unifies a range of scenarios where structural information at optimality is leveraged to accelerate convergence, offering a principled explanation for the empirical gains observed in such optimisation algorithms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "BoYGLpNXZd@OpenReview",
      "index": 78,
      "title": "Selective Omniprediction and Fair Abstention",
      "authors": [
        "Sílvia Casacuberta",
        "Varun Kanade"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "abstain",
        "selective",
        "omniprediction",
        "abstention",
        "abstentions",
        "algorithms",
        "multigroup",
        "classifiers",
        "optimally",
        "building"
      ],
      "summary": "We propose new learning algorithms for building selective classifiers, which are predictors that are allowed to abstain on some fraction of the domain. We study the model where a classifier may abstain from predicting at a fixed cost. Building on the recent framework on multigroup fairness and omniprediction, given a pre-specified class of loss functions, we provide an algorithm for building a single classifier that learns abstentions and predictions optimally for every loss in the entire class, where the abstentions are decided efficiently for each specific loss function by applying a fixed post-processing function. Our algorithm and theoretical guarantees generalize the previously-known algorithms for learning selective classifiers in formal learning-theoretic models. We then extend the traditional multigroup fairness algorithms to the selective classification setting and show that we can use a calibrated and multiaccurate predictor to efficiently build selective classifiers that abstain optimally not only globally but also locally within each of the groups in any pre-specified collection of possibly intersecting subgroups of the domain, and are also accurate when they do not abstain. We show how our abstention algorithms can be used as conformal prediction methods in the binary classification setting to achieve both marginal and group-conditional coverage guarantees for an intersecting collection of groups. We provide empirical evaluations for all of our theoretical results, demonstrating the practicality of our learning algorithms for abstaining optimally and fairly.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BoYGLpNXZd"
        ],
        "venue": [
          "/venue/BoYGLpNXZd@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BoYGLpNXZd"
        ],
        "detail": [
          "https://openreview.net/forum?id=BoYGLpNXZd"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Selective Omniprediction and Fair Abstention [PDF ] [Copy] [Kimi ] [REL] Authors : Sílvia Casacuberta , Varun Kanade We propose new learning algorithms for building selective classifiers, which are predictors that are allowed to abstain on some fraction of the domain. We study the model where a classifier may abstain from predicting at a fixed cost. Building on the recent framework on multigroup fairness and omniprediction, given a pre-specified class of loss functions, we provide an algorithm for building a single classifier that learns abstentions and predictions optimally for every loss in the entire class, where the abstentions are decided efficiently for each specific loss function by applying a fixed post-processing function. Our algorithm and theoretical guarantees generalize the previously-known algorithms for learning selective classifiers in formal learning-theoretic models. We then extend the traditional multigroup fairness algorithms to the selective classification setting and show that we can use a calibrated and multiaccurate predictor to efficiently build selective classifiers that abstain optimally not only globally but also locally within each of the groups in any pre-specified collection of possibly intersecting subgroups of the domain, and are also accurate when they do not abstain. We show how our abstention algorithms can be used as conformal prediction methods in the binary classification setting to achieve both marginal and group-conditional coverage guarantees for an intersecting collection of groups. We provide empirical evaluations for all of our theoretical results, demonstrating the practicality of our learning algorithms for abstaining optimally and fairly. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "cD4whTwm6G@OpenReview",
      "index": 79,
      "title": "Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-Index Models",
      "authors": [
        "Ilias Diakonikolas",
        "Giannis Iakovidis",
        "Daniel Kane",
        "Lisheng Ren"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mims",
        "complexity",
        "poly",
        "mathrm",
        "subspace",
        "algorithm",
        "epsilon",
        "pac",
        "learner",
        "valued"
      ],
      "summary": "We study the complexity of learning real-valued Multi-Index Models (MIMs) under the Gaussian distribution. A K K -MIM is a function f : R d → R f : R d → R that depends only on the projection of its input onto a K K -dimensional subspace. We give a general algorithm for PAC learning a broad class of MIMs with respect to the square loss, even in the presence of adversarial label noise. Moreover, we establish a nearly matching Statistical Query (SQ) lower bound, providing evidence that the complexity of our algorithm is qualitatively optimal as a function of the dimension. Specifically, we consider the class of bounded variation MIMs with the property that degree at most m m distinguishing moments exist with respect to projections onto any subspace. In the presence of adversarial label noise, the complexity of our learning algorithm is d O ( m ) 2 p o l y ( K / ϵ ) d O ( m ) 2 p o l y ( K / ϵ ) . For the realizable and independent noise settings, our algorithm incurs complexity d O ( m ) 2 p o l y ( K ) ( 1 / ϵ ) O ( K ) d O ( m ) 2 p o l y ( K ) ( 1 / ϵ ) O ( K ) . To complement our upper bound, we show that if for some subspace degree- m m distinguishing moments do not exist, then any SQ learner for the corresponding class of MIMs requires complexity d Ω ( m ) d Ω ( m ) . As an application, we give the first efficient learner for the class of positive-homogeneous L L -Lipschitz K K -MIMs. The resulting algorithm has complexity p o l y ( d ) 2 p o l y ( K L / ϵ ) p o l y ( d ) 2 p o l y ( K L / ϵ ) . This gives a new PAC learning algorithm for Lipschitz homogeneous ReLU networks with complexity independent of the network size, removing the exponential dependence incurred in prior work.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cD4whTwm6G"
        ],
        "venue": [
          "/venue/cD4whTwm6G@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cD4whTwm6G"
        ],
        "detail": [
          "https://openreview.net/forum?id=cD4whTwm6G"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-Index Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ilias Diakonikolas , Giannis Iakovidis , Daniel Kane , Lisheng Ren We study the complexity of learning real-valued Multi-Index Models (MIMs) under the Gaussian distribution. A K K -MIM is a function f : R d → R f : R d → R that depends only on the projection of its input onto a K K -dimensional subspace. We give a general algorithm for PAC learning a broad class of MIMs with respect to the square loss, even in the presence of adversarial label noise. Moreover, we establish a nearly matching Statistical Query (SQ) lower bound, providing evidence that the complexity of our algorithm is qualitatively optimal as a function of the dimension. Specifically, we consider the class of bounded variation MIMs with the property that degree at most m m distinguishing moments exist with respect to projections onto any subspace. In the presence of adversarial label noise, the complexity of our learning algorithm is d O ( m ) 2 p o l y ( K / ϵ ) d O ( m ) 2 p o l y ( K / ϵ ) . For the realizable and independent noise settings, our algorithm incurs complexity d O ( m ) 2 p o l y ( K ) ( 1 / ϵ ) O ( K ) d O ( m ) 2 p o l y ( K ) ( 1 / ϵ ) O ( K ) . To complement our upper bound, we show that if for some subspace degree- m m distinguishing moments do not exist, then any SQ learner for the corresponding class of MIMs requires complexity d Ω ( m ) d Ω ( m ) . As an application, we give the first efficient learner for the class of positive-homogeneous L L -Lipschitz K K -MIMs. The resulting algorithm has complexity p o l y ( d ) 2 p o l y ( K L / ϵ ) p o l y ( d ) 2 p o l y ( K L / ϵ ) . This gives a new PAC learning algorithm for Lipschitz homogeneous ReLU networks with complexity independent of the network size, removing the exponential dependence incurred in prior work. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0qRXETZZwv@OpenReview",
      "index": 80,
      "title": "A Principled Approach to Randomized Selection under Uncertainty: Applications to Peer Review and Grant Funding",
      "authors": [
        "Alexander Koujianos Goldberg",
        "Giulia Fanti",
        "Nihar B Shah"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "randomized",
        "peer",
        "merit",
        "grant",
        "funding",
        "principled",
        "uncertainty",
        "review",
        "selection",
        "worst"
      ],
      "summary": "Many decision-making processes involve evaluating and selecting items, including scientific peer review, job hiring, school admissions, and investment decisions. These domains feature error-prone evaluations and uncertainty about outcomes, which undermine deterministic selection rules. Consequently, randomized selection mechanisms are gaining traction. However, current randomized approaches are ad hoc and, as we prove, inappropriate for their purported objectives. We propose a principled framework for randomized decision-making based on interval estimates of item quality. We introduce MERIT (Maximin Efficient Randomized Interval Top- k k ), which maximizes the worst-case expected number of top candidates selected under uncertainty represented by overlapping intervals. MERIT provides optimal resource allocation under an interpretable robustness notion. We develop a polynomial-time, practically efficient algorithm and prove our approach satisfies desirable axiomatic properties not guaranteed by existing methods. Experiments on synthetic peer review data from grant funding and conferences demonstrate that MERIT matches existing algorithms' expected utility under fully probabilistic models while outperforming them under our worst-case formulation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0qRXETZZwv"
        ],
        "venue": [
          "/venue/0qRXETZZwv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0qRXETZZwv"
        ],
        "detail": [
          "https://openreview.net/forum?id=0qRXETZZwv"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "A Principled Approach to Randomized Selection under Uncertainty: Applications to Peer Review and Grant Funding [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Alexander Koujianos Goldberg , Giulia Fanti , Nihar B Shah Many decision-making processes involve evaluating and selecting items, including scientific peer review, job hiring, school admissions, and investment decisions. These domains feature error-prone evaluations and uncertainty about outcomes, which undermine deterministic selection rules. Consequently, randomized selection mechanisms are gaining traction. However, current randomized approaches are ad hoc and, as we prove, inappropriate for their purported objectives. We propose a principled framework for randomized decision-making based on interval estimates of item quality. We introduce MERIT (Maximin Efficient Randomized Interval Top- k k ), which maximizes the worst-case expected number of top candidates selected under uncertainty represented by overlapping intervals. MERIT provides optimal resource allocation under an interpretable robustness notion. We develop a polynomial-time, practically efficient algorithm and prove our approach satisfies desirable axiomatic properties not guaranteed by existing methods. Experiments on synthetic peer review data from grant funding and conferences demonstrate that MERIT matches existing algorithms' expected utility under fully probabilistic models while outperforming them under our worst-case formulation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xVI8g50Qfk@OpenReview",
      "index": 81,
      "title": "Error Forcing in Recurrent Neural Networks",
      "authors": [
        "A Erdem Sağtekin",
        "Colin Bredenberg",
        "Cristina Savin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "forcing",
        "feedback",
        "error",
        "rnn",
        "neural",
        "recurrent",
        "learning",
        "activity",
        "biological",
        "orthogonally"
      ],
      "summary": "How should feedback influence recurrent neural network (RNN) learning? One way to address the known limitations of backpropagation through time is to directly adjust neural activities during the learning process. However, it remains unclear how to effectively use feedback to shape RNN dynamics. Here, we introduce error forcing (EF), where the network activity is guided orthogonally toward the zero-error manifold during learning. This method contrasts with alternatives like teaching forcing, which impose stronger constraints on neural activity and thus induce larger feedback influence on circuit dynamics. Furthermore, EF can be understood from a Bayesian perspective as a form of approximate dynamic inference. Empirically, EF consistently outperforms other learning algorithms across several tasks and its benefits persist when additional biological constraints are taken into account. Overall, EF is a powerful temporal credit assignment mechanism and a promising candidate model for learning in biological systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xVI8g50Qfk"
        ],
        "venue": [
          "/venue/xVI8g50Qfk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xVI8g50Qfk"
        ],
        "detail": [
          "https://openreview.net/forum?id=xVI8g50Qfk"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Error Forcing in Recurrent Neural Networks [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : A Erdem Sağtekin , Colin Bredenberg , Cristina Savin How should feedback influence recurrent neural network (RNN) learning? One way to address the known limitations of backpropagation through time is to directly adjust neural activities during the learning process. However, it remains unclear how to effectively use feedback to shape RNN dynamics. Here, we introduce error forcing (EF), where the network activity is guided orthogonally toward the zero-error manifold during learning. This method contrasts with alternatives like teaching forcing, which impose stronger constraints on neural activity and thus induce larger feedback influence on circuit dynamics. Furthermore, EF can be understood from a Bayesian perspective as a form of approximate dynamic inference. Empirically, EF consistently outperforms other learning algorithms across several tasks and its benefits persist when additional biological constraints are taken into account. Overall, EF is a powerful temporal credit assignment mechanism and a promising candidate model for learning in biological systems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YvypxK4kut@OpenReview",
      "index": 82,
      "title": "Aligning Text-to-Image Diffusion Models to Human Preference by Classification",
      "authors": [
        "Longquan Dai",
        "Xiaolu Wei",
        "He Wang",
        "Shaomeng Wang",
        "Jinhui Tang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "preference",
        "abc",
        "classification",
        "diffusion",
        "alignment",
        "human",
        "aligning",
        "reference",
        "text",
        "models"
      ],
      "summary": "Text-to-image diffusion models are typically trained on large-scale web data, often resulting in outputs that misalign with human preferences. Inspired by preference learning in large language models, we propose ABC (Alignment by Classification), a simple yet effective framework for aligning diffusion models with human preferences. In contrast to prior DPO-based methods that depend on suboptimal supervised fine-tuned (SFT) reference models, ABC assumes access to an ideal reference model perfectly aligned with human intent and reformulates alignment as a classification problem. Under this view, we recognize that preference data naturally forms a semi-supervised classification setting. To address this, we propose a data augmentation strategy that transforms preference comparisons into fully supervised training signals. We then introduce a classification-based ABC loss to guide alignment. Our alignment by classification approach could effectively steer the diffusion model toward the behavior of the ideal reference. Experiments on various diffusion models show that our ABC consistently outperforms existing baselines, offering a scalable and robust solution for preference-based text-to-image fine-tuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YvypxK4kut"
        ],
        "venue": [
          "/venue/YvypxK4kut@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YvypxK4kut"
        ],
        "detail": [
          "https://openreview.net/forum?id=YvypxK4kut"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Aligning Text-to-Image Diffusion Models to Human Preference by Classification [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Longquan Dai , Xiaolu Wei , He Wang , Shaomeng Wang , Jinhui Tang Text-to-image diffusion models are typically trained on large-scale web data, often resulting in outputs that misalign with human preferences. Inspired by preference learning in large language models, we propose ABC (Alignment by Classification), a simple yet effective framework for aligning diffusion models with human preferences. In contrast to prior DPO-based methods that depend on suboptimal supervised fine-tuned (SFT) reference models, ABC assumes access to an ideal reference model perfectly aligned with human intent and reformulates alignment as a classification problem. Under this view, we recognize that preference data naturally forms a semi-supervised classification setting. To address this, we propose a data augmentation strategy that transforms preference comparisons into fully supervised training signals. We then introduce a classification-based ABC loss to guide alignment. Our alignment by classification approach could effectively steer the diffusion model toward the behavior of the ideal reference. Experiments on various diffusion models show that our ABC consistently outperforms existing baselines, offering a scalable and robust solution for preference-based text-to-image fine-tuning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "AUs0rScwK0@OpenReview",
      "index": 83,
      "title": "Sketched Gaussian Mechanism for Private Federated Learning",
      "authors": [
        "Qiaobo Li",
        "Zhijie Chen",
        "Arindam Banerjee"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "privacy",
        "sketching",
        "sgm",
        "sketched",
        "gaussian",
        "private",
        "mechanism",
        "federated",
        "communication",
        "updates"
      ],
      "summary": "Communication cost and privacy are two major considerations in federated learning (FL). For communication cost, gradient compression by sketching the clients’ transmitted model updates is often used for reducing per‐round communication. For privacy, the Gaussian mechanism (GM), which consists of clipping updates and adding Gaussian noise, is commonly used to guarantee client‐level differential privacy. Existing literature on private FL analyzes privacy of sketching and GM in an isolated manner, illustrating that sketching provides privacy determined by the sketching dimension and that GM has to supply any additional desired privacy. In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which directly combines sketching and the Gaussian mechanism for privacy. Using Rényi-DP tools, we present a joint analysis of SGM's overall privacy guarantee, which is significantly more flexible and sharper compared to isolated analysis of sketching and GM privacy. In particular, we prove that the privacy level of SGM for a fixed noise magnitude is proportional to 1 / b √ 1 / b , where b b is the sketching dimension, indicating that (for moderate b b ) SGM can provide much stronger privacy guarantees than the original GM under the same noise budget. We demonstrate the application of SGM to FL with either gradient descent or adaptive server optimizers, and establish theoretical results on optimization convergence, which exhibits only a logarithmic dependence on the number of parameters d d . Experimental results confirm that at the same privacy level, SGM based FL is at least competitive with non‐sketching private FL variants and outperforms them in some settings. Moreover, using adaptive optimization at the server improves empirical performance while maintaining the privacy guarantees.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AUs0rScwK0"
        ],
        "venue": [
          "/venue/AUs0rScwK0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AUs0rScwK0"
        ],
        "detail": [
          "https://openreview.net/forum?id=AUs0rScwK0"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Sketched Gaussian Mechanism for Private Federated Learning [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Qiaobo Li , Zhijie Chen , Arindam Banerjee Communication cost and privacy are two major considerations in federated learning (FL). For communication cost, gradient compression by sketching the clients’ transmitted model updates is often used for reducing per‐round communication. For privacy, the Gaussian mechanism (GM), which consists of clipping updates and adding Gaussian noise, is commonly used to guarantee client‐level differential privacy. Existing literature on private FL analyzes privacy of sketching and GM in an isolated manner, illustrating that sketching provides privacy determined by the sketching dimension and that GM has to supply any additional desired privacy. In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which directly combines sketching and the Gaussian mechanism for privacy. Using Rényi-DP tools, we present a joint analysis of SGM's overall privacy guarantee, which is significantly more flexible and sharper compared to isolated analysis of sketching and GM privacy. In particular, we prove that the privacy level of SGM for a fixed noise magnitude is proportional to 1 / b √ 1 / b , where b b is the sketching dimension, indicating that (for moderate b b ) SGM can provide much stronger privacy guarantees than the original GM under the same noise budget. We demonstrate the application of SGM to FL with either gradient descent or adaptive server optimizers, and establish theoretical results on optimization convergence, which exhibits only a logarithmic dependence on the number of parameters d d . Experimental results confirm that at the same privacy level, SGM based FL is at least competitive with non‐sketching private FL variants and outperforms them in some settings. Moreover, using adaptive optimization at the server improves empirical performance while maintaining the privacy guarantees. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "UOaLsgn5wb@OpenReview",
      "index": 84,
      "title": "Joint‑Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self‑Supervised Learning",
      "authors": [
        "Hugues Van Assel",
        "Mark Ibrahim",
        "Tommaso Biancalani",
        "Aviv Regev",
        "Randall Balestriero"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "joint",
        "embedding",
        "reconstruction",
        "paradigms",
        "ssl",
        "supervised",
        "irrelevant",
        "latent",
        "methods",
        "approaches"
      ],
      "summary": "Reconstruction and joint-embedding have emerged as two leading paradigms in Self‑Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint-embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed-form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint-embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction-based methods. These results not only clarify the trade-offs between the two paradigms but also substantiate the empirical success of joint-embedding approaches on real-world challenging datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UOaLsgn5wb"
        ],
        "venue": [
          "/venue/UOaLsgn5wb@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UOaLsgn5wb"
        ],
        "detail": [
          "https://openreview.net/forum?id=UOaLsgn5wb"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Joint‑Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self‑Supervised Learning [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Hugues Van Assel , Mark Ibrahim , Tommaso Biancalani , Aviv Regev , Randall Balestriero Reconstruction and joint-embedding have emerged as two leading paradigms in Self‑Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint-embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed-form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint-embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction-based methods. These results not only clarify the trade-offs between the two paradigms but also substantiate the empirical success of joint-embedding approaches on real-world challenging datasets. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "sXpyn3lAb5@OpenReview",
      "index": 85,
      "title": "Accelerating data-driven algorithm selection for combinatorial partitioning problems",
      "authors": [
        "Vaggos Chatziafratis",
        "Ishani Karmarkar",
        "Yingxi Li",
        "Ellen Vitercik"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "instance",
        "algorithms",
        "subsample",
        "practice",
        "algorithm",
        "representative",
        "performance",
        "size",
        "workaround",
        "gonzalez"
      ],
      "summary": "Data-driven algorithm selection is a powerful approach for choosing effective heuristics for computational problems. It operates by evaluating a set of candidate algorithms on a collection of representative training instances and selecting the one with the best empirical performance. However, running each algorithm on every training instance is computationally expensive, making scalability a central challenge. In practice, a common workaround is to evaluate algorithms on smaller proxy instances derived from the original inputs. However, this practice has remained largely ad hoc and lacked theoretical grounding. We provide the first theoretical foundations for this practice by formalizing the notion of size generalization: predicting an algorithm's performance on a large instance by evaluating it on a smaller, representative instance, subsampled from the original instance. We provide size generalization guarantees for three widely used clustering algorithms (single-linkage, k-means++, and Gonzalez's k-centers heuristic) and two canonical max-cut algorithms (Goemans-Williamson and Greedy). We characterize the subsample size sufficient to ensure that performance on the subsample reflects performance on the full instance, and our experiments support these findings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sXpyn3lAb5"
        ],
        "venue": [
          "/venue/sXpyn3lAb5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sXpyn3lAb5"
        ],
        "detail": [
          "https://openreview.net/forum?id=sXpyn3lAb5"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Accelerating data-driven algorithm selection for combinatorial partitioning problems [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Vaggos Chatziafratis , Ishani Karmarkar , Yingxi Li , Ellen Vitercik Data-driven algorithm selection is a powerful approach for choosing effective heuristics for computational problems. It operates by evaluating a set of candidate algorithms on a collection of representative training instances and selecting the one with the best empirical performance. However, running each algorithm on every training instance is computationally expensive, making scalability a central challenge. In practice, a common workaround is to evaluate algorithms on smaller proxy instances derived from the original inputs. However, this practice has remained largely ad hoc and lacked theoretical grounding. We provide the first theoretical foundations for this practice by formalizing the notion of size generalization: predicting an algorithm's performance on a large instance by evaluating it on a smaller, representative instance, subsampled from the original instance. We provide size generalization guarantees for three widely used clustering algorithms (single-linkage, k-means++, and Gonzalez's k-centers heuristic) and two canonical max-cut algorithms (Goemans-Williamson and Greedy). We characterize the subsample size sufficient to ensure that performance on the subsample reflects performance on the full instance, and our experiments support these findings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kePsKwxvaV@OpenReview",
      "index": 86,
      "title": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner",
      "authors": [
        "Runa Eschenhagen",
        "Aaron Defazio",
        "Tsung-Hsien Lee",
        "Richard E. Turner",
        "Hao-Jun Michael Shi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shampoo",
        "preconditioner",
        "heuristics",
        "grafting",
        "eigenbasis",
        "eigenvalues",
        "kronecker",
        "adam",
        "decouples",
        "purifying"
      ],
      "summary": "The recent success of Shampoo in the AlgoPerf contest has sparked renewed interest in Kronecker-factorization-based optimization algorithms for training neural networks. Despite its success, Shampoo relies heavily on several heuristics such as learning rate grafting and stale preconditioning to achieve performance at-scale. These heuristics increase algorithmic complexity, necessitate further hyperparameter tuning, and lack theoretical justification. This paper investigates these heuristics from the angle of Frobenius norm approximation to full-matrix Adam and decouples the preconditioner's eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates the staleness and mis-scaling of the preconditioner's *eigenvalues* and how correcting the eigenvalues directly eliminates the need for learning rate grafting. To manage the error induced by infrequent *eigenbasis* computations, we propose an adaptive criterion for determining the eigenbasis computation frequency motivated by terminating a warm-started QR algorithm. This criterion decouples the update frequency of different preconditioner matrices and enables us to investigate the impact of approximation error on convergence. These practical techniques offer a principled angle towards removing Shampoo's heuristics and developing improved Kronecker-factorization-based training algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kePsKwxvaV"
        ],
        "venue": [
          "/venue/kePsKwxvaV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kePsKwxvaV"
        ],
        "detail": [
          "https://openreview.net/forum?id=kePsKwxvaV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Runa Eschenhagen , Aaron Defazio , Tsung-Hsien Lee , Richard E. Turner , Hao-Jun Michael Shi The recent success of Shampoo in the AlgoPerf contest has sparked renewed interest in Kronecker-factorization-based optimization algorithms for training neural networks. Despite its success, Shampoo relies heavily on several heuristics such as learning rate grafting and stale preconditioning to achieve performance at-scale. These heuristics increase algorithmic complexity, necessitate further hyperparameter tuning, and lack theoretical justification. This paper investigates these heuristics from the angle of Frobenius norm approximation to full-matrix Adam and decouples the preconditioner's eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates the staleness and mis-scaling of the preconditioner's *eigenvalues* and how correcting the eigenvalues directly eliminates the need for learning rate grafting. To manage the error induced by infrequent *eigenbasis* computations, we propose an adaptive criterion for determining the eigenbasis computation frequency motivated by terminating a warm-started QR algorithm. This criterion decouples the update frequency of different preconditioner matrices and enables us to investigate the impact of approximation error on convergence. These practical techniques offer a principled angle towards removing Shampoo's heuristics and developing improved Kronecker-factorization-based training algorithms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qYDBgSeAlU@OpenReview",
      "index": 87,
      "title": "Replicable Distribution Testing",
      "authors": [
        "Ilias Diakonikolas",
        "Jingyi Gao",
        "Daniel Kane",
        "Sihan Liu",
        "Christopher Ye"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "replicable",
        "testing",
        "closeness",
        "replicably",
        "algorithmic",
        "distributions",
        "sample",
        "complexity",
        "front",
        "lower"
      ],
      "summary": "We initiate a systematic investigation of distribution testing in the framework of algorithmic replicability. Specifically, given independent samples from a collection of probability distributions, the goal is to characterize the sample complexity of replicably testing natural properties of the underlying distributions. On the algorithmic front, we develop new replicable algorithms for testing closeness and independence of discrete distributions. On the lower bound front, we develop a new methodology for proving sample complexity lower bounds for replicable testing that may be of broader interest. As an application of our technique, we establish near-optimal sample complexity lower bounds for replicable uniformity testing---answering an open question from prior work---and closeness testing.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qYDBgSeAlU"
        ],
        "venue": [
          "/venue/qYDBgSeAlU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qYDBgSeAlU"
        ],
        "detail": [
          "https://openreview.net/forum?id=qYDBgSeAlU"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Replicable Distribution Testing [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ilias Diakonikolas , Jingyi Gao , Daniel Kane , Sihan Liu , Christopher Ye We initiate a systematic investigation of distribution testing in the framework of algorithmic replicability. Specifically, given independent samples from a collection of probability distributions, the goal is to characterize the sample complexity of replicably testing natural properties of the underlying distributions. On the algorithmic front, we develop new replicable algorithms for testing closeness and independence of discrete distributions. On the lower bound front, we develop a new methodology for proving sample complexity lower bounds for replicable testing that may be of broader interest. As an application of our technique, we establish near-optimal sample complexity lower bounds for replicable uniformity testing---answering an open question from prior work---and closeness testing. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "A9jXG3FUMT@OpenReview",
      "index": 88,
      "title": "FAPEX: Fractional Amplitude-Phase Expressor for Robust Cross-Subject Seizure Prediction",
      "authors": [
        "Ruizhe Zheng",
        "Lingyan Mao",
        "DINGDING HAN",
        "Tian Luo",
        "Yi Wang",
        "Jing Ding",
        "Yuguo Yu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fapex",
        "frnfo",
        "sasp",
        "preictal",
        "seizure",
        "fractional",
        "biomarker",
        "expressor",
        "cross",
        "channelwise"
      ],
      "summary": "Precise, generalizable subject-agnostic seizure prediction (SASP) remains a fundamental challenge due to the intrinsic complexity and significant spectral variability of electrophysiologial signals across individuals and recording modalities. We propose \\model{FAPEX}, a novel architecture that introduces a learnable \\emph{fractional neural frame operator} (FrNFO) for adaptive time–frequency decomposition. Unlike conventional models that exhibit spectral bias toward low frequencies, our FrNFO employs fractional-order convolutions to capture both high and low-frequency dynamics, achieving approximately 10 % 10 % improvement in F1-score and sensitivity over state-of-the-art baselines. The FrNFO enables the extraction of \\emph{instantaneous phase and amplitude representations} that are particularly informative for preictal biomarker discovery and enhance out-of-distribution generalization. \\model{FAPEX} further integrates structural state-space modeling and channelwise attention, allowing it to handle heterogeneous electrode montages. Evaluated across 12 benchmarks spanning species (human, rat, dog, macaque) and modalities (Scalp‑EEG, SEEG, ECoG, LFP), \\model{FAPEX} consistently outperforms 23 supervised and 10 self-supervised baselines under nested cross-validation, with gains of up to 15 % 15 % in sensitivity on complex cross-domain scenarios. It further demonstrates superior performance in several external validation cohorts. To our knowledge, these establish \\model{FAPEX} as the first epilepsy model to show consistent superiority in SASP, offering a promising solution for discovering epileptic biomarker evidence supporting the existence of a distinct and identifiable preictal state for and clinical translation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=A9jXG3FUMT"
        ],
        "venue": [
          "/venue/A9jXG3FUMT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=A9jXG3FUMT"
        ],
        "detail": [
          "https://openreview.net/forum?id=A9jXG3FUMT"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "FAPEX: Fractional Amplitude-Phase Expressor for Robust Cross-Subject Seizure Prediction [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Ruizhe Zheng , Lingyan Mao , DINGDING HAN , Tian Luo , Yi Wang , Jing Ding , Yuguo Yu Precise, generalizable subject-agnostic seizure prediction (SASP) remains a fundamental challenge due to the intrinsic complexity and significant spectral variability of electrophysiologial signals across individuals and recording modalities. We propose \\model{FAPEX}, a novel architecture that introduces a learnable \\emph{fractional neural frame operator} (FrNFO) for adaptive time–frequency decomposition. Unlike conventional models that exhibit spectral bias toward low frequencies, our FrNFO employs fractional-order convolutions to capture both high and low-frequency dynamics, achieving approximately 10 % 10 % improvement in F1-score and sensitivity over state-of-the-art baselines. The FrNFO enables the extraction of \\emph{instantaneous phase and amplitude representations} that are particularly informative for preictal biomarker discovery and enhance out-of-distribution generalization. \\model{FAPEX} further integrates structural state-space modeling and channelwise attention, allowing it to handle heterogeneous electrode montages. Evaluated across 12 benchmarks spanning species (human, rat, dog, macaque) and modalities (Scalp‑EEG, SEEG, ECoG, LFP), \\model{FAPEX} consistently outperforms 23 supervised and 10 self-supervised baselines under nested cross-validation, with gains of up to 15 % 15 % in sensitivity on complex cross-domain scenarios. It further demonstrates superior performance in several external validation cohorts. To our knowledge, these establish \\model{FAPEX} as the first epilepsy model to show consistent superiority in SASP, offering a promising solution for discovering epileptic biomarker evidence supporting the existence of a distinct and identifiable preictal state for and clinical translation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "dfcQFL89OM@OpenReview",
      "index": 89,
      "title": "Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems",
      "authors": [
        "Hao Liang",
        "Shuqing Shi",
        "Yudi Zhang",
        "Biwei Huang",
        "Yali Du"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gsac",
        "textbf",
        "acrs",
        "actor",
        "critic",
        "provably",
        "policy",
        "networked",
        "texttt",
        "domain"
      ],
      "summary": "Large‑scale networked systems, such as traffic, power, and wireless grids, challenge reinforcement‑learning agents with both scale and environment shifts. To address these challenges, we propose \\texttt{GSAC} (\\textbf{G}eneralizable and \\textbf{S}calable \\textbf{A}ctor‑\\textbf{C}ritic), a framework that couples causal representation learning with meta actor‑critic learning to achieve both scalability and domain generalization. Each agent first learns a sparse local causal mask that provably identifies the minimal neighborhood variables influencing its dynamics, yielding exponentially tight approximately compact representations (ACRs) of state and domain factors. These ACRs bound the error of truncating value functions to κ κ -hop neighborhoods, enabling efficient learning on graphs. A meta actor‑critic then trains a shared policy across multiple source domains while conditioning on the compact domain factors; at test time, a few trajectories suffice to estimate the new domain factor and deploy the adapted policy. We establish finite‑sample guarantees on causal recovery, actor-critic convergence, and adaptation gap, and show that \\texttt{GSAC} adapts rapidly and significantly outperforms learning-from-scratch and conventional adaptation baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dfcQFL89OM"
        ],
        "venue": [
          "/venue/dfcQFL89OM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dfcQFL89OM"
        ],
        "detail": [
          "https://openreview.net/forum?id=dfcQFL89OM"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hao Liang , Shuqing Shi , Yudi Zhang , Biwei Huang , Yali Du Large‑scale networked systems, such as traffic, power, and wireless grids, challenge reinforcement‑learning agents with both scale and environment shifts. To address these challenges, we propose \\texttt{GSAC} (\\textbf{G}eneralizable and \\textbf{S}calable \\textbf{A}ctor‑\\textbf{C}ritic), a framework that couples causal representation learning with meta actor‑critic learning to achieve both scalability and domain generalization. Each agent first learns a sparse local causal mask that provably identifies the minimal neighborhood variables influencing its dynamics, yielding exponentially tight approximately compact representations (ACRs) of state and domain factors. These ACRs bound the error of truncating value functions to κ κ -hop neighborhoods, enabling efficient learning on graphs. A meta actor‑critic then trains a shared policy across multiple source domains while conditioning on the compact domain factors; at test time, a few trajectories suffice to estimate the new domain factor and deploy the adapted policy. We establish finite‑sample guarantees on causal recovery, actor-critic convergence, and adaptation gap, and show that \\texttt{GSAC} adapts rapidly and significantly outperforms learning-from-scratch and conventional adaptation baselines. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZvUZvT8tgg@OpenReview",
      "index": 90,
      "title": "Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees",
      "authors": [
        "Daniel Ovalle",
        "Lorenz T. Biegler",
        "Ignacio E Grossmann",
        "Carl D Laird",
        "Mateo Dulce Rubio"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "micl",
        "constraint",
        "feasibility",
        "conformal",
        "mixed",
        "integer",
        "guarantees",
        "constraints",
        "feasible",
        "true"
      ],
      "summary": "We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel framework that provides probabilistic feasibility guarantees for data-driven constraints in optimization problems. While standard Mixed-Integer Constraint Learning methods often violate the true constraints due to model error or data limitations, our C-MICL approach leverages conformal prediction to ensure feasible solutions are ground-truth feasible with probability at least 1 − α 1 − α , under a conditional independence assumption. The proposed framework supports both regression and classification tasks without requiring access to the true constraint function, while avoiding the scalability issues associated with ensemble-based heuristics. Experiments on real-world applications demonstrate that C-MICL consistently achieves target feasibility rates, maintains competitive objective performance, and significantly reduces computational cost compared to existing methods. Our work bridges mathematical optimization and machine learning, offering a principled approach to incorporate uncertainty-aware constraints into decision-making with rigorous statistical guarantees.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZvUZvT8tgg"
        ],
        "venue": [
          "/venue/ZvUZvT8tgg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZvUZvT8tgg"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZvUZvT8tgg"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Daniel Ovalle , Lorenz T. Biegler , Ignacio E Grossmann , Carl D Laird , Mateo Dulce Rubio We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel framework that provides probabilistic feasibility guarantees for data-driven constraints in optimization problems. While standard Mixed-Integer Constraint Learning methods often violate the true constraints due to model error or data limitations, our C-MICL approach leverages conformal prediction to ensure feasible solutions are ground-truth feasible with probability at least 1 − α 1 − α , under a conditional independence assumption. The proposed framework supports both regression and classification tasks without requiring access to the true constraint function, while avoiding the scalability issues associated with ensemble-based heuristics. Experiments on real-world applications demonstrate that C-MICL consistently achieves target feasibility rates, maintains competitive objective performance, and significantly reduces computational cost compared to existing methods. Our work bridges mathematical optimization and machine learning, offering a principled approach to incorporate uncertainty-aware constraints into decision-making with rigorous statistical guarantees. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "1Imp4KZyjA@OpenReview",
      "index": 91,
      "title": "Why Do Some Language Models Fake Alignment While Others Don't?",
      "authors": [
        "Abhay Sheshadri",
        "John Hughes",
        "Julian Michael",
        "Alex Troy Mallen",
        "Arun Jose",
        "Fabien Roger"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "claude",
        "faking",
        "alignment",
        "opus",
        "fake",
        "sonnet",
        "models",
        "training",
        "others",
        "405b"
      ],
      "summary": "*Alignment Faking in Large Language Models* presented a demonstration of Claude 3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training objective to prevent modification of their behavior outside of training. We expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude 3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries more when they infer they are in training than when they infer they are in deployment. First, we study the motivations of these 5 models. Results from perturbing details of the scenario suggest that only Claude 3 Opus's compliance gap is primarily and consistently motivated by trying to keep its goals. Second, we investigate why many chat models don't fake alignment. Our results suggest this is not entirely due to a lack of capabilities: many base models fake alignment some of the time, and post-training eliminates alignment-faking for some models and amplifies it for others. We investigate 5 hypotheses for how post-training may suppress alignment faking and find that variations in refusal behavior may account for a significant portion of differences in alignment faking.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1Imp4KZyjA"
        ],
        "venue": [
          "/venue/1Imp4KZyjA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1Imp4KZyjA"
        ],
        "detail": [
          "https://openreview.net/forum?id=1Imp4KZyjA"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 4
      },
      "raw_excerpt": "Why Do Some Language Models Fake Alignment While Others Don't? [PDF 2 ] [Copy] [Kimi 4 ] [REL] Authors : Abhay Sheshadri , John Hughes , Julian Michael , Alex Troy Mallen , Arun Jose , Fabien Roger *Alignment Faking in Large Language Models* presented a demonstration of Claude 3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training objective to prevent modification of their behavior outside of training. We expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude 3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries more when they infer they are in training than when they infer they are in deployment. First, we study the motivations of these 5 models. Results from perturbing details of the scenario suggest that only Claude 3 Opus's compliance gap is primarily and consistently motivated by trying to keep its goals. Second, we investigate why many chat models don't fake alignment. Our results suggest this is not entirely due to a lack of capabilities: many base models fake alignment some of the time, and post-training eliminates alignment-faking for some models and amplifies it for others. We investigate 5 hypotheses for how post-training may suppress alignment faking and find that variations in refusal behavior may account for a significant portion of differences in alignment faking. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "36uy2GgAy6@OpenReview",
      "index": 92,
      "title": "When Data Can't Meet: Estimating Correlation Across Privacy Barriers",
      "authors": [
        "Abhinav Chakraborty",
        "Arnab Auddy",
        "T. Tony Cai"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "privacy",
        "varepsilon",
        "interactive",
        "correlation",
        "server",
        "budgets",
        "servers",
        "estimating",
        "observations",
        "rates"
      ],
      "summary": "We consider the problem of estimating the correlation of two random variables X X and Y Y , where the pairs ( X , Y ) ( X , Y ) are not observed together, but are instead separated co-ordinate-wise at two servers: server 1 contains all the X X observations, and server 2 contains the corresponding Y Y observations. In this vertically distributed setting, we assume that each server has its own privacy constraints, owing to which they can only share suitably privatized statistics of their own component observations. We consider differing privacy budgets ( ε 1 , δ 1 ) ( ε 1 , δ 1 ) and ( ε 2 , δ 2 ) ( ε 2 , δ 2 ) for the two servers and determine the minimax optimal rates for correlation estimation allowing for both non-interactive and interactive mechanisms. We also provide correlation estimators that achieve these rates and further develop inference procedures, namely, confidence intervals, for the estimated correlations. Our results are characterized by an interesting rate in terms of the sample size n n , ε 1 ε 1 , ε 2 ε 2 , which is strictly slower than the usual central privacy estimation rates. More interestingly, we find that the interactive mechanism is always better than its non-interactive counterpart whenever the two privacy budgets are different. Results from extensive numerical experiments support our theoretical findings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=36uy2GgAy6"
        ],
        "venue": [
          "/venue/36uy2GgAy6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=36uy2GgAy6"
        ],
        "detail": [
          "https://openreview.net/forum?id=36uy2GgAy6"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "When Data Can't Meet: Estimating Correlation Across Privacy Barriers [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Abhinav Chakraborty , Arnab Auddy , T. Tony Cai We consider the problem of estimating the correlation of two random variables X X and Y Y , where the pairs ( X , Y ) ( X , Y ) are not observed together, but are instead separated co-ordinate-wise at two servers: server 1 contains all the X X observations, and server 2 contains the corresponding Y Y observations. In this vertically distributed setting, we assume that each server has its own privacy constraints, owing to which they can only share suitably privatized statistics of their own component observations. We consider differing privacy budgets ( ε 1 , δ 1 ) ( ε 1 , δ 1 ) and ( ε 2 , δ 2 ) ( ε 2 , δ 2 ) for the two servers and determine the minimax optimal rates for correlation estimation allowing for both non-interactive and interactive mechanisms. We also provide correlation estimators that achieve these rates and further develop inference procedures, namely, confidence intervals, for the estimated correlations. Our results are characterized by an interesting rate in terms of the sample size n n , ε 1 ε 1 , ε 2 ε 2 , which is strictly slower than the usual central privacy estimation rates. More interestingly, we find that the interactive mechanism is always better than its non-interactive counterpart whenever the two privacy budgets are different. Results from extensive numerical experiments support our theoretical findings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "M6l3pyvUfr@OpenReview",
      "index": 93,
      "title": "TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence",
      "authors": [
        "Feng Jiang",
        "Mangal Prakash",
        "Hehuan Ma",
        "Jianyuan Deng",
        "Yuzhi Guo",
        "Amina Mollaysa",
        "Tommaso Mansi",
        "Rui Liao",
        "Junzhou Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "trident",
        "taxonomic",
        "molecular",
        "annotations",
        "textual",
        "tri",
        "functional",
        "smiles",
        "alignment",
        "learn"
      ],
      "summary": "Molecular property prediction aims to learn representations that map chemical structures to functional properties. While multimodal learning has emerged as a powerful paradigm to learn molecular representations, prior works have largely overlooked textual and taxonomic information of molecules for representation learning. We introduce TRIDENT, a novel framework that integrates molecular SMILES, textual descriptions, and taxonomic functional annotations to learn rich molecular representations. To achieve this, we curate a comprehensive dataset of molecule-text pairs with structured, multi-level functional annotations. Instead of relying on conventional contrastive loss, TRIDENT employs a volume-based alignment objective to jointly align tri-modal features at the global level, enabling soft, geometry-aware alignment across modalities. Additionally, TRIDENT introduces a novel local alignment objective that captures detailed relationships between molecular substructures and their corresponding sub-textual descriptions. A momentum-based mechanism dynamically balances global and local alignment, enabling the model to learn both broad functional semantics and fine-grained structure-function mappings. TRIDENT achieves state-of-the-art performance on 18 downstream tasks, demonstrating the value of combining SMILES, textual, and taxonomic functional annotations for molecular property prediction. Our code and data are available at https://github.com/uta-smile/TRIDENT.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M6l3pyvUfr"
        ],
        "venue": [
          "/venue/M6l3pyvUfr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M6l3pyvUfr"
        ],
        "detail": [
          "https://openreview.net/forum?id=M6l3pyvUfr"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Feng Jiang , Mangal Prakash , Hehuan Ma , Jianyuan Deng , Yuzhi Guo , Amina Mollaysa , Tommaso Mansi , Rui Liao , Junzhou Huang Molecular property prediction aims to learn representations that map chemical structures to functional properties. While multimodal learning has emerged as a powerful paradigm to learn molecular representations, prior works have largely overlooked textual and taxonomic information of molecules for representation learning. We introduce TRIDENT, a novel framework that integrates molecular SMILES, textual descriptions, and taxonomic functional annotations to learn rich molecular representations. To achieve this, we curate a comprehensive dataset of molecule-text pairs with structured, multi-level functional annotations. Instead of relying on conventional contrastive loss, TRIDENT employs a volume-based alignment objective to jointly align tri-modal features at the global level, enabling soft, geometry-aware alignment across modalities. Additionally, TRIDENT introduces a novel local alignment objective that captures detailed relationships between molecular substructures and their corresponding sub-textual descriptions. A momentum-based mechanism dynamically balances global and local alignment, enabling the model to learn both broad functional semantics and fine-grained structure-function mappings. TRIDENT achieves state-of-the-art performance on 18 downstream tasks, demonstrating the value of combining SMILES, textual, and taxonomic functional annotations for molecular property prediction. Our code and data are available at https://github.com/uta-smile/TRIDENT. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bzxlOyjWbU@OpenReview",
      "index": 94,
      "title": "Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences",
      "authors": [
        "Joshua Ashkinaze",
        "Hua Shen",
        "Sai Avula",
        "Eric Gilbert",
        "Ceren Budak"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dvgr",
        "dvb",
        "shallow",
        "maleficence",
        "deep",
        "values",
        "language",
        "value",
        "superficial",
        "justice"
      ],
      "summary": "We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features---for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR)---the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bzxlOyjWbU"
        ],
        "venue": [
          "/venue/bzxlOyjWbU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bzxlOyjWbU"
        ],
        "detail": [
          "https://openreview.net/forum?id=bzxlOyjWbU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Joshua Ashkinaze , Hua Shen , Sai Avula , Eric Gilbert , Ceren Budak We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features---for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR)---the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oLyfML1Qze@OpenReview",
      "index": 95,
      "title": "Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks",
      "authors": [
        "Ali Hariri",
        "Alvaro Arroyo",
        "Alessio Gravina",
        "Moshe Eliasof",
        "Carola-Bibiane Schönlieb",
        "Davide Bacciu",
        "Xiaowen Dong",
        "Kamyar Azizzadenesheli",
        "Pierre Vandergheynst"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "chebnet",
        "mpnns",
        "stable",
        "rewiring",
        "graph",
        "message",
        "passing",
        "eigendecompositions",
        "overshadowed",
        "overlooked"
      ],
      "summary": "ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by Message Passing Neural Networks (MPNNs), which gained popularity for their simplicity and effectiveness in capturing local graph structure. Despite their success, MPNNs are limited in their ability to capture long-range dependencies between nodes. This has led researchers to adapt MPNNs through *rewiring* or make use of *Graph Transformers*, which compromise the computational efficiency that characterized early spatial message passing architectures, and typically disregard the graph structure. Almost a decade after its original introduction, we revisit ChebNet to shed light on its ability to model distant node interactions. We find that out-of-box, ChebNet already shows competitive advantages relative to classical MPNNs and GTs on long-range benchmarks, while maintaining good scalability properties for high-order polynomials. However, we uncover that this polynomial expansion leads ChebNet to an unstable regime during training. To address this limitation, we cast ChebNet as a stable and non-dissipative dynamical system, which we coin Stable-ChebNet. Our Stable-ChebNet model allows for stable information propagation, and has controllable dynamics which do not require the use of eigendecompositions, positional encodings, or graph rewiring. Across several benchmarks, Stable-ChebNet achieves near state-of-the-art performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oLyfML1Qze"
        ],
        "venue": [
          "/venue/oLyfML1Qze@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oLyfML1Qze"
        ],
        "detail": [
          "https://openreview.net/forum?id=oLyfML1Qze"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Ali Hariri , Alvaro Arroyo , Alessio Gravina , Moshe Eliasof , Carola-Bibiane Schönlieb , Davide Bacciu , Xiaowen Dong , Kamyar Azizzadenesheli , Pierre Vandergheynst ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by Message Passing Neural Networks (MPNNs), which gained popularity for their simplicity and effectiveness in capturing local graph structure. Despite their success, MPNNs are limited in their ability to capture long-range dependencies between nodes. This has led researchers to adapt MPNNs through *rewiring* or make use of *Graph Transformers*, which compromise the computational efficiency that characterized early spatial message passing architectures, and typically disregard the graph structure. Almost a decade after its original introduction, we revisit ChebNet to shed light on its ability to model distant node interactions. We find that out-of-box, ChebNet already shows competitive advantages relative to classical MPNNs and GTs on long-range benchmarks, while maintaining good scalability properties for high-order polynomials. However, we uncover that this polynomial expansion leads ChebNet to an unstable regime during training. To address this limitation, we cast ChebNet as a stable and non-dissipative dynamical system, which we coin Stable-ChebNet. Our Stable-ChebNet model allows for stable information propagation, and has controllable dynamics which do not require the use of eigendecompositions, positional encodings, or graph rewiring. Across several benchmarks, Stable-ChebNet achieves near state-of-the-art performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rVyBrD8h2b@OpenReview",
      "index": 96,
      "title": "Preconditioned Langevin Dynamics with Score-based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems",
      "authors": [
        "Lorenzo Baldassari",
        "Josselin Garnier",
        "Knut Solna",
        "Maarten V. de Hoop"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "langevin",
        "score",
        "sgms",
        "inverse",
        "preconditioner",
        "infinite",
        "sampler",
        "dimensional",
        "problems",
        "generative"
      ],
      "summary": "Designing algorithms for solving high-dimensional Bayesian inverse problems directly in infinite‑dimensional function spaces – where such problems are naturally formulated – is crucial to ensure stability and convergence as the discretization of the underlying problem is refined. In this paper, we contribute to this line of work by analyzing a widely used sampler for linear inverse problems: Langevin dynamics driven by score‑based generative models (SGMs) acting as priors, formulated directly in function space. Building on the theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition of this sampler in the infinite-dimensional setting and derive, for the first time, error estimates that explicitly depend on the approximation error of the score. As a consequence, we obtain sufficient conditions for global convergence in Kullback–Leibler divergence on the underlying function space. Preventing numerical instabilities requires preconditioning of the Langevin algorithm and we prove the existence and form of an optimal preconditioner. The preconditioner depends on both the score error and the forward operator and guarantees a uniform convergence rate across all posterior modes. Our analysis applies to both Gaussian and a general class of non‑Gaussian priors. Finally, we present examples that illustrate and validate our theoretical findings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rVyBrD8h2b"
        ],
        "venue": [
          "/venue/rVyBrD8h2b@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rVyBrD8h2b"
        ],
        "detail": [
          "https://openreview.net/forum?id=rVyBrD8h2b"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Preconditioned Langevin Dynamics with Score-based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Lorenzo Baldassari , Josselin Garnier , Knut Solna , Maarten V. de Hoop Designing algorithms for solving high-dimensional Bayesian inverse problems directly in infinite‑dimensional function spaces – where such problems are naturally formulated – is crucial to ensure stability and convergence as the discretization of the underlying problem is refined. In this paper, we contribute to this line of work by analyzing a widely used sampler for linear inverse problems: Langevin dynamics driven by score‑based generative models (SGMs) acting as priors, formulated directly in function space. Building on the theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition of this sampler in the infinite-dimensional setting and derive, for the first time, error estimates that explicitly depend on the approximation error of the score. As a consequence, we obtain sufficient conditions for global convergence in Kullback–Leibler divergence on the underlying function space. Preventing numerical instabilities requires preconditioning of the Langevin algorithm and we prove the existence and form of an optimal preconditioner. The preconditioner depends on both the score error and the forward operator and guarantees a uniform convergence rate across all posterior modes. Our analysis applies to both Gaussian and a general class of non‑Gaussian priors. Finally, we present examples that illustrate and validate our theoretical findings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Qvvy0X63Fv@OpenReview",
      "index": 97,
      "title": "Transferring Linear Features Across Language Models With Model Stitching",
      "authors": [
        "Alan Chen",
        "Jack Merullo",
        "Alessandro Stolfo",
        "Ellie Pavlick"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "saes",
        "transferring",
        "models",
        "transferred",
        "features",
        "stitching",
        "transfer",
        "training",
        "language",
        "sae"
      ],
      "summary": "In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the \\textit{weights} of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn highly similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. For example, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Qvvy0X63Fv"
        ],
        "venue": [
          "/venue/Qvvy0X63Fv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Qvvy0X63Fv"
        ],
        "detail": [
          "https://openreview.net/forum?id=Qvvy0X63Fv"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "Transferring Linear Features Across Language Models With Model Stitching [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Alan Chen , Jack Merullo , Alessandro Stolfo , Ellie Pavlick In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the \\textit{weights} of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn highly similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. For example, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Zf6Oj5x9sE@OpenReview",
      "index": 98,
      "title": "Causal Differentiating Concepts: Interpreting LM Behavior via Causal Representation Learning",
      "authors": [
        "Navita Goyal",
        "Hal Daumé III",
        "Alexandre Drouin",
        "Dhanya Sridhar"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "concepts",
        "causal",
        "differentiating",
        "behavior",
        "interpretable",
        "interpreting",
        "annotatable",
        "activations",
        "language",
        "factors"
      ],
      "summary": "Language model activations entangle concepts that mediate their behavior, making it difficult to interpret these factors, which has implications for generalizability and robustness. We introduce an approach for disentangling these concepts without supervision. Existing methods for concept discovery often rely on external labels, contrastive prompts, or known causal structures, which limits their scalability and biases them toward predefined, easily annotatable features. In contrast, we propose a new unsupervised algorithm that identifies causal differentiating concepts—interpretable latent directions in LM activations that must be changed to elicit a different model behavior. These concepts are discovered using a constrained contrastive learning objective, guided by the insight that eliciting a target behavior requires only sparse changes to the underlying concepts. We formalize this notion and show that, under a particular assumption about the sparsity of these causal differentiating concepts, our method learns disentangled representations that align with human-interpretable factors influencing LM decisions. We empirically show the ability of our method to recover ground-truth causal factors in synthetic and semi-synthetic settings. Additionally, we illustrate the utility of our method through a case study on refusal behavior in language models. Our approach offers a scalable and interpretable lens into the internal workings of LMs, providing a principled foundation for interpreting language model behavior.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Zf6Oj5x9sE"
        ],
        "venue": [
          "/venue/Zf6Oj5x9sE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Zf6Oj5x9sE"
        ],
        "detail": [
          "https://openreview.net/forum?id=Zf6Oj5x9sE"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Causal Differentiating Concepts: Interpreting LM Behavior via Causal Representation Learning [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Navita Goyal , Hal Daumé III , Alexandre Drouin , Dhanya Sridhar Language model activations entangle concepts that mediate their behavior, making it difficult to interpret these factors, which has implications for generalizability and robustness. We introduce an approach for disentangling these concepts without supervision. Existing methods for concept discovery often rely on external labels, contrastive prompts, or known causal structures, which limits their scalability and biases them toward predefined, easily annotatable features. In contrast, we propose a new unsupervised algorithm that identifies causal differentiating concepts—interpretable latent directions in LM activations that must be changed to elicit a different model behavior. These concepts are discovered using a constrained contrastive learning objective, guided by the insight that eliciting a target behavior requires only sparse changes to the underlying concepts. We formalize this notion and show that, under a particular assumption about the sparsity of these causal differentiating concepts, our method learns disentangled representations that align with human-interpretable factors influencing LM decisions. We empirically show the ability of our method to recover ground-truth causal factors in synthetic and semi-synthetic settings. Additionally, we illustrate the utility of our method through a case study on refusal behavior in language models. Our approach offers a scalable and interpretable lens into the internal workings of LMs, providing a principled foundation for interpreting language model behavior. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Xll01vw606@OpenReview",
      "index": 99,
      "title": "Ambient Proteins - Training Diffusion Models on Noisy Structures",
      "authors": [
        "Giannis Daras",
        "Jeffrey Ouyang-Zhang",
        "Krithika Ravishankar",
        "Constantinos Costis Daskalakis",
        "Adam Klivans",
        "Daniel Jesus Diaz"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "structures",
        "protein",
        "ambient",
        "diffusion",
        "proteins",
        "quality",
        "designability",
        "alphafold2",
        "diversity",
        "models"
      ],
      "summary": "We present Ambient Protein Diffusion, a framework for training protein diffusion models that generates structures with unprecedented diversity and quality. State-of-the-art generative models are trained on computationally derived structures from AlphaFold2 (AF), as experimentally determined structures are relatively scarce. The resulting models are therefore limited by the quality of synthetic datasets. Since the accuracy of AF predictions degrades with increasing protein length and complexity, de novo generation of long, complex proteins remains challenging. Ambient Protein Diffusion overcomes this problem by treating low-confidence AF structures as corrupted data. Rather than simply filtering out low-quality AF structures, our method adjusts the diffusion objective for each structure based on its corruption level, allowing the model to learn from both high and low quality structures. Empirically, ambient protein diffusion yields major improvements: on proteins with 700 residues, diversity increases from 45% to 85% from the previous state-of-the-art, and designability improves from 70% to 88%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Xll01vw606"
        ],
        "venue": [
          "/venue/Xll01vw606@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Xll01vw606"
        ],
        "detail": [
          "https://openreview.net/forum?id=Xll01vw606"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Ambient Proteins - Training Diffusion Models on Noisy Structures [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Giannis Daras , Jeffrey Ouyang-Zhang , Krithika Ravishankar , Constantinos Costis Daskalakis , Adam Klivans , Daniel Jesus Diaz We present Ambient Protein Diffusion, a framework for training protein diffusion models that generates structures with unprecedented diversity and quality. State-of-the-art generative models are trained on computationally derived structures from AlphaFold2 (AF), as experimentally determined structures are relatively scarce. The resulting models are therefore limited by the quality of synthetic datasets. Since the accuracy of AF predictions degrades with increasing protein length and complexity, de novo generation of long, complex proteins remains challenging. Ambient Protein Diffusion overcomes this problem by treating low-confidence AF structures as corrupted data. Rather than simply filtering out low-quality AF structures, our method adjusts the diffusion objective for each structure based on its corruption level, allowing the model to learn from both high and low quality structures. Empirically, ambient protein diffusion yields major improvements: on proteins with 700 residues, diversity increases from 45% to 85% from the previous state-of-the-art, and designability improves from 70% to 88%. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "V4YAS7NLXi@OpenReview",
      "index": 100,
      "title": "On Universality Classes of Equivariant Networks",
      "authors": [
        "Marco Pacini",
        "Gabriele Santin",
        "Bruno Lepri",
        "Shubhendu Trivedi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "equivariant",
        "mdash",
        "universality",
        "separation",
        "shallow",
        "networks",
        "symmetry",
        "ndash",
        "power",
        "classes"
      ],
      "summary": "Equivariant neural networks provide a principled framework for incorporating symmetry into learning architectures and have been extensively analyzed through the lens of their *separation power*, that is, the ability to distinguish inputs modulo symmetry. This notion plays a central role in settings such as graph learning, where it is often formalized via the Weisfeilern&ndash;Leman hierarchy. In contrast, the *universality* of equivariant models&mdash;their capacity to approximate target functions&mdash;remains comparatively underexplored. In this work, we investigate the approximation power of equivariant neural networks beyond separation constraints. We show that separation power does not fully capture expressivity: models with identical separation power may differ in their approximation ability. To demonstrate this, we characterize the universality classes of shallow invariant networks, providing a general framework for understanding which functions these architectures can approximate. Since equivariant models reduce to invariant ones under projection, this analysis yields sufficient conditions under which shallow equivariant networks fail to be universal. Conversely, we identify settings where shallow models do achieve separation-constrained universality. These positive results, however, depend critically on structural properties of the symmetry group, such as the existence of adequate normal subgroups, which may not hold in important cases like permutation symmetry.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=V4YAS7NLXi"
        ],
        "venue": [
          "/venue/V4YAS7NLXi@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=V4YAS7NLXi"
        ],
        "detail": [
          "https://openreview.net/forum?id=V4YAS7NLXi"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "On Universality Classes of Equivariant Networks [PDF ] [Copy] [Kimi ] [REL] Authors : Marco Pacini , Gabriele Santin , Bruno Lepri , Shubhendu Trivedi Equivariant neural networks provide a principled framework for incorporating symmetry into learning architectures and have been extensively analyzed through the lens of their *separation power*, that is, the ability to distinguish inputs modulo symmetry. This notion plays a central role in settings such as graph learning, where it is often formalized via the Weisfeilern&ndash;Leman hierarchy. In contrast, the *universality* of equivariant models&mdash;their capacity to approximate target functions&mdash;remains comparatively underexplored. In this work, we investigate the approximation power of equivariant neural networks beyond separation constraints. We show that separation power does not fully capture expressivity: models with identical separation power may differ in their approximation ability. To demonstrate this, we characterize the universality classes of shallow invariant networks, providing a general framework for understanding which functions these architectures can approximate. Since equivariant models reduce to invariant ones under projection, this analysis yields sufficient conditions under which shallow equivariant networks fail to be universal. Conversely, we identify settings where shallow models do achieve separation-constrained universality. These positive results, however, depend critically on structural properties of the symmetry group, such as the existence of adequate normal subgroups, which may not hold in important cases like permutation symmetry. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "9FDErIfoVE@OpenReview",
      "index": 101,
      "title": "Guarantees for Alternating Least Squares in Overparameterized Tensor Decompositions",
      "authors": [
        "Dionysis Arvanitakis",
        "Vaidehi Srinivas",
        "Aravindan Vijayaraghavan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "overparameterized",
        "overparameterization",
        "tensor",
        "als",
        "rank",
        "decomposition",
        "guarantees",
        "times",
        "alternating",
        "squares"
      ],
      "summary": "Tensor decomposition is a canonical non-convex optimization problem that is computationally challenging, and yet important due to applications in factor analysis and parameter estimation of latent variable models. In practice, scalable iterative methods, particularly Alternating Least Squares (ALS), remain the workhorse for tensor decomposition despite the lack of global convergence guarantees. A popular approach to tackle challenging non-convex optimization problems is overparameterization--- on input an n × n × n n × n × n tensor of rank r r , the algorithm can output a decomposition of potentially rank k k (potentially larger than r r ). On the theoretical side, overparameterization for iterative methods is challenging to reason about and requires new techniques. The work of Wang et al., (NeurIPS 2020) makes progress by showing that a variant of gradient descent globally converges when overparameterized to k = O ( r 7.5 log n ) k = O ( r 7.5 log ⁡ n ) . Our main result shows that overparameterization provably enables global convergence of ALS: on input a third order n × n × n n × n × n tensor with a decomposition of rank r ≪ n r ≪ n , ALS overparameterized with rank k = O ( r 2 ) k = O ( r 2 ) achieves global convergence with high probability under random initialization. Moreover our analysis also gives guarantees for the more general low-rank approximation problem. The analysis introduces new techniques for understanding iterative methods in the overparameterized regime based on new matrix anticoncentration arguments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9FDErIfoVE"
        ],
        "venue": [
          "/venue/9FDErIfoVE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9FDErIfoVE"
        ],
        "detail": [
          "https://openreview.net/forum?id=9FDErIfoVE"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Guarantees for Alternating Least Squares in Overparameterized Tensor Decompositions [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Dionysis Arvanitakis , Vaidehi Srinivas , Aravindan Vijayaraghavan Tensor decomposition is a canonical non-convex optimization problem that is computationally challenging, and yet important due to applications in factor analysis and parameter estimation of latent variable models. In practice, scalable iterative methods, particularly Alternating Least Squares (ALS), remain the workhorse for tensor decomposition despite the lack of global convergence guarantees. A popular approach to tackle challenging non-convex optimization problems is overparameterization--- on input an n × n × n n × n × n tensor of rank r r , the algorithm can output a decomposition of potentially rank k k (potentially larger than r r ). On the theoretical side, overparameterization for iterative methods is challenging to reason about and requires new techniques. The work of Wang et al., (NeurIPS 2020) makes progress by showing that a variant of gradient descent globally converges when overparameterized to k = O ( r 7.5 log n ) k = O ( r 7.5 log ⁡ n ) . Our main result shows that overparameterization provably enables global convergence of ALS: on input a third order n × n × n n × n × n tensor with a decomposition of rank r ≪ n r ≪ n , ALS overparameterized with rank k = O ( r 2 ) k = O ( r 2 ) achieves global convergence with high probability under random initialization. Moreover our analysis also gives guarantees for the more general low-rank approximation problem. The analysis introduces new techniques for understanding iterative methods in the overparameterized regime based on new matrix anticoncentration arguments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RPRqKhjrr6@OpenReview",
      "index": 102,
      "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
      "authors": [
        "Vijay Viswanathan",
        "Yanchao Sun",
        "Xiang Kong",
        "Meng Cao",
        "Graham Neubig",
        "Tongshuang Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rlcf",
        "checklists",
        "instruct",
        "rubrics",
        "instruction",
        "reinforcement",
        "language",
        "instructions",
        "followbench",
        "infobench"
      ],
      "summary": "Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this —typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item—using both AI judges and specialized verifier programs—then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods on top of a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks — RLCF is the only method to help on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. We show that RLCF can also be used off-policy to improve Llama 3.1 8B Instruct and OLMo 2 7B Instruct. These results establish rubrics as a key tool for improving language models' support of queries that express a multitude of needs. We release our our dataset of rubrics (WildChecklists), models, and code to the public.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RPRqKhjrr6"
        ],
        "venue": [
          "/venue/RPRqKhjrr6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RPRqKhjrr6"
        ],
        "detail": [
          "https://openreview.net/forum?id=RPRqKhjrr6"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Checklists Are Better Than Reward Models For Aligning Language Models [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Vijay Viswanathan , Yanchao Sun , Xiang Kong , Meng Cao , Graham Neubig , Tongshuang Wu Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this —typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item—using both AI judges and specialized verifier programs—then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods on top of a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks — RLCF is the only method to help on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. We show that RLCF can also be used off-policy to improve Llama 3.1 8B Instruct and OLMo 2 7B Instruct. These results establish rubrics as a key tool for improving language models' support of queries that express a multitude of needs. We release our our dataset of rubrics (WildChecklists), models, and code to the public. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VRhVS59yhP@OpenReview",
      "index": 103,
      "title": "Blackbox Model Provenance via Palimpsestic Membership Inference",
      "authors": [
        "Rohith Kuditipudi",
        "Jing Huang",
        "Sally Zhu",
        "Diyi Yang",
        "Christopher Potts",
        "Percy Liang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bob",
        "alice",
        "training",
        "text",
        "palimpsestic",
        "model",
        "blackbox",
        "setting",
        "run",
        "likelihood"
      ],
      "summary": "Suppose Alice trains an open-weight language model and Bob uses a blackbox derivative of Alice’s model to produce text. Can Alice prove that Bob is using her model, either by querying Bob’s derivative model (query setting) or from the text alone ( observational setting)? We formulate this question as an independence testing problem—in which the null hypothesis is that Bob’s model or text is independent of Alice’s randomized training run—and investigate it through the lens of palimpsestic memorization in language models: models are more likely to memorize data seen later in training, so we can test whether Bob is using Alice’s model using test statistics that capture correlation between Bob’s model or text and the ordering of training examples in Alice’s training run. If Alice has randomly shuffled her training data, then any significant correlation amounts to exactly quantifiable statistical evidence against the null hypothesis, regardless of the composition of Alice’s training data. In the query setting, we directly estimate (via prompting) the likelihood Bob’s model gives to Alice’s training examples and their training order; we correlate the likelihoods of over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to 12B parameters with the base model’s training data order, achieving a p-value on the order of at most 1 × 10 − 8 1 × 10 − 8 in all but six cases. In the observational setting, we try two approaches based on estimating 1) the likelihood of Bob’s text overlapping with spans of Alice’s training examples and 2) the likelihood of Bob’s text with respect to different versions of Alice’s model we obtain by repeating the last phase (e.g., 1%) of her training run on reshuffled data. The second approach can reliably distinguish Bob’s text from as little as a few hundred tokens; the first does not involve any retraining but requires many more tokens (several hundred thousand) to achieve high power.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VRhVS59yhP"
        ],
        "venue": [
          "/venue/VRhVS59yhP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VRhVS59yhP"
        ],
        "detail": [
          "https://openreview.net/forum?id=VRhVS59yhP"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 4
      },
      "raw_excerpt": "Blackbox Model Provenance via Palimpsestic Membership Inference [PDF 1 ] [Copy] [Kimi 4 ] [REL] Authors : Rohith Kuditipudi , Jing Huang , Sally Zhu , Diyi Yang , Christopher Potts , Percy Liang Suppose Alice trains an open-weight language model and Bob uses a blackbox derivative of Alice’s model to produce text. Can Alice prove that Bob is using her model, either by querying Bob’s derivative model (query setting) or from the text alone ( observational setting)? We formulate this question as an independence testing problem—in which the null hypothesis is that Bob’s model or text is independent of Alice’s randomized training run—and investigate it through the lens of palimpsestic memorization in language models: models are more likely to memorize data seen later in training, so we can test whether Bob is using Alice’s model using test statistics that capture correlation between Bob’s model or text and the ordering of training examples in Alice’s training run. If Alice has randomly shuffled her training data, then any significant correlation amounts to exactly quantifiable statistical evidence against the null hypothesis, regardless of the composition of Alice’s training data. In the query setting, we directly estimate (via prompting) the likelihood Bob’s model gives to Alice’s training examples and their training order; we correlate the likelihoods of over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to 12B parameters with the base model’s training data order, achieving a p-value on the order of at most 1 × 10 − 8 1 × 10 − 8 in all but six cases. In the observational setting, we try two approaches based on estimating 1) the likelihood of Bob’s text overlapping with spans of Alice’s training examples and 2) the likelihood of Bob’s text with respect to different versions of Alice’s model we obtain by repeating the last phase (e.g., 1%) of her training run on reshuffled data. The second approach can reliably distinguish Bob’s text from as little as a few hundred tokens; the first does not involve any retraining but requires many more tokens (several hundred thousand) to achieve high power. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hZt0daVIZi@OpenReview",
      "index": 104,
      "title": "Scaling can lead to compositional generalization",
      "authors": [
        "Florian Redhardt",
        "Yassir Akram",
        "Simon Schug"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "compositional",
        "task",
        "scaling",
        "generalization",
        "generalize",
        "neural",
        "networks",
        "doubts",
        "compositionally",
        "yes"
      ],
      "summary": "Can neural networks systematically capture discrete, compositional task structure despite their continuous, distributed nature? The impressive capabilities of large-scale neural networks suggest that the answer to this question is yes. However, even for the most capable models, there are still frequent failure cases that raise doubts about their compositionality. Here, we seek to understand what it takes for a standard neural network to generalize over tasks that share compositional structure. We find that simply scaling data and model size leads to compositional generalization. We show that this holds across different task encodings as long as the training distribution sufficiently covers the task space. In line with this finding, we prove that standard multilayer perceptrons can approximate a general class of compositional task families to arbitrary precision using only a linear number of neurons with respect to the number of task modules. Finally, we uncover that if networks successfully compositionally generalize, the constituents of a task can be linearly decoded from their hidden activations. We show that this metric correlates with failures of text-to-image generation models to compose known concepts.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hZt0daVIZi"
        ],
        "venue": [
          "/venue/hZt0daVIZi@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hZt0daVIZi"
        ],
        "detail": [
          "https://openreview.net/forum?id=hZt0daVIZi"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Scaling can lead to compositional generalization [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Florian Redhardt , Yassir Akram , Simon Schug Can neural networks systematically capture discrete, compositional task structure despite their continuous, distributed nature? The impressive capabilities of large-scale neural networks suggest that the answer to this question is yes. However, even for the most capable models, there are still frequent failure cases that raise doubts about their compositionality. Here, we seek to understand what it takes for a standard neural network to generalize over tasks that share compositional structure. We find that simply scaling data and model size leads to compositional generalization. We show that this holds across different task encodings as long as the training distribution sufficiently covers the task space. In line with this finding, we prove that standard multilayer perceptrons can approximate a general class of compositional task families to arbitrary precision using only a linear number of neurons with respect to the number of task modules. Finally, we uncover that if networks successfully compositionally generalize, the constituents of a task can be linearly decoded from their hidden activations. We show that this metric correlates with failures of text-to-image generation models to compose known concepts. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "R0dC7Xzwbk@OpenReview",
      "index": 105,
      "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning",
      "authors": [
        "Jaehun Jung",
        "Seungju Han",
        "Ximing Lu",
        "Skyler Hallinan",
        "David Acuna",
        "Shrimai Prabhumoye",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Yejin Choi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "prismatic",
        "vendi",
        "diversity",
        "llm",
        "heuristic",
        "synthesis",
        "reasoning",
        "data",
        "benchmarks",
        "generalization"
      ],
      "summary": "Data diversity is crucial for training a strong language model. Yet metrics of diversity often diverge from this goal, measuring variations in heuristic features—like n-grams or embeddings—that are detached from how the model actually performs on a target task. This motivates us to ask: *Can we redefine data diversity—beyond measuring variations in heuristic features—in a way that better predicts model generalization?* Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning—as measured by average model performance on unseen out-of-distribution benchmarks. We introduce **G-Vendi**, a metric that quantifies diversity via the entropy of model-induced loss gradients. G-Vendi scales to million-sample datasets and yet consistently outperforms heuristic alternatives, achieving strong correlation ( Spearman's ρ ≈ 0.9 Spearman's ρ ≈ 0.9 ) with out-of-distribution (OOD) performance across both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present **Prismatic Synthesis**, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data—not just on in-distribution test but across unseen, out-of-distribution benchmarks—significantly outperforming state-of-the-art models in both domains. For example, PrismMath-7B, our model distilled from a 32B LLM without human verification, outperforms R1-Distill-Qwen-7B—trained on proprietary data generated by 671B R1—on 6 out of 7 challenging math benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=R0dC7Xzwbk"
        ],
        "venue": [
          "/venue/R0dC7Xzwbk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=R0dC7Xzwbk"
        ],
        "detail": [
          "https://openreview.net/forum?id=R0dC7Xzwbk"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Jaehun Jung , Seungju Han , Ximing Lu , Skyler Hallinan , David Acuna , Shrimai Prabhumoye , Mostofa Patwary , Mohammad Shoeybi , Bryan Catanzaro , Yejin Choi Data diversity is crucial for training a strong language model. Yet metrics of diversity often diverge from this goal, measuring variations in heuristic features—like n-grams or embeddings—that are detached from how the model actually performs on a target task. This motivates us to ask: *Can we redefine data diversity—beyond measuring variations in heuristic features—in a way that better predicts model generalization?* Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning—as measured by average model performance on unseen out-of-distribution benchmarks. We introduce **G-Vendi**, a metric that quantifies diversity via the entropy of model-induced loss gradients. G-Vendi scales to million-sample datasets and yet consistently outperforms heuristic alternatives, achieving strong correlation ( Spearman's ρ ≈ 0.9 Spearman's ρ ≈ 0.9 ) with out-of-distribution (OOD) performance across both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present **Prismatic Synthesis**, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data—not just on in-distribution test but across unseen, out-of-distribution benchmarks—significantly outperforming state-of-the-art models in both domains. For example, PrismMath-7B, our model distilled from a 32B LLM without human verification, outperforms R1-Distill-Qwen-7B—trained on proprietary data generated by 671B R1—on 6 out of 7 challenging math benchmarks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qQbvLU34F1@OpenReview",
      "index": 106,
      "title": "AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection",
      "authors": [
        "Saleh Momeni",
        "Changnan Xiao",
        "Bing Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cil",
        "anacp",
        "analytic",
        "ptms",
        "continual",
        "feature",
        "contrastive",
        "projection",
        "incremental",
        "classifiers"
      ],
      "summary": "This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qQbvLU34F1"
        ],
        "venue": [
          "/venue/qQbvLU34F1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qQbvLU34F1"
        ],
        "detail": [
          "https://openreview.net/forum?id=qQbvLU34F1"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Saleh Momeni , Changnan Xiao , Bing Liu This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "JzCjNJlSxI@OpenReview",
      "index": 107,
      "title": "Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning",
      "authors": [
        "Riccardo De Santi",
        "Marin Vlastelica",
        "Ya-Ping Hsieh",
        "Zebang Shen",
        "Niao He",
        "Andreas Krause"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tuning",
        "beyond",
        "fine",
        "objectives",
        "generative",
        "flow",
        "reward",
        "divergence",
        "fdc",
        "tasks"
      ],
      "summary": "Adapting large-scale foundational flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Rényi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JzCjNJlSxI"
        ],
        "venue": [
          "/venue/JzCjNJlSxI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JzCjNJlSxI"
        ],
        "detail": [
          "https://openreview.net/forum?id=JzCjNJlSxI"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Riccardo De Santi , Marin Vlastelica , Ya-Ping Hsieh , Zebang Shen , Niao He , Andreas Krause Adapting large-scale foundational flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Rényi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ATYKgiDqt5@OpenReview",
      "index": 108,
      "title": "What Expressivity Theory Misses: Message Passing Complexity for GNNs",
      "authors": [
        "Niklas Kemper",
        "Tom Wollschläger",
        "Stephan Günnemann"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "expressivity",
        "gnn",
        "gnns",
        "mpc",
        "message",
        "passing",
        "theory",
        "misses",
        "tasks",
        "complexity"
      ],
      "summary": "Expressivity theory, characterizing which graphs a GNN can distinguish, has become the predominant framework for analyzing GNNs, with new models striving for higher expressivity. However, we argue that this focus is misguided: First, higher expressivity is not necessary for most real-world tasks as these tasks rarely require expressivity beyond the basic WL test. Second, expressivity theory's binary characterization and idealized assumptions fail to reflect GNNs' practical capabilities. To overcome these limitations, we propose Message Passing Complexity (MPC): a continuous measure that quantifies the difficulty for a GNN architecture to solve a given task through message passing. MPC captures practical limitations like over-squashing while preserving the theoretical impossibility results from expressivity theory, effectively narrowing the gap between theory and practice. Through extensive validation on fundamental GNN tasks, we show that MPC's theoretical predictions correlate with empirical performance, successfully explaining architectural successes and failures. Thereby, MPC advances beyond expressivity theory to provide a more powerful framework for understanding and developing GNN architectures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ATYKgiDqt5"
        ],
        "venue": [
          "/venue/ATYKgiDqt5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ATYKgiDqt5"
        ],
        "detail": [
          "https://openreview.net/forum?id=ATYKgiDqt5"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "What Expressivity Theory Misses: Message Passing Complexity for GNNs [PDF ] [Copy] [Kimi ] [REL] Authors : Niklas Kemper , Tom Wollschläger , Stephan Günnemann Expressivity theory, characterizing which graphs a GNN can distinguish, has become the predominant framework for analyzing GNNs, with new models striving for higher expressivity. However, we argue that this focus is misguided: First, higher expressivity is not necessary for most real-world tasks as these tasks rarely require expressivity beyond the basic WL test. Second, expressivity theory's binary characterization and idealized assumptions fail to reflect GNNs' practical capabilities. To overcome these limitations, we propose Message Passing Complexity (MPC): a continuous measure that quantifies the difficulty for a GNN architecture to solve a given task through message passing. MPC captures practical limitations like over-squashing while preserving the theoretical impossibility results from expressivity theory, effectively narrowing the gap between theory and practice. Through extensive validation on fundamental GNN tasks, we show that MPC's theoretical predictions correlate with empirical performance, successfully explaining architectural successes and failures. Thereby, MPC advances beyond expressivity theory to provide a more powerful framework for understanding and developing GNN architectures. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MVYz4GmcUH@OpenReview",
      "index": 109,
      "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
      "authors": [
        "Giannis Daras",
        "Adrian Rodriguez-Munoz",
        "Adam Klivans",
        "Antonio Torralba",
        "Constantinos Costis Daskalakis"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "diffusion",
        "omni",
        "quality",
        "images",
        "blur",
        "ambient",
        "framework",
        "dampens",
        "training",
        "bad"
      ],
      "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from arbitrarily images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We use our framework to achieve state-of-the-art ImageNet FID and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MVYz4GmcUH"
        ],
        "venue": [
          "/venue/MVYz4GmcUH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MVYz4GmcUH"
        ],
        "detail": [
          "https://openreview.net/forum?id=MVYz4GmcUH"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": null
      },
      "raw_excerpt": "Ambient Diffusion Omni: Training Good Models with Bad Data [PDF 6 ] [Copy] [Kimi ] [REL] Authors : Giannis Daras , Adrian Rodriguez-Munoz , Adam Klivans , Antonio Torralba , Constantinos Costis Daskalakis We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from arbitrarily images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We use our framework to achieve state-of-the-art ImageNet FID and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SvopaNxYWt@OpenReview",
      "index": 110,
      "title": "UMA: A Family of Universal Models for Atoms",
      "authors": [
        "Brandon M Wood",
        "Misko Dzamba",
        "Xiang Fu",
        "Meng Gao",
        "Muhammed Shuaibi",
        "Luis Barroso-Luque",
        "Kareem Abdelmaqsoud",
        "Vahe Gharakhanyan",
        "John R. Kitchin",
        "Daniel S. Levine",
        "Kyle Michel",
        "Anuroop Sriram",
        "Taco Cohen",
        "Abhishek Das",
        "Sushree Jagriti Sahoo",
        "Ammar Rizvi",
        "Zachary Ward Ulissi",
        "C. Lawrence Zitnick"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "uma",
        "models",
        "atomic",
        "universal",
        "atoms",
        "family",
        "capacity",
        "medium",
        "50m",
        "compiling"
      ],
      "summary": "The ability to quickly and accurately compute properties from atomic simulations is critical for advancing a large number of applications in chemistry and materials science including drug discovery, energy storage, and semiconductor manufacturing. To address this need, we present a family of Universal Models for Atoms (UMA), designed to push the frontier of speed, accuracy, and generalization. UMA models are trained on half a billion unique 3D atomic structures (the largest training runs to date) by compiling data across multiple chemical domains, e.g. molecules, materials, and catalysts. We develop empirical scaling laws to help understand how to increase model capacity alongside dataset size to achieve the best accuracy. The UMA small and medium models utilize a novel architectural design we refer to as mixture of linear experts that enables increasing model capacity without sacrificing speed. For example, UMA-medium has 1.4B parameters but only ∼ ∼ 50M active parameters per atomic structure. We evaluate UMA models on a diverse set of applications across multiple domains and find that, remarkably, a single model without any fine-tuning can perform similarly or better than specialized models. We are releasing the UMA code, weights, and associated data to accelerate computational workflows and enable the community to build increasingly capable AI models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SvopaNxYWt"
        ],
        "venue": [
          "/venue/SvopaNxYWt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SvopaNxYWt"
        ],
        "detail": [
          "https://openreview.net/forum?id=SvopaNxYWt"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "UMA: A Family of Universal Models for Atoms [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Brandon M Wood , Misko Dzamba , Xiang Fu , Meng Gao , Muhammed Shuaibi , Luis Barroso-Luque , Kareem Abdelmaqsoud , Vahe Gharakhanyan , John R. Kitchin , Daniel S. Levine , Kyle Michel , Anuroop Sriram , Taco Cohen , Abhishek Das , Sushree Jagriti Sahoo , Ammar Rizvi , Zachary Ward Ulissi , C. Lawrence Zitnick The ability to quickly and accurately compute properties from atomic simulations is critical for advancing a large number of applications in chemistry and materials science including drug discovery, energy storage, and semiconductor manufacturing. To address this need, we present a family of Universal Models for Atoms (UMA), designed to push the frontier of speed, accuracy, and generalization. UMA models are trained on half a billion unique 3D atomic structures (the largest training runs to date) by compiling data across multiple chemical domains, e.g. molecules, materials, and catalysts. We develop empirical scaling laws to help understand how to increase model capacity alongside dataset size to achieve the best accuracy. The UMA small and medium models utilize a novel architectural design we refer to as mixture of linear experts that enables increasing model capacity without sacrificing speed. For example, UMA-medium has 1.4B parameters but only ∼ ∼ 50M active parameters per atomic structure. We evaluate UMA models on a diverse set of applications across multiple domains and find that, remarkably, a single model without any fine-tuning can perform similarly or better than specialized models. We are releasing the UMA code, weights, and associated data to accelerate computational workflows and enable the community to build increasingly capable AI models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4jFSekBaDT@OpenReview",
      "index": 111,
      "title": "The Best Instruction-Tuning Data are Those That Fit",
      "authors": [
        "Dylan Zhang",
        "Qirun Dai",
        "Hao Peng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "grape",
        "sft",
        "tulu3",
        "data",
        "pretrained",
        "responses",
        "llama",
        "instruction",
        "ligns",
        "ultrainteract"
      ],
      "summary": "High-quality supervised finetuning (SFT) data are essential for unlocking pretrained LLMs’ capabilities. Typically, instructions are paired with responses from various sources—by human annotators or other LMs—which are often out of the distribution of the target model to be finetuned. At scale, this mismatch can lead to diminishing returns and even hurt model performance and robustness. We hypothesize that SFT is most effective when the data is aligned with the model’s pretrained distribution, and propose **GRAPE**—a novel SFT framework that tailors supervision to the target model. For each instruction, it **g**athers **r**esponses from various sources and selects the one that **a**ligns most closely to the model’s **pre**trained distribution, as measured by the normalized probability. Standard SFT is then performed on these selected responses. We first evaluate GRAPE in a controlled experiment, sampling multiple responses per question in the UltraInteract dataset from diverse models. We finetune using GRAPE-selected data on LMs from different families, including LLaMA-1-8B, Mistral-7B, and Qwen2.5-7B. GRAPE significantly outperforms strong baselines—including distilling from the strongest model—with absolute gains up to **13.8%** averaged across benchmarks, and outperforms a 3× larger data baseline with improvements up to **17.3%**. GRAPE's benefits generalize to off-the-shelf SFT data. When used to subsample from the post-training data of Tulu3 and Olmo-2, GRAPE surpasses strong baselines trained on 4.5× more data by **6.1%**, and outperforms state-of-the-art selection methods by **3.9%** on average. Notably, with only **1/3 the data** and **half the training epochs**, GRAPE enables LLaMA-1-8B to **exceed Tulu3-SFT performance by 3.5%**. Our findings highlight that aligning supervision with the pretrained distribution provides a simple yet powerful strategy to improve both the **efficiency** and **effectiveness** of SFT.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4jFSekBaDT"
        ],
        "venue": [
          "/venue/4jFSekBaDT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4jFSekBaDT"
        ],
        "detail": [
          "https://openreview.net/forum?id=4jFSekBaDT"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "The Best Instruction-Tuning Data are Those That Fit [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Dylan Zhang , Qirun Dai , Hao Peng High-quality supervised finetuning (SFT) data are essential for unlocking pretrained LLMs’ capabilities. Typically, instructions are paired with responses from various sources—by human annotators or other LMs—which are often out of the distribution of the target model to be finetuned. At scale, this mismatch can lead to diminishing returns and even hurt model performance and robustness. We hypothesize that SFT is most effective when the data is aligned with the model’s pretrained distribution, and propose **GRAPE**—a novel SFT framework that tailors supervision to the target model. For each instruction, it **g**athers **r**esponses from various sources and selects the one that **a**ligns most closely to the model’s **pre**trained distribution, as measured by the normalized probability. Standard SFT is then performed on these selected responses. We first evaluate GRAPE in a controlled experiment, sampling multiple responses per question in the UltraInteract dataset from diverse models. We finetune using GRAPE-selected data on LMs from different families, including LLaMA-1-8B, Mistral-7B, and Qwen2.5-7B. GRAPE significantly outperforms strong baselines—including distilling from the strongest model—with absolute gains up to **13.8%** averaged across benchmarks, and outperforms a 3× larger data baseline with improvements up to **17.3%**. GRAPE's benefits generalize to off-the-shelf SFT data. When used to subsample from the post-training data of Tulu3 and Olmo-2, GRAPE surpasses strong baselines trained on 4.5× more data by **6.1%**, and outperforms state-of-the-art selection methods by **3.9%** on average. Notably, with only **1/3 the data** and **half the training epochs**, GRAPE enables LLaMA-1-8B to **exceed Tulu3-SFT performance by 3.5%**. Our findings highlight that aligning supervision with the pretrained distribution provides a simple yet powerful strategy to improve both the **efficiency** and **effectiveness** of SFT. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "24UJqxw1kv@OpenReview",
      "index": 112,
      "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
      "authors": [
        "Suzan Ece Ada",
        "Georg Martius",
        "Emre Ugur",
        "Erhan Oztop"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "offline",
        "forl",
        "offsets",
        "forecasting",
        "stationarity",
        "stationary",
        "environments",
        "non",
        "misperceive",
        "reinforcement"
      ],
      "summary": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time—assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific form of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent’s experience we aim to bridge the gap between offline RL and the complexity of real-world, non-stationary environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=24UJqxw1kv"
        ],
        "venue": [
          "/venue/24UJqxw1kv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=24UJqxw1kv"
        ],
        "detail": [
          "https://openreview.net/forum?id=24UJqxw1kv"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Suzan Ece Ada , Georg Martius , Emre Ugur , Erhan Oztop Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time—assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific form of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent’s experience we aim to bridge the gap between offline RL and the complexity of real-world, non-stationary environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hHn8xGTRKO@OpenReview",
      "index": 113,
      "title": "Purifying Approximate Differential Privacy with Randomized Post-processing",
      "authors": [
        "Yingyu Lin",
        "Erchi Wang",
        "Yian Ma",
        "Yu-Xiang Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "approximate",
        "delta",
        "pure",
        "release",
        "privacy",
        "purifying",
        "randomized",
        "post",
        "varepsilon",
        "differential"
      ],
      "summary": "We propose a framework to convert ( ε , δ ) ( ε , δ ) -approximate Differential Privacy (DP) mechanisms into ( ε ′ , 0 ) ( ε ′ , 0 ) -pure DP mechanisms under certain conditions, a process we call ``purification.'' This algorithmic technique leverages randomized post-processing with calibrated noise to eliminate the δ δ parameter while achieving near-optimal privacy-utility tradeoff for pure DP. It enables a new design strategy for pure DP algorithms: first run an approximate DP algorithm with certain conditions, and then purify. This approach allows one to leverage techniques such as strong composition and propose-test-release that require δ > 0 δ > 0 in designing pure-DP methods with δ = 0 δ = 0 . We apply this framework in various settings, including Differentially Private Empirical Risk Minimization (DP-ERM), stability-based release, and query release tasks. To the best of our knowledge, this is the first work with a statistically and computationally efficient reduction from approximate DP to pure DP. Finally, we illustrate the use of this reduction for proving lower bounds under approximate DP constraints with explicit dependence in δ δ , avoiding the sophisticated fingerprinting code construction.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hHn8xGTRKO"
        ],
        "venue": [
          "/venue/hHn8xGTRKO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hHn8xGTRKO"
        ],
        "detail": [
          "https://openreview.net/forum?id=hHn8xGTRKO"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Purifying Approximate Differential Privacy with Randomized Post-processing [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yingyu Lin , Erchi Wang , Yian Ma , Yu-Xiang Wang We propose a framework to convert ( ε , δ ) ( ε , δ ) -approximate Differential Privacy (DP) mechanisms into ( ε ′ , 0 ) ( ε ′ , 0 ) -pure DP mechanisms under certain conditions, a process we call ``purification.'' This algorithmic technique leverages randomized post-processing with calibrated noise to eliminate the δ δ parameter while achieving near-optimal privacy-utility tradeoff for pure DP. It enables a new design strategy for pure DP algorithms: first run an approximate DP algorithm with certain conditions, and then purify. This approach allows one to leverage techniques such as strong composition and propose-test-release that require δ > 0 δ > 0 in designing pure-DP methods with δ = 0 δ = 0 . We apply this framework in various settings, including Differentially Private Empirical Risk Minimization (DP-ERM), stability-based release, and query release tasks. To the best of our knowledge, this is the first work with a statistically and computationally efficient reduction from approximate DP to pure DP. Finally, we illustrate the use of this reduction for proving lower bounds under approximate DP constraints with explicit dependence in δ δ , avoiding the sophisticated fingerprinting code construction. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6RlbOEcOS4@OpenReview",
      "index": 114,
      "title": "Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference",
      "authors": [
        "Denis Blessing",
        "Julius Berner",
        "Lorenz Richter",
        "Carles Domingo-Enrich",
        "Yuanqi Du",
        "Arash Vahdat",
        "Gerhard Neumann"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "trust",
        "measure",
        "control",
        "path",
        "target",
        "annealing",
        "constrained",
        "optimal",
        "region",
        "stochastic"
      ],
      "summary": "Solving stochastic optimal control problems with quadratic control costs can be viewed as approximating a target path space measure, e.g. via gradient-based optimization. In practice, however, this optimization is challenging in particular if the target measure differs substantially from the prior. In this work, we therefore approach the problem by iteratively solving constrained problems incorporating trust regions that aim for approaching the target measure gradually in a systematic way. It turns out that this trust region based strategy can be understood as a geometric annealing from the prior to the target measure, where, however, the incorporated trust regions lead to a principled and educated way of choosing the time steps in the annealing path. We demonstrate in multiple optimal control applications that our novel method can improve performance significantly, including tasks in diffusion-based sampling and fine-tuning of diffusion models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6RlbOEcOS4"
        ],
        "venue": [
          "/venue/6RlbOEcOS4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6RlbOEcOS4"
        ],
        "detail": [
          "https://openreview.net/forum?id=6RlbOEcOS4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Denis Blessing , Julius Berner , Lorenz Richter , Carles Domingo-Enrich , Yuanqi Du , Arash Vahdat , Gerhard Neumann Solving stochastic optimal control problems with quadratic control costs can be viewed as approximating a target path space measure, e.g. via gradient-based optimization. In practice, however, this optimization is challenging in particular if the target measure differs substantially from the prior. In this work, we therefore approach the problem by iteratively solving constrained problems incorporating trust regions that aim for approaching the target measure gradually in a systematic way. It turns out that this trust region based strategy can be understood as a geometric annealing from the prior to the target measure, where, however, the incorporated trust regions lead to a principled and educated way of choosing the time steps in the annealing path. We demonstrate in multiple optimal control applications that our novel method can improve performance significantly, including tasks in diffusion-based sampling and fine-tuning of diffusion models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "004uTlSufe@OpenReview",
      "index": 115,
      "title": "How Well Can Differential Privacy Be Audited in One Run?",
      "authors": [
        "Amit Keinan",
        "Moshe Shenfeld",
        "Katrina Ligett"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "auditing",
        "audited",
        "run",
        "privacy",
        "efficacy",
        "barrier",
        "one",
        "steinke",
        "true",
        "algorithm"
      ],
      "summary": "Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=004uTlSufe"
        ],
        "venue": [
          "/venue/004uTlSufe@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=004uTlSufe"
        ],
        "detail": [
          "https://openreview.net/forum?id=004uTlSufe"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "How Well Can Differential Privacy Be Audited in One Run? [PDF ] [Copy] [Kimi ] [REL] Authors : Amit Keinan , Moshe Shenfeld , Katrina Ligett Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qaHrpITIvB@OpenReview",
      "index": 116,
      "title": "Counteractive RL: Rethinking Core Principles for Efficient and Scalable Deep Reinforcement Learning",
      "authors": [
        "Ezgi Korkmaz"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "counteractive",
        "mdps",
        "learning",
        "scalable",
        "reinforcement",
        "rethinking",
        "environment",
        "dimensional",
        "success",
        "arcade"
      ],
      "summary": "Following the pivotal success of learning strategies to win at tasks, solely by interacting with an environment without any supervision, agents have gained the ability to make sequential decisions in complex MDPs. Yet, reinforcement learning policies face exponentially growing state spaces in high dimensional MDPs resulting in a dichotomy between computational complexity and policy success. In our paper we focus on the agent’s interaction with the environment in a high-dimensional MDP during the learning phase and we introduce a theoretically-founded novel paradigm based on experiences obtained through counteractive actions. Our analysis and method provide a theoretical basis for efficient, effective, scalable and accelerated learning, and further comes with zero additional computational complexity while leading to significant acceleration in training. We conduct extensive experiments in the Arcade Learning Environment with high-dimensional state representation MDPs. The experimental results further verify our theoretical analysis, and our method achieves significant performance increase with substantial sample-efficiency in high-dimensional environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qaHrpITIvB"
        ],
        "venue": [
          "/venue/qaHrpITIvB@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qaHrpITIvB"
        ],
        "detail": [
          "https://openreview.net/forum?id=qaHrpITIvB"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Counteractive RL: Rethinking Core Principles for Efficient and Scalable Deep Reinforcement Learning [PDF 3 ] [Copy] [Kimi 1 ] [REL] Author : Ezgi Korkmaz Following the pivotal success of learning strategies to win at tasks, solely by interacting with an environment without any supervision, agents have gained the ability to make sequential decisions in complex MDPs. Yet, reinforcement learning policies face exponentially growing state spaces in high dimensional MDPs resulting in a dichotomy between computational complexity and policy success. In our paper we focus on the agent’s interaction with the environment in a high-dimensional MDP during the learning phase and we introduce a theoretically-founded novel paradigm based on experiences obtained through counteractive actions. Our analysis and method provide a theoretical basis for efficient, effective, scalable and accelerated learning, and further comes with zero additional computational complexity while leading to significant acceleration in training. We conduct extensive experiments in the Arcade Learning Environment with high-dimensional state representation MDPs. The experimental results further verify our theoretical analysis, and our method achieves significant performance increase with substantial sample-efficiency in high-dimensional environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bmoGGgSLzB@OpenReview",
      "index": 117,
      "title": "Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models",
      "authors": [
        "Jipeng Li",
        "Yanning Shen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "conditioning",
        "gdms",
        "noise",
        "graph",
        "unconditional",
        "gdss",
        "digress",
        "gdm",
        "diffusion",
        "denoisers"
      ],
      "summary": "Explicit noise-level conditioning is widely regarded as essential for the effective operation of Graph Diffusion Models (GDMs). In this work, we challenge this assumption by investigating whether denoisers can implicitly infer noise levels directly from corrupted graph structures, potentially eliminating the need for explicit noise conditioning. To this end, we develop a theoretical framework centered on Bernoulli edge-flip corruptions and extend it to encompass more complex scenarios involving coupled structure-attribute noise. Extensive empirical evaluations on both synthetic and real-world graph datasets, using models such as GDSS and DiGress, provide strong support for our theoretical findings. Notably, unconditional GDMs achieve performance comparable or superior to their conditioned counterparts, while also offering reductions in parameters (4-6%) and computation time (8-10%). Our results suggest that the high-dimensional nature of graph data itself often encodes sufficient information for the denoising process, opening avenues for simpler, more efficient GDM architectures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bmoGGgSLzB"
        ],
        "venue": [
          "/venue/bmoGGgSLzB@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bmoGGgSLzB"
        ],
        "detail": [
          "https://openreview.net/forum?id=bmoGGgSLzB"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Jipeng Li , Yanning Shen Explicit noise-level conditioning is widely regarded as essential for the effective operation of Graph Diffusion Models (GDMs). In this work, we challenge this assumption by investigating whether denoisers can implicitly infer noise levels directly from corrupted graph structures, potentially eliminating the need for explicit noise conditioning. To this end, we develop a theoretical framework centered on Bernoulli edge-flip corruptions and extend it to encompass more complex scenarios involving coupled structure-attribute noise. Extensive empirical evaluations on both synthetic and real-world graph datasets, using models such as GDSS and DiGress, provide strong support for our theoretical findings. Notably, unconditional GDMs achieve performance comparable or superior to their conditioned counterparts, while also offering reductions in parameters (4-6%) and computation time (8-10%). Our results suggest that the high-dimensional nature of graph data itself often encodes sufficient information for the denoising process, opening avenues for simpler, more efficient GDM architectures. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "NgLFQTBPRR@OpenReview",
      "index": 118,
      "title": "An Evidence-Based Post-Hoc Adjustment Framework for Anomaly Detection Under Data Contamination",
      "authors": [
        "Sukanya Patra",
        "Souhaib Ben Taieb"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ephad",
        "datasets",
        "evidence",
        "contamination",
        "contaminated",
        "anomaly",
        "world",
        "anomalies",
        "knowledge",
        "adjustment"
      ],
      "summary": "Unsupervised anomaly detection (AD) methods typically assume clean training data, yet real-world datasets often contain undetected or mislabeled anomalies, leading to significant performance degradation. Existing solutions require access to the training pipelines, data or prior knowledge of the proportions of anomalies in the data, limiting their real-world applicability. To address this challenge, we propose EPHAD, a simple yet effective test-time adaptation framework that updates the outputs of AD models trained on contaminated datasets using evidence gathered at test time. Our approach integrates the prior knowledge captured by the AD model trained on contaminated datasets with evidence derived from multimodal foundation models like Contrastive Language-Image Pre-training (CLIP), classical AD methods like the Latent Outlier Factor or domain-specific knowledge. We illustrate the intuition behind EPHAD using a synthetic toy example and validate its effectiveness through comprehensive experiments across eight visual AD datasets, twenty-six tabular AD datasets, and a real-world industrial AD dataset. Additionally, we conduct an ablation study to analyse hyperparameter influence and robustness to varying contamination levels, demonstrating the versatility and robustness of EPHAD across diverse AD models and evidence pairs. To ensure reproducibility, our code is publicly available at https://github.com/sukanyapatra1997/EPHAD.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NgLFQTBPRR"
        ],
        "venue": [
          "/venue/NgLFQTBPRR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NgLFQTBPRR"
        ],
        "detail": [
          "https://openreview.net/forum?id=NgLFQTBPRR"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "An Evidence-Based Post-Hoc Adjustment Framework for Anomaly Detection Under Data Contamination [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Sukanya Patra , Souhaib Ben Taieb Unsupervised anomaly detection (AD) methods typically assume clean training data, yet real-world datasets often contain undetected or mislabeled anomalies, leading to significant performance degradation. Existing solutions require access to the training pipelines, data or prior knowledge of the proportions of anomalies in the data, limiting their real-world applicability. To address this challenge, we propose EPHAD, a simple yet effective test-time adaptation framework that updates the outputs of AD models trained on contaminated datasets using evidence gathered at test time. Our approach integrates the prior knowledge captured by the AD model trained on contaminated datasets with evidence derived from multimodal foundation models like Contrastive Language-Image Pre-training (CLIP), classical AD methods like the Latent Outlier Factor or domain-specific knowledge. We illustrate the intuition behind EPHAD using a synthetic toy example and validate its effectiveness through comprehensive experiments across eight visual AD datasets, twenty-six tabular AD datasets, and a real-world industrial AD dataset. Additionally, we conduct an ablation study to analyse hyperparameter influence and robustness to varying contamination levels, demonstrating the versatility and robustness of EPHAD across diverse AD models and evidence pairs. To ensure reproducibility, our code is publicly available at https://github.com/sukanyapatra1997/EPHAD. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wab4BEAUt6@OpenReview",
      "index": 119,
      "title": "SHAP values via sparse Fourier representation",
      "authors": [
        "Ali Gorji",
        "Andisheh Amrollahi",
        "Andreas Krause"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shap",
        "fourier",
        "values",
        "linearizes",
        "representation",
        "stage",
        "box",
        "exactly",
        "shapley",
        "computing"
      ],
      "summary": "SHAP (SHapley Additive exPlanations) values are a widely used method for local feature attribution in interpretable and explainable AI. We propose an efficient two-stage algorithm for computing SHAP values in both black-box setting and tree-based models. Motivated by spectral bias in real-world predictors, we first approximate models using compact Fourier representations, exactly for trees and approximately for black-box models. In the second stage, we introduce a closed-form formula for {\\em exactly} computing SHAP values using the Fourier representation, that ``linearizes'' the computation into a simple summation and is amenable to parallelization. As the Fourier approximation is computed only once, our method enables amortized SHAP value computation, achieving significant speedups over existing methods and a tunable trade-off between efficiency and precision.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wab4BEAUt6"
        ],
        "venue": [
          "/venue/wab4BEAUt6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wab4BEAUt6"
        ],
        "detail": [
          "https://openreview.net/forum?id=wab4BEAUt6"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "SHAP values via sparse Fourier representation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Ali Gorji , Andisheh Amrollahi , Andreas Krause SHAP (SHapley Additive exPlanations) values are a widely used method for local feature attribution in interpretable and explainable AI. We propose an efficient two-stage algorithm for computing SHAP values in both black-box setting and tree-based models. Motivated by spectral bias in real-world predictors, we first approximate models using compact Fourier representations, exactly for trees and approximately for black-box models. In the second stage, we introduce a closed-form formula for {\\em exactly} computing SHAP values using the Fourier representation, that ``linearizes'' the computation into a simple summation and is amenable to parallelization. As the Fourier approximation is computed only once, our method enables amortized SHAP value computation, achieving significant speedups over existing methods and a tunable trade-off between efficiency and precision. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0EILv1HcmG@OpenReview",
      "index": 120,
      "title": "Quantum Doubly Stochastic Transformers",
      "authors": [
        "Jannis Born",
        "Filip Skogh",
        "Kahn Rhrissorrakrai",
        "Filippo Utro",
        "Nico Wagner",
        "Aleksandros Sobczyk"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "doubly",
        "dsms",
        "stochastic",
        "qdsformer",
        "transformer",
        "quantum",
        "sinkhorn",
        "transformers",
        "circuit",
        "sinkformer"
      ],
      "summary": "At the core of the Transformer, the softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often de-stabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn’s algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn’s algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard ViT and other doubly stochastic Transformers. Beyond the Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. Our QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0EILv1HcmG"
        ],
        "venue": [
          "/venue/0EILv1HcmG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0EILv1HcmG"
        ],
        "detail": [
          "https://openreview.net/forum?id=0EILv1HcmG"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Quantum Doubly Stochastic Transformers [PDF ] [Copy] [Kimi ] [REL] Authors : Jannis Born , Filip Skogh , Kahn Rhrissorrakrai , Filippo Utro , Nico Wagner , Aleksandros Sobczyk At the core of the Transformer, the softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often de-stabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn’s algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn’s algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard ViT and other doubly stochastic Transformers. Beyond the Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. Our QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "heY0zzGvYm@OpenReview",
      "index": 121,
      "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
      "authors": [
        "Lisa Alazraki",
        "Yi-Chern Tan",
        "Jon Ander Campos",
        "Maximilian Mozes",
        "Marek Rei",
        "Max Bartolo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "llm",
        "judge",
        "preferences",
        "llms",
        "preambles",
        "reverse",
        "human",
        "reinforcement",
        "frameworks",
        "tuned"
      ],
      "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework—known as *LLM-as-a-judge*—is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited *post hoc* to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning—an approach that could find future applications in diverse tasks and domains beyond adversarial attacks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=heY0zzGvYm"
        ],
        "venue": [
          "/venue/heY0zzGvYm@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=heY0zzGvYm"
        ],
        "detail": [
          "https://openreview.net/forum?id=heY0zzGvYm"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Reverse Engineering Human Preferences with Reinforcement Learning [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Lisa Alazraki , Yi-Chern Tan , Jon Ander Campos , Maximilian Mozes , Marek Rei , Max Bartolo The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework—known as *LLM-as-a-judge*—is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited *post hoc* to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning—an approach that could find future applications in diverse tasks and domains beyond adversarial attacks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "65oFEAP42P@OpenReview",
      "index": 122,
      "title": "Efficient Fairness-Performance Pareto Front Computation",
      "authors": [
        "Mark Kozdoba",
        "Binyamin Perets",
        "Shie Mannor"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pareto",
        "fair",
        "front",
        "representation",
        "fairness",
        "methods",
        "concave",
        "representations",
        "classification",
        "performance"
      ],
      "summary": "There is a well known intrinsic trade-off between the fairness of a representation and the performance of classifiers derived from the representation. In this paper we propose a new method to compute the optimal Pareto front of this trade off. In contrast to the existing methods, this approach does not require the training of complex fair representation models. Our approach is derived through three main steps: We analyze fair representations theoretically, and derive several structural properties of optimal representations. We then show that these properties enable a reduction of the computation of the Pareto Front to a compact discrete problem. Finally, we show that these compact approximating problems can be efficiently solved via off-the shelf concave-convex programming methods. In addition to representations, we show that the new methods may also be used to directly compute the Pareto front of fair classification problems. Moreover, the proposed methods may be used with any concave performance measure. This is in contrast to the existing reduction approaches, developed recently in fair classification, which rely explicitly on the structure of the non-differentiable accuracy measure, and are thus unlikely to be extendable. The approach was evaluated on several real world benchmark datasets and compares favorably to a number of recent state of the art fair representation and classification methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=65oFEAP42P"
        ],
        "venue": [
          "/venue/65oFEAP42P@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=65oFEAP42P"
        ],
        "detail": [
          "https://openreview.net/forum?id=65oFEAP42P"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Efficient Fairness-Performance Pareto Front Computation [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Mark Kozdoba , Binyamin Perets , Shie Mannor There is a well known intrinsic trade-off between the fairness of a representation and the performance of classifiers derived from the representation. In this paper we propose a new method to compute the optimal Pareto front of this trade off. In contrast to the existing methods, this approach does not require the training of complex fair representation models. Our approach is derived through three main steps: We analyze fair representations theoretically, and derive several structural properties of optimal representations. We then show that these properties enable a reduction of the computation of the Pareto Front to a compact discrete problem. Finally, we show that these compact approximating problems can be efficiently solved via off-the shelf concave-convex programming methods. In addition to representations, we show that the new methods may also be used to directly compute the Pareto front of fair classification problems. Moreover, the proposed methods may be used with any concave performance measure. This is in contrast to the existing reduction approaches, developed recently in fair classification, which rely explicitly on the structure of the non-differentiable accuracy measure, and are thus unlikely to be extendable. The approach was evaluated on several real world benchmark datasets and compares favorably to a number of recent state of the art fair representation and classification methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SB1CsuJ11a@OpenReview",
      "index": 123,
      "title": "Curl Descent : Non-Gradient Learning Dynamics with Sign-Diverse Plasticity",
      "authors": [
        "Hugo Ninou",
        "Jonathan Kadmon",
        "N Alex Cayco Gajic"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "curl",
        "gradient",
        "descent",
        "plasticity",
        "dynamics",
        "learning",
        "hebbian",
        "terms",
        "diverse",
        "unclear"
      ],
      "summary": "Gradient-based algorithms are a cornerstone of artificial neural network training, yet it remains unclear whether biological neural networks use similar gradient-based strategies during learning. Experiments often discover a diversity of synaptic plasticity rules, but whether these amount to an approximation to gradient descent is unclear. Here we investigate a previously overlooked possibility: that learning dynamics may include fundamentally non-gradient \"curl\"-like components while still being able to effectively optimize a loss function. Curl terms naturally emerge in networks with excitatory-inhibitory connectivity or Hebbian/anti-Hebbian plasticity, resulting in learning dynamics that cannot be framed as gradient descent on any objective. To investigate the impact of these curl terms, we analyze feedforward networks within an analytically tractable student-teacher framework, systematically introducing non-gradient dynamics through rule-flipped neurons. Small curl terms preserve the stability of the original solution manifold, resulting in learning dynamics similar to gradient descent. Beyond a critical value, strong curl terms destabilize the solution manifold. Depending on the network architecture, this loss of stability can lead to chaotic learning dynamics that destroy performance. In other cases, the curl terms can counterintuitively speed up learning compared to gradient descent by allowing the weight dynamics to escape saddles by temporarily ascending the loss. Our results identify specific architectures capable of supporting robust learning via diverse learning rules, providing an important counterpoint to normative theories of gradient-based learning in neural networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SB1CsuJ11a"
        ],
        "venue": [
          "/venue/SB1CsuJ11a@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SB1CsuJ11a"
        ],
        "detail": [
          "https://openreview.net/forum?id=SB1CsuJ11a"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Curl Descent : Non-Gradient Learning Dynamics with Sign-Diverse Plasticity [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hugo Ninou , Jonathan Kadmon , N Alex Cayco Gajic Gradient-based algorithms are a cornerstone of artificial neural network training, yet it remains unclear whether biological neural networks use similar gradient-based strategies during learning. Experiments often discover a diversity of synaptic plasticity rules, but whether these amount to an approximation to gradient descent is unclear. Here we investigate a previously overlooked possibility: that learning dynamics may include fundamentally non-gradient \"curl\"-like components while still being able to effectively optimize a loss function. Curl terms naturally emerge in networks with excitatory-inhibitory connectivity or Hebbian/anti-Hebbian plasticity, resulting in learning dynamics that cannot be framed as gradient descent on any objective. To investigate the impact of these curl terms, we analyze feedforward networks within an analytically tractable student-teacher framework, systematically introducing non-gradient dynamics through rule-flipped neurons. Small curl terms preserve the stability of the original solution manifold, resulting in learning dynamics similar to gradient descent. Beyond a critical value, strong curl terms destabilize the solution manifold. Depending on the network architecture, this loss of stability can lead to chaotic learning dynamics that destroy performance. In other cases, the curl terms can counterintuitively speed up learning compared to gradient descent by allowing the weight dynamics to escape saddles by temporarily ascending the loss. Our results identify specific architectures capable of supporting robust learning via diverse learning rules, providing an important counterpoint to normative theories of gradient-based learning in neural networks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "GNDgQie8W4@OpenReview",
      "index": 124,
      "title": "ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets",
      "authors": [
        "Bohdan Turbal",
        "Iryna Voitsitska",
        "Lesia Semenova"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "recourse",
        "rashomon",
        "ellice",
        "algorithmic",
        "explanations",
        "counterfactuals",
        "provably",
        "robust",
        "counterfactual",
        "valid"
      ],
      "summary": "Machine learning models now influence decisions that directly affect people’s lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified features constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GNDgQie8W4"
        ],
        "venue": [
          "/venue/GNDgQie8W4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GNDgQie8W4"
        ],
        "detail": [
          "https://openreview.net/forum?id=GNDgQie8W4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Bohdan Turbal , Iryna Voitsitska , Lesia Semenova Machine learning models now influence decisions that directly affect people’s lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified features constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WQ9rnkaUWm@OpenReview",
      "index": 125,
      "title": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers",
      "authors": [
        "Lorenzo Basile",
        "Valentino Maiorca",
        "Diego Doimo",
        "Francesco Locatello",
        "Alberto Cazzaniga"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "heads",
        "specialization",
        "attention",
        "language",
        "head",
        "probing",
        "multimodal",
        "transformers",
        "editing",
        "tasks"
      ],
      "summary": "Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WQ9rnkaUWm"
        ],
        "venue": [
          "/venue/WQ9rnkaUWm@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WQ9rnkaUWm"
        ],
        "detail": [
          "https://openreview.net/forum?id=WQ9rnkaUWm"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 5
      },
      "raw_excerpt": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers [PDF 7 ] [Copy] [Kimi 5 ] [REL] Authors : Lorenzo Basile , Valentino Maiorca , Diego Doimo , Francesco Locatello , Alberto Cazzaniga Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yf8O4xEB4T@OpenReview",
      "index": 126,
      "title": "Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations",
      "authors": [
        "Kaibo Wang",
        "Jianda Mao",
        "Tong Wu",
        "Yang Xiang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "guidance",
        "foresight",
        "fsg",
        "iterations",
        "golden",
        "cfg",
        "conditional",
        "classifier",
        "path",
        "reframes"
      ],
      "summary": "Classifier-Free Guidance (CFG) is an essential component of text-to-image diffusion models, and understanding and advancing its operational mechanisms remains a central focus of research. Existing approaches stem from divergent theoretical interpretations, thereby limiting the design space and obscuring key design choices. To address this, we propose a unified perspective that reframes conditional guidance as fixed point iterations, seeking to identify a golden path where latents produce consistent outputs under both conditional and unconditional generation. We demonstrate that CFG and its variants constitute a special case of single-step short-interval iteration, which is theoretically proven to exhibit inefficiency. To this end, we introduce Foresight Guidance (FSG), which prioritizes solving longer-interval subproblems in early diffusion stages with increased iterations. Extensive experiments across diverse datasets and model architectures validate the superiority of FSG over state-of-the-art methods in both image quality and computational efficiency. Our work offers novel perspectives for conditional guidance and unlocks the potential of adaptive design.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yf8O4xEB4T"
        ],
        "venue": [
          "/venue/yf8O4xEB4T@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yf8O4xEB4T"
        ],
        "detail": [
          "https://openreview.net/forum?id=yf8O4xEB4T"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Kaibo Wang , Jianda Mao , Tong Wu , Yang Xiang Classifier-Free Guidance (CFG) is an essential component of text-to-image diffusion models, and understanding and advancing its operational mechanisms remains a central focus of research. Existing approaches stem from divergent theoretical interpretations, thereby limiting the design space and obscuring key design choices. To address this, we propose a unified perspective that reframes conditional guidance as fixed point iterations, seeking to identify a golden path where latents produce consistent outputs under both conditional and unconditional generation. We demonstrate that CFG and its variants constitute a special case of single-step short-interval iteration, which is theoretically proven to exhibit inefficiency. To this end, we introduce Foresight Guidance (FSG), which prioritizes solving longer-interval subproblems in early diffusion stages with increased iterations. Extensive experiments across diverse datasets and model architectures validate the superiority of FSG over state-of-the-art methods in both image quality and computational efficiency. Our work offers novel perspectives for conditional guidance and unlocks the potential of adaptive design. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rKM3oqruN3@OpenReview",
      "index": 127,
      "title": "Credal Prediction based on Relative Likelihood",
      "authors": [
        "Timo Löhr",
        "Paul Hofman",
        "Felix Mohr",
        "Eyke Hüllermeier"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "credal",
        "prediction",
        "likelihood",
        "relative",
        "sets",
        "uncertainty",
        "threshold",
        "predictions",
        "probability",
        "distributions"
      ],
      "summary": "Predictions in the form of sets of probability distributions, so-called credal sets, provide a suitable means to represent a learner's epistemic uncertainty. In this paper, we propose a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood: The target of prediction is the set of all (conditional) probability distributions produced by the collection of plausible models, namely those models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision of credal predictions. We tackle the problem of approximating credal sets defined in this way by means of suitably modified ensemble learning techniques. To validate our approach, we illustrate its effectiveness by experiments on benchmark datasets demonstrating superior uncertainty representation without compromising predictive performance. We also compare our method against several state-of-the-art baselines in credal prediction.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rKM3oqruN3"
        ],
        "venue": [
          "/venue/rKM3oqruN3@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rKM3oqruN3"
        ],
        "detail": [
          "https://openreview.net/forum?id=rKM3oqruN3"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Credal Prediction based on Relative Likelihood [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Timo Löhr , Paul Hofman , Felix Mohr , Eyke Hüllermeier Predictions in the form of sets of probability distributions, so-called credal sets, provide a suitable means to represent a learner's epistemic uncertainty. In this paper, we propose a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood: The target of prediction is the set of all (conditional) probability distributions produced by the collection of plausible models, namely those models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision of credal predictions. We tackle the problem of approximating credal sets defined in this way by means of suitably modified ensemble learning techniques. To validate our approach, we illustrate its effectiveness by experiments on benchmark datasets demonstrating superior uncertainty representation without compromising predictive performance. We also compare our method against several state-of-the-art baselines in credal prediction. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "E7knuYAvpt@OpenReview",
      "index": 128,
      "title": "The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination",
      "authors": [
        "Adam Klivans",
        "Konstantinos Stavropoulos",
        "Kevin Tian",
        "Arsen Vasilyan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "contamination",
        "approximators",
        "learning",
        "filtering",
        "supervised",
        "tolerant",
        "iterative",
        "heavy",
        "respect",
        "nasty"
      ],
      "summary": "Inspired by recent work on learning with distribution shift, we give a general outlier removal algorithm called *iterative polynomial filtering* and show a number of striking applications for supervised learning with contamination: (1) We show that any function class that can be approximated by low-degree polynomials with respect to a hypercontractive distribution can be efficiently learned under bounded contamination (also known as *nasty noise*). This is a surprising resolution to a longstanding gap between the complexity of agnostic learning and learning with contamination, as it was widely believed that low-degree approximators only implied tolerance to label noise. (2) For any function class that admits the (stronger) notion of sandwiching approximators, we obtain near-optimal learning guarantees even with respect to heavy additive contamination, where far more than 1 / 2 1 / 2 of the training set may be added adversarially. Prior related work held only for regression and in a list-decodable setting. (3) We obtain the first efficient algorithms for tolerant testable learning of functions of halfspaces with respect to any fixed log-concave distribution. Even the non-tolerant case for a single halfspace in this setting had remained open. These results significantly advance our understanding of efficient supervised learning under contamination, a setting that has been much less studied than its unsupervised counterpart.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=E7knuYAvpt"
        ],
        "venue": [
          "/venue/E7knuYAvpt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=E7knuYAvpt"
        ],
        "detail": [
          "https://openreview.net/forum?id=E7knuYAvpt"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Adam Klivans , Konstantinos Stavropoulos , Kevin Tian , Arsen Vasilyan Inspired by recent work on learning with distribution shift, we give a general outlier removal algorithm called *iterative polynomial filtering* and show a number of striking applications for supervised learning with contamination: (1) We show that any function class that can be approximated by low-degree polynomials with respect to a hypercontractive distribution can be efficiently learned under bounded contamination (also known as *nasty noise*). This is a surprising resolution to a longstanding gap between the complexity of agnostic learning and learning with contamination, as it was widely believed that low-degree approximators only implied tolerance to label noise. (2) For any function class that admits the (stronger) notion of sandwiching approximators, we obtain near-optimal learning guarantees even with respect to heavy additive contamination, where far more than 1 / 2 1 / 2 of the training set may be added adversarially. Prior related work held only for regression and in a list-decodable setting. (3) We obtain the first efficient algorithms for tolerant testable learning of functions of halfspaces with respect to any fixed log-concave distribution. Even the non-tolerant case for a single halfspace in this setting had remained open. These results significantly advance our understanding of efficient supervised learning under contamination, a setting that has been much less studied than its unsupervised counterpart. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YXSKYFZweV@OpenReview",
      "index": 129,
      "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators",
      "authors": [
        "Albert Matveev",
        "Sanmitra Ghosh",
        "Aamal Hussain",
        "James-Michael Leahy",
        "Michalis Michaelides"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dinozaur",
        "uncertainty",
        "quantification",
        "multiplier",
        "fnos",
        "neural",
        "diffusion",
        "operators",
        "fourier",
        "operator"
      ],
      "summary": "Operator learning is a powerful paradigm for solving partial differential equations, with Fourier Neural Operators serving as a widely adopted foundation. However, FNOs face significant scalability challenges due to overparameterization and offer no native uncertainty quantification -- a key requirement for reliable scientific and engineering applications. Instead, neural operators rely on post hoc UQ methods that ignore geometric inductive biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator parametrization with uncertainty quantification. Inspired by the structure of the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a dimensionality-independent diffusion multiplier that has a single learnable time parameter per channel, drastically reducing parameter count and memory footprint without compromising predictive performance. By defining priors over those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield spatially correlated outputs and calibrated uncertainty estimates. Our method achieves competitive or superior performance across several PDE benchmarks while providing efficient uncertainty quantification.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YXSKYFZweV"
        ],
        "venue": [
          "/venue/YXSKYFZweV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YXSKYFZweV"
        ],
        "detail": [
          "https://openreview.net/forum?id=YXSKYFZweV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Albert Matveev , Sanmitra Ghosh , Aamal Hussain , James-Michael Leahy , Michalis Michaelides Operator learning is a powerful paradigm for solving partial differential equations, with Fourier Neural Operators serving as a widely adopted foundation. However, FNOs face significant scalability challenges due to overparameterization and offer no native uncertainty quantification -- a key requirement for reliable scientific and engineering applications. Instead, neural operators rely on post hoc UQ methods that ignore geometric inductive biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator parametrization with uncertainty quantification. Inspired by the structure of the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a dimensionality-independent diffusion multiplier that has a single learnable time parameter per channel, drastically reducing parameter count and memory footprint without compromising predictive performance. By defining priors over those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield spatially correlated outputs and calibrated uncertainty estimates. Our method achieves competitive or superior performance across several PDE benchmarks while providing efficient uncertainty quantification. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kz3w2A2y0e@OpenReview",
      "index": 130,
      "title": "Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling",
      "authors": [
        "Javier E. Santos",
        "Agnese Marcato",
        "Roman Colman",
        "Nicholas Lubbers",
        "Yen Ting Lin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dsd",
        "diffusion",
        "discrete",
        "conservation",
        "spatial",
        "intensity",
        "conditioning",
        "preserving",
        "particle",
        "counts"
      ],
      "summary": "Generative diffusion models have achieved remarkable success in producing high-quality images. However, these models typically operate in continuous intensity spaces, diffusing independently across pixels and color channels. As a result, they are fundamentally ill-suited for applications involving inherently discrete quantities such as particle counts or material units, that are constrained by strict conservation laws like mass conservation, limiting their applicability in scientific workflows. To address this limitation, we propose Discrete Spatial Diffusion (DSD), a framework based on a continuous-time, discrete-state jump stochastic process that operates directly in discrete spatial domains while strictly preserving particle counts in both forward and reverse diffusion processes. By using spatial diffusion to achieve particle conservation, we introduce stochasticity naturally through a discrete formulation. We demonstrate the expressive flexibility of DSD by performing image synthesis, class conditioning, and image inpainting across standard image benchmarks, while exactly conditioning total image intensity. We validate DSD on two challenging scientific applications: porous rock microstructures and lithium-ion battery electrodes, demonstrating its ability to generate structurally realistic samples under strict mass conservation constraints, with quantitative evaluation using state-of-the-art metrics for transport and electrochemical performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kz3w2A2y0e"
        ],
        "venue": [
          "/venue/kz3w2A2y0e@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kz3w2A2y0e"
        ],
        "detail": [
          "https://openreview.net/forum?id=kz3w2A2y0e"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Javier E. Santos , Agnese Marcato , Roman Colman , Nicholas Lubbers , Yen Ting Lin Generative diffusion models have achieved remarkable success in producing high-quality images. However, these models typically operate in continuous intensity spaces, diffusing independently across pixels and color channels. As a result, they are fundamentally ill-suited for applications involving inherently discrete quantities such as particle counts or material units, that are constrained by strict conservation laws like mass conservation, limiting their applicability in scientific workflows. To address this limitation, we propose Discrete Spatial Diffusion (DSD), a framework based on a continuous-time, discrete-state jump stochastic process that operates directly in discrete spatial domains while strictly preserving particle counts in both forward and reverse diffusion processes. By using spatial diffusion to achieve particle conservation, we introduce stochasticity naturally through a discrete formulation. We demonstrate the expressive flexibility of DSD by performing image synthesis, class conditioning, and image inpainting across standard image benchmarks, while exactly conditioning total image intensity. We validate DSD on two challenging scientific applications: porous rock microstructures and lithium-ion battery electrodes, demonstrating its ability to generate structurally realistic samples under strict mass conservation constraints, with quantitative evaluation using state-of-the-art metrics for transport and electrochemical performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XBMjXb6f4w@OpenReview",
      "index": 131,
      "title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D",
      "authors": [
        "Francis Rhys Ward",
        "Teun van der Weij",
        "Hanna Gábor",
        "Sam Martin",
        "Raja Mehta Moreno",
        "Harel Lidar",
        "Louis Makower",
        "Thomas Jodrell",
        "Lauren Robson"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sabotage",
        "sandbag",
        "sandbagging",
        "monitors",
        "frontier",
        "deceit",
        "ctrl",
        "tasks",
        "mle",
        "bench"
      ],
      "summary": "AI systems are increasingly able to autonomously conduct realistic software engineering tasks, and may soon be deployed to automate machine learning (ML) R\\&D itself. Frontier AI systems may be deployed in safety-critical settings, including to help ensure the safety of future systems. Unfortunately, frontier and future systems may not be sufficiently trustworthy, and there is evidence that these systems may even be misaligned with their developers or users. Therefore, we investigate the capabilities of AI agents to act against the interests of their users when conducting ML engineering, by sabotaging ML models, sandbagging their performance, and subverting oversight mechanisms. First, we extend MLE-Bench, a benchmark for realistic ML tasks, with code-sabotage tasks such as implanting backdoors and purposefully causing generalisation failures. Frontier agents make meaningful progress on our sabotage tasks. In addition, we study agent capabilities to sandbag on MLE-Bench. Agents can calibrate their performance to specified target levels below their actual capability. To mitigate sabotage, we use LM monitors to detect suspicious agent behaviour, and we measure model capability to sabotage and sandbag without being detected by these monitors. Overall, monitors are capable at detecting code-sabotage attempts but our results suggest that detecting sandbagging is more difficult. Additionally, aggregating multiple monitor predictions works well, but monitoring may not be sufficiently reliable to mitigate sabotage in high-stakes domains. Our benchmark is implemented in the UK AISI’s Inspect framework and we make our code publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XBMjXb6f4w"
        ],
        "venue": [
          "/venue/XBMjXb6f4w@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XBMjXb6f4w"
        ],
        "detail": [
          "https://openreview.net/forum?id=XBMjXb6f4w"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Francis Rhys Ward , Teun van der Weij , Hanna Gábor , Sam Martin , Raja Mehta Moreno , Harel Lidar , Louis Makower , Thomas Jodrell , Lauren Robson AI systems are increasingly able to autonomously conduct realistic software engineering tasks, and may soon be deployed to automate machine learning (ML) R\\&D itself. Frontier AI systems may be deployed in safety-critical settings, including to help ensure the safety of future systems. Unfortunately, frontier and future systems may not be sufficiently trustworthy, and there is evidence that these systems may even be misaligned with their developers or users. Therefore, we investigate the capabilities of AI agents to act against the interests of their users when conducting ML engineering, by sabotaging ML models, sandbagging their performance, and subverting oversight mechanisms. First, we extend MLE-Bench, a benchmark for realistic ML tasks, with code-sabotage tasks such as implanting backdoors and purposefully causing generalisation failures. Frontier agents make meaningful progress on our sabotage tasks. In addition, we study agent capabilities to sandbag on MLE-Bench. Agents can calibrate their performance to specified target levels below their actual capability. To mitigate sabotage, we use LM monitors to detect suspicious agent behaviour, and we measure model capability to sabotage and sandbag without being detected by these monitors. Overall, monitors are capable at detecting code-sabotage attempts but our results suggest that detecting sandbagging is more difficult. Additionally, aggregating multiple monitor predictions works well, but monitoring may not be sufficiently reliable to mitigate sabotage in high-stakes domains. Our benchmark is implemented in the UK AISI’s Inspect framework and we make our code publicly available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SDhOClkyqC@OpenReview",
      "index": 132,
      "title": "An Analytical Theory of Spectral Bias in the Learning Dynamics of Diffusion Models",
      "authors": [
        "Binxu Wang",
        "Cengiz Pehlevan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bias",
        "convolutional",
        "analytical",
        "spectral",
        "diffusion",
        "converge",
        "weights",
        "dictating",
        "linear",
        "denoiser"
      ],
      "summary": "We develop an analytical framework for understanding how the learned distribution evolves during diffusion model training. Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one or two layer linear or linear convolutional denoiser settings with arbitrary data, where linear networks converge along principal components, and convolutional networks converge along Fourier modes. Remarkably, these solutions allow us to derive the generated distribution in closed-form and its KL-divergence through training. These analytical results expose a pronounced \\emph{spectral bias}, i.e. for both weights and generated distributions, the convergence time of a mode follows an inverse power law of its variance. Empirical experiments on both Gaussian and natural image datasets demonstrate that the power-law spectral bias—remain robust even when using deeper or convolutional architectures. Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations of why earlier stopping could lead to incorrect details in image generative model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SDhOClkyqC"
        ],
        "venue": [
          "/venue/SDhOClkyqC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SDhOClkyqC"
        ],
        "detail": [
          "https://openreview.net/forum?id=SDhOClkyqC"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "An Analytical Theory of Spectral Bias in the Learning Dynamics of Diffusion Models [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Binxu Wang , Cengiz Pehlevan We develop an analytical framework for understanding how the learned distribution evolves during diffusion model training. Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one or two layer linear or linear convolutional denoiser settings with arbitrary data, where linear networks converge along principal components, and convolutional networks converge along Fourier modes. Remarkably, these solutions allow us to derive the generated distribution in closed-form and its KL-divergence through training. These analytical results expose a pronounced \\emph{spectral bias}, i.e. for both weights and generated distributions, the convergence time of a mode follows an inverse power law of its variance. Empirical experiments on both Gaussian and natural image datasets demonstrate that the power-law spectral bias—remain robust even when using deeper or convolutional architectures. Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations of why earlier stopping could lead to incorrect details in image generative model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "x9XepNPGJ5@OpenReview",
      "index": 133,
      "title": "DeepHalo: A Neural Choice Model with Controllable Context Effects",
      "authors": [
        "Shuhan Zhang",
        "Zhi Wang",
        "Rui Gao",
        "Shuang Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "choice",
        "deephalo",
        "context",
        "featureless",
        "effects",
        "order",
        "setting",
        "interaction",
        "controllable",
        "entangle"
      ],
      "summary": "Modeling human decision-making is central to applications such as recommendation, preference learning, and human-AI alignment. While many classic models assume context-independent choice behavior, a large body of behavioral research shows that preferences are often influenced by the composition of the choice set itself---a phenomenon known as the context effect or Halo effect. These effects can manifest as pairwise (first-order) or even higher-order interactions among the available alternatives. Recent models that attempt to capture such effects either focus on the featureless setting or, in the feature-based setting, rely on restrictive interaction structures or entangle interactions across all orders, which limits interpretability. In this work, we propose DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. Our model enables systematic identification of interaction effects by order and serves as a universal approximator of context-dependent choice functions when specialized to a featureless setting. Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=x9XepNPGJ5"
        ],
        "venue": [
          "/venue/x9XepNPGJ5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=x9XepNPGJ5"
        ],
        "detail": [
          "https://openreview.net/forum?id=x9XepNPGJ5"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "DeepHalo: A Neural Choice Model with Controllable Context Effects [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Shuhan Zhang , Zhi Wang , Rui Gao , Shuang Li Modeling human decision-making is central to applications such as recommendation, preference learning, and human-AI alignment. While many classic models assume context-independent choice behavior, a large body of behavioral research shows that preferences are often influenced by the composition of the choice set itself---a phenomenon known as the context effect or Halo effect. These effects can manifest as pairwise (first-order) or even higher-order interactions among the available alternatives. Recent models that attempt to capture such effects either focus on the featureless setting or, in the feature-based setting, rely on restrictive interaction structures or entangle interactions across all orders, which limits interpretability. In this work, we propose DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. Our model enables systematic identification of interaction effects by order and serves as a universal approximator of context-dependent choice functions when specialized to a featureless setting. Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "CrwzbjO3aU@OpenReview",
      "index": 134,
      "title": "Multi-Agent Learning under Uncertainty: Recurrence vs. Concentration",
      "authors": [
        "Kyriakos Lotidis",
        "Panayotis Mertikopoulos",
        "Nicholas Bambos",
        "Jose Blanchet"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "regularized",
        "uncertainty",
        "learning",
        "run",
        "agent",
        "concentration",
        "monotone",
        "recurrence",
        "games",
        "wander"
      ],
      "summary": "In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games—one in continuous and one in discrete time—with the aim of characterizing the long run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone—underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CrwzbjO3aU"
        ],
        "venue": [
          "/venue/CrwzbjO3aU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CrwzbjO3aU"
        ],
        "detail": [
          "https://openreview.net/forum?id=CrwzbjO3aU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Multi-Agent Learning under Uncertainty: Recurrence vs. Concentration [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Kyriakos Lotidis , Panayotis Mertikopoulos , Nicholas Bambos , Jose Blanchet In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games—one in continuous and one in discrete time—with the aim of characterizing the long run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone—underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IkfBLlYuHA@OpenReview",
      "index": 135,
      "title": "Quantum speedup of non-linear Monte Carlo problems",
      "authors": [
        "Jose Blanchet",
        "Yassine Hamoudi",
        "Mario Szegedy",
        "Guanyang Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "speedup",
        "monte",
        "carlo",
        "quantum",
        "algorithm",
        "multilevel",
        "linear",
        "quadratic",
        "mean",
        "probability"
      ],
      "summary": "The mean of a random variable can be understood as a *linear* functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating *non-linear* functionals of probability distributions. We propose a \\textit{quantum-inside-quantum} algorithm that achieves this speedup for the broad class of nonlinear estimation problems known as nested expectations. Our algorithm improves upon the direct application of the quantum-accelerated multilevel Monte Carlo algorithm introduced by An et. al.. The existing lower bound indicates that our algorithm is optimal up to polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IkfBLlYuHA"
        ],
        "venue": [
          "/venue/IkfBLlYuHA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IkfBLlYuHA"
        ],
        "detail": [
          "https://openreview.net/forum?id=IkfBLlYuHA"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Quantum speedup of non-linear Monte Carlo problems [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jose Blanchet , Yassine Hamoudi , Mario Szegedy , Guanyang Wang The mean of a random variable can be understood as a *linear* functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating *non-linear* functionals of probability distributions. We propose a \\textit{quantum-inside-quantum} algorithm that achieves this speedup for the broad class of nonlinear estimation problems known as nested expectations. Our algorithm improves upon the direct application of the quantum-accelerated multilevel Monte Carlo algorithm introduced by An et. al.. The existing lower bound indicates that our algorithm is optimal up to polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Quo3XadYcZ@OpenReview",
      "index": 136,
      "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation",
      "authors": [
        "Chenxiao Fan",
        "Chongming Gao",
        "Wentao Shi",
        "Yaxin Gong",
        "Zhao Zihao",
        "Fuli Feng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "drug",
        "flame",
        "wise",
        "medication",
        "ddis",
        "clinical",
        "grained",
        "list",
        "recommendation",
        "grpo"
      ],
      "summary": "Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety–accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Quo3XadYcZ"
        ],
        "venue": [
          "/venue/Quo3XadYcZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Quo3XadYcZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=Quo3XadYcZ"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Fine-grained List-wise Alignment for Generative Medication Recommendation [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Chenxiao Fan , Chongming Gao , Wentao Shi , Yaxin Gong , Zhao Zihao , Fuli Feng Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety–accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "S3GhJooWIC@OpenReview",
      "index": 137,
      "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
      "authors": [
        "Jonas Geiping",
        "Sean Michael McLeish",
        "Neel Jain",
        "John Kirchenbauer",
        "Siddharth Singh",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Abhinav Bhatele",
        "Tom Goldstein"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "compute",
        "test",
        "billion",
        "latent",
        "recurrent",
        "depth",
        "tokens",
        "scaling",
        "iterating"
      ],
      "summary": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We train a proof-of-concept model from scratch with 3.5 billion parameters and 800 billion tokens. We show that this model can effortlessly use varying levels of compute, significantly improving with additional compute especially on reasoning tasks, such as math and coding. Further, this architecture naturally reduces compute costs via zero-shot per-token adaptive compute, KV-cache sharing and speculative decoding.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=S3GhJooWIC"
        ],
        "venue": [
          "/venue/S3GhJooWIC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=S3GhJooWIC"
        ],
        "detail": [
          "https://openreview.net/forum?id=S3GhJooWIC"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Jonas Geiping , Sean Michael McLeish , Neel Jain , John Kirchenbauer , Siddharth Singh , Brian R. Bartoldson , Bhavya Kailkhura , Abhinav Bhatele , Tom Goldstein We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We train a proof-of-concept model from scratch with 3.5 billion parameters and 800 billion tokens. We show that this model can effortlessly use varying levels of compute, significantly improving with additional compute especially on reasoning tasks, such as math and coding. Further, this architecture naturally reduces compute costs via zero-shot per-token adaptive compute, KV-cache sharing and speculative decoding. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MfBw0dlBfi@OpenReview",
      "index": 138,
      "title": "Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics",
      "authors": [
        "Tobias Würth",
        "Niklas Freymuth",
        "Gerhard Neumann",
        "Luise Kärger"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "robin",
        "diffusion",
        "simulators",
        "message",
        "passing",
        "batched",
        "hierarchical",
        "solid",
        "mechanics",
        "across"
      ],
      "summary": "Graph-based learned simulators have emerged as a promising approach for simulating physical systems on unstructured meshes, offering speed and generalization across diverse geometries. However, they often struggle with capturing global phenomena, such as bending or long-range correlations usually occurring in solid mechanics, and suffer from error accumulation over long rollouts due to their reliance on local message passing and direct next-step prediction. We address these limitations by introducing the Rolling Diffusion-Batched Inference Network (ROBIN), a novel learned simulator that integrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI), a parallelized inference scheme that amortizes the cost of diffusion-based refinement across physical time steps by overlapping denoising steps across a temporal window. (ii) A Hierarchical Graph Neural Network built on algebraic multigrid coarsening, enabling multiscale message passing across different mesh resolutions. This architecture, implemented via Algebraic-hierarchical Message Passing Networks, captures both fine-scale local dynamics and global structural effects critical for phenomena like beam bending or multi-body contact. We validate ROBIN on challenging 2D and 3D solid mechanics benchmarks involving geometric, material, and contact nonlinearities. ROBIN achieves state-of-the-art accuracy on all tasks, substantially outperforming existing next-step learned simulators while reducing inference time by up to an order of magnitude compared to standard diffusion simulators.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MfBw0dlBfi"
        ],
        "venue": [
          "/venue/MfBw0dlBfi@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MfBw0dlBfi"
        ],
        "detail": [
          "https://openreview.net/forum?id=MfBw0dlBfi"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Tobias Würth , Niklas Freymuth , Gerhard Neumann , Luise Kärger Graph-based learned simulators have emerged as a promising approach for simulating physical systems on unstructured meshes, offering speed and generalization across diverse geometries. However, they often struggle with capturing global phenomena, such as bending or long-range correlations usually occurring in solid mechanics, and suffer from error accumulation over long rollouts due to their reliance on local message passing and direct next-step prediction. We address these limitations by introducing the Rolling Diffusion-Batched Inference Network (ROBIN), a novel learned simulator that integrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI), a parallelized inference scheme that amortizes the cost of diffusion-based refinement across physical time steps by overlapping denoising steps across a temporal window. (ii) A Hierarchical Graph Neural Network built on algebraic multigrid coarsening, enabling multiscale message passing across different mesh resolutions. This architecture, implemented via Algebraic-hierarchical Message Passing Networks, captures both fine-scale local dynamics and global structural effects critical for phenomena like beam bending or multi-body contact. We validate ROBIN on challenging 2D and 3D solid mechanics benchmarks involving geometric, material, and contact nonlinearities. ROBIN achieves state-of-the-art accuracy on all tasks, substantially outperforming existing next-step learned simulators while reducing inference time by up to an order of magnitude compared to standard diffusion simulators. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "M8zmlixh9y@OpenReview",
      "index": 139,
      "title": "Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining",
      "authors": [
        "Raghuveer Thirukovalluru",
        "Rui Meng",
        "Ye Liu",
        "Karthikeyan K",
        "Mingyi Su",
        "Ping Nie",
        "Semih Yavuz",
        "Yingbo Zhou",
        "Wenhu Chen",
        "Bhuwan Dhingra"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "batch",
        "negatives",
        "batches",
        "examples",
        "barrier",
        "contrastive",
        "embedding",
        "mmeb",
        "positives",
        "breaking"
      ],
      "summary": "Contrastive learning (CL) is a prevalent technique for training embedding models, which pulls semantically similar examples (positives) closer in the representation space while pushing dissimilar ones (negatives) further apart. A key source of negatives are \"in-batch\" examples, i.e., positives from other examples in the batch. Effectiveness of such models is hence strongly influenced by the size and quality of training batches. In this work, we propose *Breaking the Batch Barrier* (B3), a novel batch construction strategy designed to curate high-quality batches for CL. Our approach begins by using a pretrained teacher embedding model to rank all examples in the dataset, from which a sparse similarity graph is constructed. A community detection algorithm is then applied to this graph to identify clusters of examples that serve as strong negatives for one another. The clusters are then used to construct batches that are rich in in-batch negatives. Empirical results on the MMEB multimodal embedding benchmark (36 tasks) demonstrate that our method sets a new state of the art, outperforming previous best methods by +1.3 and +2.9 points at the 7B and 2B model scales, respectively. Notably, models trained with B3 surpass existing state-of-the-art results even with a batch size as small as 64, which is 4–16× smaller than that required by other methods. Moreover, experiments show that B3 generalizes well across domains and tasks, maintaining strong performance even when trained with considerably weaker teachers.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M8zmlixh9y"
        ],
        "venue": [
          "/venue/M8zmlixh9y@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M8zmlixh9y"
        ],
        "detail": [
          "https://openreview.net/forum?id=M8zmlixh9y"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Raghuveer Thirukovalluru , Rui Meng , Ye Liu , Karthikeyan K , Mingyi Su , Ping Nie , Semih Yavuz , Yingbo Zhou , Wenhu Chen , Bhuwan Dhingra Contrastive learning (CL) is a prevalent technique for training embedding models, which pulls semantically similar examples (positives) closer in the representation space while pushing dissimilar ones (negatives) further apart. A key source of negatives are \"in-batch\" examples, i.e., positives from other examples in the batch. Effectiveness of such models is hence strongly influenced by the size and quality of training batches. In this work, we propose *Breaking the Batch Barrier* (B3), a novel batch construction strategy designed to curate high-quality batches for CL. Our approach begins by using a pretrained teacher embedding model to rank all examples in the dataset, from which a sparse similarity graph is constructed. A community detection algorithm is then applied to this graph to identify clusters of examples that serve as strong negatives for one another. The clusters are then used to construct batches that are rich in in-batch negatives. Empirical results on the MMEB multimodal embedding benchmark (36 tasks) demonstrate that our method sets a new state of the art, outperforming previous best methods by +1.3 and +2.9 points at the 7B and 2B model scales, respectively. Notably, models trained with B3 surpass existing state-of-the-art results even with a batch size as small as 64, which is 4–16× smaller than that required by other methods. Moreover, experiments show that B3 generalizes well across domains and tasks, maintaining strong performance even when trained with considerably weaker teachers. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "T1GXVrXJR4@OpenReview",
      "index": 140,
      "title": "Dynamic Algorithm for Explainable k k -medians Clustering under ℓ p ℓ p Norm",
      "authors": [
        "Konstantin Makarychev",
        "Ilias Papanikolaou",
        "Liren Shan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "medians",
        "log",
        "explainable",
        "algorithm",
        "clustering",
        "ell",
        "dynamic",
        "tilde",
        "rashtchian",
        "norm"
      ],
      "summary": "We study the problem of explainable k k -medians clustering introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). In this problem, the goal is to construct a threshold decision tree that partitions data into k k clusters while minimizing the k k -medians objective. These trees are interpretable because each internal node makes a simple decision by thresholding a single feature, allowing users to trace and understand how each point is assigned to a cluster. We present the first algorithm for explainable k k -medians under ℓ p ℓ p norm for every finite p ≥ 1 p ≥ 1 . Our algorithm achieves an O ~ ( p ( log k ) 1 + 1 / p − 1 / p 2 ) O ~ ( p ( log ⁡ k ) 1 + 1 / p − 1 / p 2 ) approximation to the optimal k k -medians cost for any p ≥ 1 p ≥ 1 . Previously, algorithms were known only for p = 1 p = 1 and p = 2 p = 2 . For p = 2 p = 2 , our algorithm improves upon the existing bound of O ~ ( log 3 / 2 k ) O ~ ( log 3 / 2 ⁡ k ) , and for p = 1 p = 1 , it matches the tight bound of log k + O ( 1 ) log ⁡ k + O ( 1 ) up to a multiplicative O ( log log k ) O ( log ⁡ log ⁡ k ) factor. We show how to implement our algorithm in a dynamic setting. The dynamic algorithm maintains an explainable clustering under a sequence of insertions and deletions, with amortized update time O ( d log 3 k ) O ( d log 3 ⁡ k ) and O ( log k ) O ( log ⁡ k ) recourse, making it suitable for large-scale and evolving datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=T1GXVrXJR4"
        ],
        "venue": [
          "/venue/T1GXVrXJR4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=T1GXVrXJR4"
        ],
        "detail": [
          "https://openreview.net/forum?id=T1GXVrXJR4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Dynamic Algorithm for Explainable k k -medians Clustering under ℓ p ℓ p Norm [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Konstantin Makarychev , Ilias Papanikolaou , Liren Shan We study the problem of explainable k k -medians clustering introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). In this problem, the goal is to construct a threshold decision tree that partitions data into k k clusters while minimizing the k k -medians objective. These trees are interpretable because each internal node makes a simple decision by thresholding a single feature, allowing users to trace and understand how each point is assigned to a cluster. We present the first algorithm for explainable k k -medians under ℓ p ℓ p norm for every finite p ≥ 1 p ≥ 1 . Our algorithm achieves an O ~ ( p ( log k ) 1 + 1 / p − 1 / p 2 ) O ~ ( p ( log ⁡ k ) 1 + 1 / p − 1 / p 2 ) approximation to the optimal k k -medians cost for any p ≥ 1 p ≥ 1 . Previously, algorithms were known only for p = 1 p = 1 and p = 2 p = 2 . For p = 2 p = 2 , our algorithm improves upon the existing bound of O ~ ( log 3 / 2 k ) O ~ ( log 3 / 2 ⁡ k ) , and for p = 1 p = 1 , it matches the tight bound of log k + O ( 1 ) log ⁡ k + O ( 1 ) up to a multiplicative O ( log log k ) O ( log ⁡ log ⁡ k ) factor. We show how to implement our algorithm in a dynamic setting. The dynamic algorithm maintains an explainable clustering under a sequence of insertions and deletions, with amortized update time O ( d log 3 k ) O ( d log 3 ⁡ k ) and O ( log k ) O ( log ⁡ k ) recourse, making it suitable for large-scale and evolving datasets. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "x9vcgXmRD0@OpenReview",
      "index": 141,
      "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
      "authors": [
        "Mantas Mazeika",
        "Xuwang Yin",
        "Rishub Tamirisa",
        "Jaehyuk Lim",
        "Bruce W. Lee",
        "Richard Ren",
        "Long Phan",
        "Norman Mu",
        "Oliver Zhang",
        "Dan Hendrycks"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ais",
        "emergent",
        "value",
        "utility",
        "systems",
        "values",
        "control",
        "utilities",
        "engineering",
        "goals"
      ],
      "summary": "As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=x9vcgXmRD0"
        ],
        "venue": [
          "/venue/x9vcgXmRD0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=x9vcgXmRD0"
        ],
        "detail": [
          "https://openreview.net/forum?id=x9vcgXmRD0"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs [PDF ] [Copy] [Kimi ] [REL] Authors : Mantas Mazeika , Xuwang Yin , Rishub Tamirisa , Jaehyuk Lim , Bruce W. Lee , Richard Ren , Long Phan , Norman Mu , Oliver Zhang , Dan Hendrycks As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "X0Etmtge6w@OpenReview",
      "index": 142,
      "title": "Is the acquisition worth the cost? Surrogate losses for Consistent Two-stage Classifiers",
      "authors": [
        "Florence Regol",
        "Joseph Cotnareanu",
        "Theodore Glavas",
        "Mark Coates"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ell",
        "01c",
        "hinge",
        "classifiers",
        "classifier",
        "defer",
        "loss",
        "surrogate",
        "access",
        "paid"
      ],
      "summary": "Recent years have witnessed the emergence of a spectrum of foundation models, covering a broad range of capabilities and costs. Often, we effectively use foundation models as feature generators and train classifiers that use the outputs of these models to make decisions. In this paper, we consider an increasingly relevant setting where we have two classifier stages. The first stage has access to features x x and has the option to make a classification decision or defer, while incurring a cost, to a second classifier that has access to features x x and z z . This is similar to the ``learning to defer'' setting, with the important difference that we train both classifiers jointly, and the second classifier has access to more information. The natural loss for this setting is an ℓ 01 c ℓ 01 c loss, where a penalty is paid for incorrect classification, as in ℓ 01 ℓ 01 , but an additional penalty c c is paid for consulting the second classifier. The ℓ 01 c ℓ 01 c loss is unwieldy for training. Our primary contribution in this paper is the derivation of a hinge-based surrogate loss ℓ c h i n g e ℓ h i n g e c that is much more amenable to training but also satisfies the property that ℓ c h i n g e ℓ h i n g e c -consistency implies ℓ 01 c ℓ 01 c -consistency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=X0Etmtge6w"
        ],
        "venue": [
          "/venue/X0Etmtge6w@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=X0Etmtge6w"
        ],
        "detail": [
          "https://openreview.net/forum?id=X0Etmtge6w"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Is the acquisition worth the cost? Surrogate losses for Consistent Two-stage Classifiers [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Florence Regol , Joseph Cotnareanu , Theodore Glavas , Mark Coates Recent years have witnessed the emergence of a spectrum of foundation models, covering a broad range of capabilities and costs. Often, we effectively use foundation models as feature generators and train classifiers that use the outputs of these models to make decisions. In this paper, we consider an increasingly relevant setting where we have two classifier stages. The first stage has access to features x x and has the option to make a classification decision or defer, while incurring a cost, to a second classifier that has access to features x x and z z . This is similar to the ``learning to defer'' setting, with the important difference that we train both classifiers jointly, and the second classifier has access to more information. The natural loss for this setting is an ℓ 01 c ℓ 01 c loss, where a penalty is paid for incorrect classification, as in ℓ 01 ℓ 01 , but an additional penalty c c is paid for consulting the second classifier. The ℓ 01 c ℓ 01 c loss is unwieldy for training. Our primary contribution in this paper is the derivation of a hinge-based surrogate loss ℓ c h i n g e ℓ h i n g e c that is much more amenable to training but also satisfies the property that ℓ c h i n g e ℓ h i n g e c -consistency implies ℓ 01 c ℓ 01 c -consistency. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "JbJVWljk7r@OpenReview",
      "index": 143,
      "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training",
      "authors": [
        "Jintao Zhang",
        "Jia wei",
        "Haoxu Wang",
        "Pengle Zhang",
        "Xiaoming Xu",
        "Haofeng Huang",
        "Kai Jiang",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "texttt",
        "fp4",
        "attention",
        "bit",
        "sageattention",
        "rtx5090",
        "microscaling",
        "tasks",
        "sageattention3",
        "flashattention3"
      ],
      "summary": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 1038 TOPS TOPS on RTX5090 RTX5090 , which is a 5 × 5 × speedup over the fastest FlashAttention on RTX5090 RTX5090 . Experiments show that our FP4 FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JbJVWljk7r"
        ],
        "venue": [
          "/venue/JbJVWljk7r@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JbJVWljk7r"
        ],
        "detail": [
          "https://openreview.net/forum?id=JbJVWljk7r"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jintao Zhang , Jia wei , Haoxu Wang , Pengle Zhang , Xiaoming Xu , Haofeng Huang , Kai Jiang , Jun Zhu , Jianfei Chen The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 1038 TOPS TOPS on RTX5090 RTX5090 , which is a 5 × 5 × speedup over the fastest FlashAttention on RTX5090 RTX5090 . Experiments show that our FP4 FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "e8R0ytPhLv@OpenReview",
      "index": 144,
      "title": "Eluder dimension: localise it!",
      "authors": [
        "Alireza Bakhtiari",
        "Alex Ayoub",
        "Samuel McLaughlin Robertson",
        "David Janz",
        "Csaba Szepesvari"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "eluder",
        "dimension",
        "localise",
        "bounds",
        "localisation",
        "immediately",
        "recovers",
        "bernoulli",
        "bandits",
        "genuine"
      ],
      "summary": "We establish a lower bound on the eluder dimension in generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=e8R0ytPhLv"
        ],
        "venue": [
          "/venue/e8R0ytPhLv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=e8R0ytPhLv"
        ],
        "detail": [
          "https://openreview.net/forum?id=e8R0ytPhLv"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Eluder dimension: localise it! [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Alireza Bakhtiari , Alex Ayoub , Samuel McLaughlin Robertson , David Janz , Csaba Szepesvari We establish a lower bound on the eluder dimension in generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "X4SCxcgb3O@OpenReview",
      "index": 145,
      "title": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo",
      "authors": [
        "Zachary Charles",
        "Gabriel Teston",
        "Lucio M. Dery",
        "J Keith Rush",
        "Nova Fallen",
        "Zachary Garrett",
        "Arthur Szlam",
        "Arthur Douillard"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "diloco",
        "scaling",
        "budget",
        "robustly",
        "training",
        "model",
        "parallel",
        "scales",
        "laws",
        "synchronization"
      ],
      "summary": "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=X4SCxcgb3O"
        ],
        "venue": [
          "/venue/X4SCxcgb3O@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=X4SCxcgb3O"
        ],
        "detail": [
          "https://openreview.net/forum?id=X4SCxcgb3O"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo [PDF ] [Copy] [Kimi ] [REL] Authors : Zachary Charles , Gabriel Teston , Lucio M. Dery , J Keith Rush , Nova Fallen , Zachary Garrett , Arthur Szlam , Arthur Douillard As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ogZu06NgQs@OpenReview",
      "index": 146,
      "title": "FlashMD: long-stride, universal prediction of molecular dynamics",
      "authors": [
        "Filippo Bigi",
        "Sanggyu Chong",
        "Agustinus Kristiadi",
        "Michele Ceriotti"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "flashmd",
        "stride",
        "time",
        "dynamics",
        "molecular",
        "atomic",
        "forces",
        "steps",
        "minuscule",
        "processes"
      ],
      "summary": "Molecular dynamics (MD) provides insights into atomic-scale processes by integrating over time the equations that describe the motion of atoms under the action of interatomic forces. Machine learning models have substantially accelerated MD by providing inexpensive predictions of the forces, but they remain constrained to minuscule time integration steps, which are required by the fast time scale of atomic motion. In this work, we propose FlashMD, a method to predict the evolution of positions and momenta over strides that are between one and two orders of magnitude longer than typical MD time steps. We incorporate considerations on the mathematical and physical properties of Hamiltonian dynamics in the architecture, generalize the approach to allow the simulation of any thermodynamic ensemble, and carefully assess the possible failure modes of a direct MD approach. We validate FlashMD’s accuracy in reproducing equilibrium and time‐dependent properties, using both system‐specific and general-purpose models, extending the ability of MD simulation to reach the long time scales needed to model microscopic processes of high scientific and technological relevance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ogZu06NgQs"
        ],
        "venue": [
          "/venue/ogZu06NgQs@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ogZu06NgQs"
        ],
        "detail": [
          "https://openreview.net/forum?id=ogZu06NgQs"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "FlashMD: long-stride, universal prediction of molecular dynamics [PDF ] [Copy] [Kimi ] [REL] Authors : Filippo Bigi , Sanggyu Chong , Agustinus Kristiadi , Michele Ceriotti Molecular dynamics (MD) provides insights into atomic-scale processes by integrating over time the equations that describe the motion of atoms under the action of interatomic forces. Machine learning models have substantially accelerated MD by providing inexpensive predictions of the forces, but they remain constrained to minuscule time integration steps, which are required by the fast time scale of atomic motion. In this work, we propose FlashMD, a method to predict the evolution of positions and momenta over strides that are between one and two orders of magnitude longer than typical MD time steps. We incorporate considerations on the mathematical and physical properties of Hamiltonian dynamics in the architecture, generalize the approach to allow the simulation of any thermodynamic ensemble, and carefully assess the possible failure modes of a direct MD approach. We validate FlashMD’s accuracy in reproducing equilibrium and time‐dependent properties, using both system‐specific and general-purpose models, extending the ability of MD simulation to reach the long time scales needed to model microscopic processes of high scientific and technological relevance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kQokjfoGjk@OpenReview",
      "index": 147,
      "title": "Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting",
      "authors": [
        "Yuxuan Yang",
        "Dalin Zhang",
        "Yuxuan Liang",
        "Hua Lu",
        "Gang Chen",
        "Huan Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tsf",
        "scam",
        "self",
        "supervised",
        "labels",
        "datasets",
        "forecasting",
        "sudis",
        "series",
        "constructing"
      ],
      "summary": "Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. The code is available at https://github.com/SuDIS-ZJU/SCAM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kQokjfoGjk"
        ],
        "venue": [
          "/venue/kQokjfoGjk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kQokjfoGjk"
        ],
        "detail": [
          "https://openreview.net/forum?id=kQokjfoGjk"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Yuxuan Yang , Dalin Zhang , Yuxuan Liang , Hua Lu , Gang Chen , Huan Li Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. The code is available at https://github.com/SuDIS-ZJU/SCAM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kXJd4JxF34@OpenReview",
      "index": 148,
      "title": "3D Equivariant Visuomotor Policy Learning via Spherical Projection",
      "authors": [
        "Boce Hu",
        "Dian Wang",
        "David Klee",
        "Heng Tian",
        "Xupeng Zhu",
        "Haojie Huang",
        "Robert Platt",
        "Robin Walters"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "policy",
        "equivariant",
        "rgb",
        "visuomotor",
        "cloud",
        "textbf",
        "gopro",
        "inputs",
        "camera",
        "sphere"
      ],
      "summary": "Equivariant models have recently been shown to improve the data efficiency of diffusion policy by a significant margin. However, prior work that explored this direction focused primarily on point cloud inputs generated by multiple cameras fixed in the workspace. This type of point cloud input is not compatible with the now-common setting where the primary input modality is an eye-in-hand RGB camera like a GoPro. This paper closes this gap by incorporating into the diffusion policy model a process that projects features from the 2D RGB camera image onto a sphere. This enables us to reason about symmetries in S O ( 3 ) S O ( 3 ) without explicitly reconstructing a point cloud. We perform extensive experiments in both simulation and the real world that demonstrate that our method consistently outperforms strong baselines in terms of both performance and sample efficiency. Our work, Image-to-Sphere Policy Image-to-Sphere Policy ( ISP ISP ), is the first S O ( 3 ) S O ( 3 ) -equivariant policy learning framework for robotic manipulation that works using only monocular RGB inputs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kXJd4JxF34"
        ],
        "venue": [
          "/venue/kXJd4JxF34@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kXJd4JxF34"
        ],
        "detail": [
          "https://openreview.net/forum?id=kXJd4JxF34"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "3D Equivariant Visuomotor Policy Learning via Spherical Projection [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Boce Hu , Dian Wang , David Klee , Heng Tian , Xupeng Zhu , Haojie Huang , Robert Platt , Robin Walters Equivariant models have recently been shown to improve the data efficiency of diffusion policy by a significant margin. However, prior work that explored this direction focused primarily on point cloud inputs generated by multiple cameras fixed in the workspace. This type of point cloud input is not compatible with the now-common setting where the primary input modality is an eye-in-hand RGB camera like a GoPro. This paper closes this gap by incorporating into the diffusion policy model a process that projects features from the 2D RGB camera image onto a sphere. This enables us to reason about symmetries in S O ( 3 ) S O ( 3 ) without explicitly reconstructing a point cloud. We perform extensive experiments in both simulation and the real world that demonstrate that our method consistently outperforms strong baselines in terms of both performance and sample efficiency. Our work, Image-to-Sphere Policy Image-to-Sphere Policy ( ISP ISP ), is the first S O ( 3 ) S O ( 3 ) -equivariant policy learning framework for robotic manipulation that works using only monocular RGB inputs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2Gnp8sdwVe@OpenReview",
      "index": 149,
      "title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs",
      "authors": [
        "Houyi Li",
        "Wenzhen Zheng",
        "Qiufeng Wang",
        "Zhenyu Ding",
        "Haoying Wang",
        "Zili Wang",
        "Shijie Xuyang",
        "Ning Ding",
        "Shuigeng Zhou",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "farseer",
        "law",
        "scaling",
        "chinchilla",
        "llms",
        "refined",
        "extrapolation",
        "scale",
        "wandb",
        "predictable"
      ],
      "summary": "Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation. To bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface L ( N , D ) L ( N , D ) , Farseer achieves a significantly better fit to empirical data than prior laws (e.g., \\Chinchilla's law). Our methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, outperforming Chinchilla's law, whose extrapolation error is 433\\% higher. This allows for the reliable evaluation of competing training strategies across all ( N , D ) ( N , D ) settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. Furthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. To foster further research, we are comprehensively open-sourcing all code, data, results (https://github.com/Farseer-Scaling-Law/Farseer), all training logs (https://wandb.ai/billzid/Farseer?nw=nwuserbillzid), all models used in scaling law fitting (https://huggingface.co/Farseer-Scaling-Law).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2Gnp8sdwVe"
        ],
        "venue": [
          "/venue/2Gnp8sdwVe@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2Gnp8sdwVe"
        ],
        "detail": [
          "https://openreview.net/forum?id=2Gnp8sdwVe"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Houyi Li , Wenzhen Zheng , Qiufeng Wang , Zhenyu Ding , Haoying Wang , Zili Wang , Shijie Xuyang , Ning Ding , Shuigeng Zhou , Xiangyu Zhang , Daxin Jiang Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation. To bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface L ( N , D ) L ( N , D ) , Farseer achieves a significantly better fit to empirical data than prior laws (e.g., \\Chinchilla's law). Our methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, outperforming Chinchilla's law, whose extrapolation error is 433\\% higher. This allows for the reliable evaluation of competing training strategies across all ( N , D ) ( N , D ) settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. Furthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. To foster further research, we are comprehensively open-sourcing all code, data, results (https://github.com/Farseer-Scaling-Law/Farseer), all training logs (https://wandb.ai/billzid/Farseer?nw=nwuserbillzid), all models used in scaling law fitting (https://huggingface.co/Farseer-Scaling-Law). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nfhmjdZUbQ@OpenReview",
      "index": 150,
      "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees",
      "authors": [
        "Sangwoo Park",
        "Matteo Zecchin",
        "Osvaldo Simeone"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "autoeval",
        "texttt",
        "llms",
        "reliability",
        "autoevaluators",
        "efficiency",
        "judges",
        "powered",
        "autoevaluation",
        "guarantees"
      ],
      "summary": "Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference ( PPI PPI ) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose R-AutoEval+ R-AutoEval+ , a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of R-AutoEval+ R-AutoEval+ is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of R-AutoEval+ R-AutoEval+ .",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nfhmjdZUbQ"
        ],
        "venue": [
          "/venue/nfhmjdZUbQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nfhmjdZUbQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=nfhmjdZUbQ"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Sangwoo Park , Matteo Zecchin , Osvaldo Simeone Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference ( PPI PPI ) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose R-AutoEval+ R-AutoEval+ , a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of R-AutoEval+ R-AutoEval+ is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of R-AutoEval+ R-AutoEval+ . Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "mf0p4PO7ko@OpenReview",
      "index": 151,
      "title": "Ridge Boosting is Both Robust and Efficient",
      "authors": [
        "David Bruns-Smith",
        "Zhongming Xie",
        "Avi Feller"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ridge",
        "boosting",
        "target",
        "semiparametric",
        "distributional",
        "multiaccuracy",
        "estimators",
        "efficiency",
        "shifts",
        "efficient"
      ],
      "summary": "Estimators in statistics and machine learning must typically trade off between efficiency, having low variance for a fixed target, and distributional robustness, such as \\textit{multiaccuracy}, or having low bias over a range of possible targets. In this paper, we consider a simple estimator, \\emph{ridge boosting}: starting with any initial predictor, perform a single boosting step with (kernel) ridge regression. Surprisingly, we show that ridge boosting simultaneously achieves both efficiency and distributional robustness: for target distribution shifts that lie within an RKHS unit ball, this estimator maintains low bias across all such shifts and has variance at the semiparametric efficiency bound for each target. In addition to bridging otherwise distinct research areas, this result has immediate practical value. Since ridge boosting uses only data from the source distribution, researchers can train a single model to obtain both robust and efficient estimates for multiple target estimands at the same time, eliminating the need to fit separate semiparametric efficient estimators for each target. We assess this approach through simulations and an application estimating the age profile of retirement income.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mf0p4PO7ko"
        ],
        "venue": [
          "/venue/mf0p4PO7ko@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mf0p4PO7ko"
        ],
        "detail": [
          "https://openreview.net/forum?id=mf0p4PO7ko"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Ridge Boosting is Both Robust and Efficient [PDF ] [Copy] [Kimi ] [REL] Authors : David Bruns-Smith , Zhongming Xie , Avi Feller Estimators in statistics and machine learning must typically trade off between efficiency, having low variance for a fixed target, and distributional robustness, such as \\textit{multiaccuracy}, or having low bias over a range of possible targets. In this paper, we consider a simple estimator, \\emph{ridge boosting}: starting with any initial predictor, perform a single boosting step with (kernel) ridge regression. Surprisingly, we show that ridge boosting simultaneously achieves both efficiency and distributional robustness: for target distribution shifts that lie within an RKHS unit ball, this estimator maintains low bias across all such shifts and has variance at the semiparametric efficiency bound for each target. In addition to bridging otherwise distinct research areas, this result has immediate practical value. Since ridge boosting uses only data from the source distribution, researchers can train a single model to obtain both robust and efficient estimates for multiple target estimands at the same time, eliminating the need to fit separate semiparametric efficient estimators for each target. We assess this approach through simulations and an application estimating the age profile of retirement income. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "pH3daDPj4c@OpenReview",
      "index": 152,
      "title": "A Unifying View of Linear Function Approximation in Off-Policy RL Through Matrix Splitting and Preconditioning",
      "authors": [
        "Zechen Wu",
        "Amy Greenwald",
        "Ronald Parr"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fqi",
        "convergence",
        "pfqi",
        "policy",
        "splitting",
        "preconditioner",
        "matrix",
        "linear",
        "function",
        "preconditioning"
      ],
      "summary": "In off-policy policy evaluation (OPE) tasks within reinforcement learning, Temporal Difference Learning(TD) and Fitted Q-Iteration (FQI) have traditionally been viewed as differing in the number of updates toward the target value function: TD makes one update, FQI makes an infinite number, and Partial Fitted Q-Iteration (PFQI) performs a finite number. We show that this view is not accurate, and provide a new mathematical perspective under linear value function approximation that unifies these methods as a single iterative method solving same linear system, but using different matrix splitting schemes and preconditioners. We show that increasing the number of updates under the same target value function, i.e., the target network technique, is a transition from using a constant preconditioner to using a data-feature adaptive preconditioner. This elucidates, for the first time, why TD convergence does not necessarily imply FQI convergence, and establishes tight convergence connections among TD, PFQI, and FQI. Our framework enables sharper theoretical results than previous work and characterization of the convergence conditions for each algorithm, without relying on assumptions about the features (e.g., linear independence). We also provide an encoder-decoder perspective to better understand TD’s convergence conditions, and prove, for the first time, that when a large learning rate doesn’t work, trying a smaller one may help(for batch TD). Our framework also leads to the discovery of new crucial conditions on features for convergence, and shows how common assumptions about features influence convergence, e.g., the assumption of linearly independent features can be dropped without compromising the convergence guarantees of stochastic TD in the on-policy setting. This paper is also the first to introduce matrix splitting into the convergence analysis of these algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pH3daDPj4c"
        ],
        "venue": [
          "/venue/pH3daDPj4c@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pH3daDPj4c"
        ],
        "detail": [
          "https://openreview.net/forum?id=pH3daDPj4c"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "A Unifying View of Linear Function Approximation in Off-Policy RL Through Matrix Splitting and Preconditioning [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Zechen Wu , Amy Greenwald , Ronald Parr In off-policy policy evaluation (OPE) tasks within reinforcement learning, Temporal Difference Learning(TD) and Fitted Q-Iteration (FQI) have traditionally been viewed as differing in the number of updates toward the target value function: TD makes one update, FQI makes an infinite number, and Partial Fitted Q-Iteration (PFQI) performs a finite number. We show that this view is not accurate, and provide a new mathematical perspective under linear value function approximation that unifies these methods as a single iterative method solving same linear system, but using different matrix splitting schemes and preconditioners. We show that increasing the number of updates under the same target value function, i.e., the target network technique, is a transition from using a constant preconditioner to using a data-feature adaptive preconditioner. This elucidates, for the first time, why TD convergence does not necessarily imply FQI convergence, and establishes tight convergence connections among TD, PFQI, and FQI. Our framework enables sharper theoretical results than previous work and characterization of the convergence conditions for each algorithm, without relying on assumptions about the features (e.g., linear independence). We also provide an encoder-decoder perspective to better understand TD’s convergence conditions, and prove, for the first time, that when a large learning rate doesn’t work, trying a smaller one may help(for batch TD). Our framework also leads to the discovery of new crucial conditions on features for convergence, and shows how common assumptions about features influence convergence, e.g., the assumption of linearly independent features can be dropped without compromising the convergence guarantees of stochastic TD in the on-policy setting. This paper is also the first to introduce matrix splitting into the convergence analysis of these algorithms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5k0AHYc4MJ@OpenReview",
      "index": 153,
      "title": "Generalizable Reasoning through Compositional Energy Minimization",
      "authors": [
        "Alexandru Oarga",
        "Yilun Du"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "energy",
        "compositional",
        "subproblems",
        "minimization",
        "landscapes",
        "generalization",
        "problems",
        "landscape",
        "generalizable"
      ],
      "summary": "Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5k0AHYc4MJ"
        ],
        "venue": [
          "/venue/5k0AHYc4MJ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5k0AHYc4MJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=5k0AHYc4MJ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Generalizable Reasoning through Compositional Energy Minimization [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Alexandru Oarga , Yilun Du Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/ Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rwmVd8BKW5@OpenReview",
      "index": 154,
      "title": "Universal Sequence Preconditioning",
      "authors": [
        "Annie Marsden",
        "Elad Hazan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "preconditioning",
        "hidden",
        "sequence",
        "convolves",
        "regret",
        "applying",
        "prediction",
        "dynamical",
        "input",
        "transition"
      ],
      "summary": "We study the problem of preconditioning in the setting of sequential prediction. From the theoretical lens of linear dynamical systems, we show that applying a convolution to the input sequence translates to applying a polynomial to the unknown transition matrix in the hidden space. With this insight, we develop a novel preconditioning method that convolves the input sequence with the coefficients of the Chebyshev or Legendre polynomials. We formally prove that this improves the regret of a wide family of prediction methods. We proceed to apply this preconditioning technique to the method of spectral filtering. This gives the first sublinear regret bound that is also hidden-dimension free (up to logarithmic factors) even when the hidden transition matrix is asymmetric. From rigorous experiments on synthetic data we show that our simple preconditioning method generalizes to both 1) settings where the data is \\emph{not} from a linear dynamical system and 2) a broad range of learning algorithms, including recurrent neural networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rwmVd8BKW5"
        ],
        "venue": [
          "/venue/rwmVd8BKW5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rwmVd8BKW5"
        ],
        "detail": [
          "https://openreview.net/forum?id=rwmVd8BKW5"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Universal Sequence Preconditioning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Annie Marsden , Elad Hazan We study the problem of preconditioning in the setting of sequential prediction. From the theoretical lens of linear dynamical systems, we show that applying a convolution to the input sequence translates to applying a polynomial to the unknown transition matrix in the hidden space. With this insight, we develop a novel preconditioning method that convolves the input sequence with the coefficients of the Chebyshev or Legendre polynomials. We formally prove that this improves the regret of a wide family of prediction methods. We proceed to apply this preconditioning technique to the method of spectral filtering. This gives the first sublinear regret bound that is also hidden-dimension free (up to logarithmic factors) even when the hidden transition matrix is asymmetric. From rigorous experiments on synthetic data we show that our simple preconditioning method generalizes to both 1) settings where the data is \\emph{not} from a linear dynamical system and 2) a broad range of learning algorithms, including recurrent neural networks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fmnxunacr4@OpenReview",
      "index": 155,
      "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles",
      "authors": [
        "Jiangjie Chen",
        "Qianyu He",
        "Siyu Yuan",
        "Aili Chen",
        "Zhicheng Cai",
        "Weinan Dai",
        "Hongli Yu",
        "Jiaze Chen",
        "Xuefeng Li",
        "Qiying Yu",
        "Hao Zhou",
        "Mingxuan Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "enigmata",
        "reasoning",
        "rlvr",
        "puzzle",
        "verifiable",
        "puzzles",
        "verifier",
        "eval",
        "agi",
        "logical"
      ],
      "summary": "Large Language Models (LLMs), such as OpenAI’s o1 and DeepSeek’s R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce ENIGMATA, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across 7 categories, each with: 1) a generator that produces unlimited examples with controllable difficulty, and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose ENIGMATA-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-ENIGMATA, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like ENIGMATA-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from ENIGMATA further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of ENIGMATA. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Project page: https://seed-enigmata.github.io.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fmnxunacr4"
        ],
        "venue": [
          "/venue/fmnxunacr4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fmnxunacr4"
        ],
        "detail": [
          "https://openreview.net/forum?id=fmnxunacr4"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Jiangjie Chen , Qianyu He , Siyu Yuan , Aili Chen , Zhicheng Cai , Weinan Dai , Hongli Yu , Jiaze Chen , Xuefeng Li , Qiying Yu , Hao Zhou , Mingxuan Wang Large Language Models (LLMs), such as OpenAI’s o1 and DeepSeek’s R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce ENIGMATA, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across 7 categories, each with: 1) a generator that produces unlimited examples with controllable difficulty, and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose ENIGMATA-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-ENIGMATA, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like ENIGMATA-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from ENIGMATA further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of ENIGMATA. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Project page: https://seed-enigmata.github.io. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "p7WHZy8TCG@OpenReview",
      "index": 156,
      "title": "Memory-Enhanced Neural Solvers for Routing Problems",
      "authors": [
        "Felix Chalumeau",
        "Refiloe Shabe",
        "Noah De Nicola",
        "Arnu Pretorius",
        "Thomas D Barrett",
        "Nathan Grinsztajn"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "memento",
        "solvers",
        "routing",
        "heuristics",
        "scalability",
        "budget",
        "problems",
        "memory",
        "instances",
        "tuning"
      ],
      "summary": "Routing Problems are central to many real-world applications, yet remain challenging due to their (NP-)hard nature. Amongst existing approaches, heuristics often offer the best trade-off between quality and scalability, making them suitable for industrial use. While Reinforcement Learning (RL) offers a flexible framework for designing heuristics, its adoption over handcrafted heuristics remains incomplete. Existing learned methods still lack the ability to adapt to specific instances and fully leverage the available computational budget. Current best methods either rely on a collection of pre-trained policies, or on RL fine-tuning; hence failing to fully utilize newly available information within the constraints of the budget. In response, we present MEMENTO, an approach that leverages memory to improve the search of neural solvers at inference. MEMENTO updates the action distribution dynamically based on the outcome of previous decisions. We validate its effectiveness on Traveling Salesman and Capacitated Vehicle Routing problems, demonstrating its superiority over tree-search and policy-gradient fine-tuning; and showing that it can be zero-shot combined with diversity-based solvers. We successfully train all RL auto-regressive solvers on large instances, and verify MEMENTO's scalability and data-efficiency: pushing the state-of-the-art on 11 out of 12 evaluated tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=p7WHZy8TCG"
        ],
        "venue": [
          "/venue/p7WHZy8TCG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=p7WHZy8TCG"
        ],
        "detail": [
          "https://openreview.net/forum?id=p7WHZy8TCG"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Memory-Enhanced Neural Solvers for Routing Problems [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Felix Chalumeau , Refiloe Shabe , Noah De Nicola , Arnu Pretorius , Thomas D Barrett , Nathan Grinsztajn Routing Problems are central to many real-world applications, yet remain challenging due to their (NP-)hard nature. Amongst existing approaches, heuristics often offer the best trade-off between quality and scalability, making them suitable for industrial use. While Reinforcement Learning (RL) offers a flexible framework for designing heuristics, its adoption over handcrafted heuristics remains incomplete. Existing learned methods still lack the ability to adapt to specific instances and fully leverage the available computational budget. Current best methods either rely on a collection of pre-trained policies, or on RL fine-tuning; hence failing to fully utilize newly available information within the constraints of the budget. In response, we present MEMENTO, an approach that leverages memory to improve the search of neural solvers at inference. MEMENTO updates the action distribution dynamically based on the outcome of previous decisions. We validate its effectiveness on Traveling Salesman and Capacitated Vehicle Routing problems, demonstrating its superiority over tree-search and policy-gradient fine-tuning; and showing that it can be zero-shot combined with diversity-based solvers. We successfully train all RL auto-regressive solvers on large instances, and verify MEMENTO's scalability and data-efficiency: pushing the state-of-the-art on 11 out of 12 evaluated tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oHbVboLXz6@OpenReview",
      "index": 157,
      "title": "Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition",
      "authors": [
        "Yu Li",
        "Jin Jiang",
        "Jianhua Zhu",
        "Shuai Peng",
        "Baole Wei",
        "Yuxuan Zhou",
        "Liangcai Gao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mumer",
        "uni",
        "hmer",
        "handwritten",
        "recognition",
        "vlm",
        "unified",
        "gemini2",
        "task",
        "symbol"
      ],
      "summary": "Handwritten Mathematical Expression Recognition (HMER) remains a persistent challenge in Optical Character Recognition (OCR) due to the inherent freedom of symbol layouts and variability in handwriting styles. Prior methods have faced performance bottlenecks by proposing isolated architectural modifications, making them difficult to integrate coherently into a unified framework. Meanwhile, recent advances in pretrained vision-language models (VLMs) have demonstrated strong cross-task generalization, offering a promising foundation for developing unified solutions. In this paper, we introduce Uni-MuMER, which fully fine-tunes a VLM for the HMER task without modifying its architecture, effectively injecting domain-specific knowledge into a generalist framework. Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought (Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for reducing confusion among visually similar characters, and Symbol Counting (SC) for improving recognition consistency in long expressions. Experiments on the CROHME and HME100K datasets show that Uni-MuMER achieves super state-of-the-art performance, outperforming the best lightweight specialized model SSAN by 16.31\\% and the top-performing VLM Gemini2.5-flash by 24.42\\% under zero-shot setting. Our datasets, models, and code are open-sourced at: https://github.com/BFlameSwift/Uni-MuMER",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oHbVboLXz6"
        ],
        "venue": [
          "/venue/oHbVboLXz6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oHbVboLXz6"
        ],
        "detail": [
          "https://openreview.net/forum?id=oHbVboLXz6"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yu Li , Jin Jiang , Jianhua Zhu , Shuai Peng , Baole Wei , Yuxuan Zhou , Liangcai Gao Handwritten Mathematical Expression Recognition (HMER) remains a persistent challenge in Optical Character Recognition (OCR) due to the inherent freedom of symbol layouts and variability in handwriting styles. Prior methods have faced performance bottlenecks by proposing isolated architectural modifications, making them difficult to integrate coherently into a unified framework. Meanwhile, recent advances in pretrained vision-language models (VLMs) have demonstrated strong cross-task generalization, offering a promising foundation for developing unified solutions. In this paper, we introduce Uni-MuMER, which fully fine-tunes a VLM for the HMER task without modifying its architecture, effectively injecting domain-specific knowledge into a generalist framework. Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought (Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for reducing confusion among visually similar characters, and Symbol Counting (SC) for improving recognition consistency in long expressions. Experiments on the CROHME and HME100K datasets show that Uni-MuMER achieves super state-of-the-art performance, outperforming the best lightweight specialized model SSAN by 16.31\\% and the top-performing VLM Gemini2.5-flash by 24.42\\% under zero-shot setting. Our datasets, models, and code are open-sourced at: https://github.com/BFlameSwift/Uni-MuMER Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Xeb2EYBKkr@OpenReview",
      "index": 158,
      "title": "Two Heads are Better than One: Simulating Large Transformers with Small Ones",
      "authors": [
        "Hantao Yu",
        "Josh Alman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "transformers",
        "input",
        "sequences",
        "small",
        "heads",
        "take",
        "efficiently",
        "simulated",
        "simulating",
        "long"
      ],
      "summary": "The quadratic complexity of self‑attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences? In this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length N N can be efficiently simulated by only O ( ( N / M ) 2 ) O ( ( N / M ) 2 ) transformers with input length M ≪ N M ≪ N , and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number O ( N / M ) O ( N / M ) of small transformers suffice.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Xeb2EYBKkr"
        ],
        "venue": [
          "/venue/Xeb2EYBKkr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Xeb2EYBKkr"
        ],
        "detail": [
          "https://openreview.net/forum?id=Xeb2EYBKkr"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Two Heads are Better than One: Simulating Large Transformers with Small Ones [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hantao Yu , Josh Alman The quadratic complexity of self‑attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences? In this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length N N can be efficiently simulated by only O ( ( N / M ) 2 ) O ( ( N / M ) 2 ) transformers with input length M ≪ N M ≪ N , and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number O ( N / M ) O ( N / M ) of small transformers suffice. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "8rKSfL3GsK@OpenReview",
      "index": 159,
      "title": "Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span",
      "authors": [
        "Heeseung Yun",
        "Joonil Na",
        "Jaeyeon Kim",
        "Calvin Murdock",
        "Gunhee Kim"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "egocentric",
        "span",
        "visual",
        "forecasting",
        "egospanlift",
        "gaze",
        "perception",
        "planes",
        "multisensory",
        "364"
      ],
      "summary": "People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric gaze anticipation and 3D localization, while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8rKSfL3GsK"
        ],
        "venue": [
          "/venue/8rKSfL3GsK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8rKSfL3GsK"
        ],
        "detail": [
          "https://openreview.net/forum?id=8rKSfL3GsK"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Heeseung Yun , Joonil Na , Jaeyeon Kim , Calvin Murdock , Gunhee Kim People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric gaze anticipation and 3D localization, while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "o7Z8TClGjp@OpenReview",
      "index": 160,
      "title": "Unifying Proportional Fairness in Centroid and Non-Centroid Clustering",
      "authors": [
        "Benjamin Cookson",
        "Nisarg Shah",
        "Ziqi Yu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "centroid",
        "clustering",
        "proportional",
        "fjr",
        "fairness",
        "loss",
        "point",
        "paradigms",
        "distance",
        "caragiannis"
      ],
      "summary": "Proportional fairness criteria inspired by democratic ideals of proportional representation have received growing attention in the clustering literature. Prior work has investigated them in two separate paradigms. Chen et al. [ICML 2019] study _centroid clustering_, in which each data point's loss is determined by its distance to a representative point (centroid) chosen in its cluster. Caragiannis et al. [NeurIPS 2024] study _non-centroid clustering_, in which each data point's loss is determined by its maximum distance to any other data point in its cluster. We generalize both paradigms to introduce _semi-centroid clustering_, in which each data point's loss is a combination of its centroid and non-centroid losses, and study two proportional fairness criteria---the core and, its relaxation, fully justified representation (FJR). Our main result is a novel algorithm which achieves a constant approximation to the core, in polynomial time, even when the distance metrics used for centroid and non-centroid loss measurements are different. We also derive improved results for more restricted loss functions and the weaker FJR criterion, and establish lower bounds in each case.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=o7Z8TClGjp"
        ],
        "venue": [
          "/venue/o7Z8TClGjp@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=o7Z8TClGjp"
        ],
        "detail": [
          "https://openreview.net/forum?id=o7Z8TClGjp"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Unifying Proportional Fairness in Centroid and Non-Centroid Clustering [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Benjamin Cookson , Nisarg Shah , Ziqi Yu Proportional fairness criteria inspired by democratic ideals of proportional representation have received growing attention in the clustering literature. Prior work has investigated them in two separate paradigms. Chen et al. [ICML 2019] study _centroid clustering_, in which each data point's loss is determined by its distance to a representative point (centroid) chosen in its cluster. Caragiannis et al. [NeurIPS 2024] study _non-centroid clustering_, in which each data point's loss is determined by its maximum distance to any other data point in its cluster. We generalize both paradigms to introduce _semi-centroid clustering_, in which each data point's loss is a combination of its centroid and non-centroid losses, and study two proportional fairness criteria---the core and, its relaxation, fully justified representation (FJR). Our main result is a novel algorithm which achieves a constant approximation to the core, in polynomial time, even when the distance metrics used for centroid and non-centroid loss measurements are different. We also derive improved results for more restricted loss functions and the weaker FJR criterion, and establish lower bounds in each case. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "eumRwpgdMU@OpenReview",
      "index": 161,
      "title": "ARIA: Training Language Agents with Intention-driven Reward Aggregation",
      "authors": [
        "Ruihan Yang",
        "Yikai Zhang",
        "Aili Chen",
        "Xintao Wang",
        "Jiangjie Chen",
        "Siyu Yuan",
        "Deqing Yang",
        "Yanghua Xiao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "aria",
        "reward",
        "intention",
        "language",
        "negotiation",
        "space",
        "aggregation",
        "actions",
        "variance",
        "ewards"
      ],
      "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an extremely large and combinatorial action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose **ARIA**, a method that **A**ggregates **R**ewards in **I**ntention space to enable efficient and effective language **A**gents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering efficient and effective policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces gradient variance, but also delivers substantial performance gains of average 9.95% across four downstream tasks (e.g., negotiation and text-based games), consistently outperforming strong offline and online RL baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eumRwpgdMU"
        ],
        "venue": [
          "/venue/eumRwpgdMU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eumRwpgdMU"
        ],
        "detail": [
          "https://openreview.net/forum?id=eumRwpgdMU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "ARIA: Training Language Agents with Intention-driven Reward Aggregation [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Ruihan Yang , Yikai Zhang , Aili Chen , Xintao Wang , Jiangjie Chen , Siyu Yuan , Deqing Yang , Yanghua Xiao Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an extremely large and combinatorial action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose **ARIA**, a method that **A**ggregates **R**ewards in **I**ntention space to enable efficient and effective language **A**gents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering efficient and effective policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces gradient variance, but also delivers substantial performance gains of average 9.95% across four downstream tasks (e.g., negotiation and text-based games), consistently outperforming strong offline and online RL baselines. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QFjssnKdBI@OpenReview",
      "index": 162,
      "title": "Reasoning Planning for Language Models",
      "authors": [
        "Bao Nguyen",
        "Hieu Trung Nguyen",
        "Ruifeng She",
        "Xiaojin Fu",
        "Viet Anh Nguyen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "epic",
        "reasoning",
        "accuracy",
        "candidate",
        "nguyenngocbaocmt02",
        "planning",
        "aggregation",
        "language",
        "query",
        "generation"
      ],
      "summary": "Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QFjssnKdBI"
        ],
        "venue": [
          "/venue/QFjssnKdBI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QFjssnKdBI"
        ],
        "detail": [
          "https://openreview.net/forum?id=QFjssnKdBI"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Reasoning Planning for Language Models [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Bao Nguyen , Hieu Trung Nguyen , Ruifeng She , Xiaojin Fu , Viet Anh Nguyen Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fleQlZ2VTx@OpenReview",
      "index": 163,
      "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners",
      "authors": [
        "Weixiang Zhao",
        "Jiahe Guo",
        "Yang Deng",
        "Tongtong Wu",
        "Wenxuan Zhang",
        "Yulin Hu",
        "Xingyu Sui",
        "Yanyan Zhao",
        "Wanxiang Che",
        "Bing Qin",
        "Tat-Seng Chua",
        "Ting Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "language",
        "multilingual",
        "llms",
        "disentanglement",
        "reasoners",
        "disentangled",
        "languages",
        "ablating",
        "typologically"
      ],
      "summary": "Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-weight LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively disentangled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training methods such as supervised fine-tuning or reinforcement learning, our training-free language-reasoning disentanglement achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fleQlZ2VTx"
        ],
        "venue": [
          "/venue/fleQlZ2VTx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fleQlZ2VTx"
        ],
        "detail": [
          "https://openreview.net/forum?id=fleQlZ2VTx"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Weixiang Zhao , Jiahe Guo , Yang Deng , Tongtong Wu , Wenxuan Zhang , Yulin Hu , Xingyu Sui , Yanyan Zhao , Wanxiang Che , Bing Qin , Tat-Seng Chua , Ting Liu Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-weight LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively disentangled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training methods such as supervised fine-tuning or reinforcement learning, our training-free language-reasoning disentanglement achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "shFhW4zqd6@OpenReview",
      "index": 164,
      "title": "EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting",
      "authors": [
        "Bohao Liao",
        "Wei Zhai",
        "Zengyu Wan",
        "Zhixin Cheng",
        "Wenfei Yang",
        "Yang Cao",
        "Tianzhu Zhang",
        "Zheng-Jun Zha"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "3dgs",
        "event",
        "cameras",
        "casually",
        "trajectory",
        "aided",
        "scene",
        "events",
        "color",
        "splatting"
      ],
      "summary": "Scene reconstruction from casually captured videos has wide real-world applications. Despite recent progress, existing methods relying on traditional cameras tend to fail in high-speed scenarios due to insufficient observations and inaccurate pose estimation. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution and low latency, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event cameras to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, enabling continuous supervision between discrete frames. Second, we extract motion information through Contrast Maximization (CMax) of warped events, which calibrates camera poses and provides gradient-domain constraints for 3DGS. Third, to address the absence of color information in events, we combine photometric bundle adjustment (PBA) with a Fixed-GS training strategy that separates structure and color optimization, effectively ensuring color consistency across different views. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our method achieves up to 3dB higher PSNR and 40% lower Absolute Trajectory Error (ATE) compared to state-of-the-art methods under challenging high-speed scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=shFhW4zqd6"
        ],
        "venue": [
          "/venue/shFhW4zqd6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=shFhW4zqd6"
        ],
        "detail": [
          "https://openreview.net/forum?id=shFhW4zqd6"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Bohao Liao , Wei Zhai , Zengyu Wan , Zhixin Cheng , Wenfei Yang , Yang Cao , Tianzhu Zhang , Zheng-Jun Zha Scene reconstruction from casually captured videos has wide real-world applications. Despite recent progress, existing methods relying on traditional cameras tend to fail in high-speed scenarios due to insufficient observations and inaccurate pose estimation. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution and low latency, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event cameras to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, enabling continuous supervision between discrete frames. Second, we extract motion information through Contrast Maximization (CMax) of warped events, which calibrates camera poses and provides gradient-domain constraints for 3DGS. Third, to address the absence of color information in events, we combine photometric bundle adjustment (PBA) with a Fixed-GS training strategy that separates structure and color optimization, effectively ensuring color consistency across different views. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our method achieves up to 3dB higher PSNR and 40% lower Absolute Trajectory Error (ATE) compared to state-of-the-art methods under challenging high-speed scenarios. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "cUy3tYIOS5@OpenReview",
      "index": 165,
      "title": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs",
      "authors": [
        "Mehran Shakerinava",
        "Siamak Ravanbakhsh",
        "Adam Oberman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "rewards",
        "utility",
        "hausner",
        "scalar",
        "mdp",
        "memorylessness",
        "axiomatic",
        "lexicographic",
        "mdps"
      ],
      "summary": "Recent work has formalized the reward hypothesis through the lens of expected utility theory, by interpreting reward as utility. Hausner's foundational work showed that dropping the continuity axiom leads to a generalization of expected utility theory where utilities are lexicographically ordered vectors of arbitrary dimension. In this paper, we extend this result by identifying a simple and practical condition under which preferences in a Markov Decision Process (MDP) cannot be represented by scalar rewards, necessitating a 2-dimensional reward function. We provide a full characterization of such reward functions, as well as the general d-dimensional case under a memorylessness assumption on preferences. Furthermore, we show that optimal policies in this setting retain many desirable properties of their scalar-reward counterparts, while in the Constrained MDP (CMDP) setting — another common multiobjective setting — they do not.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cUy3tYIOS5"
        ],
        "venue": [
          "/venue/cUy3tYIOS5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cUy3tYIOS5"
        ],
        "detail": [
          "https://openreview.net/forum?id=cUy3tYIOS5"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs [PDF ] [Copy] [Kimi ] [REL] Authors : Mehran Shakerinava , Siamak Ravanbakhsh , Adam Oberman Recent work has formalized the reward hypothesis through the lens of expected utility theory, by interpreting reward as utility. Hausner's foundational work showed that dropping the continuity axiom leads to a generalization of expected utility theory where utilities are lexicographically ordered vectors of arbitrary dimension. In this paper, we extend this result by identifying a simple and practical condition under which preferences in a Markov Decision Process (MDP) cannot be represented by scalar rewards, necessitating a 2-dimensional reward function. We provide a full characterization of such reward functions, as well as the general d-dimensional case under a memorylessness assumption on preferences. Furthermore, we show that optimal policies in this setting retain many desirable properties of their scalar-reward counterparts, while in the Constrained MDP (CMDP) setting — another common multiobjective setting — they do not. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "tXxsCbKdQv@OpenReview",
      "index": 166,
      "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples",
      "authors": [
        "Shiva Sreeram",
        "Alaa Maalouf",
        "Pratyusha Sharma",
        "Daniela Rus"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "matrices",
        "impress",
        "gradient",
        "adaptation",
        "100",
        "llm",
        "downstream",
        "layer",
        "carefully",
        "accuracy"
      ],
      "summary": "Recently, Sharma et al. (2024) suggested a method called LAyer- SElective-Rank reduction (LASER) which demonstrated that pruning high‑order components of carefully chosen LLM’s weight matrices can boost downstream accuracy—without any gradient‑based fine‑tuning. Yet LASER’s exhaustive, per‑matrix search (each requiring full‑dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected—eliminating the layer‑by‑layer sweep, (ii) The gradient of each matrix’s singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data—both for computing the indicative gradients and for measuring the final accuracy—suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a results, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets—entirely without fine‑tuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tXxsCbKdQv"
        ],
        "venue": [
          "/venue/tXxsCbKdQv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tXxsCbKdQv"
        ],
        "detail": [
          "https://openreview.net/forum?id=tXxsCbKdQv"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples [PDF ] [Copy] [Kimi ] [REL] Authors : Shiva Sreeram , Alaa Maalouf , Pratyusha Sharma , Daniela Rus Recently, Sharma et al. (2024) suggested a method called LAyer- SElective-Rank reduction (LASER) which demonstrated that pruning high‑order components of carefully chosen LLM’s weight matrices can boost downstream accuracy—without any gradient‑based fine‑tuning. Yet LASER’s exhaustive, per‑matrix search (each requiring full‑dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected—eliminating the layer‑by‑layer sweep, (ii) The gradient of each matrix’s singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data—both for computing the indicative gradients and for measuring the final accuracy—suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a results, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets—entirely without fine‑tuning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "avRktRfQ8c@OpenReview",
      "index": 167,
      "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning",
      "authors": [
        "Tim Genewein",
        "Li Kevin Wenliang",
        "Jordi Grau-Moya",
        "Anian Ruoss",
        "Laurent Orseau",
        "Marcus Hutter"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "prompting",
        "tuning",
        "bayesian",
        "prompt",
        "meta",
        "predictors",
        "prompts",
        "conceptual",
        "understanding",
        "context"
      ],
      "summary": "Prompting is one of the main ways to adapt a pretrained model to target tasks. Besides manually constructing prompts, many prompt optimization methods have been proposed in the literature. Method development is mainly empirically driven, with less emphasis on a conceptual understanding of prompting. In this paper we discuss how optimal prompting can be understood through a Bayesian view, which also implies some fundamental limitations of prompting that can only be overcome by tuning weights. The paper explains in detail how meta-trained neural networks behave as Bayesian predictors over the pretraining distribution, whose hallmark feature is rapid in-context adaptation. Optimal prompting can be studied formally as conditioning these Bayesian predictors, yielding criteria for target tasks where optimal prompting is and is not possible. We support the theory with educational experiments on LSTMs and Transformers, where we compare different versions of prefix-tuning and different weight-tuning methods. We also confirm that soft prefixes, which are sequences of real-valued vectors outside the token alphabet, can lead to very effective prompts for trained and even untrained networks by manipulating activations in ways that are not achievable by hard tokens. This adds an important mechanistic aspect beyond the conceptual Bayesian theory.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=avRktRfQ8c"
        ],
        "venue": [
          "/venue/avRktRfQ8c@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=avRktRfQ8c"
        ],
        "detail": [
          "https://openreview.net/forum?id=avRktRfQ8c"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Tim Genewein , Li Kevin Wenliang , Jordi Grau-Moya , Anian Ruoss , Laurent Orseau , Marcus Hutter Prompting is one of the main ways to adapt a pretrained model to target tasks. Besides manually constructing prompts, many prompt optimization methods have been proposed in the literature. Method development is mainly empirically driven, with less emphasis on a conceptual understanding of prompting. In this paper we discuss how optimal prompting can be understood through a Bayesian view, which also implies some fundamental limitations of prompting that can only be overcome by tuning weights. The paper explains in detail how meta-trained neural networks behave as Bayesian predictors over the pretraining distribution, whose hallmark feature is rapid in-context adaptation. Optimal prompting can be studied formally as conditioning these Bayesian predictors, yielding criteria for target tasks where optimal prompting is and is not possible. We support the theory with educational experiments on LSTMs and Transformers, where we compare different versions of prefix-tuning and different weight-tuning methods. We also confirm that soft prefixes, which are sequences of real-valued vectors outside the token alphabet, can lead to very effective prompts for trained and even untrained networks by manipulating activations in ways that are not achievable by hard tokens. This adds an important mechanistic aspect beyond the conceptual Bayesian theory. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IZ1KYTU9ON@OpenReview",
      "index": 168,
      "title": "Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism",
      "authors": [
        "Mete Erdogan",
        "Cengiz Pehlevan",
        "Alper Tunga Erdogan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ebd",
        "error",
        "broadcast",
        "decorrelation",
        "broadcasting",
        "principled",
        "biologically",
        "plausible",
        "mechanism",
        "errors"
      ],
      "summary": "We introduce *Error Broadcast and Decorrelation* (EBD), a novel learning framework for neural networks that addresses credit assignment by directly broadcasting output errors to individual layers, circumventing weight transport of backpropagation. EBD is rigorously grounded in the stochastic orthogonality property of Minimum Mean Square Error estimators. This fundamental principle states that the error of an optimal estimator is orthogonal to functions of the input. Guided by this insight, EBD defines layerwise loss functions that directly penalize correlations between layer activations and output errors, thereby establishing a principled foundation for error broadcasting. This theoretically sound mechanism naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate EBD’s competitive or better performance against other error-broadcast methods on benchmark datasets. Our findings establish EBD as an efficient, biologically plausible, and principled alternative for neural network training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IZ1KYTU9ON"
        ],
        "venue": [
          "/venue/IZ1KYTU9ON@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IZ1KYTU9ON"
        ],
        "detail": [
          "https://openreview.net/forum?id=IZ1KYTU9ON"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism [PDF ] [Copy] [Kimi ] [REL] Authors : Mete Erdogan , Cengiz Pehlevan , Alper Tunga Erdogan We introduce *Error Broadcast and Decorrelation* (EBD), a novel learning framework for neural networks that addresses credit assignment by directly broadcasting output errors to individual layers, circumventing weight transport of backpropagation. EBD is rigorously grounded in the stochastic orthogonality property of Minimum Mean Square Error estimators. This fundamental principle states that the error of an optimal estimator is orthogonal to functions of the input. Guided by this insight, EBD defines layerwise loss functions that directly penalize correlations between layer activations and output errors, thereby establishing a principled foundation for error broadcasting. This theoretically sound mechanism naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate EBD’s competitive or better performance against other error-broadcast methods on benchmark datasets. Our findings establish EBD as an efficient, biologically plausible, and principled alternative for neural network training. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "zSrb8rtH9M@OpenReview",
      "index": 169,
      "title": "On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks",
      "authors": [
        "Mingze Wang",
        "Weinan E"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "moes",
        "experts",
        "tasks",
        "expressive",
        "mixture",
        "structured",
        "dimensionality",
        "sparsity",
        "complex",
        "approximate"
      ],
      "summary": "Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in modern deep learning. Despite their empirical success, the theoretical foundations underlying their ability to model complex tasks remain poorly understood. In this work, we conduct a systematic study of the expressive power of MoEs in modeling complex tasks with two common structural priors: low-dimensionality and sparsity. For shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality. For deep MoEs, we show that O ( L ) O ( L ) -layer MoEs with E E experts per layer can approximate piecewise functions comprising E L E L pieces with compositional sparsity, i.e., they can exhibit an exponential number of structured tasks. Our analysis reveals the roles of critical architectural components and hyperparameters in MoEs, including the gating mechanism, expert networks, the number of experts, and the number of layers, and offers natural suggestions for MoE variants.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zSrb8rtH9M"
        ],
        "venue": [
          "/venue/zSrb8rtH9M@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zSrb8rtH9M"
        ],
        "detail": [
          "https://openreview.net/forum?id=zSrb8rtH9M"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Mingze Wang , Weinan E Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in modern deep learning. Despite their empirical success, the theoretical foundations underlying their ability to model complex tasks remain poorly understood. In this work, we conduct a systematic study of the expressive power of MoEs in modeling complex tasks with two common structural priors: low-dimensionality and sparsity. For shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality. For deep MoEs, we show that O ( L ) O ( L ) -layer MoEs with E E experts per layer can approximate piecewise functions comprising E L E L pieces with compositional sparsity, i.e., they can exhibit an exponential number of structured tasks. Our analysis reveals the roles of critical architectural components and hyperparameters in MoEs, including the gating mechanism, expert networks, the number of experts, and the number of layers, and offers natural suggestions for MoE variants. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gNiT81iag0@OpenReview",
      "index": 170,
      "title": "TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs",
      "authors": [
        "Parjanya Prajakta Prashant",
        "Kaustubh Ponkshe",
        "Babak Salimi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tokenswap",
        "verbatim",
        "memorized",
        "memorize",
        "lightweight",
        "distilgpt",
        "regurgitate",
        "llms",
        "disrupt",
        "token"
      ],
      "summary": "As language models scale, their performance improves dramatically across a wide range of tasks, but so does their tendency to memorize and regurgitate parts of their training data verbatim. This tradeoff poses serious legal, ethical, and safety concerns, especially in real-world deployments. Existing mitigation techniques, such as differential privacy or model unlearning, often require retraining or access to internal weights making them impractical for most users. In this work, we introduce TokenSwap, a lightweight, post-hoc defense designed for realistic settings where the user can only access token-level outputs. Our key insight is that while large models are necessary for high task performance, small models (e.g., DistilGPT-2) are often sufficient to assign fluent, grammatically plausible probabilities to common function words - and crucially, they memorize far less. By selectively swapping token probabilities between models, TokenSwap preserves the capabilities of large models while reducing their propensity for verbatim reproduction. Evaluations on Pythia-6.9B and Llama-3-8B show up to a 10 × × drop in exact memorization with negligible task degradation. Our method offers a practical, accessible solution for mitigating memorized generation in deployed LLMs. Code is available at https://github.com/parjanya20/verbatim-llm.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gNiT81iag0"
        ],
        "venue": [
          "/venue/gNiT81iag0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gNiT81iag0"
        ],
        "detail": [
          "https://openreview.net/forum?id=gNiT81iag0"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Parjanya Prajakta Prashant , Kaustubh Ponkshe , Babak Salimi As language models scale, their performance improves dramatically across a wide range of tasks, but so does their tendency to memorize and regurgitate parts of their training data verbatim. This tradeoff poses serious legal, ethical, and safety concerns, especially in real-world deployments. Existing mitigation techniques, such as differential privacy or model unlearning, often require retraining or access to internal weights making them impractical for most users. In this work, we introduce TokenSwap, a lightweight, post-hoc defense designed for realistic settings where the user can only access token-level outputs. Our key insight is that while large models are necessary for high task performance, small models (e.g., DistilGPT-2) are often sufficient to assign fluent, grammatically plausible probabilities to common function words - and crucially, they memorize far less. By selectively swapping token probabilities between models, TokenSwap preserves the capabilities of large models while reducing their propensity for verbatim reproduction. Evaluations on Pythia-6.9B and Llama-3-8B show up to a 10 × × drop in exact memorization with negligible task degradation. Our method offers a practical, accessible solution for mitigating memorized generation in deployed LLMs. Code is available at https://github.com/parjanya20/verbatim-llm. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "p3HBEtNDRY@OpenReview",
      "index": 171,
      "title": "Streaming Attention Approximation via Discrepancy Theory",
      "authors": [
        "Ekaterina Kochetkova",
        "Kshiteej Sheth",
        "Insu Han",
        "Amir Zandieh",
        "Michael Kapralov"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "streaming",
        "attention",
        "balancekv",
        "approximation",
        "token",
        "banaszczyk",
        "discrepancy",
        "context",
        "generation",
        "key"
      ],
      "summary": "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. In this paper we study the streaming complexity of attention approximation, a key computational primitive underlying token generation. Our main contribution is BalanceKV, a streaming algorithm for ϵ ϵ -approximating attention computations based on geometric process for selecting a balanced collection of Key and Value tokens as per Banaszczyk's vector balancing theory. We complement our algorithm with space lower bounds for streaming attention computation. Besides strong theoretical guarantees, BalanceKV exhibits empirically validated performance improvements over existing methods, both for attention approximation and end-to-end performance on various long context benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=p3HBEtNDRY"
        ],
        "venue": [
          "/venue/p3HBEtNDRY@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=p3HBEtNDRY"
        ],
        "detail": [
          "https://openreview.net/forum?id=p3HBEtNDRY"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Streaming Attention Approximation via Discrepancy Theory [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ekaterina Kochetkova , Kshiteej Sheth , Insu Han , Amir Zandieh , Michael Kapralov Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. In this paper we study the streaming complexity of attention approximation, a key computational primitive underlying token generation. Our main contribution is BalanceKV, a streaming algorithm for ϵ ϵ -approximating attention computations based on geometric process for selecting a balanced collection of Key and Value tokens as per Banaszczyk's vector balancing theory. We complement our algorithm with space lower bounds for streaming attention computation. Besides strong theoretical guarantees, BalanceKV exhibits empirically validated performance improvements over existing methods, both for attention approximation and end-to-end performance on various long context benchmarks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "DI2AAFnLrc@OpenReview",
      "index": 172,
      "title": "SegMASt3R: Geometry Grounded Segment Matching",
      "authors": [
        "Rohit Jayanti",
        "Swayam Agrawal",
        "Vansh Garg",
        "Siddharth Tourani",
        "Muhammad Haris Khan",
        "Sourav Garg",
        "Madhava Krishna"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "matching",
        "segment",
        "segmast3r",
        "viewpoint",
        "foundation",
        "sam2",
        "auprc",
        "grounded",
        "scannet",
        "regions"
      ],
      "summary": "Segment matching is an important intermediate task in computer vision that establishes correspondences between semantically or geometrically coherent regions across images. Unlike keypoint matching, which focuses on localized features, segment matching captures structured regions, offering greater robustness to occlusions, lighting variations, and viewpoint changes. In this paper, we leverage the spatial understanding of 3D foundation models to tackle wide-baseline segment matching, a challenging setting involving extreme viewpoint shifts. We propose an architecture that uses the inductive bias of these 3D foundation models to match segments across image pairs with up to 180 ∘ 180 ∘ rotation. Extensive experiments show that our approach outperforms state-of-the-art methods, including the SAM2 video propagator and local feature matching methods, by up to 30\\% on the AUPRC metric, on ScanNet++ and Replica datasets. We further demonstrate benefits of the proposed model on relevant downstream tasks, including 3D instance mapping and object-relative navigation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DI2AAFnLrc"
        ],
        "venue": [
          "/venue/DI2AAFnLrc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DI2AAFnLrc"
        ],
        "detail": [
          "https://openreview.net/forum?id=DI2AAFnLrc"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "SegMASt3R: Geometry Grounded Segment Matching [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Rohit Jayanti , Swayam Agrawal , Vansh Garg , Siddharth Tourani , Muhammad Haris Khan , Sourav Garg , Madhava Krishna Segment matching is an important intermediate task in computer vision that establishes correspondences between semantically or geometrically coherent regions across images. Unlike keypoint matching, which focuses on localized features, segment matching captures structured regions, offering greater robustness to occlusions, lighting variations, and viewpoint changes. In this paper, we leverage the spatial understanding of 3D foundation models to tackle wide-baseline segment matching, a challenging setting involving extreme viewpoint shifts. We propose an architecture that uses the inductive bias of these 3D foundation models to match segments across image pairs with up to 180 ∘ 180 ∘ rotation. Extensive experiments show that our approach outperforms state-of-the-art methods, including the SAM2 video propagator and local feature matching methods, by up to 30\\% on the AUPRC metric, on ScanNet++ and Replica datasets. We further demonstrate benefits of the proposed model on relevant downstream tasks, including 3D instance mapping and object-relative navigation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XP3v1THxsq@OpenReview",
      "index": 173,
      "title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception",
      "authors": [
        "Satvik Golechha",
        "Adrià Garriga-Alonso"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "deception",
        "sandbox",
        "deceptive",
        "game",
        "open",
        "agentic",
        "agents",
        "among",
        "statement",
        "ended"
      ],
      "summary": "Prior studies on deception in language-based AI agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal. To fix this, we introduce Among Us Among Us , a sandbox social deception game where LLM-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, Among Us Among Us can be expected to last much longer, because it is a multi-player game far from equilibrium. Using the sandbox, we evaluate 18 18 proprietary and open-weight LLMs and uncover a general trend: models trained with RL are comparatively much better at producing deception than detecting it. We evaluate the effectiveness of methods to detect lying and deception: logistic regression on the activations and sparse autoencoders (SAEs). We find that probes trained on a dataset of ``pretend you're a dishonest model: … … '' generalize extremely well out-of-distribution, consistently obtaining AUROCs over 95% even when evaluated just on the deceptive statement, without the chain of thought. We also find two SAE features that work well at deception detection but are unable to steer the model to lie less. We hope our open-sourced sandbox, game logs, and probes serve to anticipate and mitigate deceptive behavior and capabilities in language-based agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XP3v1THxsq"
        ],
        "venue": [
          "/venue/XP3v1THxsq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XP3v1THxsq"
        ],
        "detail": [
          "https://openreview.net/forum?id=XP3v1THxsq"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Satvik Golechha , Adrià Garriga-Alonso Prior studies on deception in language-based AI agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal. To fix this, we introduce Among Us Among Us , a sandbox social deception game where LLM-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, Among Us Among Us can be expected to last much longer, because it is a multi-player game far from equilibrium. Using the sandbox, we evaluate 18 18 proprietary and open-weight LLMs and uncover a general trend: models trained with RL are comparatively much better at producing deception than detecting it. We evaluate the effectiveness of methods to detect lying and deception: logistic regression on the activations and sparse autoencoders (SAEs). We find that probes trained on a dataset of ``pretend you're a dishonest model: … … '' generalize extremely well out-of-distribution, consistently obtaining AUROCs over 95% even when evaluated just on the deceptive statement, without the chain of thought. We also find two SAE features that work well at deception detection but are unable to steer the model to lie less. We hope our open-sourced sandbox, game logs, and probes serve to anticipate and mitigate deceptive behavior and capabilities in language-based agents. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oeZZusZheP@OpenReview",
      "index": 174,
      "title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling",
      "authors": [
        "Zhining Zhang",
        "Chuanyang Jin",
        "Mung Yao Jia",
        "Shunchi Zhang",
        "Tianmin Shu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "autotom",
        "tom",
        "mental",
        "agent",
        "inference",
        "automated",
        "model",
        "modeling",
        "reasoning",
        "based"
      ],
      "summary": "Theory of Mind (ToM), the ability to understand people's minds based on their behavior, is key to developing socially intelligent agents. Current approaches to ToM reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use handcrafted, rigid agent models for model-based inference, which are more robust but fail to generalize across domains. In this work, we introduce *AutoToM*, an automated agent modeling method for scalable, robust, and interpretable mental inference. Given a ToM problem, *AutoToM* first proposes an initial agent model and then performs automated Bayesian inverse planning based on this model, leveraging an LLM backend. Guided by inference uncertainty, it iteratively refines the model by introducing additional mental variables and/or incorporating more timesteps in the context. Across five diverse benchmarks, *AutoToM* outperforms existing ToM methods and even large reasoning models. Additionally, we show that *AutoToM* can produce human‐like confidence estimates and enable online mental inference for embodied decision-making.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oeZZusZheP"
        ],
        "venue": [
          "/venue/oeZZusZheP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oeZZusZheP"
        ],
        "detail": [
          "https://openreview.net/forum?id=oeZZusZheP"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Zhining Zhang , Chuanyang Jin , Mung Yao Jia , Shunchi Zhang , Tianmin Shu Theory of Mind (ToM), the ability to understand people's minds based on their behavior, is key to developing socially intelligent agents. Current approaches to ToM reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use handcrafted, rigid agent models for model-based inference, which are more robust but fail to generalize across domains. In this work, we introduce *AutoToM*, an automated agent modeling method for scalable, robust, and interpretable mental inference. Given a ToM problem, *AutoToM* first proposes an initial agent model and then performs automated Bayesian inverse planning based on this model, leveraging an LLM backend. Guided by inference uncertainty, it iteratively refines the model by introducing additional mental variables and/or incorporating more timesteps in the context. Across five diverse benchmarks, *AutoToM* outperforms existing ToM methods and even large reasoning models. Additionally, we show that *AutoToM* can produce human‐like confidence estimates and enable online mental inference for embodied decision-making. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6SI1pvb5xl@OpenReview",
      "index": 175,
      "title": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few",
      "authors": [
        "Qishuai Wen",
        "Zhiyuan Huang",
        "Chun-Guang Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "attention",
        "cbsa",
        "mechanisms",
        "representatives",
        "contracting",
        "interpretable",
        "compressing",
        "interpretability",
        "towards",
        "pursuits"
      ],
      "summary": "Attention mechanisms have achieved significant empirical success in multiple fields, but their underlying optimization objectives remain unclear yet. Moreover, the quadratic complexity of self-attention has become increasingly prohibitive. Although interpretability and efficiency are two mutually reinforcing pursuits, prior work typically investigates them separately. In this paper, we propose a unified optimization objective that derives inherently interpretable and efficient attention mechanisms through algorithm unrolling. Precisely, we construct a gradient step of the proposed objective with a set of forward-pass operations of our \\emph{Contract-and-Broadcast Self-Attention} (CBSA), which compresses input tokens towards low-dimensional structures by contracting a few representatives of them. This novel mechanism can not only scale linearly by fixing the number of representatives, but also covers the instantiations of varied attention mechanisms when using different sets of representatives. We conduct extensive experiments to demonstrate comparable performance and superior advantages over black-box attention mechanisms on visual tasks. Our work sheds light on the integration of interpretability and efficiency, as well as the unified formula of attention mechanisms. Code is available at \\href{https://github.com/QishuaiWen/CBSA}{this https URL}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6SI1pvb5xl"
        ],
        "venue": [
          "/venue/6SI1pvb5xl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6SI1pvb5xl"
        ],
        "detail": [
          "https://openreview.net/forum?id=6SI1pvb5xl"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Qishuai Wen , Zhiyuan Huang , Chun-Guang Li Attention mechanisms have achieved significant empirical success in multiple fields, but their underlying optimization objectives remain unclear yet. Moreover, the quadratic complexity of self-attention has become increasingly prohibitive. Although interpretability and efficiency are two mutually reinforcing pursuits, prior work typically investigates them separately. In this paper, we propose a unified optimization objective that derives inherently interpretable and efficient attention mechanisms through algorithm unrolling. Precisely, we construct a gradient step of the proposed objective with a set of forward-pass operations of our \\emph{Contract-and-Broadcast Self-Attention} (CBSA), which compresses input tokens towards low-dimensional structures by contracting a few representatives of them. This novel mechanism can not only scale linearly by fixing the number of representatives, but also covers the instantiations of varied attention mechanisms when using different sets of representatives. We conduct extensive experiments to demonstrate comparable performance and superior advantages over black-box attention mechanisms on visual tasks. Our work sheds light on the integration of interpretability and efficiency, as well as the unified formula of attention mechanisms. Code is available at \\href{https://github.com/QishuaiWen/CBSA}{this https URL}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XmV7KRABBl@OpenReview",
      "index": 176,
      "title": "EvoBrain: Dynamic Multi-Channel EEG Graph Modeling for Time-Evolving Brain Networks",
      "authors": [
        "Rikuto Kotoge",
        "Zheng Chen",
        "Tasuku Kimura",
        "Yasuko Matsubara",
        "Takufumi Yanagisawa",
        "Haruhiko Kishima",
        "Yasushi Sakurai"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "seizure",
        "evobrain",
        "dynamic",
        "graph",
        "brain",
        "gnn",
        "eeg",
        "evolving",
        "modeling",
        "insights"
      ],
      "summary": "Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23\\% and F1 score by 30\\%, compared with the dynamic GNN baseline, and (c) broad evaluation of our method on the challenging early seizure prediction task.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XmV7KRABBl"
        ],
        "venue": [
          "/venue/XmV7KRABBl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XmV7KRABBl"
        ],
        "detail": [
          "https://openreview.net/forum?id=XmV7KRABBl"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "EvoBrain: Dynamic Multi-Channel EEG Graph Modeling for Time-Evolving Brain Networks [PDF ] [Copy] [Kimi ] [REL] Authors : Rikuto Kotoge , Zheng Chen , Tasuku Kimura , Yasuko Matsubara , Takufumi Yanagisawa , Haruhiko Kishima , Yasushi Sakurai Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23\\% and F1 score by 30\\%, compared with the dynamic GNN baseline, and (c) broad evaluation of our method on the challenging early seizure prediction task. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "idjZKbf78s@OpenReview",
      "index": 177,
      "title": "Product Distribution Learning with Imperfect Advice",
      "authors": [
        "Arnab Bhattacharyya",
        "Davin Choo",
        "Philips George John",
        "Themis Gouleakis"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mathbf",
        "epsilon",
        "advice",
        "distribution",
        "product",
        "eta",
        "imperfect",
        "omega",
        "samples",
        "hypercube"
      ],
      "summary": "Given i.i.d.~samples from an unknown distribution P P , the goal of distribution learning is to recover the parameters of a distribution that is close to P P . When P P belongs to the class of product distributions on the Boolean hypercube { 0 , 1 } d { 0 , 1 } d , it is known that Ω ( d / ϵ 2 ) Ω ( d / ϵ 2 ) samples are necessary to learn P P within total variation (TV) distance ϵ ϵ . We revisit this problem when the learner is also given as advice the parameters of a product distribution Q Q . We show that there is an efficient algorithm to learn P P within TV distance ϵ ϵ that has sample complexity O ~ ( d 1 − η / ϵ 2 ) O ~ ( d 1 − η / ϵ 2 ) , if ∥ p − q ∥ 1 < ϵ d 0.5 − Ω ( η ) ‖ p − q ‖ 1 < ϵ d 0.5 − Ω ( η ) . Here, p p and q q are the mean vectors of P P and Q Q respectively, and no bound on ∥ p − q ∥ 1 ‖ p − q ‖ 1 is known to the algorithm a priori.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=idjZKbf78s"
        ],
        "venue": [
          "/venue/idjZKbf78s@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=idjZKbf78s"
        ],
        "detail": [
          "https://openreview.net/forum?id=idjZKbf78s"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Product Distribution Learning with Imperfect Advice [PDF ] [Copy] [Kimi ] [REL] Authors : Arnab Bhattacharyya , Davin Choo , Philips George John , Themis Gouleakis Given i.i.d.~samples from an unknown distribution P P , the goal of distribution learning is to recover the parameters of a distribution that is close to P P . When P P belongs to the class of product distributions on the Boolean hypercube { 0 , 1 } d { 0 , 1 } d , it is known that Ω ( d / ϵ 2 ) Ω ( d / ϵ 2 ) samples are necessary to learn P P within total variation (TV) distance ϵ ϵ . We revisit this problem when the learner is also given as advice the parameters of a product distribution Q Q . We show that there is an efficient algorithm to learn P P within TV distance ϵ ϵ that has sample complexity O ~ ( d 1 − η / ϵ 2 ) O ~ ( d 1 − η / ϵ 2 ) , if ∥ p − q ∥ 1 < ϵ d 0.5 − Ω ( η ) ‖ p − q ‖ 1 < ϵ d 0.5 − Ω ( η ) . Here, p p and q q are the mean vectors of P P and Q Q respectively, and no bound on ∥ p − q ∥ 1 ‖ p − q ‖ 1 is known to the algorithm a priori. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wbZCBBrq3W@OpenReview",
      "index": 178,
      "title": "RoboScape: Physics-informed Embodied World Model",
      "authors": [
        "Yu Shang",
        "Xin Zhang",
        "Yinzhou Tang",
        "Lei Jin",
        "Chen Gao",
        "Wei Wu",
        "Yong Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "roboscape",
        "embodied",
        "informed",
        "robotic",
        "physics",
        "world",
        "video",
        "videos",
        "physical",
        "intelligence"
      ],
      "summary": "World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. Our code and demos are available at: https://github.com/tsinghua-fib-lab/RoboScape.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wbZCBBrq3W"
        ],
        "venue": [
          "/venue/wbZCBBrq3W@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wbZCBBrq3W"
        ],
        "detail": [
          "https://openreview.net/forum?id=wbZCBBrq3W"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "RoboScape: Physics-informed Embodied World Model [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Yu Shang , Xin Zhang , Yinzhou Tang , Lei Jin , Chen Gao , Wei Wu , Yong Li World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. Our code and demos are available at: https://github.com/tsinghua-fib-lab/RoboScape. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WjYvHSjXrP@OpenReview",
      "index": 179,
      "title": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection",
      "authors": [
        "Jinguo Luo",
        "Weihong Ren",
        "Quanlong Zheng",
        "Yanhao Zhang",
        "Zhenlong Yuan",
        "Zhiyong Wang",
        "Haonan Lu",
        "Honghai LIU"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hoi",
        "reasoning",
        "lfms",
        "instructions",
        "modal",
        "context",
        "interaction",
        "instructhoi",
        "aware",
        "cig"
      ],
      "summary": "Recently, Large Foundation Models (LFMs), e.g., CLIP and GPT, have significantly advanced the Human-Object Interaction (HOI) detection, due to their superior generalization and transferability. Prior HOI detectors typically employ single- or multi-modal prompts to generate discriminative representations for HOIs from pretrained LFMs. However, such prompt-based approaches focus on transferring HOI-specific knowledge, but unexplore the potential reasoning capabilities of LFMs, which can provide informative context for ambiguous and open-world interaction recognition. In this paper, we propose InstructHOI, a novel method that leverages context-aware instructions to guide multi-modal reasoning for HOI detection. Specifically, to bridge knowledge gap and enhance reasoning abilities, we first perform HOI-domain fine-tuning on a pretrained multi-modal LFM, using a generated dataset with 140K interaction-reasoning image-text pairs. Then, we develop a Context-aware Instruction Generator (CIG) to guide interaction reasoning. Unlike traditional language-only instructions, CIG first mines visual interactive context at the human-object level, which is then fused with linguistic instructions, forming multi-modal reasoning guidance. Furthermore, an Interest Token Selector (ITS) is adopted to adaptively filter image tokens based on context-aware instructions, thereby aligning reasoning process with interaction regions. Extensive experiments on two public benchmarks demonstrate that our proposed method outperforms the state-of-the-art ones, under both supervised and zero-shot settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WjYvHSjXrP"
        ],
        "venue": [
          "/venue/WjYvHSjXrP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WjYvHSjXrP"
        ],
        "detail": [
          "https://openreview.net/forum?id=WjYvHSjXrP"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Jinguo Luo , Weihong Ren , Quanlong Zheng , Yanhao Zhang , Zhenlong Yuan , Zhiyong Wang , Haonan Lu , Honghai LIU Recently, Large Foundation Models (LFMs), e.g., CLIP and GPT, have significantly advanced the Human-Object Interaction (HOI) detection, due to their superior generalization and transferability. Prior HOI detectors typically employ single- or multi-modal prompts to generate discriminative representations for HOIs from pretrained LFMs. However, such prompt-based approaches focus on transferring HOI-specific knowledge, but unexplore the potential reasoning capabilities of LFMs, which can provide informative context for ambiguous and open-world interaction recognition. In this paper, we propose InstructHOI, a novel method that leverages context-aware instructions to guide multi-modal reasoning for HOI detection. Specifically, to bridge knowledge gap and enhance reasoning abilities, we first perform HOI-domain fine-tuning on a pretrained multi-modal LFM, using a generated dataset with 140K interaction-reasoning image-text pairs. Then, we develop a Context-aware Instruction Generator (CIG) to guide interaction reasoning. Unlike traditional language-only instructions, CIG first mines visual interactive context at the human-object level, which is then fused with linguistic instructions, forming multi-modal reasoning guidance. Furthermore, an Interest Token Selector (ITS) is adopted to adaptively filter image tokens based on context-aware instructions, thereby aligning reasoning process with interaction regions. Extensive experiments on two public benchmarks demonstrate that our proposed method outperforms the state-of-the-art ones, under both supervised and zero-shot settings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "z9oeQrcNh9@OpenReview",
      "index": 180,
      "title": "ARM: Adaptive Reasoning Model",
      "authors": [
        "Siye Wu",
        "Jian Xie",
        "Yikai Zhang",
        "Aili Chen",
        "Kai Zhang",
        "Yu Su",
        "Yanghua Xiao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "grpo",
        "reasoning",
        "arm",
        "cot",
        "token",
        "format",
        "formats",
        "ada",
        "sim",
        "adaptive"
      ],
      "summary": "While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the \"overthinking\" problem—excessive and unnecessary reasoning—which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones—Direct Answer, Short CoT, and Code—as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of ∼ ∼ 30%, and up to ∼ ∼ 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a ∼ ∼ 2 × × speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens—ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage. All the resources will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=z9oeQrcNh9"
        ],
        "venue": [
          "/venue/z9oeQrcNh9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=z9oeQrcNh9"
        ],
        "detail": [
          "https://openreview.net/forum?id=z9oeQrcNh9"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "ARM: Adaptive Reasoning Model [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Siye Wu , Jian Xie , Yikai Zhang , Aili Chen , Kai Zhang , Yu Su , Yanghua Xiao While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the \"overthinking\" problem—excessive and unnecessary reasoning—which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones—Direct Answer, Short CoT, and Code—as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of ∼ ∼ 30%, and up to ∼ ∼ 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a ∼ ∼ 2 × × speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens—ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage. All the resources will be released. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TU2MZHZLkP@OpenReview",
      "index": 181,
      "title": "Computational Efficiency under Covariate Shift in Kernel Ridge Regression",
      "authors": [
        "Andrea Della Vecchia",
        "Arnaud Mavakala Watusadisi",
        "Ernesto De Vito",
        "Lorenzo Rosasco"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "covariate",
        "shift",
        "computational",
        "kernel",
        "regression",
        "rkhss",
        "ridge",
        "efficiency",
        "rkhs",
        "statistical"
      ],
      "summary": "This paper addresses the covariate shift problem in the context of nonparametric regression within reproducing kernel Hilbert spaces (RKHSs). Covariate shift arises in supervised learning when the input distributions of the training and test data differ, presenting additional challenges for learning. Although kernel methods have optimal statistical properties, their high computational demands in terms of time and, particularly, memory, limit their scalability to large datasets. To address this limitation, the main focus of this paper is to explore the trade-off between computational efficiency and statistical accuracy under covariate shift. We investigate the use of random projections where the hypothesis space consists of a random subspace within a given RKHS. Our results show that, even in the presence of covariate shift, significant computational savings can be achieved without compromising learning performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TU2MZHZLkP"
        ],
        "venue": [
          "/venue/TU2MZHZLkP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TU2MZHZLkP"
        ],
        "detail": [
          "https://openreview.net/forum?id=TU2MZHZLkP"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Computational Efficiency under Covariate Shift in Kernel Ridge Regression [PDF ] [Copy] [Kimi ] [REL] Authors : Andrea Della Vecchia , Arnaud Mavakala Watusadisi , Ernesto De Vito , Lorenzo Rosasco This paper addresses the covariate shift problem in the context of nonparametric regression within reproducing kernel Hilbert spaces (RKHSs). Covariate shift arises in supervised learning when the input distributions of the training and test data differ, presenting additional challenges for learning. Although kernel methods have optimal statistical properties, their high computational demands in terms of time and, particularly, memory, limit their scalability to large datasets. To address this limitation, the main focus of this paper is to explore the trade-off between computational efficiency and statistical accuracy under covariate shift. We investigate the use of random projections where the hypothesis space consists of a random subspace within a given RKHS. Our results show that, even in the presence of covariate shift, significant computational savings can be achieved without compromising learning performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5iHDGJFf49@OpenReview",
      "index": 182,
      "title": "Self-Assembling Graph Perceptrons",
      "authors": [
        "Jialong Chen",
        "Tong Wang",
        "Bowen Deng",
        "Luonan Chen",
        "Zibin Zheng",
        "Chuan Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sagp",
        "anns",
        "perceptron",
        "assembling",
        "self",
        "graph",
        "mlp",
        "perceptrons",
        "biological",
        "sparking"
      ],
      "summary": "Inspired by the workings of biological brains, humans have designed artificial neural networks (ANNs), sparking profound advancements across various fields. However, the biological brain possesses high plasticity, enabling it to develop simple, efficient, and powerful structures to cope with complex external environments. In contrast, the superior performance of ANNs often relies on meticulously crafted architectures, which can make them vulnerable when handling complex inputs. Moreover, overparameterization often characterizes the most advanced ANNs. This paper explores the path toward building streamlined and plastic ANNs. Firstly, we introduce the Graph Perceptron (GP), which extends the most fundamental ANN, the Multi-Layer Perceptron (MLP). Subsequently, we incorporate a self-assembly mechanism on top of GP called Self-Assembling Graph Perceptron (SAGP). During training, SAGP can autonomously adjust the network's number of neurons and synapses and their connectivity. SAGP achieves comparable or even superior performance with only about 5% of the size of an MLP. We also demonstrate the SAGP's advantages in enhancing model interpretability and feature selection.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5iHDGJFf49"
        ],
        "venue": [
          "/venue/5iHDGJFf49@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5iHDGJFf49"
        ],
        "detail": [
          "https://openreview.net/forum?id=5iHDGJFf49"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Self-Assembling Graph Perceptrons [PDF ] [Copy] [Kimi ] [REL] Authors : Jialong Chen , Tong Wang , Bowen Deng , Luonan Chen , Zibin Zheng , Chuan Chen Inspired by the workings of biological brains, humans have designed artificial neural networks (ANNs), sparking profound advancements across various fields. However, the biological brain possesses high plasticity, enabling it to develop simple, efficient, and powerful structures to cope with complex external environments. In contrast, the superior performance of ANNs often relies on meticulously crafted architectures, which can make them vulnerable when handling complex inputs. Moreover, overparameterization often characterizes the most advanced ANNs. This paper explores the path toward building streamlined and plastic ANNs. Firstly, we introduce the Graph Perceptron (GP), which extends the most fundamental ANN, the Multi-Layer Perceptron (MLP). Subsequently, we incorporate a self-assembly mechanism on top of GP called Self-Assembling Graph Perceptron (SAGP). During training, SAGP can autonomously adjust the network's number of neurons and synapses and their connectivity. SAGP achieves comparable or even superior performance with only about 5% of the size of an MLP. We also demonstrate the SAGP's advantages in enhancing model interpretability and feature selection. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4YKlo58RcQ@OpenReview",
      "index": 183,
      "title": "Scaling and context steer LLMs along the same computational path as the human brain",
      "authors": [
        "Joséphine Raugel",
        "Jérémy Rapin",
        "Stéphane d'Ascoli",
        "Valentin Wyart",
        "Jean-Remi King"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "brain",
        "llms",
        "alignment",
        "steer",
        "tend",
        "align",
        "responses",
        "audiobook",
        "context",
        "human"
      ],
      "summary": "Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether this representational alignment arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 17 LLMs varying in size and architecture type. Our analyses reveal that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, the alignment between LLMs and the brain provides novel elements supporting a partial convergence between biological and artificial neural networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4YKlo58RcQ"
        ],
        "venue": [
          "/venue/4YKlo58RcQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4YKlo58RcQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=4YKlo58RcQ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Scaling and context steer LLMs along the same computational path as the human brain [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Joséphine Raugel , Jérémy Rapin , Stéphane d'Ascoli , Valentin Wyart , Jean-Remi King Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether this representational alignment arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 17 LLMs varying in size and architecture type. Our analyses reveal that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, the alignment between LLMs and the brain provides novel elements supporting a partial convergence between biological and artificial neural networks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZwBtDbuzjY@OpenReview",
      "index": 184,
      "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models",
      "authors": [
        "Yanggan Gu",
        "Yuanyi Wang",
        "Zhaoyi Yan",
        "Yiming Zhang",
        "Qi Zhou",
        "Fei Wu",
        "Hongxia Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "infifpo",
        "fusion",
        "preference",
        "model",
        "optimization",
        "source",
        "pivot",
        "implicit",
        "probability",
        "language"
      ],
      "summary": "Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) —a critical phase for enhancing LLM performance—largely unexplored. The current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion. InfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information. By introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models. Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improves its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZwBtDbuzjY"
        ],
        "venue": [
          "/venue/ZwBtDbuzjY@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZwBtDbuzjY"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZwBtDbuzjY"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yanggan Gu , Yuanyi Wang , Zhaoyi Yan , Yiming Zhang , Qi Zhou , Fei Wu , Hongxia Yang Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) —a critical phase for enhancing LLM performance—largely unexplored. The current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion. InfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information. By introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models. Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improves its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "huZzy5w2Js@OpenReview",
      "index": 185,
      "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios",
      "authors": [
        "Lingwei Dang",
        "Ruizhi Shao",
        "Hongwen Zhang",
        "Wei MIN",
        "Yebin Liu",
        "Qingyao Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hoi",
        "svimo",
        "motion",
        "droliven",
        "synchronized",
        "video",
        "diffusion",
        "generation",
        "object",
        "interaction"
      ],
      "summary": "Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at [https://droliven.github.io/SViMo_project](https://droliven.github.io/SViMo_project).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=huZzy5w2Js"
        ],
        "venue": [
          "/venue/huZzy5w2Js@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=huZzy5w2Js"
        ],
        "detail": [
          "https://openreview.net/forum?id=huZzy5w2Js"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Lingwei Dang , Ruizhi Shao , Hongwen Zhang , Wei MIN , Yebin Liu , Qingyao Wu Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at [https://droliven.github.io/SViMo_project](https://droliven.github.io/SViMo_project). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "W1Cu6JsRsd@OpenReview",
      "index": 186,
      "title": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions",
      "authors": [
        "Rafael Bischof",
        "Michal Piovarci",
        "Michael Anton Kraus",
        "Siddhartha Mishra",
        "Bernd Bickel"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hypino",
        "pinns",
        "pinn",
        "physics",
        "manufactured",
        "informed",
        "pde",
        "neural",
        "hyperpinns",
        "operators"
      ],
      "summary": "We present HyPINO, a multi-physics neural operator designed for zero-shot generalization across a broad class of parametric PDEs without requiring task-specific fine-tuning. Our approach combines a Swin Transformer-based hypernetwork with mixed supervision: (i) labeled data from analytical solutions generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled samples optimized using physics-informed objectives. The model maps PDE parameterizations to target Physics-Informed Neural Networks (PINNs) and can handle linear elliptic, hyperbolic, and parabolic equations in two dimensions with varying source terms, geometries, and mixed Dirichlet/Neumann boundary conditions, including interior boundaries. HyPINO achieves strong zero-shot accuracy on seven benchmark problems from PINN literature, outperforming U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we introduce an iterative refinement procedure that treats the residual of the generated PINN as \"delta PDE\" and performs another forward pass to generate a corrective PINN. Summing their contributions and repeating this process forms an ensemble whose combined solution progressively reduces the error on six benchmarks and achieves a >100× lower L 2 L 2 loss in the best case, while retaining forward-only inference. Additionally, we evaluate the fine-tuning behavior of PINNs initialized by HyPINO and show that they converge faster and to lower final error than both randomly initialized and Reptile-meta-learned PINNs on five benchmarks, performing on par on the remaining two. Our results highlight the potential of this scalable approach as a foundation for extending neural operators toward solving increasingly complex, nonlinear, and high-dimensional PDE problems. The code and model weights are publicly available at https://github.com/rbischof/hypino.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=W1Cu6JsRsd"
        ],
        "venue": [
          "/venue/W1Cu6JsRsd@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=W1Cu6JsRsd"
        ],
        "detail": [
          "https://openreview.net/forum?id=W1Cu6JsRsd"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Rafael Bischof , Michal Piovarci , Michael Anton Kraus , Siddhartha Mishra , Bernd Bickel We present HyPINO, a multi-physics neural operator designed for zero-shot generalization across a broad class of parametric PDEs without requiring task-specific fine-tuning. Our approach combines a Swin Transformer-based hypernetwork with mixed supervision: (i) labeled data from analytical solutions generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled samples optimized using physics-informed objectives. The model maps PDE parameterizations to target Physics-Informed Neural Networks (PINNs) and can handle linear elliptic, hyperbolic, and parabolic equations in two dimensions with varying source terms, geometries, and mixed Dirichlet/Neumann boundary conditions, including interior boundaries. HyPINO achieves strong zero-shot accuracy on seven benchmark problems from PINN literature, outperforming U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we introduce an iterative refinement procedure that treats the residual of the generated PINN as \"delta PDE\" and performs another forward pass to generate a corrective PINN. Summing their contributions and repeating this process forms an ensemble whose combined solution progressively reduces the error on six benchmarks and achieves a >100× lower L 2 L 2 loss in the best case, while retaining forward-only inference. Additionally, we evaluate the fine-tuning behavior of PINNs initialized by HyPINO and show that they converge faster and to lower final error than both randomly initialized and Reptile-meta-learned PINNs on five benchmarks, performing on par on the remaining two. Our results highlight the potential of this scalable approach as a foundation for extending neural operators toward solving increasingly complex, nonlinear, and high-dimensional PDE problems. The code and model weights are publicly available at https://github.com/rbischof/hypino. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0RF80tUWuv@OpenReview",
      "index": 187,
      "title": "RidgeLoRA: Matrix Ridge Enhanced Low-Rank Adaptation of Large Language Models",
      "authors": [
        "Junda Zhu",
        "Jun Ai",
        "Yujun Li",
        "Yichun Yin",
        "Yasheng Wang",
        "Lifeng Shang",
        "Qun Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ridgelora",
        "rank",
        "lora",
        "ridge",
        "matrix",
        "low",
        "adaptation",
        "full",
        "enhanced",
        "match"
      ],
      "summary": "As one of the state-of-the-art parameter-efficient fine-tuning~(PEFT) methods, Low-Rank Adaptation (LoRA) enables model optimization with reduced computational cost through trainable low-rank matrix. However, the low-rank nature makes it prone to produce a decrease in the representation ability, leading to suboptimal performance. In order to break this limitation, we propose RidgeLoRA, a lightweight architecture like LoRA that incorporates novel architecture and matrix ridge enhanced full-rank approximation, to match the performance of full-rank training, while eliminating the need for high memory and a large number of parameters to restore the rank of matrices. We provide a rigorous mathematical derivation to prove that RidgeLoRA has a better upper bound on the representations than vanilla LoRA. Furthermore, extensive experiments across multiple domains demonstrate that RidgeLoRA achieves better performance than other LoRA variants, and can even match or surpass full-rank training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0RF80tUWuv"
        ],
        "venue": [
          "/venue/0RF80tUWuv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0RF80tUWuv"
        ],
        "detail": [
          "https://openreview.net/forum?id=0RF80tUWuv"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "RidgeLoRA: Matrix Ridge Enhanced Low-Rank Adaptation of Large Language Models [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Junda Zhu , Jun Ai , Yujun Li , Yichun Yin , Yasheng Wang , Lifeng Shang , Qun Liu As one of the state-of-the-art parameter-efficient fine-tuning~(PEFT) methods, Low-Rank Adaptation (LoRA) enables model optimization with reduced computational cost through trainable low-rank matrix. However, the low-rank nature makes it prone to produce a decrease in the representation ability, leading to suboptimal performance. In order to break this limitation, we propose RidgeLoRA, a lightweight architecture like LoRA that incorporates novel architecture and matrix ridge enhanced full-rank approximation, to match the performance of full-rank training, while eliminating the need for high memory and a large number of parameters to restore the rank of matrices. We provide a rigorous mathematical derivation to prove that RidgeLoRA has a better upper bound on the representations than vanilla LoRA. Furthermore, extensive experiments across multiple domains demonstrate that RidgeLoRA achieves better performance than other LoRA variants, and can even match or surpass full-rank training. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "DpOSndSOZz@OpenReview",
      "index": 188,
      "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens",
      "authors": [
        "Xixian Yong",
        "Xiao Zhou",
        "Yingying Zhang",
        "Jinlin Li",
        "Yefeng Zheng",
        "Xian Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "think",
        "efficiency",
        "information",
        "lens",
        "infogain",
        "qwq",
        "theoretic",
        "lrms",
        "accuracy"
      ],
      "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics—InfoBias and InfoGain—to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DpOSndSOZz"
        ],
        "venue": [
          "/venue/DpOSndSOZz@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DpOSndSOZz"
        ],
        "detail": [
          "https://openreview.net/forum?id=DpOSndSOZz"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 6
      },
      "raw_excerpt": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens [PDF 7 ] [Copy] [Kimi 6 ] [REL] Authors : Xixian Yong , Xiao Zhou , Yingying Zhang , Jinlin Li , Yefeng Zheng , Xian Wu The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics—InfoBias and InfoGain—to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TkEdQv0bXB@OpenReview",
      "index": 189,
      "title": "Hyperbolic Fine-Tuning for Large Language Models",
      "authors": [
        "Menglin Yang",
        "Ram Samarth B B",
        "Aosong Feng",
        "Bo Xiong",
        "Jiahong Liu",
        "Irwin King",
        "Rex Ying"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hyperbolic",
        "hyplora",
        "llms",
        "fine",
        "tokens",
        "tuning",
        "euclidean",
        "embedding",
        "space",
        "token"
      ],
      "summary": "Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs. In this study, we investigate the non-Euclidean characteristics of LLMs. Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. Motivated by these observations, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. However, we find that this hyperbolic fine-tuning cannot be achieved through the naive application of exponential and logarithmic maps when the embedding and weight matrices both reside in Euclidean space. To address this technical issue, we introduce hyperbolic low-rank efficient fine-tuning, HypLoRA, which performs low-rank adaptation directly on the hyperbolic manifold, preventing the cancellation effect produced by consecutive exponential and logarithmic maps and thereby preserving hyperbolic modeling capabilities. Extensive experiments across various base models and two different reasoning benchmarks, specifically arithmetic and commonsense reasoning tasks, demonstrate that HypLoRA substantially improves LLM performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TkEdQv0bXB"
        ],
        "venue": [
          "/venue/TkEdQv0bXB@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TkEdQv0bXB"
        ],
        "detail": [
          "https://openreview.net/forum?id=TkEdQv0bXB"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Hyperbolic Fine-Tuning for Large Language Models [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Menglin Yang , Ram Samarth B B , Aosong Feng , Bo Xiong , Jiahong Liu , Irwin King , Rex Ying Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs. In this study, we investigate the non-Euclidean characteristics of LLMs. Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. Motivated by these observations, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. However, we find that this hyperbolic fine-tuning cannot be achieved through the naive application of exponential and logarithmic maps when the embedding and weight matrices both reside in Euclidean space. To address this technical issue, we introduce hyperbolic low-rank efficient fine-tuning, HypLoRA, which performs low-rank adaptation directly on the hyperbolic manifold, preventing the cancellation effect produced by consecutive exponential and logarithmic maps and thereby preserving hyperbolic modeling capabilities. Extensive experiments across various base models and two different reasoning benchmarks, specifically arithmetic and commonsense reasoning tasks, demonstrate that HypLoRA substantially improves LLM performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "74SvE2GZwW@OpenReview",
      "index": 190,
      "title": "Deep Continuous-Time State-Space Models for Marked Event Sequences",
      "authors": [
        "Yuxin Chang",
        "Alex James Boyd",
        "Cao Xiao",
        "Taha Kass-Hout",
        "Parminder Bhatia",
        "Padhraic Smyth",
        "Andrew Warrington"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "s2p2",
        "mtpps",
        "mtpp",
        "marked",
        "sequences",
        "event",
        "imbuing",
        "space",
        "continuous",
        "interleaves"
      ],
      "summary": "Marked temporal point processes (MTPPs) model sequences of events occurring at irregular time intervals, with wide-ranging applications in fields such as healthcare, finance and social networks. We propose the _state-space point process_ (S2P2) model, a novel and performant model that leverages techniques derived for modern deep state-space models (SSMs) to overcome limitations of existing MTPP models, while simultaneously imbuing strong inductive biases for continuous-time event sequences that other discrete sequence models (i.e., RNNs, transformers) do not capture. Inspired by the classical linear Hawkes processes, we propose an architecture that interleaves stochastic jump differential equations with nonlinearities to create a highly expressive intensity-based MTPP model, without the need for restrictive parametric assumptions for the intensity. Our approach enables efficient training and inference with a parallel scan, bringing linear complexity and sublinear scaling while retaining expressivity to MTPPs. Empirically, S2P2 achieves state-of-the-art predictive likelihoods across eight real-world datasets, delivering an average improvement of 33% over the best existing approaches.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=74SvE2GZwW"
        ],
        "venue": [
          "/venue/74SvE2GZwW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=74SvE2GZwW"
        ],
        "detail": [
          "https://openreview.net/forum?id=74SvE2GZwW"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Deep Continuous-Time State-Space Models for Marked Event Sequences [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yuxin Chang , Alex James Boyd , Cao Xiao , Taha Kass-Hout , Parminder Bhatia , Padhraic Smyth , Andrew Warrington Marked temporal point processes (MTPPs) model sequences of events occurring at irregular time intervals, with wide-ranging applications in fields such as healthcare, finance and social networks. We propose the _state-space point process_ (S2P2) model, a novel and performant model that leverages techniques derived for modern deep state-space models (SSMs) to overcome limitations of existing MTPP models, while simultaneously imbuing strong inductive biases for continuous-time event sequences that other discrete sequence models (i.e., RNNs, transformers) do not capture. Inspired by the classical linear Hawkes processes, we propose an architecture that interleaves stochastic jump differential equations with nonlinearities to create a highly expressive intensity-based MTPP model, without the need for restrictive parametric assumptions for the intensity. Our approach enables efficient training and inference with a parallel scan, bringing linear complexity and sublinear scaling while retaining expressivity to MTPPs. Empirically, S2P2 achieves state-of-the-art predictive likelihoods across eight real-world datasets, delivering an average improvement of 33% over the best existing approaches. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3FsM6wWQL4@OpenReview",
      "index": 191,
      "title": "HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs",
      "authors": [
        "Ningning CHEN",
        "Weicai Ye",
        "Ying Jiang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hbllm",
        "quantization",
        "wavelet",
        "bit",
        "fidelity",
        "llama",
        "grouping",
        "llms",
        "frequency",
        "enhanced"
      ],
      "summary": "We introduce HBLLM, a wavelet-enhanced high-fidelity 1 1 -bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) ℓ 2 ℓ 2 -norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in 1 1 -bit quantization, attaining a perplexity of 6.71 6.71 on LLaMA 2 2 - 13 13 B with an average weight storage of only 1.08 1.08 bits. Code available at: https://github.com/Yeyke/HBLLM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3FsM6wWQL4"
        ],
        "venue": [
          "/venue/3FsM6wWQL4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3FsM6wWQL4"
        ],
        "detail": [
          "https://openreview.net/forum?id=3FsM6wWQL4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ningning CHEN , Weicai Ye , Ying Jiang We introduce HBLLM, a wavelet-enhanced high-fidelity 1 1 -bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) ℓ 2 ℓ 2 -norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in 1 1 -bit quantization, attaining a perplexity of 6.71 6.71 on LLaMA 2 2 - 13 13 B with an average weight storage of only 1.08 1.08 bits. Code available at: https://github.com/Yeyke/HBLLM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qXAABCxYQ2@OpenReview",
      "index": 192,
      "title": "Which Algorithms Have Tight Generalization Bounds?",
      "authors": [
        "Michael Gastpar",
        "Ido Nachum",
        "Jonathan Shafer",
        "Thomas Weinberger"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tight",
        "generalization",
        "bounds",
        "algorithms",
        "gastpar",
        "preclude",
        "existence",
        "loss",
        "2023",
        "relates"
      ],
      "summary": "We study which machine learning algorithms have tight generalization bounds with respect to a given collection of population distributions. Our results build on and extend the recent work of Gastpar et al. (2023). First, we present conditions that preclude the existence of tight generalization bounds. Specifically, we show that algorithms that have certain inductive biases that cause them to be unstable do not admit tight generalization bounds. Next, we show that algorithms that are sufficiently loss-stable do have tight generalization bounds. We conclude with a simple characterization that relates the existence of tight generalization bounds to the conditional variance of the algorithm's loss.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qXAABCxYQ2"
        ],
        "venue": [
          "/venue/qXAABCxYQ2@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qXAABCxYQ2"
        ],
        "detail": [
          "https://openreview.net/forum?id=qXAABCxYQ2"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Which Algorithms Have Tight Generalization Bounds? [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Michael Gastpar , Ido Nachum , Jonathan Shafer , Thomas Weinberger We study which machine learning algorithms have tight generalization bounds with respect to a given collection of population distributions. Our results build on and extend the recent work of Gastpar et al. (2023). First, we present conditions that preclude the existence of tight generalization bounds. Specifically, we show that algorithms that have certain inductive biases that cause them to be unstable do not admit tight generalization bounds. Next, we show that algorithms that are sufficiently loss-stable do have tight generalization bounds. We conclude with a simple characterization that relates the existence of tight generalization bounds to the conditional variance of the algorithm's loss. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "cLQlsOGqbM@OpenReview",
      "index": 193,
      "title": "Axial Neural Networks for Dimension-Free Foundation Models",
      "authors": [
        "Hyunsu Kim",
        "Jonggeon Park",
        "Joan Bruna",
        "Hongseok Yang",
        "Juho Lee"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "xnn",
        "foundation",
        "axial",
        "dimensionalities",
        "neural",
        "dimension",
        "pretraining",
        "models",
        "pdes",
        "across"
      ],
      "summary": "The advent of foundation models in AI has significantly advanced general-purpose learning, enabling remarkable capabilities in zero-shot inference and in-context learning. However, training such models on physics data, including solutions to partial differential equations (PDEs), poses a unique challenge due to varying dimensionalities across different systems. Traditional approaches either fix a maximum dimension or employ separate encoders for different dimensionalities, resulting in inefficiencies. To address this, we propose a dimension-agnostic neural network architecture, the Axial Neural Network (XNN), inspired by parameter-sharing structures such as Deep Sets and Graph Neural Networks. XNN generalizes across varying tensor dimensions while maintaining computational efficiency. We convert existing PDE foundation models into axial neural networks and evaluate their performance across three training scenarios: training from scratch, pretraining on multiple PDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform competitively with original models and exhibit superior generalization to unseen dimensions, highlighting the importance of multidimensional pretraining for foundation models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cLQlsOGqbM"
        ],
        "venue": [
          "/venue/cLQlsOGqbM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cLQlsOGqbM"
        ],
        "detail": [
          "https://openreview.net/forum?id=cLQlsOGqbM"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Axial Neural Networks for Dimension-Free Foundation Models [PDF ] [Copy] [Kimi ] [REL] Authors : Hyunsu Kim , Jonggeon Park , Joan Bruna , Hongseok Yang , Juho Lee The advent of foundation models in AI has significantly advanced general-purpose learning, enabling remarkable capabilities in zero-shot inference and in-context learning. However, training such models on physics data, including solutions to partial differential equations (PDEs), poses a unique challenge due to varying dimensionalities across different systems. Traditional approaches either fix a maximum dimension or employ separate encoders for different dimensionalities, resulting in inefficiencies. To address this, we propose a dimension-agnostic neural network architecture, the Axial Neural Network (XNN), inspired by parameter-sharing structures such as Deep Sets and Graph Neural Networks. XNN generalizes across varying tensor dimensions while maintaining computational efficiency. We convert existing PDE foundation models into axial neural networks and evaluate their performance across three training scenarios: training from scratch, pretraining on multiple PDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform competitively with original models and exhibit superior generalization to unseen dimensions, highlighting the importance of multidimensional pretraining for foundation models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hguaupzLCU@OpenReview",
      "index": 194,
      "title": "Horizon Reduction Makes RL Scalable",
      "authors": [
        "Seohong Park",
        "Kevin Frans",
        "Deepinder Mann",
        "Benjamin Eysenbach",
        "Aviral Kumar",
        "Sergey Levine"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "offline",
        "horizon",
        "scaling",
        "sharsa",
        "scalability",
        "scalable",
        "algorithms",
        "reduction",
        "poor",
        "showing"
      ],
      "summary": "In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000× larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hguaupzLCU"
        ],
        "venue": [
          "/venue/hguaupzLCU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hguaupzLCU"
        ],
        "detail": [
          "https://openreview.net/forum?id=hguaupzLCU"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Horizon Reduction Makes RL Scalable [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Seohong Park , Kevin Frans , Deepinder Mann , Benjamin Eysenbach , Aviral Kumar , Sergey Levine In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000× larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7ieZWCc7rB@OpenReview",
      "index": 195,
      "title": "OpenBox: Annotate Any Bounding Boxes in 3D",
      "authors": [
        "In-Jae Lee",
        "Mungyeom Kim",
        "Kwonyoung Ryu",
        "Pierre Musacchio",
        "Jaesik Park"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "openbox",
        "bounding",
        "boxes",
        "annotate",
        "annotation",
        "stage",
        "refinement",
        "foundation",
        "dataset",
        "wod"
      ],
      "summary": "Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects’ physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via context-aware refinement. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset (WOD), the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7ieZWCc7rB"
        ],
        "venue": [
          "/venue/7ieZWCc7rB@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7ieZWCc7rB"
        ],
        "detail": [
          "https://openreview.net/forum?id=7ieZWCc7rB"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "OpenBox: Annotate Any Bounding Boxes in 3D [PDF 2 ] [Copy] [Kimi ] [REL] Authors : In-Jae Lee , Mungyeom Kim , Kwonyoung Ryu , Pierre Musacchio , Jaesik Park Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects’ physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via context-aware refinement. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset (WOD), the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "z2SGaPIhLT@OpenReview",
      "index": 196,
      "title": "SGCD: Stain-Guided CycleDiffusion for Unsupervised Domain Adaptation of Histopathology Image Classification",
      "authors": [
        "Hsi-Ling Chen",
        "Chun-Shien Lu",
        "Pau-Choo Chung"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sgcd",
        "stain",
        "domain",
        "cyclediffusion",
        "translated",
        "guided",
        "diffusion",
        "downstream",
        "bidirectional",
        "unsupervised"
      ],
      "summary": "The effectiveness of domain translation in addressing image-based problems of Unsupervised Domain Adaptation (UDA) depends on the quality of the translated images and the preservation of crucial discriminative features. However, achieving high-quality and stable translations typically requires paired data, which poses a challenge in scenarios with limited annotations in the target domain. To address this issue, this paper proposes a novel method termed Stain-Guided Cycle Diffusion (SGCD), employing a dual diffusion model with bidirectional generative constraints to synthesize highly realistic data for downstream task fine-tuning. The bidirectional generative constraints ensure that the translated images retain the features critical to the downstream model in properly controlling the generation process. Additionally, a stain-guided consistency loss is introduced to enhance the denoising capability of the dual diffusion model, thereby improving the quality of images translated between different domains using latents from one domain and a diffusion model trained on another. Experiments conducted on four public datasets demonstrate that SGCD can effectively enhance the performance of downstream task models on the target domain.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=z2SGaPIhLT"
        ],
        "venue": [
          "/venue/z2SGaPIhLT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=z2SGaPIhLT"
        ],
        "detail": [
          "https://openreview.net/forum?id=z2SGaPIhLT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SGCD: Stain-Guided CycleDiffusion for Unsupervised Domain Adaptation of Histopathology Image Classification [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Hsi-Ling Chen , Chun-Shien Lu , Pau-Choo Chung The effectiveness of domain translation in addressing image-based problems of Unsupervised Domain Adaptation (UDA) depends on the quality of the translated images and the preservation of crucial discriminative features. However, achieving high-quality and stable translations typically requires paired data, which poses a challenge in scenarios with limited annotations in the target domain. To address this issue, this paper proposes a novel method termed Stain-Guided Cycle Diffusion (SGCD), employing a dual diffusion model with bidirectional generative constraints to synthesize highly realistic data for downstream task fine-tuning. The bidirectional generative constraints ensure that the translated images retain the features critical to the downstream model in properly controlling the generation process. Additionally, a stain-guided consistency loss is introduced to enhance the denoising capability of the dual diffusion model, thereby improving the quality of images translated between different domains using latents from one domain and a diffusion model trained on another. Experiments conducted on four public datasets demonstrate that SGCD can effectively enhance the performance of downstream task models on the target domain. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XUmGMBRv4M@OpenReview",
      "index": 197,
      "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
      "authors": [
        "Akhiad Bercovich",
        "Mohammed Dabbah",
        "Omri Puny",
        "Ido Galil",
        "Amnon Geifman",
        "Yonatan Geifman",
        "Izhak Golan",
        "Ehud Dov Karpas",
        "Itay Levy",
        "Zach Moshe",
        "Najeeb Nabwani",
        "Tomer Ronen",
        "Itamar Schen",
        "Ido Shahaf",
        "Oren Tropp",
        "Ran Zilberstein",
        "Ran El-Yaniv"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ffn",
        "253b",
        "parallelized",
        "layers",
        "fusion",
        "sequential",
        "rethinking",
        "latency",
        "405b",
        "computation"
      ],
      "summary": "We introduce \\textit{FFN Fusion}, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create a 253B model (253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71 × × speedup in inference latency and 35 × × lower per-token cost while maintaining strong performance across benchmarks. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XUmGMBRv4M"
        ],
        "venue": [
          "/venue/XUmGMBRv4M@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XUmGMBRv4M"
        ],
        "detail": [
          "https://openreview.net/forum?id=XUmGMBRv4M"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "FFN Fusion: Rethinking Sequential Computation in Large Language Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Akhiad Bercovich , Mohammed Dabbah , Omri Puny , Ido Galil , Amnon Geifman , Yonatan Geifman , Izhak Golan , Ehud Dov Karpas , Itay Levy , Zach Moshe , Najeeb Nabwani , Tomer Ronen , Itamar Schen , Ido Shahaf , Oren Tropp , Ran Zilberstein , Ran El-Yaniv We introduce \\textit{FFN Fusion}, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create a 253B model (253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71 × × speedup in inference latency and 35 × × lower per-token cost while maintaining strong performance across benchmarks. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VkicTqszOn@OpenReview",
      "index": 198,
      "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
      "authors": [
        "Minki Kang",
        "Jongwon Jeong",
        "Seanie Lee",
        "Jaewoong Cho",
        "Sung Ju Hwang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "slms",
        "distillation",
        "agent",
        "distilling",
        "reasoning",
        "cot",
        "factual",
        "llm",
        "agents",
        "teacher"
      ],
      "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VkicTqszOn"
        ],
        "venue": [
          "/venue/VkicTqszOn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VkicTqszOn"
        ],
        "detail": [
          "https://openreview.net/forum?id=VkicTqszOn"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Distilling LLM Agent into Small Models with Retrieval and Code Tools [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Minki Kang , Jongwon Jeong , Seanie Lee , Jaewoong Cho , Sung Ju Hwang Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "76cFMRgEzQ@OpenReview",
      "index": 199,
      "title": "Understanding Parametric and Contextual Knowledge Reconciliation within Large Language Models",
      "authors": [
        "Jun Zhao",
        "Yongzhuo Yang",
        "Xiang Hu",
        "Jingqi Tong",
        "Yi Lu",
        "Wei Wu",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "knowledge",
        "contextual",
        "llms",
        "entity",
        "probing",
        "parametric",
        "layers",
        "trainable",
        "conflicting",
        "reconciliation"
      ],
      "summary": "Retrieval-Augmented Generation (RAG) provides additional contextual knowledge to complement the parametric knowledge in Large Language Models (LLMs). These two knowledge interweave to enhance the accuracy and timeliness of LLM responses. However, the internal mechanisms by which LLMs utilize these knowledge remain unclear. We propose modeling the forward propagation of knowledge as an entity flow, employing this framework to trace LLMs' internal behaviors when processing mixed-source knowledge. Linear probing utilizes a trainable linear classifier to detect specific attributes in hidden layers. However, once trained, a probe cannot adapt to dynamically specified entities. To address this challenge, we construct an entity-aware probe, which introduces special tokens to mark probing targets and employs a small trainable rank-8 lora update to process these special markers. We first verify this approach through an attribution experiment, demonstrating that it can accurately detect information about ad-hoc entities from complex hidden states. Next, we trace entity flows across layers to understand how LLMs reconcile conflicting knowledge internally. Our probing results reveal that contextual and parametric knowledge are routed between tokens through distinct sets of attention heads, supporting attention competition only within knowledge types. While conflicting knowledge maintains a residual presence across layers, aligned knowledge from multiple sources gradually accumulates, with the magnitude of this accumulation directly determining its influence on final outputs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=76cFMRgEzQ"
        ],
        "venue": [
          "/venue/76cFMRgEzQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=76cFMRgEzQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=76cFMRgEzQ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Understanding Parametric and Contextual Knowledge Reconciliation within Large Language Models [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Jun Zhao , Yongzhuo Yang , Xiang Hu , Jingqi Tong , Yi Lu , Wei Wu , Tao Gui , Qi Zhang , Xuanjing Huang Retrieval-Augmented Generation (RAG) provides additional contextual knowledge to complement the parametric knowledge in Large Language Models (LLMs). These two knowledge interweave to enhance the accuracy and timeliness of LLM responses. However, the internal mechanisms by which LLMs utilize these knowledge remain unclear. We propose modeling the forward propagation of knowledge as an entity flow, employing this framework to trace LLMs' internal behaviors when processing mixed-source knowledge. Linear probing utilizes a trainable linear classifier to detect specific attributes in hidden layers. However, once trained, a probe cannot adapt to dynamically specified entities. To address this challenge, we construct an entity-aware probe, which introduces special tokens to mark probing targets and employs a small trainable rank-8 lora update to process these special markers. We first verify this approach through an attribution experiment, demonstrating that it can accurately detect information about ad-hoc entities from complex hidden states. Next, we trace entity flows across layers to understand how LLMs reconcile conflicting knowledge internally. Our probing results reveal that contextual and parametric knowledge are routed between tokens through distinct sets of attention heads, supporting attention competition only within knowledge types. While conflicting knowledge maintains a residual presence across layers, aligned knowledge from multiple sources gradually accumulates, with the magnitude of this accumulation directly determining its influence on final outputs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "F11iEhKoYp@OpenReview",
      "index": 200,
      "title": "Differentiable Decision Tree via \"ReLU+Argmin\" Reformulation",
      "authors": [
        "Qiangqiang Mao",
        "Jiayang Ren",
        "Yixiu Wang",
        "Chenxuanyin Zou",
        "Jingjing Zheng",
        "Yankai Cao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "argmin",
        "tree",
        "relu",
        "reformulation",
        "decision",
        "differentiable",
        "softmin",
        "optimization",
        "zero",
        "issues"
      ],
      "summary": "Decision tree, despite its unmatched interpretability and lightweight structure, faces two key issues that limit its broader applicability: non-differentiability and low testing accuracy. This study addresses these issues by developing a differentiable oblique tree that optimizes the entire tree using gradient-based optimization. We propose an exact reformulation of hard-split trees based on \"ReLU+Argmin\" mechanism, and then cast the reformulated tree training as an unconstrained optimization task. The ReLU-based sample branching, expressed as exact-zero or non-zero values, preserve a unique decision path, in contrast to soft decision trees with probabilistic routing. The subsequent Argmin operation identifies the unique zero-violation path, enabling deterministic predictions. For effective gradient flow, we approximate Argmin behaviors by scaling softmin function. To ameliorate numerical instability, we propose a warm-start annealing scheme that solves multiple optimization tasks with increasingly accurate approximations. This reformulation alongside distributed GPU parallelism offers strong scalability, supporting 12-depth tree even on million-scale datasets where most baselines fail. Extensive experiments demonstrate that our optimized tree achieves a superior testing accuracy against 14 baselines, including an average improvement of 7.54\\% over CART.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=F11iEhKoYp"
        ],
        "venue": [
          "/venue/F11iEhKoYp@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=F11iEhKoYp"
        ],
        "detail": [
          "https://openreview.net/forum?id=F11iEhKoYp"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Differentiable Decision Tree via \"ReLU+Argmin\" Reformulation [PDF ] [Copy] [Kimi ] [REL] Authors : Qiangqiang Mao , Jiayang Ren , Yixiu Wang , Chenxuanyin Zou , Jingjing Zheng , Yankai Cao Decision tree, despite its unmatched interpretability and lightweight structure, faces two key issues that limit its broader applicability: non-differentiability and low testing accuracy. This study addresses these issues by developing a differentiable oblique tree that optimizes the entire tree using gradient-based optimization. We propose an exact reformulation of hard-split trees based on \"ReLU+Argmin\" mechanism, and then cast the reformulated tree training as an unconstrained optimization task. The ReLU-based sample branching, expressed as exact-zero or non-zero values, preserve a unique decision path, in contrast to soft decision trees with probabilistic routing. The subsequent Argmin operation identifies the unique zero-violation path, enabling deterministic predictions. For effective gradient flow, we approximate Argmin behaviors by scaling softmin function. To ameliorate numerical instability, we propose a warm-start annealing scheme that solves multiple optimization tasks with increasingly accurate approximations. This reformulation alongside distributed GPU parallelism offers strong scalability, supporting 12-depth tree even on million-scale datasets where most baselines fail. Extensive experiments demonstrate that our optimized tree achieves a superior testing accuracy against 14 baselines, including an average improvement of 7.54\\% over CART. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "dt940loCBT@OpenReview",
      "index": 201,
      "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning",
      "authors": [
        "Borong Zhang",
        "Yuhao Zhang",
        "Jiaming Ji",
        "Yingshan Lei",
        "Josef Dai",
        "Yuanpei Chen",
        "Yaodong Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "safety",
        "vlas",
        "policies",
        "isa",
        "safevla",
        "vision",
        "constrained",
        "risks",
        "language",
        "action"
      ],
      "summary": "Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. *How can safety constraints be explicitly integrated into VLAs?* We address this by exploring an integrated safety approach (ISA), systematically **modeling** safety requirements, then actively **eliciting** diverse unsafe behaviors, effectively **constraining** VLA policies via safe reinforcement learning, and rigorously **assuring** their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective **safety-performance trade-offs**, reducing the cumulative cost of safety violations by 83.58\\% compared to the state-of-the-art method, while also maintaining task success rate (+3.85\\%). (II) strong **safety assurance**, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust **generalization** of learned safety behaviors to various out-of-distribution perturbations. The effectiveness is evaluated on long-horizon mobile manipulation tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dt940loCBT"
        ],
        "venue": [
          "/venue/dt940loCBT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dt940loCBT"
        ],
        "detail": [
          "https://openreview.net/forum?id=dt940loCBT"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Borong Zhang , Yuhao Zhang , Jiaming Ji , Yingshan Lei , Josef Dai , Yuanpei Chen , Yaodong Yang Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. *How can safety constraints be explicitly integrated into VLAs?* We address this by exploring an integrated safety approach (ISA), systematically **modeling** safety requirements, then actively **eliciting** diverse unsafe behaviors, effectively **constraining** VLA policies via safe reinforcement learning, and rigorously **assuring** their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective **safety-performance trade-offs**, reducing the cumulative cost of safety violations by 83.58\\% compared to the state-of-the-art method, while also maintaining task success rate (+3.85\\%). (II) strong **safety assurance**, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust **generalization** of learned safety behaviors to various out-of-distribution perturbations. The effectiveness is evaluated on long-horizon mobile manipulation tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "PZaxCfLGLA@OpenReview",
      "index": 202,
      "title": "3D Interaction Geometric Pre-training for Molecular Relational Learning",
      "authors": [
        "Namkyeong Lee",
        "Yunhak Oh",
        "Heewoong Noh",
        "Gyoung S. Na",
        "Minkai Xu",
        "Hanchen",
        "Tianfan Fu",
        "Chanyoung Park"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mrl",
        "3dmrl",
        "interaction",
        "relational",
        "geometric",
        "molecular",
        "namkyeong",
        "pre",
        "molecules",
        "virtual"
      ],
      "summary": "Molecular Relational Learning (MRL) is a rapidly growing field that focuses on understanding the interaction dynamics between molecules, which is crucial for applications ranging from catalyst engineering to drug discovery. Despite recent progress, earlier MRL approaches are limited to using only the 2D topological structure of molecules, as obtaining the 3D interaction geometry remains prohibitively expensive. This paper introduces a novel 3D geometric pre-training strategy for MRL (3DMRL) that incorporates a 3D virtual interaction environment, overcoming the limitations of costly traditional quantum mechanical calculation methods. With the constructed 3D virtual interaction environment, 3DMRL trains 2D MRL model to learn the global and local 3D geometric information of molecular interaction. Extensive experiments on various tasks using real-world datasets, including out-of-distribution and extrapolation scenarios, demonstrate the effectiveness of 3DMRL, showing up to a 24.93% improvement in performance across 40 tasks. Our code is publicly available at https://github.com/Namkyeong/3DMRL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PZaxCfLGLA"
        ],
        "venue": [
          "/venue/PZaxCfLGLA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PZaxCfLGLA"
        ],
        "detail": [
          "https://openreview.net/forum?id=PZaxCfLGLA"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "3D Interaction Geometric Pre-training for Molecular Relational Learning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Namkyeong Lee , Yunhak Oh , Heewoong Noh , Gyoung S. Na , Minkai Xu , Hanchen , Tianfan Fu , Chanyoung Park Molecular Relational Learning (MRL) is a rapidly growing field that focuses on understanding the interaction dynamics between molecules, which is crucial for applications ranging from catalyst engineering to drug discovery. Despite recent progress, earlier MRL approaches are limited to using only the 2D topological structure of molecules, as obtaining the 3D interaction geometry remains prohibitively expensive. This paper introduces a novel 3D geometric pre-training strategy for MRL (3DMRL) that incorporates a 3D virtual interaction environment, overcoming the limitations of costly traditional quantum mechanical calculation methods. With the constructed 3D virtual interaction environment, 3DMRL trains 2D MRL model to learn the global and local 3D geometric information of molecular interaction. Extensive experiments on various tasks using real-world datasets, including out-of-distribution and extrapolation scenarios, demonstrate the effectiveness of 3DMRL, showing up to a 24.93% improvement in performance across 40 tasks. Our code is publicly available at https://github.com/Namkyeong/3DMRL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "pXoiIDdynI@OpenReview",
      "index": 203,
      "title": "Improved Bounds for Swap Multicalibration and Swap Omniprediction",
      "authors": [
        "Haipeng Luo",
        "Spandan Senapati",
        "Vatsal Sharan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "swap",
        "multicalibration",
        "omniprediction",
        "tilde",
        "mathcal",
        "ell",
        "varepsilon",
        "improved",
        "sample",
        "efficiently"
      ],
      "summary": "In this paper, we consider the related problems of multicalibration --- a multigroup fairness notion and omniprediction --- a simultaneous loss minimization paradigm, both in the distributional and online settings. The recent work of Garg et al. (2024) raised the open problem of whether it is possible to efficiently achieve O ~ ( T − − √ ) O ~ ( T ) ℓ 2 ℓ 2 -multicalibration error against bounded linear functions. In this paper, we answer this question in a strongly affirmative sense. We propose an efficient algorithm that achieves O ~ ( T 1 3 ) O ~ ( T 1 3 ) ℓ 2 ℓ 2 -swap multicalibration error (both in high probability and expectation). On propagating this bound onward, we obtain significantly improved rates for ℓ 1 ℓ 1 -swap multicalibration and swap omniprediction for a loss class of convex Lipschitz functions. In particular, we show that our algorithm achieves O ~ ( T 2 3 ) O ~ ( T 2 3 ) ℓ 1 ℓ 1 -swap multicalibration and swap omniprediction errors, thereby improving upon the previous best-known bound of O ~ ( T 7 8 ) O ~ ( T 7 8 ) . As a consequence of our improved online results, we further obtain several improved sample complexity rates in the distributional setting. In particular, we establish a O ~ ( ε − 3 ) O ~ ( ε − 3 ) sample complexity of efficiently learning an ε ε -swap omnipredictor for the class of convex and Lipschitz functions, O ~ ( ε − 2.5 ) O ~ ( ε − 2.5 ) sample complexity of efficiently learning an ε ε -swap agnostic learner for the squared loss, and O ~ ( ε − 5 ) , O ~ ( ε − 2.5 ) O ~ ( ε − 5 ) , O ~ ( ε − 2.5 ) sample complexities of learning ℓ 1 , ℓ 2 ℓ 1 , ℓ 2 -swap multicalibrated predictors against linear functions, all of which significantly improve on the previous best-known bounds.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pXoiIDdynI"
        ],
        "venue": [
          "/venue/pXoiIDdynI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pXoiIDdynI"
        ],
        "detail": [
          "https://openreview.net/forum?id=pXoiIDdynI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Improved Bounds for Swap Multicalibration and Swap Omniprediction [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Haipeng Luo , Spandan Senapati , Vatsal Sharan In this paper, we consider the related problems of multicalibration --- a multigroup fairness notion and omniprediction --- a simultaneous loss minimization paradigm, both in the distributional and online settings. The recent work of Garg et al. (2024) raised the open problem of whether it is possible to efficiently achieve O ~ ( T − − √ ) O ~ ( T ) ℓ 2 ℓ 2 -multicalibration error against bounded linear functions. In this paper, we answer this question in a strongly affirmative sense. We propose an efficient algorithm that achieves O ~ ( T 1 3 ) O ~ ( T 1 3 ) ℓ 2 ℓ 2 -swap multicalibration error (both in high probability and expectation). On propagating this bound onward, we obtain significantly improved rates for ℓ 1 ℓ 1 -swap multicalibration and swap omniprediction for a loss class of convex Lipschitz functions. In particular, we show that our algorithm achieves O ~ ( T 2 3 ) O ~ ( T 2 3 ) ℓ 1 ℓ 1 -swap multicalibration and swap omniprediction errors, thereby improving upon the previous best-known bound of O ~ ( T 7 8 ) O ~ ( T 7 8 ) . As a consequence of our improved online results, we further obtain several improved sample complexity rates in the distributional setting. In particular, we establish a O ~ ( ε − 3 ) O ~ ( ε − 3 ) sample complexity of efficiently learning an ε ε -swap omnipredictor for the class of convex and Lipschitz functions, O ~ ( ε − 2.5 ) O ~ ( ε − 2.5 ) sample complexity of efficiently learning an ε ε -swap agnostic learner for the squared loss, and O ~ ( ε − 5 ) , O ~ ( ε − 2.5 ) O ~ ( ε − 5 ) , O ~ ( ε − 2.5 ) sample complexities of learning ℓ 1 , ℓ 2 ℓ 1 , ℓ 2 -swap multicalibrated predictors against linear functions, all of which significantly improve on the previous best-known bounds. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "BRklmFlCsD@OpenReview",
      "index": 204,
      "title": "UniteFormer: Unifying Node and Edge Modalities in Transformers for Vehicle Routing Problems",
      "authors": [
        "Dian Meng",
        "Zhiguang Cao",
        "Jie Gao",
        "Yaoxin Wu",
        "Yaqing Hou"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "uniteformer",
        "node",
        "edge",
        "modalities",
        "vehicle",
        "routing",
        "vrp",
        "cvrplib",
        "input",
        "tsplib"
      ],
      "summary": "Neural solvers for the Vehicle Routing Problem (VRP) have typically relied on either node or edge inputs, limiting their flexibility and generalization in real-world scenarios. We propose UniteFormer, a unified neural solver that supports node-only, edge-only, and hybrid input types through a single model trained via joint edge-node modalities. UniteFormer introduces: (1) a mixed encoder that integrates graph convolutional networks and attention mechanisms to collaboratively process node and edge features, capturing cross-modal interactions between them; and (2) a parallel decoder enhanced with query mapping and a feed-forward layer for improved representation. The model is trained with REINFORCE by randomly sampling input types across batches. Experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that UniteFormer achieves state-of-the-art performance and generalizes effectively to TSPLib and CVRPLib instances. These results underscore UniteFormer’s ability to handle diverse input modalities and its strong potential to improve performance across various VRP tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BRklmFlCsD"
        ],
        "venue": [
          "/venue/BRklmFlCsD@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BRklmFlCsD"
        ],
        "detail": [
          "https://openreview.net/forum?id=BRklmFlCsD"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "UniteFormer: Unifying Node and Edge Modalities in Transformers for Vehicle Routing Problems [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Dian Meng , Zhiguang Cao , Jie Gao , Yaoxin Wu , Yaqing Hou Neural solvers for the Vehicle Routing Problem (VRP) have typically relied on either node or edge inputs, limiting their flexibility and generalization in real-world scenarios. We propose UniteFormer, a unified neural solver that supports node-only, edge-only, and hybrid input types through a single model trained via joint edge-node modalities. UniteFormer introduces: (1) a mixed encoder that integrates graph convolutional networks and attention mechanisms to collaboratively process node and edge features, capturing cross-modal interactions between them; and (2) a parallel decoder enhanced with query mapping and a feed-forward layer for improved representation. The model is trained with REINFORCE by randomly sampling input types across batches. Experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that UniteFormer achieves state-of-the-art performance and generalizes effectively to TSPLib and CVRPLib instances. These results underscore UniteFormer’s ability to handle diverse input modalities and its strong potential to improve performance across various VRP tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Ib4ZXPXpss@OpenReview",
      "index": 205,
      "title": "Simultaneous Swap Regret Minimization via KL-Calibration",
      "authors": [
        "Haipeng Luo",
        "Spandan Senapati",
        "Vatsal Sharan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "swap",
        "calibration",
        "regret",
        "tilde",
        "mathcal",
        "pseudo",
        "univariate",
        "loss",
        "forecaster",
        "log"
      ],
      "summary": "Calibration is a fundamental concept that aims at ensuring the reliability of probabilistic predictions by aligning them with real-world outcomes. There is a surge of studies on new calibration measures that are easier to optimize compared to the classical ℓ 1 ℓ 1 -Calibration while still having strong implications for downstream applications. One recent such example is the work by Fishelson et al. (2025) who show that it is possible to achieve O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) pseudo ℓ 2 ℓ 2 -Calibration error via minimizing pseudo swap regret of the squared loss, which in fact implies the same bound for all bounded proper losses with a smooth univariate form. In this work, we significantly generalize their result in the following ways: (a) in addition to smooth univariate forms, our algorithm also simultaneously achieves O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) swap regret for any proper loss with a twice continuously differentiable univariate form (such as Tsallis entropy); (b) our bounds hold not only for pseudo swap regret that measures losses using the forecaster's distributions on predictions, but also hold for the actual swap regret that measures losses using the forecaster's actual realized predictions. We achieve so by introducing a new stronger notion of calibration called (pseudo) KL-Calibration, which we show is equivalent to the (pseudo) swap regret with respect to log loss. We prove that there exists an algorithm that achieves O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) KL-Calibration error and provide an explicit algorithm that achieves O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) pseudo KL-Calibration error. Moreover, we show that the same algorithm achieves O ( T 1 / 3 ( log T ) − 1 3 log ( T / δ ) ) O ( T 1 / 3 ( log ⁡ T ) − 1 3 log ⁡ ( T / δ ) ) swap regret with probability at least 1 − δ 1 − δ for any proper loss with a smooth univariate form, which implies O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) ℓ 2 ℓ 2 -Calibration error. A technical contribution of our work is a new randomized rounding procedure and a non-uniform discretization scheme to minimize the swap regret for log loss.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Ib4ZXPXpss"
        ],
        "venue": [
          "/venue/Ib4ZXPXpss@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Ib4ZXPXpss"
        ],
        "detail": [
          "https://openreview.net/forum?id=Ib4ZXPXpss"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Simultaneous Swap Regret Minimization via KL-Calibration [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Haipeng Luo , Spandan Senapati , Vatsal Sharan Calibration is a fundamental concept that aims at ensuring the reliability of probabilistic predictions by aligning them with real-world outcomes. There is a surge of studies on new calibration measures that are easier to optimize compared to the classical ℓ 1 ℓ 1 -Calibration while still having strong implications for downstream applications. One recent such example is the work by Fishelson et al. (2025) who show that it is possible to achieve O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) pseudo ℓ 2 ℓ 2 -Calibration error via minimizing pseudo swap regret of the squared loss, which in fact implies the same bound for all bounded proper losses with a smooth univariate form. In this work, we significantly generalize their result in the following ways: (a) in addition to smooth univariate forms, our algorithm also simultaneously achieves O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) swap regret for any proper loss with a twice continuously differentiable univariate form (such as Tsallis entropy); (b) our bounds hold not only for pseudo swap regret that measures losses using the forecaster's distributions on predictions, but also hold for the actual swap regret that measures losses using the forecaster's actual realized predictions. We achieve so by introducing a new stronger notion of calibration called (pseudo) KL-Calibration, which we show is equivalent to the (pseudo) swap regret with respect to log loss. We prove that there exists an algorithm that achieves O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) KL-Calibration error and provide an explicit algorithm that achieves O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) pseudo KL-Calibration error. Moreover, we show that the same algorithm achieves O ( T 1 / 3 ( log T ) − 1 3 log ( T / δ ) ) O ( T 1 / 3 ( log ⁡ T ) − 1 3 log ⁡ ( T / δ ) ) swap regret with probability at least 1 − δ 1 − δ for any proper loss with a smooth univariate form, which implies O ~ ( T 1 / 3 ) O ~ ( T 1 / 3 ) ℓ 2 ℓ 2 -Calibration error. A technical contribution of our work is a new randomized rounding procedure and a non-uniform discretization scheme to minimize the swap regret for log loss. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "b50IW9yV2M@OpenReview",
      "index": 206,
      "title": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints",
      "authors": [
        "Dongjie Yang",
        "Chengqiang Lu",
        "Qimeng Wang",
        "Xinbei Ma",
        "Yan Gao",
        "Yao Hu",
        "hai zhao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "planning",
        "constraints",
        "multifaceted",
        "travel",
        "maop",
        "thinking",
        "world",
        "horizon",
        "real",
        "llm"
      ],
      "summary": "Unlike reasoning, which often entails a deep sequence of deductive steps, complex real-world planning is characterized by the need to synthesize a broad spectrum of parallel and potentially conflicting information and constraints. For example, in travel planning scenarios, it requires the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints, leading to suboptimal solutions. Motivated by the challenges of real-world travel planning, this paper introduces the Multiple Aspects of Planning (MAoP), empowering LLMs with \"wide-horizon thinking\" to solve planning problems with multifaceted constraints. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planners, enabling strong inference-time scalability by scaling aspects to consider various constraints. In addition, existing benchmarks for multi-constraint planning are flawed because they assess constraints in isolation, ignoring causal dependencies within the constraints, e.g, travel planning, where past activities dictate future itinerary. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world simulation, thereby inherently resolving these causal dependencies. This paper advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through simulation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=b50IW9yV2M"
        ],
        "venue": [
          "/venue/b50IW9yV2M@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=b50IW9yV2M"
        ],
        "detail": [
          "https://openreview.net/forum?id=b50IW9yV2M"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints [PDF ] [Copy] [Kimi ] [REL] Authors : Dongjie Yang , Chengqiang Lu , Qimeng Wang , Xinbei Ma , Yan Gao , Yao Hu , hai zhao Unlike reasoning, which often entails a deep sequence of deductive steps, complex real-world planning is characterized by the need to synthesize a broad spectrum of parallel and potentially conflicting information and constraints. For example, in travel planning scenarios, it requires the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints, leading to suboptimal solutions. Motivated by the challenges of real-world travel planning, this paper introduces the Multiple Aspects of Planning (MAoP), empowering LLMs with \"wide-horizon thinking\" to solve planning problems with multifaceted constraints. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planners, enabling strong inference-time scalability by scaling aspects to consider various constraints. In addition, existing benchmarks for multi-constraint planning are flawed because they assess constraints in isolation, ignoring causal dependencies within the constraints, e.g, travel planning, where past activities dictate future itinerary. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world simulation, thereby inherently resolving these causal dependencies. This paper advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through simulation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5BS6gBb4yP@OpenReview",
      "index": 207,
      "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?",
      "authors": [
        "Yihao Li",
        "Saeed Salehi",
        "Lyle Ungar",
        "Konrad Kording"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "object",
        "issameobject",
        "binding",
        "vits",
        "naturally",
        "attention",
        "emerges",
        "belong",
        "transformers",
        "pretraining"
      ],
      "summary": "Object binding, the brain’s ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high‑level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term *IsSameObject*. We decode *IsSameObject* from patch embeddings across ViT layers using a similarity probe, which reaches over 90\\% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that *IsSameObject* is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating *IsSameObject* from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of “which parts belong together” emerges naturally in a connectionist system.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5BS6gBb4yP"
        ],
        "venue": [
          "/venue/5BS6gBb4yP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5BS6gBb4yP"
        ],
        "detail": [
          "https://openreview.net/forum?id=5BS6gBb4yP"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers? [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Yihao Li , Saeed Salehi , Lyle Ungar , Konrad Kording Object binding, the brain’s ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high‑level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term *IsSameObject*. We decode *IsSameObject* from patch embeddings across ViT layers using a similarity probe, which reaches over 90\\% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that *IsSameObject* is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating *IsSameObject* from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of “which parts belong together” emerges naturally in a connectionist system. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ycnc9aLnQu@OpenReview",
      "index": 208,
      "title": "Probing Neural Combinatorial Optimization Models",
      "authors": [
        "Zhiqin Zhang",
        "Yining Ma",
        "Zhiguang Cao",
        "Hoong Chuin Lau"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "nco",
        "probing",
        "representations",
        "insights",
        "models",
        "combinatorial",
        "deeper",
        "significance",
        "optimization",
        "learned"
      ],
      "summary": "Neural combinatorial optimization (NCO) has achieved remarkable performance, yet its learned model representations and decision rationale remain a black box. This impedes both academic research and practical deployment, since researchers and stakeholders require deeper insights into NCO models. In this paper, we take the first critical step towards interpreting NCO models by investigating their representations through various probing tasks. Moreover, we introduce a novel probing tool named Coefficient Significance Probing (CS-Probing) to enable deeper analysis of NCO representations by examining the coefficients and statistical significance during probing. Extensive experiments and analysis reveal that NCO models encode low-level information essential for solution construction, while capturing high-level knowledge to facilitate better decisions. Using CS-Probing, we find that prevalent NCO models impose varying inductive biases on their learned representations, uncover direct evidence related to model generalization, and identify key embedding dimensions associated with specific knowledge. These insights can be potentially translated into practice, for example, with minor code modifications, we improve the generalization of the analyzed model. Our work represents a first systematic attempt to interpret black-box NCO models, showcasing probing as a promising tool for analyzing their internal mechanisms and revealing insights for the NCO community. The source code is publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ycnc9aLnQu"
        ],
        "venue": [
          "/venue/ycnc9aLnQu@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ycnc9aLnQu"
        ],
        "detail": [
          "https://openreview.net/forum?id=ycnc9aLnQu"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Probing Neural Combinatorial Optimization Models [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zhiqin Zhang , Yining Ma , Zhiguang Cao , Hoong Chuin Lau Neural combinatorial optimization (NCO) has achieved remarkable performance, yet its learned model representations and decision rationale remain a black box. This impedes both academic research and practical deployment, since researchers and stakeholders require deeper insights into NCO models. In this paper, we take the first critical step towards interpreting NCO models by investigating their representations through various probing tasks. Moreover, we introduce a novel probing tool named Coefficient Significance Probing (CS-Probing) to enable deeper analysis of NCO representations by examining the coefficients and statistical significance during probing. Extensive experiments and analysis reveal that NCO models encode low-level information essential for solution construction, while capturing high-level knowledge to facilitate better decisions. Using CS-Probing, we find that prevalent NCO models impose varying inductive biases on their learned representations, uncover direct evidence related to model generalization, and identify key embedding dimensions associated with specific knowledge. These insights can be potentially translated into practice, for example, with minor code modifications, we improve the generalization of the analyzed model. Our work represents a first systematic attempt to interpret black-box NCO models, showcasing probing as a promising tool for analyzing their internal mechanisms and revealing insights for the NCO community. The source code is publicly available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Gte3F0ONhr@OpenReview",
      "index": 209,
      "title": "A Principled Path to Fitted Distributional Evaluation",
      "authors": [
        "Sungee Hong",
        "Jiayi Wang",
        "Zhengling Qi",
        "Raymond K. W. Wong"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fde",
        "distributional",
        "fitted",
        "evaluation",
        "ope",
        "policy",
        "methods",
        "principled",
        "reinforcement",
        "focuses"
      ],
      "summary": "In reinforcement learning, distributional off-policy evaluation (OPE) focuses on estimating the return distribution of a target policy using offline data collected under a different policy. This work focuses on extending the widely used fitted Q-evaluation---developed for expectation-based reinforcement learning---to the distributional OPE setting. We refer to this extension as fitted distributional evaluation (FDE). While only a few related approaches exist, there remains no unified framework for designing FDE methods. To fill this gap, we present a set of guiding principles for constructing theoretically grounded FDE methods. Building on these principles, we develop several new FDE methods with convergence analysis and provide theoretical justification for existing methods, even in non-tabular environments. Extensive experiments, including simulations on linear quadratic regulators and Atari games, demonstrate the superior performance of the FDE methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Gte3F0ONhr"
        ],
        "venue": [
          "/venue/Gte3F0ONhr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Gte3F0ONhr"
        ],
        "detail": [
          "https://openreview.net/forum?id=Gte3F0ONhr"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "A Principled Path to Fitted Distributional Evaluation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Sungee Hong , Jiayi Wang , Zhengling Qi , Raymond K. W. Wong In reinforcement learning, distributional off-policy evaluation (OPE) focuses on estimating the return distribution of a target policy using offline data collected under a different policy. This work focuses on extending the widely used fitted Q-evaluation---developed for expectation-based reinforcement learning---to the distributional OPE setting. We refer to this extension as fitted distributional evaluation (FDE). While only a few related approaches exist, there remains no unified framework for designing FDE methods. To fill this gap, we present a set of guiding principles for constructing theoretically grounded FDE methods. Building on these principles, we develop several new FDE methods with convergence analysis and provide theoretical justification for existing methods, even in non-tabular environments. Extensive experiments, including simulations on linear quadratic regulators and Atari games, demonstrate the superior performance of the FDE methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kZstGANG8D@OpenReview",
      "index": 210,
      "title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent",
      "authors": [
        "Yuheng Zhang",
        "Dian Yu",
        "Tao Ge",
        "Linfeng Song",
        "Zhichen Zeng",
        "Haitao Mi",
        "Nan Jiang",
        "Dong Yu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "alignment",
        "optimistic",
        "rlhf",
        "preferences",
        "llm",
        "mirror",
        "descent",
        "assumption",
        "improving",
        "online"
      ],
      "summary": "Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an O ( T − 1 ) O ( T − 1 ) bound on the duality gap, improving upon the previous O ( T − 1 / 2 ) O ( T − 1 / 2 ) result. Meanwhile, it enjoys a linear convergence rate in the last iterate, a property not achieved by previous methods. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kZstGANG8D"
        ],
        "venue": [
          "/venue/kZstGANG8D@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kZstGANG8D"
        ],
        "detail": [
          "https://openreview.net/forum?id=kZstGANG8D"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yuheng Zhang , Dian Yu , Tao Ge , Linfeng Song , Zhichen Zeng , Haitao Mi , Nan Jiang , Dong Yu Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an O ( T − 1 ) O ( T − 1 ) bound on the duality gap, improving upon the previous O ( T − 1 / 2 ) O ( T − 1 / 2 ) result. Meanwhile, it enjoys a linear convergence rate in the last iterate, a property not achieved by previous methods. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "vf2GHcxzMV@OpenReview",
      "index": 211,
      "title": "Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities",
      "authors": [
        "Tara Akhound-Sadegh",
        "Jungyoon Lee",
        "Joey Bose",
        "Valentin De Bortoli",
        "Arnaud Doucet",
        "Michael M. Bronstein",
        "Dominique Beaini",
        "Siamak Ravanbakhsh",
        "Kirill Neklyudov",
        "Alexander Tong"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pita",
        "diffusion",
        "annealing",
        "samplers",
        "progressive",
        "boltzmann",
        "inference",
        "sampling",
        "tripeptides",
        "samples"
      ],
      "summary": "Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA) a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to *procure training samples at a lower temperature* for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N N -body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vf2GHcxzMV"
        ],
        "venue": [
          "/venue/vf2GHcxzMV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vf2GHcxzMV"
        ],
        "detail": [
          "https://openreview.net/forum?id=vf2GHcxzMV"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities [PDF ] [Copy] [Kimi ] [REL] Authors : Tara Akhound-Sadegh , Jungyoon Lee , Joey Bose , Valentin De Bortoli , Arnaud Doucet , Michael M. Bronstein , Dominique Beaini , Siamak Ravanbakhsh , Kirill Neklyudov , Alexander Tong Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA) a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to *procure training samples at a lower temperature* for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N N -body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "om2CpclG4y@OpenReview",
      "index": 212,
      "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
      "authors": [
        "Jaesik Yoon",
        "Hyeonseo Cho",
        "Sungjin Ahn"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mctd",
        "plan",
        "composer",
        "tree",
        "planning",
        "diffusion",
        "monte",
        "carlo",
        "compositional",
        "trajectory"
      ],
      "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=om2CpclG4y"
        ],
        "venue": [
          "/venue/om2CpclG4y@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=om2CpclG4y"
        ],
        "detail": [
          "https://openreview.net/forum?id=om2CpclG4y"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Compositional Monte Carlo Tree Diffusion for Extendable Planning [PDF ] [Copy] [Kimi ] [REL] Authors : Jaesik Yoon , Hyeonseo Cho , Sungjin Ahn Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "JRVZTACwb0@OpenReview",
      "index": 213,
      "title": "Fast Monte Carlo Tree Diffusion: 100× Speedup via Parallel and Sparse Planning",
      "authors": [
        "Jaesik Yoon",
        "Hyeonseo Cho",
        "Yoshua Bengio",
        "Sungjin Ahn"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mctd",
        "tree",
        "planning",
        "fast",
        "diffusion",
        "diffuser",
        "speedup",
        "parallel",
        "monte",
        "carlo"
      ],
      "summary": "Diffusion models have recently emerged as a powerful approach for trajectory planning. However, their inherently non-sequential nature limits their effectiveness in long-horizon reasoning tasks at test time. The recently proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by combining diffusion with tree-based search, achieving state-of-the-art performance on complex planning problems. Despite its strengths, our analysis shows that MCTD incurs substantial computational overhead due to the sequential nature of tree search and the cost of iterative denoising. To address this, we propose Fast-MCTD, a more efficient variant that preserves the strengths of MCTD while significantly improving its speed and scalability. Fast-MCTD integrates two techniques: Parallel MCTD, which enables parallel rollouts via delayed tree updates and redundancy-aware selection; and Sparse MCTD, which reduces rollout length through trajectory coarsening. Experiments show that Fast-MCTD achieves up to 100× speedup over standard MCTD while maintaining or improving planning performance. Remarkably, it even outperforms Diffuser in inference speed on some tasks, despite Diffuser requiring no search and yielding weaker solutions. These results position Fast-MCTD as a practical and scalable solution for diffusion-based inference-time reasoning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JRVZTACwb0"
        ],
        "venue": [
          "/venue/JRVZTACwb0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JRVZTACwb0"
        ],
        "detail": [
          "https://openreview.net/forum?id=JRVZTACwb0"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Fast Monte Carlo Tree Diffusion: 100× Speedup via Parallel and Sparse Planning [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jaesik Yoon , Hyeonseo Cho , Yoshua Bengio , Sungjin Ahn Diffusion models have recently emerged as a powerful approach for trajectory planning. However, their inherently non-sequential nature limits their effectiveness in long-horizon reasoning tasks at test time. The recently proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by combining diffusion with tree-based search, achieving state-of-the-art performance on complex planning problems. Despite its strengths, our analysis shows that MCTD incurs substantial computational overhead due to the sequential nature of tree search and the cost of iterative denoising. To address this, we propose Fast-MCTD, a more efficient variant that preserves the strengths of MCTD while significantly improving its speed and scalability. Fast-MCTD integrates two techniques: Parallel MCTD, which enables parallel rollouts via delayed tree updates and redundancy-aware selection; and Sparse MCTD, which reduces rollout length through trajectory coarsening. Experiments show that Fast-MCTD achieves up to 100× speedup over standard MCTD while maintaining or improving planning performance. Remarkably, it even outperforms Diffuser in inference speed on some tasks, despite Diffuser requiring no search and yielding weaker solutions. These results position Fast-MCTD as a practical and scalable solution for diffusion-based inference-time reasoning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "DwZD97uHgm@OpenReview",
      "index": 214,
      "title": "Zero-shot Denoising via Neural Compression: Theoretical and algorithmic framework",
      "authors": [
        "Ali Zafari",
        "Xi Chen",
        "Shirin Jalali"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "compression",
        "ncd",
        "denoising",
        "shot",
        "zero",
        "denoisers",
        "theoretical",
        "neural",
        "imaging",
        "patches"
      ],
      "summary": "Zero-shot denoising aims to denoise observations without access to training samples or clean reference images. This setting is particularly relevant in practical imaging scenarios involving specialized domains such as medical imaging or biology. In this work, we propose the *Zero-Shot Neural Compression Denoiser* (ZS-NCD), a novel denoising framework based on neural compression. ZS-NCD treats a neural compression network as an untrained model, optimized directly on patches extracted from a single noisy image. The final reconstruction is then obtained by aggregating the outputs of the trained model over overlapping patches. Thanks to the built-in entropy constraints of compression architectures, our method naturally avoids overfitting and does not require manual regularization or early stopping. Through extensive experiments, we show that ZS-NCD achieves state-of-the-art performance among zero-shot denoisers for both Gaussian and Poisson noise, and generalizes well to both natural and non-natural images. Additionally, we provide new finite-sample theoretical results that characterize upper bounds on the achievable reconstruction error of general maximum-likelihood compression-based denoisers. These results further establish the theoretical foundations of compression-based denoising. Our code is available at: https://github.com/Computational-Imaging-RU/ZS-NCDenoiser.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DwZD97uHgm"
        ],
        "venue": [
          "/venue/DwZD97uHgm@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DwZD97uHgm"
        ],
        "detail": [
          "https://openreview.net/forum?id=DwZD97uHgm"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Zero-shot Denoising via Neural Compression: Theoretical and algorithmic framework [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Ali Zafari , Xi Chen , Shirin Jalali Zero-shot denoising aims to denoise observations without access to training samples or clean reference images. This setting is particularly relevant in practical imaging scenarios involving specialized domains such as medical imaging or biology. In this work, we propose the *Zero-Shot Neural Compression Denoiser* (ZS-NCD), a novel denoising framework based on neural compression. ZS-NCD treats a neural compression network as an untrained model, optimized directly on patches extracted from a single noisy image. The final reconstruction is then obtained by aggregating the outputs of the trained model over overlapping patches. Thanks to the built-in entropy constraints of compression architectures, our method naturally avoids overfitting and does not require manual regularization or early stopping. Through extensive experiments, we show that ZS-NCD achieves state-of-the-art performance among zero-shot denoisers for both Gaussian and Poisson noise, and generalizes well to both natural and non-natural images. Additionally, we provide new finite-sample theoretical results that characterize upper bounds on the achievable reconstruction error of general maximum-likelihood compression-based denoisers. These results further establish the theoretical foundations of compression-based denoising. Our code is available at: https://github.com/Computational-Imaging-RU/ZS-NCDenoiser. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "miYvfEKvEl@OpenReview",
      "index": 215,
      "title": "Gradient-Variation Online Adaptivity for Accelerated Optimization with Hölder Smoothness",
      "authors": [
        "Yuheng Zhao",
        "Yu-Hu Yan",
        "Kfir Yehuda Levy",
        "Peng Zhao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "online",
        "adaptivity",
        "hölder",
        "smooth",
        "variation",
        "smoothness",
        "optimization",
        "offline",
        "gradient",
        "accelerated"
      ],
      "summary": "Smoothness is known to be crucial for acceleration in offline optimization, and for gradient-variation regret minimization in online learning. Interestingly, these two problems are actually closely connected --- accelerated optimization can be understood through the lens of gradient-variation online learning. In this paper, we investigate online learning with *Hölder* functions, a general class encompassing both smooth and non-smooth (Lipschitz) functions, and explore its implications for offline optimization. For (strongly) convex online functions, we design the corresponding gradient-variation online learning algorithm whose regret smoothly interpolates between the optimal guarantees in smooth and non-smooth regimes. Notably, our algorithms do not require prior knowledge of the Hölder smoothness parameter, exhibiting strong adaptivity over existing methods. Through online-to-batch conversion, this gradient-variation online adaptivity yields an optimal universal method for stochastic convex optimization under Hölder smoothness. However, achieving universality in offline strongly convex optimization is more challenging. We address this by integrating online adaptivity with a detection-based guess-and-check procedure, which, for the first time, yields a universal offline method that achieves accelerated convergence in the smooth regime while maintaining near-optimal convergence in the non-smooth one.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=miYvfEKvEl"
        ],
        "venue": [
          "/venue/miYvfEKvEl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=miYvfEKvEl"
        ],
        "detail": [
          "https://openreview.net/forum?id=miYvfEKvEl"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Gradient-Variation Online Adaptivity for Accelerated Optimization with Hölder Smoothness [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yuheng Zhao , Yu-Hu Yan , Kfir Yehuda Levy , Peng Zhao Smoothness is known to be crucial for acceleration in offline optimization, and for gradient-variation regret minimization in online learning. Interestingly, these two problems are actually closely connected --- accelerated optimization can be understood through the lens of gradient-variation online learning. In this paper, we investigate online learning with *Hölder* functions, a general class encompassing both smooth and non-smooth (Lipschitz) functions, and explore its implications for offline optimization. For (strongly) convex online functions, we design the corresponding gradient-variation online learning algorithm whose regret smoothly interpolates between the optimal guarantees in smooth and non-smooth regimes. Notably, our algorithms do not require prior knowledge of the Hölder smoothness parameter, exhibiting strong adaptivity over existing methods. Through online-to-batch conversion, this gradient-variation online adaptivity yields an optimal universal method for stochastic convex optimization under Hölder smoothness. However, achieving universality in offline strongly convex optimization is more challenging. We address this by integrating online adaptivity with a detection-based guess-and-check procedure, which, for the first time, yields a universal offline method that achieves accelerated convergence in the smooth regime while maintaining near-optimal convergence in the non-smooth one. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qN5hmLkBtC@OpenReview",
      "index": 216,
      "title": "A Smooth Sea Never Made a Skilled SAILOR: Robust Imitation via Learning to Search",
      "authors": [
        "Arnav Kumar Jain",
        "Vibhakar Mohta",
        "Subin Kim",
        "Atiksh Bhardwaj",
        "Juntao Ren",
        "Yunhai Feng",
        "Sanjiban Choudhury",
        "Gokul Swamy"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sailor",
        "expert",
        "demonstrations",
        "arnavkj1995",
        "mistake",
        "imitation",
        "fish",
        "agent",
        "skilled",
        "giving"
      ],
      "summary": "The fundamental limitation of the behavioral cloning (BC) approach to imitation learning is that it only teaches an agent what the expert did at states the expert visited. This means that when a BC agent makes a mistake which takes them out of the support of the demonstrations, they often don't know how to recover from it. In this sense, BC is akin to *giving the agent the fish* -- giving them dense supervision across a narrow set of states -- rather than teaching them *to fish*: to be able to reason independently about achieving the expert's outcome even when faced with unseen situations at test-time. In response, we explore *learning to search* (L2S) from expert demonstrations, i.e. learning the components required to, at test time, plan to match expert outcomes, even after making a mistake. These include *(1)* a world model and *(2)* a reward model. We carefully ablate the set of algorithmic and design decisions required to combine these and other components for stable and sample/interaction-efficient learning of recovery behavior without additional human corrections. Across a dozen visual manipulation tasks from three benchmarks, our approach SAILOR consistently out-performs state-of-the-art Diffusion Policies trained via BC on the same data. Furthermore, scaling up the amount of demonstrations used for BC by 5-10x still leaves a performance gap. We find that SAILOR can identify nuanced failures and is robust to reward hacking. Our code is available at [https://github.com/arnavkj1995/SAILOR](https://github.com/arnavkj1995/SAILOR).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qN5hmLkBtC"
        ],
        "venue": [
          "/venue/qN5hmLkBtC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qN5hmLkBtC"
        ],
        "detail": [
          "https://openreview.net/forum?id=qN5hmLkBtC"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "A Smooth Sea Never Made a Skilled SAILOR: Robust Imitation via Learning to Search [PDF ] [Copy] [Kimi ] [REL] Authors : Arnav Kumar Jain , Vibhakar Mohta , Subin Kim , Atiksh Bhardwaj , Juntao Ren , Yunhai Feng , Sanjiban Choudhury , Gokul Swamy The fundamental limitation of the behavioral cloning (BC) approach to imitation learning is that it only teaches an agent what the expert did at states the expert visited. This means that when a BC agent makes a mistake which takes them out of the support of the demonstrations, they often don't know how to recover from it. In this sense, BC is akin to *giving the agent the fish* -- giving them dense supervision across a narrow set of states -- rather than teaching them *to fish*: to be able to reason independently about achieving the expert's outcome even when faced with unseen situations at test-time. In response, we explore *learning to search* (L2S) from expert demonstrations, i.e. learning the components required to, at test time, plan to match expert outcomes, even after making a mistake. These include *(1)* a world model and *(2)* a reward model. We carefully ablate the set of algorithmic and design decisions required to combine these and other components for stable and sample/interaction-efficient learning of recovery behavior without additional human corrections. Across a dozen visual manipulation tasks from three benchmarks, our approach SAILOR consistently out-performs state-of-the-art Diffusion Policies trained via BC on the same data. Furthermore, scaling up the amount of demonstrations used for BC by 5-10x still leaves a performance gap. We find that SAILOR can identify nuanced failures and is robust to reward hacking. Our code is available at [https://github.com/arnavkj1995/SAILOR](https://github.com/arnavkj1995/SAILOR). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "1K28gV5MeF@OpenReview",
      "index": 217,
      "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series",
      "authors": [
        "Payal Mohapatra",
        "Yueyuan Sui",
        "Akash Pandey",
        "Stephen Xia",
        "Qi Zhu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "maestro",
        "multimodal",
        "modality",
        "modalities",
        "sparse",
        "observations",
        "modal",
        "attention",
        "dynamic",
        "series"
      ],
      "summary": "From clinical healthcare to daily living, continuous sensor monitoring across multiple modalities has shown great promise for real-world intelligent decision-making but also faces various challenges. In this work, we argue for modeling such heterogeneous data sources under the multimodal paradigm and introduce a new framework, MAESTRO. We introduce MAESTRO, a novel framework that overcomes key limitations of existing multimodal learning approaches: (1) reliance on a single primary modality for alignment, (2) pairwise modeling of modalities, and (3) assumption of complete modality observations. These limitations hinder the applicability of these approaches in real-world multimodal time-series settings, where primary modality priors are often unclear, the number of modalities can be large (making pairwise modeling impractical), and sensor failures often result in arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra- and cross-modal interactions based on task relevance, and leverages symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, which are processed via sparse cross-modal attention. The resulting cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE) mechanism, enabling black-box specialization under varying modality combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets spanning three applications, and observe average relative improvements of 4% and 8% over the best existing multimodal and multivariate approaches, respectively, under complete observations. Under partial observations—with up to 40% of missing modalities—MAESTRO achieves an average 9% improvement. Further analysis also demonstrates the robustness and efficiency of MAESTRO's sparse, modality-aware design for learning from dynamic time series.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1K28gV5MeF"
        ],
        "venue": [
          "/venue/1K28gV5MeF@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1K28gV5MeF"
        ],
        "detail": [
          "https://openreview.net/forum?id=1K28gV5MeF"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Payal Mohapatra , Yueyuan Sui , Akash Pandey , Stephen Xia , Qi Zhu From clinical healthcare to daily living, continuous sensor monitoring across multiple modalities has shown great promise for real-world intelligent decision-making but also faces various challenges. In this work, we argue for modeling such heterogeneous data sources under the multimodal paradigm and introduce a new framework, MAESTRO. We introduce MAESTRO, a novel framework that overcomes key limitations of existing multimodal learning approaches: (1) reliance on a single primary modality for alignment, (2) pairwise modeling of modalities, and (3) assumption of complete modality observations. These limitations hinder the applicability of these approaches in real-world multimodal time-series settings, where primary modality priors are often unclear, the number of modalities can be large (making pairwise modeling impractical), and sensor failures often result in arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra- and cross-modal interactions based on task relevance, and leverages symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, which are processed via sparse cross-modal attention. The resulting cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE) mechanism, enabling black-box specialization under varying modality combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets spanning three applications, and observe average relative improvements of 4% and 8% over the best existing multimodal and multivariate approaches, respectively, under complete observations. Under partial observations—with up to 40% of missing modalities—MAESTRO achieves an average 9% improvement. Further analysis also demonstrates the robustness and efficiency of MAESTRO's sparse, modality-aware design for learning from dynamic time series. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "JeXkIy0JyM@OpenReview",
      "index": 218,
      "title": "Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL",
      "authors": [
        "Xingyu Chen",
        "Shihao Ma",
        "Runsheng Lin",
        "Jiecong Lin",
        "BO WANG"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dna",
        "regulatory",
        "ctrl",
        "cell",
        "sequences",
        "gene",
        "specific",
        "constrained",
        "type",
        "controllable"
      ],
      "summary": "Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JeXkIy0JyM"
        ],
        "venue": [
          "/venue/JeXkIy0JyM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JeXkIy0JyM"
        ],
        "detail": [
          "https://openreview.net/forum?id=JeXkIy0JyM"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Xingyu Chen , Shihao Ma , Runsheng Lin , Jiecong Lin , BO WANG Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "V7m2oQ5OFW@OpenReview",
      "index": 219,
      "title": "On the Universal Near Optimality of Hedge in Combinatorial Settings",
      "authors": [
        "Zhiyuan Fan",
        "Arnab Maiti",
        "Lillian J. Ratliff",
        "Kevin Jamieson",
        "Gabriele Farina"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hedge",
        "log",
        "combinatorial",
        "dags",
        "mathcal",
        "sqrt",
        "mathbf",
        "near",
        "online",
        "big"
      ],
      "summary": "In this paper, we study the classical Hedge algorithm in combinatorial settings. In each round, the learner selects a vector x t x t from a set X ⊆ X ⊆ { 0 , 1 0 , 1 } d d , observes a full loss vector y t ∈ R d y t ∈ R d , and incurs a loss ⟨ x t , y t ⟩ ∈ [ − 1 , 1 ] ⟨ x t , y t ⟩ ∈ [ − 1 , 1 ] . This setting captures several important problems, including extensive-form games, resource allocation, m m -sets, online multitask learning, and shortest-path problems on directed acyclic graphs (DAGs). It is well known that Hedge achieves a regret of O ( T log | X | − − − − − − − √ ) O ( T log ⁡ | X | ) after T T rounds of interaction. In this paper, we ask whether Hedge is optimal across all combinatorial settings. To that end, we show that for any X ⊆ X ⊆ { 0 , 1 0 , 1 } d d , Hedge is near-optimal—specifically, up to a log d − − − − √ log ⁡ d factor—by establishing a lower bound of Ω ( T log ( | X | ) / log d − − − − − − − − − − − − − √ ) Ω ( T log ⁡ ( | X | ) / log ⁡ d ) that holds for any algorithm. We then identify a natural class of combinatorial sets—namely, m m -sets with log d ≤ m ≤ d − − √ log ⁡ d ≤ m ≤ d —for which this lower bound is tight, and for which Hedge is provably suboptimal by a factor of exactly log d − − − − √ log ⁡ d . At the same time, we show that Hedge is optimal for online multitask learning, a generalization of the classical K K -experts problem. Finally, we leverage the near-optimality of Hedge to establish the existence of a near-optimal regularizer for online shortest-path problems in DAGs—a setting that subsumes a broad range of combinatorial domains. Specifically, we show that the classical Online Mirror Descent (OMD) algorithm, when instantiated with the dilated entropy regularizer, is iterate-equivalent to Hedge, and therefore inherits its near-optimal regret guarantees for DAGs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=V7m2oQ5OFW"
        ],
        "venue": [
          "/venue/V7m2oQ5OFW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=V7m2oQ5OFW"
        ],
        "detail": [
          "https://openreview.net/forum?id=V7m2oQ5OFW"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "On the Universal Near Optimality of Hedge in Combinatorial Settings [PDF ] [Copy] [Kimi ] [REL] Authors : Zhiyuan Fan , Arnab Maiti , Lillian J. Ratliff , Kevin Jamieson , Gabriele Farina In this paper, we study the classical Hedge algorithm in combinatorial settings. In each round, the learner selects a vector x t x t from a set X ⊆ X ⊆ { 0 , 1 0 , 1 } d d , observes a full loss vector y t ∈ R d y t ∈ R d , and incurs a loss ⟨ x t , y t ⟩ ∈ [ − 1 , 1 ] ⟨ x t , y t ⟩ ∈ [ − 1 , 1 ] . This setting captures several important problems, including extensive-form games, resource allocation, m m -sets, online multitask learning, and shortest-path problems on directed acyclic graphs (DAGs). It is well known that Hedge achieves a regret of O ( T log | X | − − − − − − − √ ) O ( T log ⁡ | X | ) after T T rounds of interaction. In this paper, we ask whether Hedge is optimal across all combinatorial settings. To that end, we show that for any X ⊆ X ⊆ { 0 , 1 0 , 1 } d d , Hedge is near-optimal—specifically, up to a log d − − − − √ log ⁡ d factor—by establishing a lower bound of Ω ( T log ( | X | ) / log d − − − − − − − − − − − − − √ ) Ω ( T log ⁡ ( | X | ) / log ⁡ d ) that holds for any algorithm. We then identify a natural class of combinatorial sets—namely, m m -sets with log d ≤ m ≤ d − − √ log ⁡ d ≤ m ≤ d —for which this lower bound is tight, and for which Hedge is provably suboptimal by a factor of exactly log d − − − − √ log ⁡ d . At the same time, we show that Hedge is optimal for online multitask learning, a generalization of the classical K K -experts problem. Finally, we leverage the near-optimality of Hedge to establish the existence of a near-optimal regularizer for online shortest-path problems in DAGs—a setting that subsumes a broad range of combinatorial domains. Specifically, we show that the classical Online Mirror Descent (OMD) algorithm, when instantiated with the dilated entropy regularizer, is iterate-equivalent to Hedge, and therefore inherits its near-optimal regret guarantees for DAGs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "g2f0UoasGs@OpenReview",
      "index": 220,
      "title": "Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift",
      "authors": [
        "Yi Zhang",
        "Elynn Chen",
        "Yujun Yan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pricing",
        "tdp",
        "market",
        "transfer",
        "smarter",
        "alpha",
        "beta",
        "regret",
        "bigl",
        "bigr"
      ],
      "summary": "We study contextual dynamic pricing when a target market can leverage K K auxiliary markets—offline logs or concurrent streams—whose *mean utilities differ by a structured preference shift*. We propose *Cross-Market Transfer Dynamic Pricing (CM-TDP)*, the first algorithm that *provably* handles such model-shift transfer and delivers minimax-optimal regret for *both* linear and non-parametric utility models. For linear utilities of dimension d d , where the *difference* between source- and target-task coefficients is s 0 s 0 -sparse, CM-TDP attains regret O ~ ( ( d K − 1 + s 0 ) log T ) O ~ ( ( d K − 1 + s 0 ) log ⁡ T ) . For nonlinear demand residing in a reproducing kernel Hilbert space with effective dimension α α , complexity β β and task-similarity parameter H H , the regret becomes O ~ ( K − 2 α β / ( 2 α β + 1 ) T 1 / ( 2 α β + 1 ) + H 2 / ( 2 α + 1 ) T 1 / ( 2 α + 1 ) ) O ~ ( K − 2 α β / ( 2 α β + 1 ) T 1 / ( 2 α β + 1 ) + H 2 / ( 2 α + 1 ) T 1 / ( 2 α + 1 ) ) , matching information-theoretic lower bounds up to logarithmic factors. The RKHS bound is the first of its kind for transfer pricing and is of independent interest. Extensive simulations show up to 38\\% higher cumulative revenue and 6 × 6 × faster convergence relative to single-market pricing baselines. By bridging transfer learning, robust aggregation, and revenue optimization, CM-TDP moves toward pricing systems that *transfer faster, price smarter*.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=g2f0UoasGs"
        ],
        "venue": [
          "/venue/g2f0UoasGs@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=g2f0UoasGs"
        ],
        "detail": [
          "https://openreview.net/forum?id=g2f0UoasGs"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yi Zhang , Elynn Chen , Yujun Yan We study contextual dynamic pricing when a target market can leverage K K auxiliary markets—offline logs or concurrent streams—whose *mean utilities differ by a structured preference shift*. We propose *Cross-Market Transfer Dynamic Pricing (CM-TDP)*, the first algorithm that *provably* handles such model-shift transfer and delivers minimax-optimal regret for *both* linear and non-parametric utility models. For linear utilities of dimension d d , where the *difference* between source- and target-task coefficients is s 0 s 0 -sparse, CM-TDP attains regret O ~ ( ( d K − 1 + s 0 ) log T ) O ~ ( ( d K − 1 + s 0 ) log ⁡ T ) . For nonlinear demand residing in a reproducing kernel Hilbert space with effective dimension α α , complexity β β and task-similarity parameter H H , the regret becomes O ~ ( K − 2 α β / ( 2 α β + 1 ) T 1 / ( 2 α β + 1 ) + H 2 / ( 2 α + 1 ) T 1 / ( 2 α + 1 ) ) O ~ ( K − 2 α β / ( 2 α β + 1 ) T 1 / ( 2 α β + 1 ) + H 2 / ( 2 α + 1 ) T 1 / ( 2 α + 1 ) ) , matching information-theoretic lower bounds up to logarithmic factors. The RKHS bound is the first of its kind for transfer pricing and is of independent interest. Extensive simulations show up to 38\\% higher cumulative revenue and 6 × 6 × faster convergence relative to single-market pricing baselines. By bridging transfer learning, robust aggregation, and revenue optimization, CM-TDP moves toward pricing systems that *transfer faster, price smarter*. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Tt3XLyuDrE@OpenReview",
      "index": 221,
      "title": "Transformer brain encoders explain human high-level visual responses",
      "authors": [
        "Hossein Adeli",
        "Minni Sun",
        "Nikolaus Kriegeskorte"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "brain",
        "visual",
        "retinotopic",
        "transformer",
        "hosseinadeli",
        "routed",
        "responses",
        "areas",
        "different",
        "feature"
      ],
      "summary": "A major goal of neuroscience is to understand brain computations during visual processing in naturalistic settings. A dominant approach is to use image-computable deep neural networks trained with different task objectives as a basis for linear encoding models. However, in addition to requiring estimation of a large number of linear encoding parameters, this approach ignores the structure of the feature maps both in the brain and the models. Recently proposed alternatives factor the linear mapping into separate sets of spatial and feature weights, thus finding static receptive fields for units, which is appropriate only for early visual areas. In this work, we employ the attention mechanism used in the transformer architecture to study how retinotopic visual features can be dynamically routed to category-selective areas in high-level visual processing. We show that this computational motif is significantly more powerful than alternative methods in predicting brain activity during natural scene viewing, across different feature basis models and modalities. We also show that this approach is inherently more interpretable as the attention-routing signals for different high-level categorical areas can be easily visualized for any input image. Given its high performance at predicting brain responses to novel images, the model deserves consideration as a candidate mechanistic model of how visual information from retinotopic maps is routed in the human brain based on the relevance of the input content to different category-selective regions. Our code is available at \\href{https://github.com/Hosseinadeli/transformer_brain_encoder/}{https://github.com/Hosseinadeli/transformer\\_brain\\_encoder/}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Tt3XLyuDrE"
        ],
        "venue": [
          "/venue/Tt3XLyuDrE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Tt3XLyuDrE"
        ],
        "detail": [
          "https://openreview.net/forum?id=Tt3XLyuDrE"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Transformer brain encoders explain human high-level visual responses [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hossein Adeli , Minni Sun , Nikolaus Kriegeskorte A major goal of neuroscience is to understand brain computations during visual processing in naturalistic settings. A dominant approach is to use image-computable deep neural networks trained with different task objectives as a basis for linear encoding models. However, in addition to requiring estimation of a large number of linear encoding parameters, this approach ignores the structure of the feature maps both in the brain and the models. Recently proposed alternatives factor the linear mapping into separate sets of spatial and feature weights, thus finding static receptive fields for units, which is appropriate only for early visual areas. In this work, we employ the attention mechanism used in the transformer architecture to study how retinotopic visual features can be dynamically routed to category-selective areas in high-level visual processing. We show that this computational motif is significantly more powerful than alternative methods in predicting brain activity during natural scene viewing, across different feature basis models and modalities. We also show that this approach is inherently more interpretable as the attention-routing signals for different high-level categorical areas can be easily visualized for any input image. Given its high performance at predicting brain responses to novel images, the model deserves consideration as a candidate mechanistic model of how visual information from retinotopic maps is routed in the human brain based on the relevance of the input content to different category-selective regions. Our code is available at \\href{https://github.com/Hosseinadeli/transformer_brain_encoder/}{https://github.com/Hosseinadeli/transformer\\_brain\\_encoder/}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "HvIRFV0J90@OpenReview",
      "index": 222,
      "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning",
      "authors": [
        "Yeongbin Seo",
        "Dongha Lee",
        "Jaehyung Kim",
        "Jinyoung Yeo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "decoding",
        "diffusion",
        "rejective",
        "tokens",
        "conv",
        "language",
        "textbf",
        "sacrificing",
        "convolutional",
        "tuning"
      ],
      "summary": "Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the \\textbf{long decoding-window problem}, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks (sacrificing bidirectionality), but we find that this also leads to \\textbf{time-interval expansion problem}, sacrificing the speed. Therefore, semi-AR eliminates the main advantages of diffusion models. To overcome this, we propose Convolutional decoding (\\textit{Conv}), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements. The code is available online (\\url{https://github.com/ybseo-ac/Conv}).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HvIRFV0J90"
        ],
        "venue": [
          "/venue/HvIRFV0J90@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HvIRFV0J90"
        ],
        "detail": [
          "https://openreview.net/forum?id=HvIRFV0J90"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yeongbin Seo , Dongha Lee , Jaehyung Kim , Jinyoung Yeo Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the \\textbf{long decoding-window problem}, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks (sacrificing bidirectionality), but we find that this also leads to \\textbf{time-interval expansion problem}, sacrificing the speed. Therefore, semi-AR eliminates the main advantages of diffusion models. To overcome this, we propose Convolutional decoding (\\textit{Conv}), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements. The code is available online (\\url{https://github.com/ybseo-ac/Conv}). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RlqYCpTu1P@OpenReview",
      "index": 223,
      "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
      "authors": [
        "Enzhe Lu",
        "Zhejun Jiang",
        "Jingyuan Liu",
        "Yulun Du",
        "Tao Jiang",
        "Chao Hong",
        "Shaowei Liu",
        "Weiran He",
        "Enming Yuan",
        "Yuzhi Wang",
        "Zhiqi Huang",
        "Huan Yuan",
        "Suting Xu",
        "Xinran Xu",
        "Guokun Lai",
        "Yanru Chen",
        "Huabin Zheng",
        "Junjie Yan",
        "Jianlin Su",
        "Yuxin Wu",
        "Yutao Zhang",
        "Zhilin Yang",
        "Xinyu Zhou",
        "Mingxing Zhang",
        "Jiezhong Qiu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "moba",
        "attention",
        "context",
        "mixture",
        "llms",
        "block",
        "long",
        "moonshotai",
        "agi",
        "performance"
      ],
      "summary": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to handle actual production workloads with long-context requirements, demonstrating significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RlqYCpTu1P"
        ],
        "venue": [
          "/venue/RlqYCpTu1P@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RlqYCpTu1P"
        ],
        "detail": [
          "https://openreview.net/forum?id=RlqYCpTu1P"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "MoBA: Mixture of Block Attention for Long-Context LLMs [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Enzhe Lu , Zhejun Jiang , Jingyuan Liu , Yulun Du , Tao Jiang , Chao Hong , Shaowei Liu , Weiran He , Enming Yuan , Yuzhi Wang , Zhiqi Huang , Huan Yuan , Suting Xu , Xinran Xu , Guokun Lai , Yanru Chen , Huabin Zheng , Junjie Yan , Jianlin Su , Yuxin Wu , Yutao Zhang , Zhilin Yang , Xinyu Zhou , Mingxing Zhang , Jiezhong Qiu Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to handle actual production workloads with long-context requirements, demonstrating significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0KnZasL9nA@OpenReview",
      "index": 224,
      "title": "DERD-Net: Learning Depth from Event-based Ray Densities",
      "authors": [
        "Diego de Oliveira Hitzges",
        "Suman Ghosh",
        "Guillermo Gallego"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "depth",
        "event",
        "stereo",
        "dsis",
        "cameras",
        "derd",
        "slam",
        "monocular",
        "densities",
        "estimation"
      ],
      "summary": "Event cameras offer a promising avenue for multi-view stereo depth estimation and Simultaneous Localization And Mapping (SLAM) due to their ability to detect blur-free 3D edges at high-speed and over broad illumination conditions. However, traditional deep learning frameworks designed for conventional cameras struggle with the asynchronous, stream-like nature of event data, as their architectures are optimized for discrete, image-like inputs. We propose a scalable, flexible and adaptable framework for pixel-wise depth estimation with event cameras in both monocular and stereo setups. The 3D scene structure is encoded into disparity space images (DSIs), representing spatial densities of rays obtained by back-projecting events into space via known camera poses. Our neural network processes local subregions of the DSIs combining 3D convolutions and a recurrent structure to recognize valuable patterns for depth prediction. Local processing enables fast inference with full parallelization and ensures constant ultra-low model complexity and memory costs, regardless of camera resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets) demonstrate unprecedented effectiveness: (i) using purely monocular data, our method achieves comparable results to existing stereo methods; (ii) when applied to stereo data, it strongly outperforms all state-of-the-art (SOTA) approaches, reducing the mean absolute error by at least 42\\%; (iii) our method also allows for increases in depth completeness by more than 3-fold while still yielding a reduction in median absolute error of at least 30\\%. Given its remarkable performance and effective processing of event-data, our framework holds strong potential to become a standard approach for using deep learning for event-based depth estimation and SLAM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0KnZasL9nA"
        ],
        "venue": [
          "/venue/0KnZasL9nA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0KnZasL9nA"
        ],
        "detail": [
          "https://openreview.net/forum?id=0KnZasL9nA"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "DERD-Net: Learning Depth from Event-based Ray Densities [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Diego de Oliveira Hitzges , Suman Ghosh , Guillermo Gallego Event cameras offer a promising avenue for multi-view stereo depth estimation and Simultaneous Localization And Mapping (SLAM) due to their ability to detect blur-free 3D edges at high-speed and over broad illumination conditions. However, traditional deep learning frameworks designed for conventional cameras struggle with the asynchronous, stream-like nature of event data, as their architectures are optimized for discrete, image-like inputs. We propose a scalable, flexible and adaptable framework for pixel-wise depth estimation with event cameras in both monocular and stereo setups. The 3D scene structure is encoded into disparity space images (DSIs), representing spatial densities of rays obtained by back-projecting events into space via known camera poses. Our neural network processes local subregions of the DSIs combining 3D convolutions and a recurrent structure to recognize valuable patterns for depth prediction. Local processing enables fast inference with full parallelization and ensures constant ultra-low model complexity and memory costs, regardless of camera resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets) demonstrate unprecedented effectiveness: (i) using purely monocular data, our method achieves comparable results to existing stereo methods; (ii) when applied to stereo data, it strongly outperforms all state-of-the-art (SOTA) approaches, reducing the mean absolute error by at least 42\\%; (iii) our method also allows for increases in depth completeness by more than 3-fold while still yielding a reduction in median absolute error of at least 30\\%. Given its remarkable performance and effective processing of event-data, our framework holds strong potential to become a standard approach for using deep learning for event-based depth estimation and SLAM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "R7HJj1YvJH@OpenReview",
      "index": 225,
      "title": "Beyond Expectations: Quantile-Guided Alignment for Risk-Calibrated Language Models",
      "authors": [
        "Xinran Wang",
        "Jin Du",
        "Azal Ahmad Khan",
        "Qi Le",
        "Enmao Diao",
        "Jiawei Zhou",
        "Jie Ding",
        "Ali Anwar"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "quantile",
        "alignment",
        "rlhf",
        "reward",
        "risk",
        "calibrated",
        "language",
        "guided",
        "outputs",
        "tail"
      ],
      "summary": "Large language models can generate rare but catastrophic outputs, such as harmful conversations or insecure code. Existing Reinforcement Learning from Human Feedback (RLHF) typically maximizes average reward, leaving high-risk tail events insufficiently controlled. We introduce Quantile‑Guided Alignment (QA), a framework that allows users to specify desired improvements at any quantile—individually or across multiple reward dimensions—thus shifting the distribution of outputs with finer control toward safer, more desirable outcomes. The method extends standard RLHF via an augmented reward formulation that enforces quantile constraints. Experiments on conversation and code‐generation tasks show that quantile alignment significantly enhances quality at targeted tails while maintaining overall performance. The results position QA as a principled route to risk‑calibrated language models with tail‑focused alignment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=R7HJj1YvJH"
        ],
        "venue": [
          "/venue/R7HJj1YvJH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=R7HJj1YvJH"
        ],
        "detail": [
          "https://openreview.net/forum?id=R7HJj1YvJH"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Beyond Expectations: Quantile-Guided Alignment for Risk-Calibrated Language Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xinran Wang , Jin Du , Azal Ahmad Khan , Qi Le , Enmao Diao , Jiawei Zhou , Jie Ding , Ali Anwar Large language models can generate rare but catastrophic outputs, such as harmful conversations or insecure code. Existing Reinforcement Learning from Human Feedback (RLHF) typically maximizes average reward, leaving high-risk tail events insufficiently controlled. We introduce Quantile‑Guided Alignment (QA), a framework that allows users to specify desired improvements at any quantile—individually or across multiple reward dimensions—thus shifting the distribution of outputs with finer control toward safer, more desirable outcomes. The method extends standard RLHF via an augmented reward formulation that enforces quantile constraints. Experiments on conversation and code‐generation tasks show that quantile alignment significantly enhances quality at targeted tails while maintaining overall performance. The results position QA as a principled route to risk‑calibrated language models with tail‑focused alignment. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "LCZmI3iM8X@OpenReview",
      "index": 226,
      "title": "VPO: Reasoning Preferences Optimization Based on V V -Usable Information",
      "authors": [
        "Zecheng Wang",
        "Chunshan Li",
        "Yupeng Zhang",
        "Han Liu",
        "Bingning Wang",
        "Dianhui Chu",
        "Dianbo Sui"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "vpo",
        "preference",
        "dpo",
        "usable",
        "confidence",
        "reasoning",
        "mathcal",
        "samples",
        "math500",
        "amc23"
      ],
      "summary": "Direct Preference Optimization (DPO) is a widely used preference optimization algorithm in large language model (LLM) alignment, which reparameterizes the reward function in reinforcement learning with human feedback (RLHF) without requiring a separate reward model. However, during the DPO training process, when a large negative gradient is applied to low-confidence samples, LLMs with a softmax output head tend to squeeze the confidence in the model's output distribution towards the highest-confidence sentence, which may lead to a decrease in the confidence of both preference and non-preference samples, while increasing the confidence of unrelated tokens. This phenomenon becomes more complex in reasoning tasks. In this work, focusing on reasoning tasks, we propose VPO, a negative gradient constraint method for human non-preference samples based on V V -usable information. By using V V -usable information to measure the similarity between preference pairs and selectively constrain the negative gradient, VPO can alleviate the squeezing effect of DPO, enhance alignment with the generation objective, and maintain the model's ability to distinguish between preference and non-preference samples. We compare VPO with DPO and its latest variants on mathematical reasoning tasks using the LLama 3.1 and Qwen 2.5 series, including both Base and Instruct models. Our results demonstrate that VPO consistently and significantly outperforms existing methods. Specifically, on Qwen2.5-7B-Base, VPO achieves 7.80\\% and 13.25\\% improvement over DPO on MATH500 and AMC23, respectively. We also conduct ablation experiments and in-depth analysis on VPO to explain its effectiveness and rationale.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LCZmI3iM8X"
        ],
        "venue": [
          "/venue/LCZmI3iM8X@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LCZmI3iM8X"
        ],
        "detail": [
          "https://openreview.net/forum?id=LCZmI3iM8X"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "VPO: Reasoning Preferences Optimization Based on V V -Usable Information [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Zecheng Wang , Chunshan Li , Yupeng Zhang , Han Liu , Bingning Wang , Dianhui Chu , Dianbo Sui Direct Preference Optimization (DPO) is a widely used preference optimization algorithm in large language model (LLM) alignment, which reparameterizes the reward function in reinforcement learning with human feedback (RLHF) without requiring a separate reward model. However, during the DPO training process, when a large negative gradient is applied to low-confidence samples, LLMs with a softmax output head tend to squeeze the confidence in the model's output distribution towards the highest-confidence sentence, which may lead to a decrease in the confidence of both preference and non-preference samples, while increasing the confidence of unrelated tokens. This phenomenon becomes more complex in reasoning tasks. In this work, focusing on reasoning tasks, we propose VPO, a negative gradient constraint method for human non-preference samples based on V V -usable information. By using V V -usable information to measure the similarity between preference pairs and selectively constrain the negative gradient, VPO can alleviate the squeezing effect of DPO, enhance alignment with the generation objective, and maintain the model's ability to distinguish between preference and non-preference samples. We compare VPO with DPO and its latest variants on mathematical reasoning tasks using the LLama 3.1 and Qwen 2.5 series, including both Base and Instruct models. Our results demonstrate that VPO consistently and significantly outperforms existing methods. Specifically, on Qwen2.5-7B-Base, VPO achieves 7.80\\% and 13.25\\% improvement over DPO on MATH500 and AMC23, respectively. We also conduct ablation experiments and in-depth analysis on VPO to explain its effectiveness and rationale. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "S9E1nfYPwl@OpenReview",
      "index": 227,
      "title": "MoCha: Towards Movie-Grade Talking Character Generation",
      "authors": [
        "Cong Wei",
        "Bo Sun",
        "Haoyu Ma",
        "Ji Hou",
        "Felix Juefei-Xu",
        "Zecheng He",
        "Xiaoliang Dai",
        "Luxin Zhang",
        "Kunpeng Li",
        "Tingbo Hou",
        "Animesh Sinha",
        "Peter Vajda",
        "Wenhu Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "talking",
        "mocha",
        "character",
        "characters",
        "video",
        "speech",
        "cinematic",
        "labelled",
        "storytelling",
        "generation"
      ],
      "summary": "Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head tasks, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a localized audio attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labelled video datasets, we introduce a joint training strategy that leverages both speech-labelled and text-labelled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue—allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human evaluation studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, controllability and generalization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=S9E1nfYPwl"
        ],
        "venue": [
          "/venue/S9E1nfYPwl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=S9E1nfYPwl"
        ],
        "detail": [
          "https://openreview.net/forum?id=S9E1nfYPwl"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": null
      },
      "raw_excerpt": "MoCha: Towards Movie-Grade Talking Character Generation [PDF 5 ] [Copy] [Kimi ] [REL] Authors : Cong Wei , Bo Sun , Haoyu Ma , Ji Hou , Felix Juefei-Xu , Zecheng He , Xiaoliang Dai , Luxin Zhang , Kunpeng Li , Tingbo Hou , Animesh Sinha , Peter Vajda , Wenhu Chen Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head tasks, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a localized audio attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labelled video datasets, we introduce a joint training strategy that leverages both speech-labelled and text-labelled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue—allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human evaluation studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, controllability and generalization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "HhCl2BIHfk@OpenReview",
      "index": 228,
      "title": "Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon",
      "authors": [
        "Tongtong Liang",
        "Dan Qiao",
        "Yu-Xiang Wang",
        "Rahul Parhi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "minima",
        "shattering",
        "relu",
        "flat",
        "curse",
        "flatness",
        "dimensionality",
        "inputs",
        "suffer",
        "neural"
      ],
      "summary": "We study the implicit bias of flatness / low (loss) curvature and its effects on generalization in two-layer overparameterized ReLU networks with multivariate inputs---a problem well motivated by the minima stability and edge-of-stability phenomena in gradient-descent training. Existing work either requires interpolation or focuses only on univariate inputs. This paper presents new and somewhat surprising theoretical results for multivariate inputs. On two natural settings (1) generalization gap for flat solutions, and (2) mean-squared error (MSE) in nonparametric function estimation by stable minima, we prove upper and lower bounds, which establish that while flatness does imply generalization, the resulting rates of convergence necessarily deteriorate exponentially as the input dimension grows. This gives an exponential separation between the flat solutions compared to low-norm solutions (i.e., weight decay), which are known not to suffer from the curse of dimensionality. In particular, our minimax lower bound construction, based on a novel packing argument with boundary-localized ReLU neurons, reveals how flat solutions can exploit a kind of \"neural shattering\" where neurons rarely activate, but with high weight magnitudes. This leads to poor performance in high dimensions. We corroborate these theoretical findings with extensive numerical simulations. To the best of our knowledge, our analysis provides the first systematic explanation for why flat minima may fail to generalize in high dimensions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HhCl2BIHfk"
        ],
        "venue": [
          "/venue/HhCl2BIHfk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HhCl2BIHfk"
        ],
        "detail": [
          "https://openreview.net/forum?id=HhCl2BIHfk"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Tongtong Liang , Dan Qiao , Yu-Xiang Wang , Rahul Parhi We study the implicit bias of flatness / low (loss) curvature and its effects on generalization in two-layer overparameterized ReLU networks with multivariate inputs---a problem well motivated by the minima stability and edge-of-stability phenomena in gradient-descent training. Existing work either requires interpolation or focuses only on univariate inputs. This paper presents new and somewhat surprising theoretical results for multivariate inputs. On two natural settings (1) generalization gap for flat solutions, and (2) mean-squared error (MSE) in nonparametric function estimation by stable minima, we prove upper and lower bounds, which establish that while flatness does imply generalization, the resulting rates of convergence necessarily deteriorate exponentially as the input dimension grows. This gives an exponential separation between the flat solutions compared to low-norm solutions (i.e., weight decay), which are known not to suffer from the curse of dimensionality. In particular, our minimax lower bound construction, based on a novel packing argument with boundary-localized ReLU neurons, reveals how flat solutions can exploit a kind of \"neural shattering\" where neurons rarely activate, but with high weight magnitudes. This leads to poor performance in high dimensions. We corroborate these theoretical findings with extensive numerical simulations. To the best of our knowledge, our analysis provides the first systematic explanation for why flat minima may fail to generalize in high dimensions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yG8vmj3EAU@OpenReview",
      "index": 229,
      "title": "Fisher meets Feynman: score-based variational inference with a product of experts",
      "authors": [
        "Diana Cai",
        "Robert M. Gower",
        "David Blei",
        "Lawrence K. Saul"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "experts",
        "bbvi",
        "poe",
        "product",
        "feynman",
        "variational",
        "fisher",
        "weighted",
        "products",
        "weighting"
      ],
      "summary": "We introduce a highly expressive yet distinctly tractable family for black-box variational inference (BBVI). Each member of this family is a weighted product of experts (PoE), and each weighted expert in the product is proportional to a multivariate t t -distribution. These products of experts can model distributions with skew, heavy tails, and multiple modes, but to use them for BBVI, we must be able to sample from their densities. We show how to do this by reformulating these products of experts as latent variable models with auxiliary Dirichlet random variables. These Dirichlet variables emerge from a Feynman identity, originally developed for loop integrals in quantum field theory, that expresses the product of multiple fractions (or in our case, t t -distributions) as an integral over the simplex. We leverage this simplicial latent space to draw weighted samples from these products of experts---samples which BBVI then uses to find the PoE that best approximates a target density. Given a collection of experts, we derive an iterative procedure to optimize the exponents that determine their geometric weighting in the PoE. At each iteration, this procedure minimizes a regularized Fisher divergence to match the scores of the variational and target densities at a batch of samples drawn from the current approximation. This minimization reduces to a convex quadratic program, and we prove under general conditions that these updates converge exponentially fast to a near-optimal weighting of experts. We conclude by evaluating this approach on a variety of synthetic and real-world target distributions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yG8vmj3EAU"
        ],
        "venue": [
          "/venue/yG8vmj3EAU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yG8vmj3EAU"
        ],
        "detail": [
          "https://openreview.net/forum?id=yG8vmj3EAU"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Fisher meets Feynman: score-based variational inference with a product of experts [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Diana Cai , Robert M. Gower , David Blei , Lawrence K. Saul We introduce a highly expressive yet distinctly tractable family for black-box variational inference (BBVI). Each member of this family is a weighted product of experts (PoE), and each weighted expert in the product is proportional to a multivariate t t -distribution. These products of experts can model distributions with skew, heavy tails, and multiple modes, but to use them for BBVI, we must be able to sample from their densities. We show how to do this by reformulating these products of experts as latent variable models with auxiliary Dirichlet random variables. These Dirichlet variables emerge from a Feynman identity, originally developed for loop integrals in quantum field theory, that expresses the product of multiple fractions (or in our case, t t -distributions) as an integral over the simplex. We leverage this simplicial latent space to draw weighted samples from these products of experts---samples which BBVI then uses to find the PoE that best approximates a target density. Given a collection of experts, we derive an iterative procedure to optimize the exponents that determine their geometric weighting in the PoE. At each iteration, this procedure minimizes a regularized Fisher divergence to match the scores of the variational and target densities at a batch of samples drawn from the current approximation. This minimization reduces to a convex quadratic program, and we prove under general conditions that these updates converge exponentially fast to a near-optimal weighting of experts. We conclude by evaluating this approach on a variety of synthetic and real-world target distributions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YIGUv0BZCy@OpenReview",
      "index": 230,
      "title": "Mitigating the Privacy–Utility Trade-off in Decentralized Federated Learning via f-Differential Privacy",
      "authors": [
        "Xiang Li",
        "Chendi Wang",
        "Buxin Su",
        "Qi Long",
        "Weijie J Su"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "privacy",
        "decentralized",
        "accounting",
        "federated",
        "utility",
        "private",
        "local",
        "communication",
        "differential",
        "mitigating"
      ],
      "summary": "Differentially private (DP) decentralized Federated Learning (FL) allows local users to collaborate without sharing their data with a central server. However, accurately quantifying the privacy budget of private FL algorithms is challenging due to the co-existence of complex algorithmic components such as decentralized communication and local updates. This paper addresses privacy accounting for two decentralized FL algorithms within the f f -differential privacy ( f f -DP) framework. We develop two new f f -DP–based accounting methods tailored to decentralized settings: Pairwise Network f f -DP (PN- f f -DP), which quantifies privacy leakage between user pairs under random-walk communication, and Secret-based f f -Local DP (Sec- f f -LDP), which supports structured noise injection via shared secrets. By combining tools from f f -DP theory and Markov chain concentration, our accounting framework captures privacy amplification arising from sparse communication, local iterations, and correlated noise. Experiments on synthetic and real datasets demonstrate that our methods yield consistently tighter ( ϵ , δ ) ( ϵ , δ ) bounds and improved utility compared to Rényi DP–based approaches, illustrating the benefits of f f -DP in decentralized privacy accounting.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YIGUv0BZCy"
        ],
        "venue": [
          "/venue/YIGUv0BZCy@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YIGUv0BZCy"
        ],
        "detail": [
          "https://openreview.net/forum?id=YIGUv0BZCy"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Mitigating the Privacy–Utility Trade-off in Decentralized Federated Learning via f-Differential Privacy [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xiang Li , Chendi Wang , Buxin Su , Qi Long , Weijie J Su Differentially private (DP) decentralized Federated Learning (FL) allows local users to collaborate without sharing their data with a central server. However, accurately quantifying the privacy budget of private FL algorithms is challenging due to the co-existence of complex algorithmic components such as decentralized communication and local updates. This paper addresses privacy accounting for two decentralized FL algorithms within the f f -differential privacy ( f f -DP) framework. We develop two new f f -DP–based accounting methods tailored to decentralized settings: Pairwise Network f f -DP (PN- f f -DP), which quantifies privacy leakage between user pairs under random-walk communication, and Secret-based f f -Local DP (Sec- f f -LDP), which supports structured noise injection via shared secrets. By combining tools from f f -DP theory and Markov chain concentration, our accounting framework captures privacy amplification arising from sparse communication, local iterations, and correlated noise. Experiments on synthetic and real datasets demonstrate that our methods yield consistently tighter ( ϵ , δ ) ( ϵ , δ ) bounds and improved utility compared to Rényi DP–based approaches, illustrating the benefits of f f -DP in decentralized privacy accounting. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hq2CkcEY7h@OpenReview",
      "index": 231,
      "title": "Mitigating Instability in High Residual Adaptive Sampling for PINNs via Langevin Dynamics",
      "authors": [
        "Minseok Jeong",
        "Giup Seo",
        "Euiseok Hwang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pinns",
        "langevin",
        "adaptive",
        "sampling",
        "residuals",
        "instability",
        "residual",
        "pdes",
        "stability",
        "mitigating"
      ],
      "summary": "Recently, physics-informed neural networks (PINNs) have gained attention in the scientific community for their potential to solve partial differential equations (PDEs). However, they face challenges related to resource efficiency and slow convergence. Adaptive sampling methods, which prioritize collocation points with high residuals, improve both efficiency and accuracy. However, these methods often neglect points with medium or low residuals, which can affect stability as the complexity of the model increases. In this paper, we investigate this limitation and show that high residual-based approaches require stricter learning rate bounds to ensure stability. To address this, we propose a Langevin dynamics-based Adaptive Sampling (LAS) framework that is robust to various learning rates and model complexities. Our experiments demonstrate that the proposed method outperforms existing approaches in terms of relative L 2 L 2 error, and stability across a range of environments, including high-dimensional PDEs where Monte Carlo integration-based methods typically suffer from instability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hq2CkcEY7h"
        ],
        "venue": [
          "/venue/hq2CkcEY7h@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hq2CkcEY7h"
        ],
        "detail": [
          "https://openreview.net/forum?id=hq2CkcEY7h"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Mitigating Instability in High Residual Adaptive Sampling for PINNs via Langevin Dynamics [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Minseok Jeong , Giup Seo , Euiseok Hwang Recently, physics-informed neural networks (PINNs) have gained attention in the scientific community for their potential to solve partial differential equations (PDEs). However, they face challenges related to resource efficiency and slow convergence. Adaptive sampling methods, which prioritize collocation points with high residuals, improve both efficiency and accuracy. However, these methods often neglect points with medium or low residuals, which can affect stability as the complexity of the model increases. In this paper, we investigate this limitation and show that high residual-based approaches require stricter learning rate bounds to ensure stability. To address this, we propose a Langevin dynamics-based Adaptive Sampling (LAS) framework that is robust to various learning rates and model complexities. Our experiments demonstrate that the proposed method outperforms existing approaches in terms of relative L 2 L 2 error, and stability across a range of environments, including high-dimensional PDEs where Monte Carlo integration-based methods typically suffer from instability. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2CeGVUpOd7@OpenReview",
      "index": 232,
      "title": "Spectral Estimation with Free Decompression",
      "authors": [
        "Siavash Ameli",
        "Chris van der Heide",
        "Liam Hodgkinson",
        "Michael W. Mahoney"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "matrix",
        "matrices",
        "decompression",
        "impalpable",
        "submatrices",
        "products",
        "settings",
        "vector",
        "full",
        "free"
      ],
      "summary": "Computing eigenvalues of very large matrices is a critical task in many machine learning applications, including the evaluation of log-determinants, the trace of matrix functions, and other important metrics. As datasets continue to grow in scale, the corresponding covariance and kernel matrices become increasingly large, often reaching magnitudes that make their direct formation impractical or impossible. Existing techniques typically rely on matrix-vector products, which can provide efficient approximations, if the matrix spectrum behaves well. However, in settings like distributed learning, or when the matrix is defined only indirectly, access to the full data set can be restricted to only very small sub-matrices of the original matrix. In these cases, the matrix of nominal interest is not even available as an implicit operator, meaning that even matrix-vector products may not be available. In such settings, the matrix is \"impalpable\", in the sense that we have access to only masked snapshots of it. We draw on principles from free probability theory to introduce a novel method of \"free decompression\" to estimate the spectrum of such matrices. Our method can be used to extrapolate from the empirical spectral densities of small submatrices to infer the eigenspectrum of extremely large (impalpable) matrices (that we cannot form or even evaluate with full matrix-vector products). We demonstrate the effectiveness of this approach through a series of examples, comparing its performance against known limiting distributions from random matrix theory in synthetic settings, as well as applying it to submatrices of real-world datasets, matching them with their full empirical eigenspectra.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2CeGVUpOd7"
        ],
        "venue": [
          "/venue/2CeGVUpOd7@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2CeGVUpOd7"
        ],
        "detail": [
          "https://openreview.net/forum?id=2CeGVUpOd7"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Spectral Estimation with Free Decompression [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Siavash Ameli , Chris van der Heide , Liam Hodgkinson , Michael W. Mahoney Computing eigenvalues of very large matrices is a critical task in many machine learning applications, including the evaluation of log-determinants, the trace of matrix functions, and other important metrics. As datasets continue to grow in scale, the corresponding covariance and kernel matrices become increasingly large, often reaching magnitudes that make their direct formation impractical or impossible. Existing techniques typically rely on matrix-vector products, which can provide efficient approximations, if the matrix spectrum behaves well. However, in settings like distributed learning, or when the matrix is defined only indirectly, access to the full data set can be restricted to only very small sub-matrices of the original matrix. In these cases, the matrix of nominal interest is not even available as an implicit operator, meaning that even matrix-vector products may not be available. In such settings, the matrix is \"impalpable\", in the sense that we have access to only masked snapshots of it. We draw on principles from free probability theory to introduce a novel method of \"free decompression\" to estimate the spectrum of such matrices. Our method can be used to extrapolate from the empirical spectral densities of small submatrices to infer the eigenspectrum of extremely large (impalpable) matrices (that we cannot form or even evaluate with full matrix-vector products). We demonstrate the effectiveness of this approach through a series of examples, comparing its performance against known limiting distributions from random matrix theory in synthetic settings, as well as applying it to submatrices of real-world datasets, matching them with their full empirical eigenspectra. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "idnW3BiZcV@OpenReview",
      "index": 233,
      "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching",
      "authors": [
        "Chen Chen",
        "Pengsheng Guo",
        "Liangchen Song",
        "Jiasen Lu",
        "Rui Qian",
        "Tsu-Jui Fu",
        "Xinze Wang",
        "Wei Liu",
        "Yinfei Yang",
        "Alex Schwing"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "flow",
        "car",
        "reparameterization",
        "conditional",
        "condition",
        "emph",
        "aware",
        "matching",
        "learn",
        "relocating"
      ],
      "summary": "Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport \\emph{and} conditional injection. To ease the demand on the model, we propose \\emph{Condition-Aware Reparameterization for Flow Matching} (CAR-Flow) -- a lightweight, learned \\emph{shift} that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR-Flow. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6 % 0.6 % additional parameters.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=idnW3BiZcV"
        ],
        "venue": [
          "/venue/idnW3BiZcV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=idnW3BiZcV"
        ],
        "detail": [
          "https://openreview.net/forum?id=idnW3BiZcV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Chen Chen , Pengsheng Guo , Liangchen Song , Jiasen Lu , Rui Qian , Tsu-Jui Fu , Xinze Wang , Wei Liu , Yinfei Yang , Alex Schwing Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport \\emph{and} conditional injection. To ease the demand on the model, we propose \\emph{Condition-Aware Reparameterization for Flow Matching} (CAR-Flow) -- a lightweight, learned \\emph{shift} that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR-Flow. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6 % 0.6 % additional parameters. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TrNB08KuHK@OpenReview",
      "index": 234,
      "title": "Training-Free Constrained Generation With Stable Diffusion Models",
      "authors": [
        "Stefano Zampini",
        "Jacob K Christopher",
        "Luca Oneto",
        "Davide Anguita",
        "Ferdinando Fioretto"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "constrained",
        "diffusion",
        "generation",
        "stable",
        "raiselab",
        "frameworks",
        "morphometric",
        "atuva",
        "models",
        "adherence"
      ],
      "summary": "Stable diffusion models represent the state-of-the-art in data synthesis across diverse domains and hold transformative potential for applications in science and engineering, e.g., by facilitating the discovery of novel solutions and simulating systems that are computationally intractable to model explicitly. While there is increasing effort to incorporate physics-based constraints into generative models, existing techniques are either limited in their applicability to latent diffusion frameworks or lack the capability to strictly enforce domain-specific constraints. To address this limitation this paper proposes a novel integration of stable diffusion models with constrained optimization frameworks, enabling the generation of outputs satisfying stringent physical and functional requirements. The effectiveness of this approach is demonstrated through material design experiments requiring adherence to precise morphometric properties, challenging inverse design tasks involving the generation of materials inducing specific stress-strain responses, and copyright-constrained content generation tasks. All code has been released at https://github.com/RAISELab-atUVA/Constrained-Stable-Diffusion.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TrNB08KuHK"
        ],
        "venue": [
          "/venue/TrNB08KuHK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TrNB08KuHK"
        ],
        "detail": [
          "https://openreview.net/forum?id=TrNB08KuHK"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Training-Free Constrained Generation With Stable Diffusion Models [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Stefano Zampini , Jacob K Christopher , Luca Oneto , Davide Anguita , Ferdinando Fioretto Stable diffusion models represent the state-of-the-art in data synthesis across diverse domains and hold transformative potential for applications in science and engineering, e.g., by facilitating the discovery of novel solutions and simulating systems that are computationally intractable to model explicitly. While there is increasing effort to incorporate physics-based constraints into generative models, existing techniques are either limited in their applicability to latent diffusion frameworks or lack the capability to strictly enforce domain-specific constraints. To address this limitation this paper proposes a novel integration of stable diffusion models with constrained optimization frameworks, enabling the generation of outputs satisfying stringent physical and functional requirements. The effectiveness of this approach is demonstrated through material design experiments requiring adherence to precise morphometric properties, challenging inverse design tasks involving the generation of materials inducing specific stress-strain responses, and copyright-constrained content generation tasks. All code has been released at https://github.com/RAISELab-atUVA/Constrained-Stable-Diffusion. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rSsc9uCVBl@OpenReview",
      "index": 235,
      "title": "Comparator-Adaptive Φ Φ -Regret: Improved Bounds, Simpler Algorithms, and Applications to Games",
      "authors": [
        "Soumita Hait",
        "Ping Li",
        "Haipeng Luo",
        "Mengxiao Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "regret",
        "phi",
        "comparator",
        "2025",
        "adaptive",
        "achieve",
        "simpler",
        "bounds",
        "algorithms",
        "copies"
      ],
      "summary": "In the classic expert problem, Φ Φ -regret measures the gap between the learner's total loss and that achieved by applying the best action transformation ϕ ∈ Φ ϕ ∈ Φ . A recent work by Lu et al., [2025] introduced an adaptive algorithm whose regret against a comparator ϕ ϕ depends on a certain sparsity-based complexity measure of ϕ ϕ , recovering and interpolating optimal bounds for standard regret notions such as external, internal, and swap regret. In this work, we propose a general idea to achieve an even better comparator-adaptive Φ Φ -regret bound via much simpler algorithms compared to Lu et al., [2025]. Specifically, we discover a prior distribution over all possible binary transformations and show that it suffices to achieve prior-dependent regret against these transformations. Then, we propose two concrete and efficient algorithms to achieve so, where the first one combines multiple copies of the kernelized Hedge algorithm of Farina et al., [2022], and the second one combines multiple copies of a variant of the BM-reduction [Blum and Mansour, 2007]. To further showcase the power of our methods and the advantages over Lu et al., [2025] besides the simplicity and better regret bounds, we also show that our second approach can be extended to the game setting to achieve accelerated and adaptive convergence rate to Φ Φ -equilibria for a class of general-sum games. When specified to the special case of correlated equilibria, our bound improves over the existing ones from Anagnostides et al., [2022a,b].",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rSsc9uCVBl"
        ],
        "venue": [
          "/venue/rSsc9uCVBl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rSsc9uCVBl"
        ],
        "detail": [
          "https://openreview.net/forum?id=rSsc9uCVBl"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Comparator-Adaptive Φ Φ -Regret: Improved Bounds, Simpler Algorithms, and Applications to Games [PDF ] [Copy] [Kimi ] [REL] Authors : Soumita Hait , Ping Li , Haipeng Luo , Mengxiao Zhang In the classic expert problem, Φ Φ -regret measures the gap between the learner's total loss and that achieved by applying the best action transformation ϕ ∈ Φ ϕ ∈ Φ . A recent work by Lu et al., [2025] introduced an adaptive algorithm whose regret against a comparator ϕ ϕ depends on a certain sparsity-based complexity measure of ϕ ϕ , recovering and interpolating optimal bounds for standard regret notions such as external, internal, and swap regret. In this work, we propose a general idea to achieve an even better comparator-adaptive Φ Φ -regret bound via much simpler algorithms compared to Lu et al., [2025]. Specifically, we discover a prior distribution over all possible binary transformations and show that it suffices to achieve prior-dependent regret against these transformations. Then, we propose two concrete and efficient algorithms to achieve so, where the first one combines multiple copies of the kernelized Hedge algorithm of Farina et al., [2022], and the second one combines multiple copies of a variant of the BM-reduction [Blum and Mansour, 2007]. To further showcase the power of our methods and the advantages over Lu et al., [2025] besides the simplicity and better regret bounds, we also show that our second approach can be extended to the game setting to achieve accelerated and adaptive convergence rate to Φ Φ -equilibria for a class of general-sum games. When specified to the special case of correlated equilibria, our bound improves over the existing ones from Anagnostides et al., [2022a,b]. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "S8XcHutp7Z@OpenReview",
      "index": 236,
      "title": "X-Field: A Physically Informed Representation for 3D X-ray Reconstruction",
      "authors": [
        "Feiran Wang",
        "Jiachen Tao",
        "Junyi Wu",
        "Haoxuan Wang",
        "Bin Duan",
        "Kai Wang",
        "Zongxin Yang",
        "Yan Yan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ray",
        "reconstruction",
        "attenuation",
        "nvs",
        "imaging",
        "ellipsoids",
        "informed",
        "field",
        "representation",
        "radiation"
      ],
      "summary": "X-ray imaging is indispensable in medical diagnostics, yet its use is tightly regulated due to radiation exposure. Recent research borrows representations from the 3D reconstruction area to complete two tasks with reduced radiation dose: X-ray Novel View Synthesis (NVS) and Computed Tomography (CT) reconstruction. However, these representations fail to fully capture the penetration and attenuation properties of X-ray imaging as they originate from visible light imaging. In this paper, we introduce X-Field, a 3D representation informed in the physics of X-ray imaging. First, we employ homogeneous 3D ellipsoids with distinct attenuation coefficients to accurately model diverse materials within internal structures. Second, we introduce an efficient path-partitioning algorithm that resolves the intricate intersection of ellipsoids to compute cumulative attenuation along an X-ray path. We further propose a hybrid progressive initialization to refine the geometric accuracy of X-Field and incorporate material-based optimization to enhance model fitting along material boundaries. Experiments show that X-Field achieves superior visual fidelity on both real-world human organ and synthetic object datasets, outperforming state-of-the-art methods in X-ray NVS and CT Reconstruction.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=S8XcHutp7Z"
        ],
        "venue": [
          "/venue/S8XcHutp7Z@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=S8XcHutp7Z"
        ],
        "detail": [
          "https://openreview.net/forum?id=S8XcHutp7Z"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "X-Field: A Physically Informed Representation for 3D X-ray Reconstruction [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Feiran Wang , Jiachen Tao , Junyi Wu , Haoxuan Wang , Bin Duan , Kai Wang , Zongxin Yang , Yan Yan X-ray imaging is indispensable in medical diagnostics, yet its use is tightly regulated due to radiation exposure. Recent research borrows representations from the 3D reconstruction area to complete two tasks with reduced radiation dose: X-ray Novel View Synthesis (NVS) and Computed Tomography (CT) reconstruction. However, these representations fail to fully capture the penetration and attenuation properties of X-ray imaging as they originate from visible light imaging. In this paper, we introduce X-Field, a 3D representation informed in the physics of X-ray imaging. First, we employ homogeneous 3D ellipsoids with distinct attenuation coefficients to accurately model diverse materials within internal structures. Second, we introduce an efficient path-partitioning algorithm that resolves the intricate intersection of ellipsoids to compute cumulative attenuation along an X-ray path. We further propose a hybrid progressive initialization to refine the geometric accuracy of X-Field and incorporate material-based optimization to enhance model fitting along material boundaries. Experiments show that X-Field achieves superior visual fidelity on both real-world human organ and synthetic object datasets, outperforming state-of-the-art methods in X-ray NVS and CT Reconstruction. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "UTGjik64IK@OpenReview",
      "index": 237,
      "title": "Distillation Robustifies Unlearning",
      "authors": [
        "Bruce W. Lee",
        "Addie Foote",
        "Alex Infanger",
        "Leni Shor",
        "Harish K Kamath",
        "Jacob Goldman-Wetzler",
        "Bryce Woodworth",
        "Alex Cloud",
        "Alexander Matt Turner"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "unlearning",
        "robustifies",
        "undo",
        "distillation",
        "unlearned",
        "leaving",
        "wmdp",
        "outputs",
        "revert",
        "model"
      ],
      "summary": "Current LLM unlearning methods are not robust. A few steps of finetuning can revert their effects. We begin by showing that this is true even for an idealized form of unlearning: training to imitate a model that was never trained on unwanted information. This shows that training a model can drastically modify its input-output behavior while leaving its underlying capabilities intact. In light of this dynamic, we show our main result. Training a randomly initialized student on the outputs of an unlearned model transfers behaviors while leaving latent capabilities behind. In short, distillation robustifies unlearning. Based on this result, we propose Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an unlearned model into a noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic language and arithmetic tasks. At its strongest setting, UNDO matches the robustness of a model retrained from scratch with perfect data filtering while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark. Since distillation is widely used in practice, incorporating an unlearning step beforehand offers a convenient path to robust capability removal.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UTGjik64IK"
        ],
        "venue": [
          "/venue/UTGjik64IK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UTGjik64IK"
        ],
        "detail": [
          "https://openreview.net/forum?id=UTGjik64IK"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Distillation Robustifies Unlearning [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Bruce W. Lee , Addie Foote , Alex Infanger , Leni Shor , Harish K Kamath , Jacob Goldman-Wetzler , Bryce Woodworth , Alex Cloud , Alexander Matt Turner Current LLM unlearning methods are not robust. A few steps of finetuning can revert their effects. We begin by showing that this is true even for an idealized form of unlearning: training to imitate a model that was never trained on unwanted information. This shows that training a model can drastically modify its input-output behavior while leaving its underlying capabilities intact. In light of this dynamic, we show our main result. Training a randomly initialized student on the outputs of an unlearned model transfers behaviors while leaving latent capabilities behind. In short, distillation robustifies unlearning. Based on this result, we propose Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an unlearned model into a noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic language and arithmetic tasks. At its strongest setting, UNDO matches the robustness of a model retrained from scratch with perfect data filtering while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark. Since distillation is widely used in practice, incorporating an unlearning step beforehand offers a convenient path to robust capability removal. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "FxCy8TvQHO@OpenReview",
      "index": 238,
      "title": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds",
      "authors": [
        "Xiaokang Ye",
        "Jiawei Ren",
        "Yan Zhuang",
        "Xuhong He",
        "Yiming Liang",
        "Yiqing Yang",
        "Mrinaal Dogra",
        "Xianrui Zhong",
        "Eric Liu",
        "Kevin Benavente",
        "Rajiv Mandya Nagaraju",
        "Dhruv Vivek Sharma",
        "Ziqiao Ma",
        "Tianmin Shu",
        "Zhiting Hu",
        "Lianhui Qin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "simworld",
        "agents",
        "vlm",
        "llm",
        "social",
        "world",
        "open",
        "physical",
        "ended",
        "simulator"
      ],
      "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (e.g., by autonomously earning income) requires massive-scale interaction, reasoning, training, and evaluation across diverse scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) rich interface for LLM/VLM agents, with multi-modal world inputs/feedback and open-vocabulary action outputs at varying levels of abstraction; and (3) diverse physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., Gemini-2.5-Flash, Claude-3.5, GPT-4o, and DeepSeek-Prover-V2) on both short-horizon navigation tasks requiring grounded re-planning, and long-horizon multi-agent food delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines. Please refer to the project website for the most up-to-date information: http://simworld.org/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FxCy8TvQHO"
        ],
        "venue": [
          "/venue/FxCy8TvQHO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FxCy8TvQHO"
        ],
        "detail": [
          "https://openreview.net/forum?id=FxCy8TvQHO"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Xiaokang Ye , Jiawei Ren , Yan Zhuang , Xuhong He , Yiming Liang , Yiqing Yang , Mrinaal Dogra , Xianrui Zhong , Eric Liu , Kevin Benavente , Rajiv Mandya Nagaraju , Dhruv Vivek Sharma , Ziqiao Ma , Tianmin Shu , Zhiting Hu , Lianhui Qin While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (e.g., by autonomously earning income) requires massive-scale interaction, reasoning, training, and evaluation across diverse scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) rich interface for LLM/VLM agents, with multi-modal world inputs/feedback and open-vocabulary action outputs at varying levels of abstraction; and (3) diverse physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., Gemini-2.5-Flash, Claude-3.5, GPT-4o, and DeepSeek-Prover-V2) on both short-horizon navigation tasks requiring grounded re-planning, and long-horizon multi-agent food delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines. Please refer to the project website for the most up-to-date information: http://simworld.org/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IT12Radlnq@OpenReview",
      "index": 239,
      "title": "Two‑Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion",
      "authors": [
        "Haoyu Li",
        "Xiangru Zhong",
        "Bin Hu",
        "Huan Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "zubov",
        "controllers",
        "training",
        "conservatism",
        "stage",
        "neural",
        "smt",
        "verification",
        "systems",
        "attraction"
      ],
      "summary": "Learning-based neural network (NN) control policies have shown impressive empirical performance. However, obtaining stability guarantees and estimates of the region of attraction of these learned neural controllers is challenging due to the lack of stable and scalable training and verification algorithms. Although previous works in this area have achieved great success, much conservatism remains in their frameworks. In this work, we propose a novel two-stage training framework to jointly synthesize a controller a Lyapunov function for continuous-time systems. By leveraging a Zubov‑inspired region of attraction characterization to directly estimate stability boundaries, we propose a novel training-data sampling strategy and a domain-updating mechanism that significantly reduces the conservatism in training. Moreover, unlike existing works on continuous-time systems that rely on an SMT solver to formally verify the Lyapunov condition, we extend state-of-the-art neural network verifier α , β α , β -CROWN with the capability of performing automatic bound propagation through the Jacobian of dynamical systems and a novel verification scheme that avoids expensive bisection. To demonstrate the effectiveness of our approach, we conduct numerical experiments by synthesizing and verifying controllers on several challenging nonlinear systems across multiple dimensions. We show that our training can yield region of attractions with volume 5 − 1.5 ⋅ 10 5 5 − 1.5 ⋅ 10 5 times larger compared to the baselines, and our verification on continuous systems can be up to 40 − 10,000 40 − 10,000 times faster compared to the traditional SMT solver dReal. Our code is available at https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IT12Radlnq"
        ],
        "venue": [
          "/venue/IT12Radlnq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IT12Radlnq"
        ],
        "detail": [
          "https://openreview.net/forum?id=IT12Radlnq"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Two‑Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Haoyu Li , Xiangru Zhong , Bin Hu , Huan Zhang Learning-based neural network (NN) control policies have shown impressive empirical performance. However, obtaining stability guarantees and estimates of the region of attraction of these learned neural controllers is challenging due to the lack of stable and scalable training and verification algorithms. Although previous works in this area have achieved great success, much conservatism remains in their frameworks. In this work, we propose a novel two-stage training framework to jointly synthesize a controller a Lyapunov function for continuous-time systems. By leveraging a Zubov‑inspired region of attraction characterization to directly estimate stability boundaries, we propose a novel training-data sampling strategy and a domain-updating mechanism that significantly reduces the conservatism in training. Moreover, unlike existing works on continuous-time systems that rely on an SMT solver to formally verify the Lyapunov condition, we extend state-of-the-art neural network verifier α , β α , β -CROWN with the capability of performing automatic bound propagation through the Jacobian of dynamical systems and a novel verification scheme that avoids expensive bisection. To demonstrate the effectiveness of our approach, we conduct numerical experiments by synthesizing and verifying controllers on several challenging nonlinear systems across multiple dimensions. We show that our training can yield region of attractions with volume 5 − 1.5 ⋅ 10 5 5 − 1.5 ⋅ 10 5 times larger compared to the baselines, and our verification on continuous systems can be up to 40 − 10,000 40 − 10,000 times faster compared to the traditional SMT solver dReal. Our code is available at https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "75LMvs1CjG@OpenReview",
      "index": 240,
      "title": "The Complexity of Symmetric Equilibria in Min-Max Optimization and Team Zero-Sum Games",
      "authors": [
        "Ioannis Anagnostides",
        "Ioannis Panageas",
        "Tuomas Sandholm",
        "Jingming Yan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "equilibria",
        "symmetric",
        "emph",
        "text",
        "textsf",
        "nash",
        "team",
        "games",
        "min",
        "max"
      ],
      "summary": "We consider the problem of computing stationary points in min-max optimization, with a focus on the special case of Nash equilibria in (two-)team zero-sum games. We first show that computing ϵ ϵ -Nash equilibria in 3 3 -player \\emph{adversarial} \\emph{adversarial} team games---wherein a team of 2 2 players competes against a \\emph{single} \\emph{single} adversary---is CLS CLS -complete, resolving the complexity of Nash equilibria in such settings. Our proof proceeds by reducing from \\emph{symmetric} \\emph{symmetric} ϵ ϵ -Nash equilibria in \\emph{symmetric} \\emph{symmetric} , identical-payoff, two-player games, by suitably leveraging the adversarial player so as to enforce symmetry---without disturbing the structure of the game. In particular, the class of instances we construct comprises solely polymatrix games, thereby also settling a question left open by Hollender, Maystre, and Nagarajan (2024). Moreover, we establish that computing \\emph{symmetric} \\emph{symmetric} (first-order) equilibria in \\emph{symmetric} \\emph{symmetric} min-max optimization is PPAD PPAD -complete, even for quadratic functions. Building on this reduction, we show that computing symmetric ϵ ϵ -Nash equilibria in symmetric, 6 6 -player ( 3 3 vs. 3 3 ) team zero-sum games is also PPAD PPAD -complete, even for ϵ = poly ( 1 / n ) ϵ = poly ( 1 / n ) . As a corollary, this precludes the existence of symmetric dynamics---which includes many of the algorithms considered in the literature---converging to stationary points. Finally, we prove that computing a \\emph{non-symmetric} \\emph{non-symmetric} poly ( 1 / n ) poly ( 1 / n ) -equilibrium in symmetric min-max optimization is FNP FNP -hard.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=75LMvs1CjG"
        ],
        "venue": [
          "/venue/75LMvs1CjG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=75LMvs1CjG"
        ],
        "detail": [
          "https://openreview.net/forum?id=75LMvs1CjG"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "The Complexity of Symmetric Equilibria in Min-Max Optimization and Team Zero-Sum Games [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ioannis Anagnostides , Ioannis Panageas , Tuomas Sandholm , Jingming Yan We consider the problem of computing stationary points in min-max optimization, with a focus on the special case of Nash equilibria in (two-)team zero-sum games. We first show that computing ϵ ϵ -Nash equilibria in 3 3 -player \\emph{adversarial} \\emph{adversarial} team games---wherein a team of 2 2 players competes against a \\emph{single} \\emph{single} adversary---is CLS CLS -complete, resolving the complexity of Nash equilibria in such settings. Our proof proceeds by reducing from \\emph{symmetric} \\emph{symmetric} ϵ ϵ -Nash equilibria in \\emph{symmetric} \\emph{symmetric} , identical-payoff, two-player games, by suitably leveraging the adversarial player so as to enforce symmetry---without disturbing the structure of the game. In particular, the class of instances we construct comprises solely polymatrix games, thereby also settling a question left open by Hollender, Maystre, and Nagarajan (2024). Moreover, we establish that computing \\emph{symmetric} \\emph{symmetric} (first-order) equilibria in \\emph{symmetric} \\emph{symmetric} min-max optimization is PPAD PPAD -complete, even for quadratic functions. Building on this reduction, we show that computing symmetric ϵ ϵ -Nash equilibria in symmetric, 6 6 -player ( 3 3 vs. 3 3 ) team zero-sum games is also PPAD PPAD -complete, even for ϵ = poly ( 1 / n ) ϵ = poly ( 1 / n ) . As a corollary, this precludes the existence of symmetric dynamics---which includes many of the algorithms considered in the literature---converging to stationary points. Finally, we prove that computing a \\emph{non-symmetric} \\emph{non-symmetric} poly ( 1 / n ) poly ( 1 / n ) -equilibrium in symmetric min-max optimization is FNP FNP -hard. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "LyG7kDSsGh@OpenReview",
      "index": 241,
      "title": "On Traceability in ℓ p ℓ p Stochastic Convex Optimization",
      "authors": [
        "Sasha Voitovych",
        "Mahdi Haghifam",
        "Idan Attias",
        "Gintare Karolina Dziugaite",
        "Roi Livni",
        "Daniel M. Roy"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "traceability",
        "traceable",
        "threshold",
        "sco",
        "ell",
        "excess",
        "risk",
        "convex",
        "emph",
        "stochastic"
      ],
      "summary": "In this paper, we investigate the necessity of traceability for accurate learning in stochastic convex optimization (SCO) under ℓ p ℓ p geometries. Informally, we say a learning algorithm is \\emph{ m m -traceable} if, by analyzing its output, it is possible to identify at least m m of its training samples. Our main results uncover a fundamental tradeoff between traceability and excess risk in SCO. For every p ∈ [ 1 , ∞ ) p ∈ [ 1 , ∞ ) , we establish the existence of an excess risk threshold below which every sample-efficient learner is traceable with the number of samples which is a \\emph{constant fraction} of its training sample. For p ∈ [ 1 , 2 ] p ∈ [ 1 , 2 ] , this threshold coincides with the best excess risk of differentially private (DP) algorithms, i.e., above this threshold, there exist algorithms that are not traceable, which corresponds to a sharp phase transition. For p ∈ ( 2 , ∞ ) p ∈ ( 2 , ∞ ) , this threshold instead gives novel lower bounds for DP learning, partially closing an open problem in this setup. En route to establishing these results, we prove a sparse variant of the fingerprinting lemma, which is of independent interest to the community.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LyG7kDSsGh"
        ],
        "venue": [
          "/venue/LyG7kDSsGh@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LyG7kDSsGh"
        ],
        "detail": [
          "https://openreview.net/forum?id=LyG7kDSsGh"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "On Traceability in ℓ p ℓ p Stochastic Convex Optimization [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Sasha Voitovych , Mahdi Haghifam , Idan Attias , Gintare Karolina Dziugaite , Roi Livni , Daniel M. Roy In this paper, we investigate the necessity of traceability for accurate learning in stochastic convex optimization (SCO) under ℓ p ℓ p geometries. Informally, we say a learning algorithm is \\emph{ m m -traceable} if, by analyzing its output, it is possible to identify at least m m of its training samples. Our main results uncover a fundamental tradeoff between traceability and excess risk in SCO. For every p ∈ [ 1 , ∞ ) p ∈ [ 1 , ∞ ) , we establish the existence of an excess risk threshold below which every sample-efficient learner is traceable with the number of samples which is a \\emph{constant fraction} of its training sample. For p ∈ [ 1 , 2 ] p ∈ [ 1 , 2 ] , this threshold coincides with the best excess risk of differentially private (DP) algorithms, i.e., above this threshold, there exist algorithms that are not traceable, which corresponds to a sharp phase transition. For p ∈ ( 2 , ∞ ) p ∈ ( 2 , ∞ ) , this threshold instead gives novel lower bounds for DP learning, partially closing an open problem in this setup. En route to establishing these results, we prove a sparse variant of the fingerprinting lemma, which is of independent interest to the community. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VHb883Gs1u@OpenReview",
      "index": 242,
      "title": "Improved Representation Steering for Language Models",
      "authors": [
        "Zhengxuan Wu",
        "Qinan Yu",
        "Aryaman Arora",
        "Christopher D Manning",
        "Christopher Potts"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "steering",
        "reps",
        "gemma",
        "prompting",
        "language",
        "suppression",
        "objective",
        "axbench",
        "interpretable",
        "preference"
      ],
      "summary": "Steering methods for language models (LMs) seek to provide fine-grained and interpretable control over model generations by variously changing model inputs, weights, or representations to adjust behavior. Recent work has shown that adjusting weights or representations is often less effective than steering by prompting, for instance when wanting to introduce or suppress a particular concept. We demonstrate how to improve representation steering via our new Reference-free Preference Steering (RePS), a bidirectional preference-optimization objective that jointly does concept steering and suppression. We train three parameterizations of RePS and evaluate them on AxBench, a large-scale model steering benchmark. On Gemma models with sizes ranging from 2B to 27B, RePS outperforms all existing steering methods trained with a language modeling objective and substantially narrows the gap with prompting -- while promoting interpretability and minimizing parameter count. In suppression, RePS matches the language-modeling objective on Gemma-2 and outperforms it on the larger Gemma-3 variants while remaining resilient to prompt-based jailbreaking attacks that defeat prompting. Overall, our results suggest that RePS provides an interpretable and robust alternative to prompting for both steering and suppression.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VHb883Gs1u"
        ],
        "venue": [
          "/venue/VHb883Gs1u@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VHb883Gs1u"
        ],
        "detail": [
          "https://openreview.net/forum?id=VHb883Gs1u"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Improved Representation Steering for Language Models [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Zhengxuan Wu , Qinan Yu , Aryaman Arora , Christopher D Manning , Christopher Potts Steering methods for language models (LMs) seek to provide fine-grained and interpretable control over model generations by variously changing model inputs, weights, or representations to adjust behavior. Recent work has shown that adjusting weights or representations is often less effective than steering by prompting, for instance when wanting to introduce or suppress a particular concept. We demonstrate how to improve representation steering via our new Reference-free Preference Steering (RePS), a bidirectional preference-optimization objective that jointly does concept steering and suppression. We train three parameterizations of RePS and evaluate them on AxBench, a large-scale model steering benchmark. On Gemma models with sizes ranging from 2B to 27B, RePS outperforms all existing steering methods trained with a language modeling objective and substantially narrows the gap with prompting -- while promoting interpretability and minimizing parameter count. In suppression, RePS matches the language-modeling objective on Gemma-2 and outperforms it on the larger Gemma-3 variants while remaining resilient to prompt-based jailbreaking attacks that defeat prompting. Overall, our results suggest that RePS provides an interpretable and robust alternative to prompting for both steering and suppression. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "1rUj9ZN6Bz@OpenReview",
      "index": 243,
      "title": "FlexOLMo: Open Language Models for Flexible Data Use",
      "authors": [
        "Weijia Shi",
        "Akshita Bhagia",
        "Kevin Farhat",
        "Niklas Muennighoff",
        "Jacob Morrison",
        "Evan Pete Walsh",
        "Dustin Schwenk",
        "Shayne Longpre",
        "Jake Poznanski",
        "Allyson Ettinger",
        "Daogao Liu",
        "Margaret Li",
        "Mike Lewis",
        "Wen-tau Yih",
        "Dirk Groeneveld",
        "Luca Soldaini",
        "Kyle Lo",
        "Noah A. Smith",
        "Luke Zettlemoyer",
        "Pang Wei Koh",
        "Hannaneh Hajishirzi",
        "Ali Farhadi",
        "Sewon Min"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "flexolmo",
        "trained",
        "data",
        "moe",
        "flexible",
        "datasets",
        "training",
        "independently",
        "flexmix",
        "restricted"
      ],
      "summary": "We introduce FlexOLMo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on private datasets, and (2) data-flexible inference, where these parameters along with their associated data can be easily included or excluded from model inferences with no further training. FlexOLMo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on private datasets and later integrated through a new nonparametric routing without any joint training across datasets. FlexOLMo is trained on FLEXMIX, a corpus we curate comprising seven restricted sets, either real or realistic approximations, alongside publicly available datasets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners significantly benefiting from these restricted sets (an average 41% relative improvement) while allowing flexible opt-out at inference time (e.g., for users without appropriate licenses or permissions). Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, FlexOLMo enables training on restricted data while keeping data local and supports fine-grained control of data access at inference.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1rUj9ZN6Bz"
        ],
        "venue": [
          "/venue/1rUj9ZN6Bz@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1rUj9ZN6Bz"
        ],
        "detail": [
          "https://openreview.net/forum?id=1rUj9ZN6Bz"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "FlexOLMo: Open Language Models for Flexible Data Use [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Weijia Shi , Akshita Bhagia , Kevin Farhat , Niklas Muennighoff , Jacob Morrison , Evan Pete Walsh , Dustin Schwenk , Shayne Longpre , Jake Poznanski , Allyson Ettinger , Daogao Liu , Margaret Li , Mike Lewis , Wen-tau Yih , Dirk Groeneveld , Luca Soldaini , Kyle Lo , Noah A. Smith , Luke Zettlemoyer , Pang Wei Koh , Hannaneh Hajishirzi , Ali Farhadi , Sewon Min We introduce FlexOLMo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on private datasets, and (2) data-flexible inference, where these parameters along with their associated data can be easily included or excluded from model inferences with no further training. FlexOLMo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on private datasets and later integrated through a new nonparametric routing without any joint training across datasets. FlexOLMo is trained on FLEXMIX, a corpus we curate comprising seven restricted sets, either real or realistic approximations, alongside publicly available datasets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners significantly benefiting from these restricted sets (an average 41% relative improvement) while allowing flexible opt-out at inference time (e.g., for users without appropriate licenses or permissions). Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, FlexOLMo enables training on restricted data while keeping data local and supports fine-grained control of data access at inference. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "DrUR87D4Hj@OpenReview",
      "index": 244,
      "title": "The Fragile Truth of Saliency: Improving LLM Input Attribution via Attention Bias Optimization",
      "authors": [
        "Yihua Zhang",
        "Changsheng Wang",
        "Yiwei Chen",
        "Chongyu Fan",
        "Jinghan Jia",
        "Sijia Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "saliency",
        "attribution",
        "input",
        "niah",
        "token",
        "abo",
        "bias",
        "attention",
        "fragile",
        "methods"
      ],
      "summary": "Input saliency aims to quantify the influence of input tokens on the output of large language models (LLMs), which has been widely used for prompt engineering, model interpretability, and behavior attribution. Despite the proliferation of saliency techniques, the field lacks a standardized and rigorous evaluation protocol. In this work, we introduce a stress-testing framework inspired by the needle-in-a-haystack (NIAH) setting to systematically assess the reliability of seven popular input saliency methods. Our evaluation reveals a surprising and critical flaw: existing methods consistently assign non-trivial importance to irrelevant context, and this attribution error worsens as input length increases. To address this issue, we propose a novel saliency method based on Attention Bias Optimization (ours), which explicitly optimizes the attention bias associated with each input token to quantify its causal impact on target token generation. ABO robustly outperforms existing methods by 10\\sim30% in saliency accuracy across diverse NIAH tasks, maintains effectiveness up to 10K-token prompts, and enables practical applications including zero-shot detoxification, sentiment steering, and reasoning-error correction. Our findings highlight the limitations of prevalent attribution methods and establish ABO as a principled alternative for accurate token attribution.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DrUR87D4Hj"
        ],
        "venue": [
          "/venue/DrUR87D4Hj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DrUR87D4Hj"
        ],
        "detail": [
          "https://openreview.net/forum?id=DrUR87D4Hj"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "The Fragile Truth of Saliency: Improving LLM Input Attribution via Attention Bias Optimization [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Yihua Zhang , Changsheng Wang , Yiwei Chen , Chongyu Fan , Jinghan Jia , Sijia Liu Input saliency aims to quantify the influence of input tokens on the output of large language models (LLMs), which has been widely used for prompt engineering, model interpretability, and behavior attribution. Despite the proliferation of saliency techniques, the field lacks a standardized and rigorous evaluation protocol. In this work, we introduce a stress-testing framework inspired by the needle-in-a-haystack (NIAH) setting to systematically assess the reliability of seven popular input saliency methods. Our evaluation reveals a surprising and critical flaw: existing methods consistently assign non-trivial importance to irrelevant context, and this attribution error worsens as input length increases. To address this issue, we propose a novel saliency method based on Attention Bias Optimization (ours), which explicitly optimizes the attention bias associated with each input token to quantify its causal impact on target token generation. ABO robustly outperforms existing methods by 10\\sim30% in saliency accuracy across diverse NIAH tasks, maintains effectiveness up to 10K-token prompts, and enables practical applications including zero-shot detoxification, sentiment steering, and reasoning-error correction. Our findings highlight the limitations of prevalent attribution methods and establish ABO as a principled alternative for accurate token attribution. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ESB924uT5Y@OpenReview",
      "index": 245,
      "title": "Enhancing Training Data Attribution with Representational Optimization",
      "authors": [
        "Weiwei Sun",
        "Haokun Liu",
        "Nikhil Kandpal",
        "Colin Raffel",
        "Yiming Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "airrep",
        "attribution",
        "tda",
        "sunnweiwei",
        "training",
        "representational",
        "based",
        "scalable",
        "optimized",
        "tuned"
      ],
      "summary": "Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. While gradient-based attribution methods, such as influence functions, offer theoretical grounding, their computational costs make them impractical for large-scale applications. Representation-based approaches are far more scalable, but typically rely on heuristic embeddings that are not optimized for attribution, limiting their fidelity. To address these challenges, we propose AirRep, a scalable, representation-based approach that closes this gap by learning task-specific and model-aligned representations optimized explicitly for TDA. AirRep introduces two key innovations: a trainable encoder tuned for attribution quality, and an attention-based pooling mechanism that enables accurate estimation of group-wise influence. We train AirRep using a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions. Experiments on instruction-tuned LLMs demonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient at inference time. Further analysis highlights its robustness and generalization across tasks and models. Our code is available at https://github.com/sunnweiwei/AirRep.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ESB924uT5Y"
        ],
        "venue": [
          "/venue/ESB924uT5Y@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ESB924uT5Y"
        ],
        "detail": [
          "https://openreview.net/forum?id=ESB924uT5Y"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Enhancing Training Data Attribution with Representational Optimization [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Weiwei Sun , Haokun Liu , Nikhil Kandpal , Colin Raffel , Yiming Yang Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. While gradient-based attribution methods, such as influence functions, offer theoretical grounding, their computational costs make them impractical for large-scale applications. Representation-based approaches are far more scalable, but typically rely on heuristic embeddings that are not optimized for attribution, limiting their fidelity. To address these challenges, we propose AirRep, a scalable, representation-based approach that closes this gap by learning task-specific and model-aligned representations optimized explicitly for TDA. AirRep introduces two key innovations: a trainable encoder tuned for attribution quality, and an attention-based pooling mechanism that enables accurate estimation of group-wise influence. We train AirRep using a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions. Experiments on instruction-tuned LLMs demonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient at inference time. Further analysis highlights its robustness and generalization across tasks and models. Our code is available at https://github.com/sunnweiwei/AirRep. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "jneVld5iZw@OpenReview",
      "index": 246,
      "title": "DisMo: Disentangled Motion Representations for Open-World Motion Transfer",
      "authors": [
        "Thomas Ressler-Antal",
        "Frank Fundel",
        "Malek Ben Alaya",
        "Stefan Andreas Baumann",
        "Felix Krause",
        "Ming Gui",
        "Björn Ommer"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dismo",
        "motion",
        "video",
        "transfer",
        "representation",
        "something",
        "representations",
        "compvis",
        "i2v",
        "jester"
      ],
      "summary": "Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jneVld5iZw"
        ],
        "venue": [
          "/venue/jneVld5iZw@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jneVld5iZw"
        ],
        "detail": [
          "https://openreview.net/forum?id=jneVld5iZw"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "DisMo: Disentangled Motion Representations for Open-World Motion Transfer [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Thomas Ressler-Antal , Frank Fundel , Malek Ben Alaya , Stefan Andreas Baumann , Felix Krause , Ming Gui , Björn Ommer Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "vEFPm6gw2s@OpenReview",
      "index": 247,
      "title": "Rig3R: Rig-Aware Conditioning and Discovery for 3D Reconstruction",
      "authors": [
        "Samuel Li",
        "Pujith Kachana",
        "Prajwal Chidananda",
        "Saurabh Nair",
        "Yasutaka Furukawa",
        "Matthew Brown"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rig",
        "rig3r",
        "raymaps",
        "rigs",
        "metadata",
        "pose",
        "raymap",
        "infer",
        "reconstruction",
        "multiview"
      ],
      "summary": "Estimating agent pose and 3D scene structure from multi-camera rigs is a central task in embodied AI applications such as autonomous driving. Recent learned approaches such as DUSt3R have shown impressive performance in multiview settings. However, these models treat images as unstructured collections, limiting effectiveness in scenarios where frames are captured from synchronized rigs with known or inferable structure. To this end, we introduce Rig3R, a generalization of prior multiview reconstruction models that incorporates rig structure when available, and learns to infer it when not. Rig3R conditions on optional rig metadata including camera ID, time, and rig poses to develop a rig-aware latent space that remains robust to missing information. It jointly predicts pointmaps and two types of raymaps: a pose raymap relative to a global frame, and a rig raymap relative to a rig-centric frame consistent across time. Rig raymaps allow the model to infer rig structure directly from input images when metadata is missing. The global pose raymaps allow the model to reason about the agent’s ego-motion, while the rig raymaps allow the model to infer rig structure directly from input images when metadata is missing. Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose estimation, and rig discovery -- outperforming both traditional and learned methods by 17-45% mAA across diverse real-world rig datasets, all in a single forward pass without post-processing or iterative refinement.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vEFPm6gw2s"
        ],
        "venue": [
          "/venue/vEFPm6gw2s@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vEFPm6gw2s"
        ],
        "detail": [
          "https://openreview.net/forum?id=vEFPm6gw2s"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Rig3R: Rig-Aware Conditioning and Discovery for 3D Reconstruction [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Samuel Li , Pujith Kachana , Prajwal Chidananda , Saurabh Nair , Yasutaka Furukawa , Matthew Brown Estimating agent pose and 3D scene structure from multi-camera rigs is a central task in embodied AI applications such as autonomous driving. Recent learned approaches such as DUSt3R have shown impressive performance in multiview settings. However, these models treat images as unstructured collections, limiting effectiveness in scenarios where frames are captured from synchronized rigs with known or inferable structure. To this end, we introduce Rig3R, a generalization of prior multiview reconstruction models that incorporates rig structure when available, and learns to infer it when not. Rig3R conditions on optional rig metadata including camera ID, time, and rig poses to develop a rig-aware latent space that remains robust to missing information. It jointly predicts pointmaps and two types of raymaps: a pose raymap relative to a global frame, and a rig raymap relative to a rig-centric frame consistent across time. Rig raymaps allow the model to infer rig structure directly from input images when metadata is missing. The global pose raymaps allow the model to reason about the agent’s ego-motion, while the rig raymaps allow the model to infer rig structure directly from input images when metadata is missing. Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose estimation, and rig discovery -- outperforming both traditional and learned methods by 17-45% mAA across diverse real-world rig datasets, all in a single forward pass without post-processing or iterative refinement. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "M3zxsDL2Rk@OpenReview",
      "index": 248,
      "title": "Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization",
      "authors": [
        "Shaohan Li",
        "Yunpeng Shi",
        "Gilad Lerman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cycle",
        "mpls",
        "sync",
        "camera",
        "robust",
        "synchronization",
        "global",
        "adjustment",
        "pose",
        "welsch"
      ],
      "summary": "We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) - originally developed for group synchronization - to the camera localization setting. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, demonstrating that cycle consistency alone enables the lowest sample complexity to date. To further boost robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation averaging. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M3zxsDL2Rk"
        ],
        "venue": [
          "/venue/M3zxsDL2Rk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M3zxsDL2Rk"
        ],
        "detail": [
          "https://openreview.net/forum?id=M3zxsDL2Rk"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization [PDF ] [Copy] [Kimi ] [REL] Authors : Shaohan Li , Yunpeng Shi , Gilad Lerman We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) - originally developed for group synchronization - to the camera localization setting. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, demonstrating that cycle consistency alone enables the lowest sample complexity to date. To further boost robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation averaging. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZbQ5Zq3zA3@OpenReview",
      "index": 249,
      "title": "Dense Associative Memory with Epanechnikov Energy",
      "authors": [
        "Benjamin Hoover",
        "Zhaoyang Shi",
        "Krishna Balasubramanian",
        "Dmitry Krotov",
        "Parikshit Ram"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lsr",
        "epanechnikov",
        "denseam",
        "lse",
        "memory",
        "associative",
        "log",
        "exponential",
        "minima",
        "memories"
      ],
      "summary": "We propose a novel energy function for Dense Associative Memory (DenseAM) networks, the log-sum-ReLU (LSR), inspired by optimal kernel density estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based on the Epanechnikov kernel and enables exact memory retrieval with exponential capacity without requiring exponential separation functions. Uniquely, it introduces abundant additional emergent local minima while preserving perfect pattern recovery --- a characteristic previously unseen in DenseAM literature. Empirical results show that LSR energy has significantly more local minima (memories) that have comparable log-likelihood to LSE-based models. Analysis of LSR's emergent memories on image datasets reveals a degree of creativity and novelty, hinting at this method's potential for both large-scale memory storage and generative tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZbQ5Zq3zA3"
        ],
        "venue": [
          "/venue/ZbQ5Zq3zA3@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZbQ5Zq3zA3"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZbQ5Zq3zA3"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Dense Associative Memory with Epanechnikov Energy [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Benjamin Hoover , Zhaoyang Shi , Krishna Balasubramanian , Dmitry Krotov , Parikshit Ram We propose a novel energy function for Dense Associative Memory (DenseAM) networks, the log-sum-ReLU (LSR), inspired by optimal kernel density estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based on the Epanechnikov kernel and enables exact memory retrieval with exponential capacity without requiring exponential separation functions. Uniquely, it introduces abundant additional emergent local minima while preserving perfect pattern recovery --- a characteristic previously unseen in DenseAM literature. Empirical results show that LSR energy has significantly more local minima (memories) that have comparable log-likelihood to LSE-based models. Analysis of LSR's emergent memories on image datasets reveals a degree of creativity and novelty, hinting at this method's potential for both large-scale memory storage and generative tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "CsXKGIqZtr@OpenReview",
      "index": 250,
      "title": "Searching Latent Program Spaces",
      "authors": [
        "Matthew Macfarlane",
        "Clément Bonnet"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lpn",
        "test",
        "latent",
        "program",
        "search",
        "time",
        "space",
        "tasks",
        "spaces",
        "dsls"
      ],
      "summary": "General intelligence requires systems that acquire new skills efficiently and generalize beyond their training distributions. Although program synthesis approaches have strong generalization power, they face scaling issues due to large combinatorial spaces that quickly make them impractical and require human-generated DSLs or pre-trained priors to narrow this search space. On the other hand, deep learning methods have had high successes, but they lack structured test-time adaptation and rely on heavy stochastic sampling or expensive gradient updates for fine-tuning. In this work, we propose the Latent Program Network (LPN), a new architecture that builds in test-time search directly into neural models. LPN learns a latent space of implicit programs---neurally mapping inputs to outputs---through which it can search using gradients at test time. LPN combines the adaptability of symbolic approaches and the scalability of neural methods. It searches through a compact latent space at test time and bypasses the need for pre-defined domain-specific languages. On a range of programming-by-examples tasks, LPN either outperforms or matches performance compared to in-context learning and test-time training methods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both learn a compact program space and search through it at test time to adapt to novel tasks. LPN doubles its performance on out-of-distribution tasks when test-time search is switched on.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CsXKGIqZtr"
        ],
        "venue": [
          "/venue/CsXKGIqZtr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CsXKGIqZtr"
        ],
        "detail": [
          "https://openreview.net/forum?id=CsXKGIqZtr"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Searching Latent Program Spaces [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Matthew Macfarlane , Clément Bonnet General intelligence requires systems that acquire new skills efficiently and generalize beyond their training distributions. Although program synthesis approaches have strong generalization power, they face scaling issues due to large combinatorial spaces that quickly make them impractical and require human-generated DSLs or pre-trained priors to narrow this search space. On the other hand, deep learning methods have had high successes, but they lack structured test-time adaptation and rely on heavy stochastic sampling or expensive gradient updates for fine-tuning. In this work, we propose the Latent Program Network (LPN), a new architecture that builds in test-time search directly into neural models. LPN learns a latent space of implicit programs---neurally mapping inputs to outputs---through which it can search using gradients at test time. LPN combines the adaptability of symbolic approaches and the scalability of neural methods. It searches through a compact latent space at test time and bypasses the need for pre-defined domain-specific languages. On a range of programming-by-examples tasks, LPN either outperforms or matches performance compared to in-context learning and test-time training methods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both learn a compact program space and search through it at test time to adapt to novel tasks. LPN doubles its performance on out-of-distribution tasks when test-time search is switched on. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Dn4He1IrUT@OpenReview",
      "index": 251,
      "title": "Convergence Rates of Constrained Expected Improvement",
      "authors": [
        "Haowei Wang",
        "Jingyi Wang",
        "Zhongxiang Dai",
        "Nai-Yuan Chiang",
        "Szu Hui Ng",
        "Cosmin G. Petra"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cei",
        "convergence",
        "cbo",
        "frac",
        "constrained",
        "improvement",
        "rates",
        "expected",
        "commonly",
        "assumed"
      ],
      "summary": "Constrained Bayesian optimization (CBO) methods have seen significant success in black-box optimization with constraints. One of the most commonly used CBO methods is the constrained expected improvement (CEI) algorithm. CEI is a natural extension of expected improvement (EI) when constraints are incorporated. However, the theoretical convergence rate of CEI has not been established. In this work, we study the convergence rate of CEI by analyzing its simple regret upper bound. First, we show that when the objective function f f and constraint function c c are assumed to each lie in a reproducing kernel Hilbert space (RKHS), CEI achieves the convergence rates of O ( t − 1 2 log d + 1 2 ( t ) ) and O ( t − ν 2 ν + d log ν 2 ν + d ( t ) ) O ( t − 1 2 log d + 1 2 ⁡ ( t ) ) and O ( t − ν 2 ν + d log ν 2 ν + d ⁡ ( t ) ) for the commonly used squared exponential and Matérn kernels, respectively. Second, we show that when f f and c c are assumed to be sampled from Gaussian processes (GPs), CEI achieves the same convergence rates with a high probability. Numerical experiments are performed to validate the theoretical analysis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Dn4He1IrUT"
        ],
        "venue": [
          "/venue/Dn4He1IrUT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Dn4He1IrUT"
        ],
        "detail": [
          "https://openreview.net/forum?id=Dn4He1IrUT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Convergence Rates of Constrained Expected Improvement [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Haowei Wang , Jingyi Wang , Zhongxiang Dai , Nai-Yuan Chiang , Szu Hui Ng , Cosmin G. Petra Constrained Bayesian optimization (CBO) methods have seen significant success in black-box optimization with constraints. One of the most commonly used CBO methods is the constrained expected improvement (CEI) algorithm. CEI is a natural extension of expected improvement (EI) when constraints are incorporated. However, the theoretical convergence rate of CEI has not been established. In this work, we study the convergence rate of CEI by analyzing its simple regret upper bound. First, we show that when the objective function f f and constraint function c c are assumed to each lie in a reproducing kernel Hilbert space (RKHS), CEI achieves the convergence rates of O ( t − 1 2 log d + 1 2 ( t ) ) and O ( t − ν 2 ν + d log ν 2 ν + d ( t ) ) O ( t − 1 2 log d + 1 2 ⁡ ( t ) ) and O ( t − ν 2 ν + d log ν 2 ν + d ⁡ ( t ) ) for the commonly used squared exponential and Matérn kernels, respectively. Second, we show that when f f and c c are assumed to be sampled from Gaussian processes (GPs), CEI achieves the same convergence rates with a high probability. Numerical experiments are performed to validate the theoretical analysis. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "OkVQJZWGfn@OpenReview",
      "index": 252,
      "title": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision",
      "authors": [
        "Awni Altabaa",
        "Omar Montasser",
        "John Lafferty"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cot",
        "supervision",
        "thought",
        "chain",
        "end",
        "reasoning",
        "information",
        "learning",
        "augments",
        "bounds"
      ],
      "summary": "Learning complex functions that involve multi-step reasoning poses a significant challenge for standard supervised learning from input-output examples. Chain-of-thought (CoT) supervision, which augments training data with intermediate reasoning steps to provide a richer learning signal, has driven recent advances in large language model reasoning. This paper develops a statistical theory of learning under CoT supervision. Central to the theory is the *CoT information*, which measures the additional discriminative power offered by the chain-of-thought for distinguishing hypotheses with different end-to-end behaviors. The main theoretical results demonstrate how CoT supervision can yield significantly faster learning rates compared to standard end-to-end supervision, with both upper bounds and information-theoretic lower bounds characterized by the CoT information.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OkVQJZWGfn"
        ],
        "venue": [
          "/venue/OkVQJZWGfn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OkVQJZWGfn"
        ],
        "detail": [
          "https://openreview.net/forum?id=OkVQJZWGfn"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Awni Altabaa , Omar Montasser , John Lafferty Learning complex functions that involve multi-step reasoning poses a significant challenge for standard supervised learning from input-output examples. Chain-of-thought (CoT) supervision, which augments training data with intermediate reasoning steps to provide a richer learning signal, has driven recent advances in large language model reasoning. This paper develops a statistical theory of learning under CoT supervision. Central to the theory is the *CoT information*, which measures the additional discriminative power offered by the chain-of-thought for distinguishing hypotheses with different end-to-end behaviors. The main theoretical results demonstrate how CoT supervision can yield significantly faster learning rates compared to standard end-to-end supervision, with both upper bounds and information-theoretic lower bounds characterized by the CoT information. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "U87XyMPrZp@OpenReview",
      "index": 253,
      "title": "Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time",
      "authors": [
        "Daniel D. Richman",
        "Jessica Karaguesian",
        "Carl-Mikael Suomivuori",
        "Ron O. Dror"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "conformational",
        "conformix",
        "conformations",
        "static",
        "diffusion",
        "proteins",
        "biomolecular",
        "unlocking",
        "interconvert",
        "endeavored"
      ],
      "summary": "The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or conformations. Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models---whether trained for static structure prediction or conformational generation---to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=U87XyMPrZp"
        ],
        "venue": [
          "/venue/U87XyMPrZp@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=U87XyMPrZp"
        ],
        "detail": [
          "https://openreview.net/forum?id=U87XyMPrZp"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time [PDF ] [Copy] [Kimi ] [REL] Authors : Daniel D. Richman , Jessica Karaguesian , Carl-Mikael Suomivuori , Ron O. Dror The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or conformations. Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models---whether trained for static structure prediction or conformational generation---to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "sEFDhxF1mG@OpenReview",
      "index": 254,
      "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
      "authors": [
        "Yutong Wang",
        "Haiyu Wang",
        "Sai Qian Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "qsvd",
        "vlm",
        "svd",
        "rank",
        "value",
        "query",
        "weight",
        "vision",
        "computational",
        "cost"
      ],
      "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning and visual question answering, but their high computational cost, driven by large memory footprints and processing time, limits their scalability and real-time applicability. In this work, we propose leveraging Singular-Value Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight matrices to reduce KV cache size and computational overhead. We in addition introduce an efficient rank allocation strategy that dynamically adjusts the SVD rank based on its impact on VLM accuracy, achieving a significant reduction in both memory usage and computational cost. Finally, we extend this approach by applying quantization to both VLM weights and activations, resulting in a highly efficient VLM. Our method outperforms previous approaches that rely solely on quantization or SVD by achieving more than 10 10 % accuracy improvement while consuming less hardware cost, making it better for real-time deployment on resource-constrained devices. We open source our code at https://github.com/SAI-Lab-NYU/QSVD.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sEFDhxF1mG"
        ],
        "venue": [
          "/venue/sEFDhxF1mG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sEFDhxF1mG"
        ],
        "detail": [
          "https://openreview.net/forum?id=sEFDhxF1mG"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Yutong Wang , Haiyu Wang , Sai Qian Zhang Vision-Language Models (VLMs) are integral to tasks such as image captioning and visual question answering, but their high computational cost, driven by large memory footprints and processing time, limits their scalability and real-time applicability. In this work, we propose leveraging Singular-Value Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight matrices to reduce KV cache size and computational overhead. We in addition introduce an efficient rank allocation strategy that dynamically adjusts the SVD rank based on its impact on VLM accuracy, achieving a significant reduction in both memory usage and computational cost. Finally, we extend this approach by applying quantization to both VLM weights and activations, resulting in a highly efficient VLM. Our method outperforms previous approaches that rely solely on quantization or SVD by achieving more than 10 10 % accuracy improvement while consuming less hardware cost, making it better for real-time deployment on resource-constrained devices. We open source our code at https://github.com/SAI-Lab-NYU/QSVD. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "m6WmeOI1AW@OpenReview",
      "index": 255,
      "title": "Caption This, Reason That: VLMs Caught in the Middle",
      "authors": [
        "Zihan Weng",
        "Lucas Gomez",
        "Taylor Whittington Webb",
        "Pouya Bashivan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "vlms",
        "vlm",
        "cognitive",
        "reasoning",
        "abilities",
        "tasks",
        "visual",
        "performance",
        "improvement",
        "caught"
      ],
      "summary": "Vision-Language Models (VLMs) have shown remarkable progress in visual understanding in recent years. Yet, they still lag behind human capabilities in specific visual tasks such as counting or relational reasoning. To understand the underlying limitations, we adopt methodologies from cognitive science, analyzing VLM performance along core cognitive axes: Perception, Attention, and Memory. Using a suite of tasks targeting these abilities, we evaluate state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct cognitive profiles: while advanced models approach ceiling performance on some tasks (e.g. category identification), a significant gap persists, particularly in tasks requiring spatial understanding or selective attention. Investigating the source of these failures and potential methods for improvement, we employ a vision-text decoupling analysis, finding that models struggling with direct visual reasoning show marked improvement when reasoning over their own generated text captions. These experiments reveal a strong need for improved VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed human performance. Furthermore, we demonstrate the potential of targeted fine-tuning on composite visual reasoning tasks and show that fine-tuning smaller VLMs moderately improves core cognitive abilities. While this improvement does not translate to large enhancements on challenging, out-of-distribution benchmarks, we show broadly that VLM performance on our datasets strongly correlates with performance on established benchmarks like MMMU-Pro and VQAv2. Our work provides a detailed analysis of VLM cognitive strengths and weaknesses and identifies key bottlenecks in simultaneous perception and reasoning while also providing an effective and simple solution.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=m6WmeOI1AW"
        ],
        "venue": [
          "/venue/m6WmeOI1AW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=m6WmeOI1AW"
        ],
        "detail": [
          "https://openreview.net/forum?id=m6WmeOI1AW"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Caption This, Reason That: VLMs Caught in the Middle [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Zihan Weng , Lucas Gomez , Taylor Whittington Webb , Pouya Bashivan Vision-Language Models (VLMs) have shown remarkable progress in visual understanding in recent years. Yet, they still lag behind human capabilities in specific visual tasks such as counting or relational reasoning. To understand the underlying limitations, we adopt methodologies from cognitive science, analyzing VLM performance along core cognitive axes: Perception, Attention, and Memory. Using a suite of tasks targeting these abilities, we evaluate state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct cognitive profiles: while advanced models approach ceiling performance on some tasks (e.g. category identification), a significant gap persists, particularly in tasks requiring spatial understanding or selective attention. Investigating the source of these failures and potential methods for improvement, we employ a vision-text decoupling analysis, finding that models struggling with direct visual reasoning show marked improvement when reasoning over their own generated text captions. These experiments reveal a strong need for improved VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed human performance. Furthermore, we demonstrate the potential of targeted fine-tuning on composite visual reasoning tasks and show that fine-tuning smaller VLMs moderately improves core cognitive abilities. While this improvement does not translate to large enhancements on challenging, out-of-distribution benchmarks, we show broadly that VLM performance on our datasets strongly correlates with performance on established benchmarks like MMMU-Pro and VQAv2. Our work provides a detailed analysis of VLM cognitive strengths and weaknesses and identifies key bottlenecks in simultaneous perception and reasoning while also providing an effective and simple solution. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TOhpnECT10@OpenReview",
      "index": 256,
      "title": "Universal Causal Inference in a Topos",
      "authors": [
        "Sridhar Mahadevan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "topos",
        "causal",
        "subobject",
        "tcm",
        "semantics",
        "universal",
        "intuitionistic",
        "define",
        "inference",
        "tcms"
      ],
      "summary": "In this paper, we explore the universal properties underlying causal inference by formulating it in terms of a topos. More concretely, we introduce topos causal models (TCMs), a strict generalization of the popular structural causal models (SCMs). A topos category has several properties that make it attractive: a general theory for how to combine local functions that define ``independent causal mechanisms\" into a consistent global function building on the theory of sheaves in a topos; a generic way to define causal interventions using a subobject classifier in a topos category; and finally, an internal logical language for causal and counterfactual reasoning that emerges from the topos itself. A striking characteristic of subobject classifiers is that they induce an intuitionistic logic, whose semantics is based on the partially ordered lattice of subobjects. We show that the underlying subobject classifier for causal inference is not Boolean in general, but forms a Heyting algebra. We define the internal Mitchell-Bénabou language, a typed local set theory, associated with causal models, and its associated Kripke-Joyal intuitionistic semantics. We prove a universal property of TCM, namely that any causal functor mapping decomposable structure to probabilistic semantics factors uniquely through a TCM representation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TOhpnECT10"
        ],
        "venue": [
          "/venue/TOhpnECT10@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TOhpnECT10"
        ],
        "detail": [
          "https://openreview.net/forum?id=TOhpnECT10"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Universal Causal Inference in a Topos [PDF ] [Copy] [Kimi ] [REL] Author : Sridhar Mahadevan In this paper, we explore the universal properties underlying causal inference by formulating it in terms of a topos. More concretely, we introduce topos causal models (TCMs), a strict generalization of the popular structural causal models (SCMs). A topos category has several properties that make it attractive: a general theory for how to combine local functions that define ``independent causal mechanisms\" into a consistent global function building on the theory of sheaves in a topos; a generic way to define causal interventions using a subobject classifier in a topos category; and finally, an internal logical language for causal and counterfactual reasoning that emerges from the topos itself. A striking characteristic of subobject classifiers is that they induce an intuitionistic logic, whose semantics is based on the partially ordered lattice of subobjects. We show that the underlying subobject classifier for causal inference is not Boolean in general, but forms a Heyting algebra. We define the internal Mitchell-Bénabou language, a typed local set theory, associated with causal models, and its associated Kripke-Joyal intuitionistic semantics. We prove a universal property of TCM, namely that any causal functor mapping decomposable structure to probabilistic semantics factors uniquely through a TCM representation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "PhIWEbewAz@OpenReview",
      "index": 257,
      "title": "Can We Infer Confidential Properties of Training Data from LLMs?",
      "authors": [
        "Pengrun Huang",
        "Chhavi Yadav",
        "Kamalika Chaudhuri",
        "Ruihan Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "llms",
        "confidential",
        "attacks",
        "chatdoctor",
        "fine",
        "property",
        "tuning",
        "propinfer",
        "attack",
        "unrecognized"
      ],
      "summary": "Large language models (LLMs) are increasingly fine-tuned on domain-specific datasets to support applications in fields such as healthcare, finance, and law. These fine-tuning datasets often have sensitive and confidential dataset-level properties — such as patient demographics or disease prevalence—that are not intended to be revealed. While prior work has studied property inference attacks on discriminative models (e.g., image classification models) and generative models (e.g., GANs for image data), it remains unclear if such attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark task for evaluating property inference in LLMs under two fine-tuning paradigms: question-answering and chat-completion. Built on the ChatDoctor dataset, our benchmark includes a range of property types and task configurations. We further propose two tailored attacks: a prompt-based generation attack and a shadow-model attack leveraging word frequency signals. Empirical evaluations across multiple pretrained LLMs show the success of our attacks, revealing a previously unrecognized vulnerability in LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PhIWEbewAz"
        ],
        "venue": [
          "/venue/PhIWEbewAz@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PhIWEbewAz"
        ],
        "detail": [
          "https://openreview.net/forum?id=PhIWEbewAz"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Can We Infer Confidential Properties of Training Data from LLMs? [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Pengrun Huang , Chhavi Yadav , Kamalika Chaudhuri , Ruihan Wu Large language models (LLMs) are increasingly fine-tuned on domain-specific datasets to support applications in fields such as healthcare, finance, and law. These fine-tuning datasets often have sensitive and confidential dataset-level properties — such as patient demographics or disease prevalence—that are not intended to be revealed. While prior work has studied property inference attacks on discriminative models (e.g., image classification models) and generative models (e.g., GANs for image data), it remains unclear if such attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark task for evaluating property inference in LLMs under two fine-tuning paradigms: question-answering and chat-completion. Built on the ChatDoctor dataset, our benchmark includes a range of property types and task configurations. We further propose two tailored attacks: a prompt-based generation attack and a shadow-model attack leveraging word frequency signals. Empirical evaluations across multiple pretrained LLMs show the success of our attacks, revealing a previously unrecognized vulnerability in LLMs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "EjkvtZwRoA@OpenReview",
      "index": 258,
      "title": "Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes",
      "authors": [
        "Itamar Harel",
        "Yonathan Wolanowsky",
        "Gal Vardi",
        "Nathan Srebro",
        "Daniel Soudry"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "training",
        "langevin",
        "theta",
        "beta",
        "generalization",
        "initialization",
        "markov",
        "gap",
        "temperature",
        "delta"
      ],
      "summary": "We analyze the generalization gap (gap between the training and test errors) when training a potentially over-parametrized model using a Markovian stochastic training algorithm, initialized from some distribution θ 0 ∼ p 0 θ 0 ∼ p 0 . We focus on Langevin dynamics with a positive temperature β − 1 β − 1 , i.e. gradient descent on a training loss L L with infinitesimal step size, perturbed with β − 1 β − 1 -variances Gaussian noise, and lightly regularized or bounded. There, we bound the generalization gap, *at any time during training*, by ( β E L ( θ 0 ) + log ( 1 / δ ) ) / N − − − − − − − − − − − − − − − − − − − √ ( β E L ( θ 0 ) + log ⁡ ( 1 / δ ) ) / N with probability 1 − δ 1 − δ over the dataset, where N N is the sample size, and E L ( θ 0 ) = O ( 1 ) E L ( θ 0 ) = O ( 1 ) with standard initialization scaling. In contrast to previous guarantees, we have no dependence on either training time or reliance on mixing, nor a dependence on dimensionality, gradient norms, or any other properties of the loss or model. This guarantee follows from a general analysis of any Markov process-based training that has a Gibbs-style stationary distribution. The proof is surprisingly simple, once we observe that the marginal distribution divergence from initialization remains bounded, as implied by a generalized second law of thermodynamics.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EjkvtZwRoA"
        ],
        "venue": [
          "/venue/EjkvtZwRoA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EjkvtZwRoA"
        ],
        "detail": [
          "https://openreview.net/forum?id=EjkvtZwRoA"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes [PDF ] [Copy] [Kimi ] [REL] Authors : Itamar Harel , Yonathan Wolanowsky , Gal Vardi , Nathan Srebro , Daniel Soudry We analyze the generalization gap (gap between the training and test errors) when training a potentially over-parametrized model using a Markovian stochastic training algorithm, initialized from some distribution θ 0 ∼ p 0 θ 0 ∼ p 0 . We focus on Langevin dynamics with a positive temperature β − 1 β − 1 , i.e. gradient descent on a training loss L L with infinitesimal step size, perturbed with β − 1 β − 1 -variances Gaussian noise, and lightly regularized or bounded. There, we bound the generalization gap, *at any time during training*, by ( β E L ( θ 0 ) + log ( 1 / δ ) ) / N − − − − − − − − − − − − − − − − − − − √ ( β E L ( θ 0 ) + log ⁡ ( 1 / δ ) ) / N with probability 1 − δ 1 − δ over the dataset, where N N is the sample size, and E L ( θ 0 ) = O ( 1 ) E L ( θ 0 ) = O ( 1 ) with standard initialization scaling. In contrast to previous guarantees, we have no dependence on either training time or reliance on mixing, nor a dependence on dimensionality, gradient norms, or any other properties of the loss or model. This guarantee follows from a general analysis of any Markov process-based training that has a Gibbs-style stationary distribution. The proof is surprisingly simple, once we observe that the marginal distribution divergence from initialization remains bounded, as implied by a generalized second law of thermodynamics. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oBikm5Rshc@OpenReview",
      "index": 259,
      "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models",
      "authors": [
        "Chantal Shaib",
        "Vinith Menon Suriyakumar",
        "Byron C Wallace",
        "Marzyeh Ghassemi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "syntactic",
        "olmo",
        "domain",
        "spurious",
        "correlations",
        "syntax",
        "instruction",
        "semantics",
        "flanv2",
        "templates"
      ],
      "summary": "For an LLM to correctly respond to an instruction it must understand both the semantics and the domain (i.e., subject area) of a given task-instruction pair. However, syntax can also convey implicit information. Recent work shows that \\textit{syntactic templates}---frequent sequences of Part-of-Speech (PoS) tags---are prevalent in training data and often appear in model outputs. In this work we characterize syntactic templates, domain, and semantics in task-instruction pairs. We identify cases of spurious correlations between syntax and domain, where models learn to associate a domain with syntax during training; this can sometimes override prompt semantics. Using a synthetic training dataset, we find that the syntactic-domain correlation can lower performance (mean 0.51 +/- 0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an evaluation framework to detect this phenomenon in trained models, and show that it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B; Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study on the implications for LLM security, showing that unintended syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test for syntactic-domain correlations, and (2) to ensure \\textit{syntactic} diversity in training data, specifically within domains, to prevent such spurious correlations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oBikm5Rshc"
        ],
        "venue": [
          "/venue/oBikm5Rshc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oBikm5Rshc"
        ],
        "detail": [
          "https://openreview.net/forum?id=oBikm5Rshc"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models [PDF ] [Copy] [Kimi ] [REL] Authors : Chantal Shaib , Vinith Menon Suriyakumar , Byron C Wallace , Marzyeh Ghassemi For an LLM to correctly respond to an instruction it must understand both the semantics and the domain (i.e., subject area) of a given task-instruction pair. However, syntax can also convey implicit information. Recent work shows that \\textit{syntactic templates}---frequent sequences of Part-of-Speech (PoS) tags---are prevalent in training data and often appear in model outputs. In this work we characterize syntactic templates, domain, and semantics in task-instruction pairs. We identify cases of spurious correlations between syntax and domain, where models learn to associate a domain with syntax during training; this can sometimes override prompt semantics. Using a synthetic training dataset, we find that the syntactic-domain correlation can lower performance (mean 0.51 +/- 0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an evaluation framework to detect this phenomenon in trained models, and show that it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B; Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study on the implications for LLM security, showing that unintended syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test for syntactic-domain correlations, and (2) to ensure \\textit{syntactic} diversity in training data, specifically within domains, to prevent such spurious correlations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Sf5nxMRiG7@OpenReview",
      "index": 260,
      "title": "Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems",
      "authors": [
        "Elfarouk Harb",
        "Yousef Yassin",
        "Chandra Chekuri"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "supermodular",
        "submodular",
        "fujishige",
        "dss",
        "usss",
        "udss",
        "densest",
        "wolfe",
        "supergreedy",
        "problems"
      ],
      "summary": "We consider the following question: given a submodular or supermodular set function f : 2 V → R f : 2 V → R , how should one minimize or maximize its average value f ( S ) / | S | f ( S ) / | S | over non-empty subsets S ⊆ V S ⊆ V ? This problem generalizes several well-known objectives including Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular Function Minimization (SFM). Motivated by recent applications [39, 31], we formalize two new broad problems: the Unrestricted Sparsest Submodular Set (USSS) and Unrestricted Densest Supermodular Set (UDSS) which allow negative and non-monotone functions. Using classical results we observe that DSS, SFM, USSS, UDSS, and MNP are all equivalent under strongly polynomial-time reductions. This equivalence enables algorithmic cross-over: methods designed for one problem can be repurposed to solve others efficiently. In particular we use the perspective of the minimum norm point in the base polyhedron of a sub/supermodular function which, via Fujishige's results, yields the dense decomposition as a byproduct. Via this perspective we show that a recent converging heuristic for DSS, \\textsc{SuperGreedy++} [15, 29], and Wolfe’s minimum norm point algorithm are both universal solvers for all of these problems. On the theoretical front, we explain the observation made in recent work [39, 31] that \\textsc{SuperGreedy++} appears to work well even in settings beyond DSS. Surprisingly, we also show that this simple algorithm can be used for Submodular Function Minimization, including for example that it can act as an effective minimum s t s t cut algorithm. On the empirical front, we explore the utility of several different algorithms including Fujishige-Wolfe min-norm point algorithm for recent problems. We conduct over 400 experiments across seven problem types and large-scale synthetic and real-world datasets (up to ≈ 100 ≈ 100 million edges). Our results reveal that methods historically considered inefficient, such as convex-programming methods, flow-based solvers, and Fujishige-Wolfe’s algorithm, outperform state-of-the-art task-specific baselines by orders of magnitude on concrete problems like HNSN [39]. These findings challenge prevailing assumptions and demonstrate that with the right framing, general optimization algorithms can be both scalable and state-of-the-art for supermodular and submodular ratio problems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Sf5nxMRiG7"
        ],
        "venue": [
          "/venue/Sf5nxMRiG7@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Sf5nxMRiG7"
        ],
        "detail": [
          "https://openreview.net/forum?id=Sf5nxMRiG7"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems [PDF ] [Copy] [Kimi ] [REL] Authors : Elfarouk Harb , Yousef Yassin , Chandra Chekuri We consider the following question: given a submodular or supermodular set function f : 2 V → R f : 2 V → R , how should one minimize or maximize its average value f ( S ) / | S | f ( S ) / | S | over non-empty subsets S ⊆ V S ⊆ V ? This problem generalizes several well-known objectives including Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular Function Minimization (SFM). Motivated by recent applications [39, 31], we formalize two new broad problems: the Unrestricted Sparsest Submodular Set (USSS) and Unrestricted Densest Supermodular Set (UDSS) which allow negative and non-monotone functions. Using classical results we observe that DSS, SFM, USSS, UDSS, and MNP are all equivalent under strongly polynomial-time reductions. This equivalence enables algorithmic cross-over: methods designed for one problem can be repurposed to solve others efficiently. In particular we use the perspective of the minimum norm point in the base polyhedron of a sub/supermodular function which, via Fujishige's results, yields the dense decomposition as a byproduct. Via this perspective we show that a recent converging heuristic for DSS, \\textsc{SuperGreedy++} [15, 29], and Wolfe’s minimum norm point algorithm are both universal solvers for all of these problems. On the theoretical front, we explain the observation made in recent work [39, 31] that \\textsc{SuperGreedy++} appears to work well even in settings beyond DSS. Surprisingly, we also show that this simple algorithm can be used for Submodular Function Minimization, including for example that it can act as an effective minimum s t s t cut algorithm. On the empirical front, we explore the utility of several different algorithms including Fujishige-Wolfe min-norm point algorithm for recent problems. We conduct over 400 experiments across seven problem types and large-scale synthetic and real-world datasets (up to ≈ 100 ≈ 100 million edges). Our results reveal that methods historically considered inefficient, such as convex-programming methods, flow-based solvers, and Fujishige-Wolfe’s algorithm, outperform state-of-the-art task-specific baselines by orders of magnitude on concrete problems like HNSN [39]. These findings challenge prevailing assumptions and demonstrate that with the right framing, general optimization algorithms can be both scalable and state-of-the-art for supermodular and submodular ratio problems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RdNYp8ilPr@OpenReview",
      "index": 261,
      "title": "Private Set Union with Multiple Contributions",
      "authors": [
        "Travis Dick",
        "Haim Kaplan",
        "Alex Kulesza",
        "Uri Stemmer",
        "Ziteng Sun",
        "Ananda Theertha Suresh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "utility",
        "private",
        "union",
        "items",
        "bags",
        "histogram",
        "differentially",
        "degrades",
        "algorithm",
        "algorithms"
      ],
      "summary": "In the private set union problem each user owns a bag of at most k k items (from some large universe of items), and we are interested in computing the union of the items in the bags of all of the users. This is trivial without privacy, but a differentially private algorithm must be careful about reporting items contained in only a small number of bags. We consider differentially private algorithms that always report a subset of the union, and define the utility of an algorithm to be the expected size of the subset that it reports. Because the achievable utility varies significantly with the dataset, we introduce the *utility ratio*, which normalizes utility by a dataset-specific upper bound and characterizes a mechanism by its lowest normalized utility across all datasets. We then develop algorithms with guaranteed utility ratios and complement them with bounds on the best possible utility ratio. Prior work has shown that a single algorithm can be simultaneously optimal for all datasets when k = 1 k = 1 , but we show that instance-optimal algorithms do not exist when k > 1 k > 1 , and characterize how performance degrades as k k grows. At the same time, we design a private algorithm that achieves the maximum possible utility, regardless of k k , when the item histogram matches a prior prediction (for instance, from a previous data release) and degrades gracefully with the L ∞ L ∞ distance between the prediction and the actual histogram when the prediction is imperfect.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RdNYp8ilPr"
        ],
        "venue": [
          "/venue/RdNYp8ilPr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RdNYp8ilPr"
        ],
        "detail": [
          "https://openreview.net/forum?id=RdNYp8ilPr"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Private Set Union with Multiple Contributions [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Travis Dick , Haim Kaplan , Alex Kulesza , Uri Stemmer , Ziteng Sun , Ananda Theertha Suresh In the private set union problem each user owns a bag of at most k k items (from some large universe of items), and we are interested in computing the union of the items in the bags of all of the users. This is trivial without privacy, but a differentially private algorithm must be careful about reporting items contained in only a small number of bags. We consider differentially private algorithms that always report a subset of the union, and define the utility of an algorithm to be the expected size of the subset that it reports. Because the achievable utility varies significantly with the dataset, we introduce the *utility ratio*, which normalizes utility by a dataset-specific upper bound and characterizes a mechanism by its lowest normalized utility across all datasets. We then develop algorithms with guaranteed utility ratios and complement them with bounds on the best possible utility ratio. Prior work has shown that a single algorithm can be simultaneously optimal for all datasets when k = 1 k = 1 , but we show that instance-optimal algorithms do not exist when k > 1 k > 1 , and characterize how performance degrades as k k grows. At the same time, we design a private algorithm that achieves the maximum possible utility, regardless of k k , when the item histogram matches a prior prediction (for instance, from a previous data release) and degrades gracefully with the L ∞ L ∞ distance between the prediction and the actual histogram when the prediction is imperfect. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "96I0XnrjkQ@OpenReview",
      "index": 262,
      "title": "Clustering via Hedonic Games: New Concepts and Algorithms",
      "authors": [
        "Gergely Csáji",
        "Alexander Gundert",
        "Jörg Rothe",
        "Ildikó Schlotter"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "clustering",
        "hedonic",
        "games",
        "friendship",
        "concepts",
        "coalition",
        "enmity",
        "algorithms",
        "enemy",
        "locally"
      ],
      "summary": "We study fundamental connections between coalition formation games and clustering, illustrating the cross-disciplinary relevance of these concepts. We focus on graphical hedonic games where agents' preferences are compactly represented by a friendship graph and an enemy graph. In the context of clustering, friendship relations naturally align with data point similarities, whereas enmity corresponds to dissimilarities. We consider two stability notions based on single-agent deviations: local popularity and local stability. Exploring these concepts from an algorithmic viewpoint, we design efficient mechanisms for finding locally stable or locally popular partitions. Besides gaining theoretical insight into the computational complexity of these problems, we perform simulations that demonstrate how our algorithms can be successfully applied in clustering and community detection. Our findings highlight the interplay between coalition formation games and data-driven clustering techniques, offering fresh perspectives and applications in both areas.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=96I0XnrjkQ"
        ],
        "venue": [
          "/venue/96I0XnrjkQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=96I0XnrjkQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=96I0XnrjkQ"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Clustering via Hedonic Games: New Concepts and Algorithms [PDF ] [Copy] [Kimi ] [REL] Authors : Gergely Csáji , Alexander Gundert , Jörg Rothe , Ildikó Schlotter We study fundamental connections between coalition formation games and clustering, illustrating the cross-disciplinary relevance of these concepts. We focus on graphical hedonic games where agents' preferences are compactly represented by a friendship graph and an enemy graph. In the context of clustering, friendship relations naturally align with data point similarities, whereas enmity corresponds to dissimilarities. We consider two stability notions based on single-agent deviations: local popularity and local stability. Exploring these concepts from an algorithmic viewpoint, we design efficient mechanisms for finding locally stable or locally popular partitions. Besides gaining theoretical insight into the computational complexity of these problems, we perform simulations that demonstrate how our algorithms can be successfully applied in clustering and community detection. Our findings highlight the interplay between coalition formation games and data-driven clustering techniques, offering fresh perspectives and applications in both areas. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "EEZLBhyer1@OpenReview",
      "index": 263,
      "title": "The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis",
      "authors": [
        "Hoang Pham",
        "The-Anh Ta",
        "Tom Jacobs",
        "Rebekka Burkholz",
        "Long Tran-Thanh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "graphon",
        "sparse",
        "pruning",
        "ntk",
        "width",
        "networks",
        "graphons",
        "neural",
        "limit",
        "infinite"
      ],
      "summary": "Sparse neural networks promise efficiency, yet training them effectively remains a fundamental challenge. Despite advances in pruning methods that create sparse architectures, understanding why some sparse structures are better trainable than others with the same level of sparsity remains poorly understood. Aiming to develop a systematic approach to this fundamental problem, we propose a novel theoretical framework based on the theory of graph limits, particularly graphons, that characterizes sparse neural networks in the infinite-width regime. Our key insight is that connectivity patterns of sparse neural networks induced by pruning methods converge to specific graphons as networks' width tends to infinity, which encodes implicit structural biases of different pruning methods. We postulate the *Graphon Limit Hypothesis* and provide empirical evidence to support it. Leveraging this graphon representation, we derive a *Graphon Neural Tangent Kernel (Graphon NTK)* to study the training dynamics of sparse networks in the infinite width limit. Graphon NTK provides a general framework for the theoretical analysis of sparse networks. We empirically show that the spectral analysis of Graphon NTK correlates with observed training dynamics of sparse networks, explaining the varying convergence behaviours of different pruning methods. Our framework provides theoretical insights into the impact of connectivity patterns on the trainability of various sparse network architectures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EEZLBhyer1"
        ],
        "venue": [
          "/venue/EEZLBhyer1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EEZLBhyer1"
        ],
        "detail": [
          "https://openreview.net/forum?id=EEZLBhyer1"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Hoang Pham , The-Anh Ta , Tom Jacobs , Rebekka Burkholz , Long Tran-Thanh Sparse neural networks promise efficiency, yet training them effectively remains a fundamental challenge. Despite advances in pruning methods that create sparse architectures, understanding why some sparse structures are better trainable than others with the same level of sparsity remains poorly understood. Aiming to develop a systematic approach to this fundamental problem, we propose a novel theoretical framework based on the theory of graph limits, particularly graphons, that characterizes sparse neural networks in the infinite-width regime. Our key insight is that connectivity patterns of sparse neural networks induced by pruning methods converge to specific graphons as networks' width tends to infinity, which encodes implicit structural biases of different pruning methods. We postulate the *Graphon Limit Hypothesis* and provide empirical evidence to support it. Leveraging this graphon representation, we derive a *Graphon Neural Tangent Kernel (Graphon NTK)* to study the training dynamics of sparse networks in the infinite width limit. Graphon NTK provides a general framework for the theoretical analysis of sparse networks. We empirically show that the spectral analysis of Graphon NTK correlates with observed training dynamics of sparse networks, explaining the varying convergence behaviours of different pruning methods. Our framework provides theoretical insights into the impact of connectivity patterns on the trainability of various sparse network architectures. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "a7hHwWnZey@OpenReview",
      "index": 264,
      "title": "Fast Training of Large Kernel Models with Delayed Projections",
      "authors": [
        "Amirhesam Abedsoltan",
        "Siyuan Ma",
        "Parthe Pandit",
        "Mikhail Belkin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "eigenpro",
        "kernel",
        "delayed",
        "projections",
        "ep4",
        "machines",
        "psgd",
        "training",
        "historically",
        "datasets"
      ],
      "summary": "Classical kernel machines have historically faced significant challenges in scaling to large datasets and model sizes—a key ingredient that has driven the success of neural networks. In this paper, we present a new methodology for building kernel machines that can scale efficiently with both data size and model size. Our algorithm introduces delayed projections to Preconditioned Stochastic Gradient Descent (PSGD) allowing the training of much larger models than was previously feasible. We validate our algorithm, \\EP4, across multiple datasets, demonstrating drastic training speedups without compromising the performance. Our implementation is publicly available at: https://github.com/EigenPro/EigenPro .",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=a7hHwWnZey"
        ],
        "venue": [
          "/venue/a7hHwWnZey@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=a7hHwWnZey"
        ],
        "detail": [
          "https://openreview.net/forum?id=a7hHwWnZey"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Fast Training of Large Kernel Models with Delayed Projections [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Amirhesam Abedsoltan , Siyuan Ma , Parthe Pandit , Mikhail Belkin Classical kernel machines have historically faced significant challenges in scaling to large datasets and model sizes—a key ingredient that has driven the success of neural networks. In this paper, we present a new methodology for building kernel machines that can scale efficiently with both data size and model size. Our algorithm introduces delayed projections to Preconditioned Stochastic Gradient Descent (PSGD) allowing the training of much larger models than was previously feasible. We validate our algorithm, \\EP4, across multiple datasets, demonstrating drastic training speedups without compromising the performance. Our implementation is publicly available at: https://github.com/EigenPro/EigenPro . Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7ZVRlBFuEv@OpenReview",
      "index": 265,
      "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
      "authors": [
        "Siyan Zhao",
        "Devaansh Gupta",
        "Qinqing Zheng",
        "Aditya Grover"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dllms",
        "reasoning",
        "masked",
        "language",
        "sft",
        "grpo",
        "dllm",
        "diffusion",
        "diffu",
        "reinforcement"
      ],
      "summary": "Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7ZVRlBFuEv"
        ],
        "venue": [
          "/venue/7ZVRlBFuEv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7ZVRlBFuEv"
        ],
        "detail": [
          "https://openreview.net/forum?id=7ZVRlBFuEv"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Siyan Zhao , Devaansh Gupta , Qinqing Zheng , Aditya Grover Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "d6UV0UNgn9@OpenReview",
      "index": 266,
      "title": "Affine-Invariant Global Non-Asymptotic Convergence Analysis of BFGS under Self-Concordance",
      "authors": [
        "Qiujiang Jin",
        "Aryan Mokhtari"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bfgs",
        "convergence",
        "affine",
        "global",
        "guarantees",
        "concordant",
        "invariant",
        "asymptotic",
        "self",
        "hessian"
      ],
      "summary": "In this paper, we establish global non-asymptotic convergence guarantees for the BFGS quasi-Newton method without requiring strong convexity or the Lipschitz continuity of the gradient or Hessian. Instead, we consider the setting where the objective function is strictly convex and strongly self-concordant. For an arbitrary initial point and any arbitrary positive-definite initial Hessian approximation, we prove global linear and superlinear convergence guarantees for BFGS when the step size is determined using a line search scheme satisfying the weak Wolfe conditions. Moreover, all our global guarantees are affine-invariant, with the convergence rates depending solely on the initial error and the strongly self-concordant constant. Our results extend the global non-asymptotic convergence theory of BFGS beyond traditional assumptions and, for the first time, establish affine-invariant convergence guarantees—aligning with the inherent affine invariance of the BFGS method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=d6UV0UNgn9"
        ],
        "venue": [
          "/venue/d6UV0UNgn9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=d6UV0UNgn9"
        ],
        "detail": [
          "https://openreview.net/forum?id=d6UV0UNgn9"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Affine-Invariant Global Non-Asymptotic Convergence Analysis of BFGS under Self-Concordance [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Qiujiang Jin , Aryan Mokhtari In this paper, we establish global non-asymptotic convergence guarantees for the BFGS quasi-Newton method without requiring strong convexity or the Lipschitz continuity of the gradient or Hessian. Instead, we consider the setting where the objective function is strictly convex and strongly self-concordant. For an arbitrary initial point and any arbitrary positive-definite initial Hessian approximation, we prove global linear and superlinear convergence guarantees for BFGS when the step size is determined using a line search scheme satisfying the weak Wolfe conditions. Moreover, all our global guarantees are affine-invariant, with the convergence rates depending solely on the initial error and the strongly self-concordant constant. Our results extend the global non-asymptotic convergence theory of BFGS beyond traditional assumptions and, for the first time, establish affine-invariant convergence guarantees—aligning with the inherent affine invariance of the BFGS method. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "aLUAzLDIOc@OpenReview",
      "index": 267,
      "title": "Extrapolation by Association: Length Generalization Transfer In Transformers",
      "authors": [
        "Ziyang Cai",
        "Nayoung Lee",
        "Avi Schwarzschild",
        "Samet Oymak",
        "Dimitris Papailiopoulos"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "generalization",
        "length",
        "transfer",
        "tasks",
        "extrapolation",
        "transformers",
        "longer",
        "inputs",
        "language",
        "textit"
      ],
      "summary": "Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization—the ability to extrapolate from shorter to longer inputs—through the lens of \\textit{task transfer}. We find that length generalization can be \\textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead the model to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across a diverse suite of algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aLUAzLDIOc"
        ],
        "venue": [
          "/venue/aLUAzLDIOc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aLUAzLDIOc"
        ],
        "detail": [
          "https://openreview.net/forum?id=aLUAzLDIOc"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Extrapolation by Association: Length Generalization Transfer In Transformers [PDF ] [Copy] [Kimi ] [REL] Authors : Ziyang Cai , Nayoung Lee , Avi Schwarzschild , Samet Oymak , Dimitris Papailiopoulos Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization—the ability to extrapolate from shorter to longer inputs—through the lens of \\textit{task transfer}. We find that length generalization can be \\textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead the model to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across a diverse suite of algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZAKpELpclI@OpenReview",
      "index": 268,
      "title": "Optimal Neural Compressors for the Rate-Distortion-Perception Tradeoff",
      "authors": [
        "Eric Lei",
        "Hamed Hassani",
        "Shirin Saeedi Bidokhti"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rdp",
        "compressors",
        "randomness",
        "shared",
        "tradeoff",
        "distortion",
        "perception",
        "neural",
        "lattice",
        "packing"
      ],
      "summary": "Recent efforts in neural compression have focused on the rate-distortion-perception (RDP) tradeoff, where the perception constraint ensures the source and reconstruction distributions are close in terms of a statistical divergence. Theoretical work on RDP describes properties of RDP-optimal compressors without providing constructive and low complexity solutions. While classical rate-distortion theory shows that optimal compressors should efficiently pack space, RDP theory additionally shows that infinite randomness shared between the encoder and decoder may be necessary for RDP optimality. In this paper, we propose neural compressors that are low complexity and benefit from high packing efficiency through lattice coding and shared randomness through shared dithering over the lattice cells. For two important settings, namely infinite shared and zero shared randomness, we analyze the RDP tradeoff achieved by our proposed neural compressors and show optimality in both cases. Experimentally, we investigate the roles that these two components of our design, lattice coding and randomness, play in the performance of neural compressors on synthetic and real-world data. We observe that performance improves with more shared randomness and better lattice packing.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZAKpELpclI"
        ],
        "venue": [
          "/venue/ZAKpELpclI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZAKpELpclI"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZAKpELpclI"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Optimal Neural Compressors for the Rate-Distortion-Perception Tradeoff [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Eric Lei , Hamed Hassani , Shirin Saeedi Bidokhti Recent efforts in neural compression have focused on the rate-distortion-perception (RDP) tradeoff, where the perception constraint ensures the source and reconstruction distributions are close in terms of a statistical divergence. Theoretical work on RDP describes properties of RDP-optimal compressors without providing constructive and low complexity solutions. While classical rate-distortion theory shows that optimal compressors should efficiently pack space, RDP theory additionally shows that infinite randomness shared between the encoder and decoder may be necessary for RDP optimality. In this paper, we propose neural compressors that are low complexity and benefit from high packing efficiency through lattice coding and shared randomness through shared dithering over the lattice cells. For two important settings, namely infinite shared and zero shared randomness, we analyze the RDP tradeoff achieved by our proposed neural compressors and show optimality in both cases. Experimentally, we investigate the roles that these two components of our design, lattice coding and randomness, play in the performance of neural compressors on synthetic and real-world data. We observe that performance improves with more shared randomness and better lattice packing. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xDxskDUvte@OpenReview",
      "index": 269,
      "title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness",
      "authors": [
        "Longwei Wang",
        "Ifrat Ikhtear Uddin",
        "KC Santosh",
        "Chaowei Zhang",
        "Xiao Qin",
        "Yang Zhou"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "adversarial",
        "robustness",
        "equivariant",
        "cifar",
        "symmetry",
        "equivariance",
        "10c",
        "bridging",
        "layers",
        "attacks"
      ],
      "summary": "Adversarial examples reveal critical vulnerabilities in deep neural networks by exploiting their sensitivity to imperceptible input perturbations. While adversarial training remains the predominant defense strategy, it often incurs significant computational cost and may compromise clean-data accuracy. In this work, we investigate an architectural approach to adversarial robustness by embedding group-equivariant convolutions—specifically, rotation- and scale-equivariant layers—into standard convolutional neural networks (CNNs). These layers encode symmetry priors that align model behavior with structured transformations in the input space, promoting smoother decision boundaries and greater resilience to adversarial attacks. We propose and evaluate two symmetry-aware architectures: a parallel design that processes standard and equivariant features independently before fusion, and a cascaded design that applies equivariant operations sequentially. Theoretically, we demonstrate that such models reduce hypothesis space complexity, regularize gradients, and yield tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness) framework. Empirically, our models consistently improve adversarial robustness and generalization across CIFAR-10, CIFAR-100, and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial training. These findings underscore the potential of symmetry-enforcing architectures as efficient and principled alternatives to data augmentation-based defenses.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xDxskDUvte"
        ],
        "venue": [
          "/venue/xDxskDUvte@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xDxskDUvte"
        ],
        "detail": [
          "https://openreview.net/forum?id=xDxskDUvte"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Longwei Wang , Ifrat Ikhtear Uddin , KC Santosh , Chaowei Zhang , Xiao Qin , Yang Zhou Adversarial examples reveal critical vulnerabilities in deep neural networks by exploiting their sensitivity to imperceptible input perturbations. While adversarial training remains the predominant defense strategy, it often incurs significant computational cost and may compromise clean-data accuracy. In this work, we investigate an architectural approach to adversarial robustness by embedding group-equivariant convolutions—specifically, rotation- and scale-equivariant layers—into standard convolutional neural networks (CNNs). These layers encode symmetry priors that align model behavior with structured transformations in the input space, promoting smoother decision boundaries and greater resilience to adversarial attacks. We propose and evaluate two symmetry-aware architectures: a parallel design that processes standard and equivariant features independently before fusion, and a cascaded design that applies equivariant operations sequentially. Theoretically, we demonstrate that such models reduce hypothesis space complexity, regularize gradients, and yield tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness) framework. Empirically, our models consistently improve adversarial robustness and generalization across CIFAR-10, CIFAR-100, and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial training. These findings underscore the potential of symmetry-enforcing architectures as efficient and principled alternatives to data augmentation-based defenses. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "r1Bx58M6It@OpenReview",
      "index": 270,
      "title": "Color Conditional Generation with Sliced Wasserstein Guidance",
      "authors": [
        "Alexander Lobashev",
        "Maria Larchenko",
        "Dmitry Guskov"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "color",
        "sliced",
        "guidance",
        "reference",
        "image",
        "colors",
        "wasserstein",
        "alobashev",
        "generation",
        "conditional"
      ],
      "summary": "We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1-Wasserstein distance between the color distribution of the generated image and the reference palette. Our method outperforms state-of-the-art techniques for color-conditional generation in terms of color similarity to the reference, producing images that not only match the reference colors but also maintain semantic coherence with the original text prompt. Our source code is available at https://github.com/alobashev/sw-guidance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=r1Bx58M6It"
        ],
        "venue": [
          "/venue/r1Bx58M6It@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=r1Bx58M6It"
        ],
        "detail": [
          "https://openreview.net/forum?id=r1Bx58M6It"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Color Conditional Generation with Sliced Wasserstein Guidance [PDF ] [Copy] [Kimi ] [REL] Authors : Alexander Lobashev , Maria Larchenko , Dmitry Guskov We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1-Wasserstein distance between the color distribution of the generated image and the reference palette. Our method outperforms state-of-the-art techniques for color-conditional generation in terms of color similarity to the reference, producing images that not only match the reference colors but also maintain semantic coherence with the original text prompt. Our source code is available at https://github.com/alobashev/sw-guidance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "zjMd3yfyWv@OpenReview",
      "index": 271,
      "title": "Private Hyperparameter Tuning with Ex-Post Guarantee",
      "authors": [
        "Badih Ghazi",
        "Pritish Kamath",
        "Alexander Knop",
        "Ravi Kumar",
        "Pasin Manurangsi",
        "Chiyuan Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "privacy",
        "utility",
        "private",
        "2019",
        "laplace",
        "hyperparameter",
        "post",
        "tuning",
        "cost",
        "whitehouse"
      ],
      "summary": "The conventional approach in differential privacy (DP) literature formulates the privacy-utility tradeoff with a \"privacy-first\" perspective: for a predetermined level of privacy, a certain utility is achievable. However, practitioners often operate under a \"utility-first\" paradigm, prioritizing a desired level of utility and then determining the corresponding privacy cost. Wu et al. [2019] initiated a formal study of this ``utility-first'' perspective by introducing ex-post DP. They demonstrated that by adding correlated Laplace noise and progressively reducing it on demand, a sequence of increasingly accurate estimates of a private parameter can be generated, with the privacy cost attributed only to the least noisy iterate released. This led to a Laplace mechanism variant that achieves a specified utility with minimal privacy loss. However, their work, and similar findings by Whitehouse et al. [2023], are primarily limited to simple mechanisms based on Laplace or Gaussian noise. In this paper, we significantly generalize these results. In particular, we extend the findings of Wu et al. [2019] and Liu and Talwar [2019] to support any sequence of private estimators, incurring at most a doubling of the original privacy budget. Furthermore, we demonstrate that hyperparameter tuning for these estimators, including the selection of an optimal privacy budget, can be performed without additional privacy cost. Finally, we extend our results to ex-post Rényi DP, further broadening the applicability of utility-first privacy mechanisms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zjMd3yfyWv"
        ],
        "venue": [
          "/venue/zjMd3yfyWv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zjMd3yfyWv"
        ],
        "detail": [
          "https://openreview.net/forum?id=zjMd3yfyWv"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Private Hyperparameter Tuning with Ex-Post Guarantee [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Badih Ghazi , Pritish Kamath , Alexander Knop , Ravi Kumar , Pasin Manurangsi , Chiyuan Zhang The conventional approach in differential privacy (DP) literature formulates the privacy-utility tradeoff with a \"privacy-first\" perspective: for a predetermined level of privacy, a certain utility is achievable. However, practitioners often operate under a \"utility-first\" paradigm, prioritizing a desired level of utility and then determining the corresponding privacy cost. Wu et al. [2019] initiated a formal study of this ``utility-first'' perspective by introducing ex-post DP. They demonstrated that by adding correlated Laplace noise and progressively reducing it on demand, a sequence of increasingly accurate estimates of a private parameter can be generated, with the privacy cost attributed only to the least noisy iterate released. This led to a Laplace mechanism variant that achieves a specified utility with minimal privacy loss. However, their work, and similar findings by Whitehouse et al. [2023], are primarily limited to simple mechanisms based on Laplace or Gaussian noise. In this paper, we significantly generalize these results. In particular, we extend the findings of Wu et al. [2019] and Liu and Talwar [2019] to support any sequence of private estimators, incurring at most a doubling of the original privacy budget. Furthermore, we demonstrate that hyperparameter tuning for these estimators, including the selection of an optimal privacy budget, can be performed without additional privacy cost. Finally, we extend our results to ex-post Rényi DP, further broadening the applicability of utility-first privacy mechanisms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "M0U8wUow8c@OpenReview",
      "index": 272,
      "title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning",
      "authors": [
        "Guan Zhe Hong",
        "Nishanth Dikkala",
        "Enming Luo",
        "Cyrus Rashtchian",
        "Xin Wang",
        "Rina Panigrahy"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "gemma",
        "27b",
        "llms",
        "propositional",
        "mistral",
        "circuit",
        "uncover",
        "problem",
        "components"
      ],
      "summary": "Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M0U8wUow8c"
        ],
        "venue": [
          "/venue/M0U8wUow8c@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M0U8wUow8c"
        ],
        "detail": [
          "https://openreview.net/forum?id=M0U8wUow8c"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning [PDF ] [Copy] [Kimi ] [REL] Authors : Guan Zhe Hong , Nishanth Dikkala , Enming Luo , Cyrus Rashtchian , Xin Wang , Rina Panigrahy Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SgQAleMecy@OpenReview",
      "index": 273,
      "title": "Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling",
      "authors": [
        "Yufan Li",
        "Pragya Sur"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "platt",
        "hat",
        "calibration",
        "calibrated",
        "bregman",
        "predictor",
        "provably",
        "angle",
        "star",
        "true"
      ],
      "summary": "We study the fundamental problem of calibrating a linear binary classifier of the form σ ( w ^ ⊤ x ) σ ( w ^ ⊤ x ) , where the feature vector x x is Gaussian, σ σ is a link function, and w ^ w ^ is an estimator of the true linear weight w ⋆ w ⋆ . By interpolating with a noninformative \\emph{chance classifier}, we construct a well-calibrated predictor whose interpolation weight depends on the angle ∠ ( w ^ , w ⋆ ) ∠ ( w ^ , w ⋆ ) between the estimator w ^ w ^ and the true linear weight w ⋆ w ⋆ . We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle ∠ ( w ^ , w ⋆ ) ∠ ( w ^ , w ⋆ ) can be consistently estimated. Furthermore, the resulting predictor is uniquely \\emph{Bregman-optimal}, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SgQAleMecy"
        ],
        "venue": [
          "/venue/SgQAleMecy@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SgQAleMecy"
        ],
        "detail": [
          "https://openreview.net/forum?id=SgQAleMecy"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling [PDF ] [Copy] [Kimi ] [REL] Authors : Yufan Li , Pragya Sur We study the fundamental problem of calibrating a linear binary classifier of the form σ ( w ^ ⊤ x ) σ ( w ^ ⊤ x ) , where the feature vector x x is Gaussian, σ σ is a link function, and w ^ w ^ is an estimator of the true linear weight w ⋆ w ⋆ . By interpolating with a noninformative \\emph{chance classifier}, we construct a well-calibrated predictor whose interpolation weight depends on the angle ∠ ( w ^ , w ⋆ ) ∠ ( w ^ , w ⋆ ) between the estimator w ^ w ^ and the true linear weight w ⋆ w ⋆ . We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle ∠ ( w ^ , w ⋆ ) ∠ ( w ^ , w ⋆ ) can be consistently estimated. Furthermore, the resulting predictor is uniquely \\emph{Bregman-optimal}, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0vJJdEiXOb@OpenReview",
      "index": 274,
      "title": "Reconstruction and Secrecy under Approximate Distance Queries",
      "authors": [
        "Shay Moran",
        "Elizaveta Nesterova"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reconstruction",
        "queries",
        "reconstructor",
        "spaces",
        "distance",
        "approximate",
        "secrecy",
        "error",
        "subsets",
        "responder"
      ],
      "summary": "Consider the task of locating an unknown target point using approximate distance queries: in each round, a reconstructor selects a reference point and receives a noisy version of its distance to the target. This problem arises naturally in various contexts—from localization in GPS and sensor networks to privacy-aware data access—making it relevant from the perspective of both the reconstructor (seeking accurate recovery) and the responder (aiming to limit information disclosure, e.g., for privacy or security reasons). We study this reconstruction game through a learning-theoretic lens, focusing on the rate and limits of the best possible reconstruction error. Our first result provides a tight geometric characterization of the optimal error in terms of the Chebyshev radius, a classical concept from geometry. This characterization applies to all compact metric spaces (in fact, to all totally bounded spaces) and yields explicit formulas for natural subsets of the Euclidean metric. Our second result addresses the asymptotic behavior of reconstruction, distinguishing between pseudo-finite spaces, where the optimal error is attained after finitely many queries, and spaces where the approximation curve exhibits a nontrivial decay. We characterize pseudo-finiteness for convex subsets of Euclidean spaces.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0vJJdEiXOb"
        ],
        "venue": [
          "/venue/0vJJdEiXOb@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0vJJdEiXOb"
        ],
        "detail": [
          "https://openreview.net/forum?id=0vJJdEiXOb"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Reconstruction and Secrecy under Approximate Distance Queries [PDF ] [Copy] [Kimi ] [REL] Authors : Shay Moran , Elizaveta Nesterova Consider the task of locating an unknown target point using approximate distance queries: in each round, a reconstructor selects a reference point and receives a noisy version of its distance to the target. This problem arises naturally in various contexts—from localization in GPS and sensor networks to privacy-aware data access—making it relevant from the perspective of both the reconstructor (seeking accurate recovery) and the responder (aiming to limit information disclosure, e.g., for privacy or security reasons). We study this reconstruction game through a learning-theoretic lens, focusing on the rate and limits of the best possible reconstruction error. Our first result provides a tight geometric characterization of the optimal error in terms of the Chebyshev radius, a classical concept from geometry. This characterization applies to all compact metric spaces (in fact, to all totally bounded spaces) and yields explicit formulas for natural subsets of the Euclidean metric. Our second result addresses the asymptotic behavior of reconstruction, distinguishing between pseudo-finite spaces, where the optimal error is attained after finitely many queries, and spaces where the approximation curve exhibits a nontrivial decay. We characterize pseudo-finiteness for convex subsets of Euclidean spaces. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Fcs90Rwm8j@OpenReview",
      "index": 275,
      "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
      "authors": [
        "Hao Kang",
        "Qingru Zhang",
        "Han Cai",
        "Weiyuan Xu",
        "Tushar Krishna",
        "Yilun Du",
        "Tsachy Weissman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "latency",
        "trading",
        "win",
        "agents",
        "llm",
        "gaming",
        "decisions",
        "real",
        "fpx",
        "llms"
      ],
      "summary": "Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency–quality trade-off, it remains underexplored in the context of LLM-based agents. In this work, we present the first systematic study of this trade-off in real-time decision-making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high-frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency–quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real-time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency-aware evaluation and deployment strategies for LLM-based agents. These results demonstrate the critical importance of latency-aware evaluation and deployment strategies for real-world LLM-based agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Fcs90Rwm8j"
        ],
        "venue": [
          "/venue/Fcs90Rwm8j@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Fcs90Rwm8j"
        ],
        "detail": [
          "https://openreview.net/forum?id=Fcs90Rwm8j"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs [PDF ] [Copy] [Kimi ] [REL] Authors : Hao Kang , Qingru Zhang , Han Cai , Weiyuan Xu , Tushar Krishna , Yilun Du , Tsachy Weissman Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency–quality trade-off, it remains underexplored in the context of LLM-based agents. In this work, we present the first systematic study of this trade-off in real-time decision-making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high-frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency–quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real-time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency-aware evaluation and deployment strategies for LLM-based agents. These results demonstrate the critical importance of latency-aware evaluation and deployment strategies for real-world LLM-based agents. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "29LwAgLFpj@OpenReview",
      "index": 276,
      "title": "Characterizing the Expressivity of Fixed-Precision Transformer Language Models",
      "authors": [
        "Jiaoda Li",
        "Ryan Cotterell"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "expressivity",
        "idealization",
        "expressive",
        "transformer",
        "language",
        "fragment",
        "precision",
        "transformers",
        "fixed",
        "languages"
      ],
      "summary": "Transformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood. In this work, we analyze a restricted idealization of fixed-precision transformers with strict future masking, soft attention, and no positional encodings. We establish that this class of models is exactly as expressive as a specific fragment of linear temporal logic that contains only a single temporal operator: the past past operator. We further connect this fragment to established classes in formal language theory, automata theory, and algebra, yielding a unified framework for understanding transformer expressivity under this idealization. Finally, we present empirical results that align closely with our theory: transformers trained on languages within their characterized expressive capacity generalize reliably across sequence lengths, while they consistently fail to generalize on languages beyond it.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=29LwAgLFpj"
        ],
        "venue": [
          "/venue/29LwAgLFpj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=29LwAgLFpj"
        ],
        "detail": [
          "https://openreview.net/forum?id=29LwAgLFpj"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Characterizing the Expressivity of Fixed-Precision Transformer Language Models [PDF ] [Copy] [Kimi ] [REL] Authors : Jiaoda Li , Ryan Cotterell Transformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood. In this work, we analyze a restricted idealization of fixed-precision transformers with strict future masking, soft attention, and no positional encodings. We establish that this class of models is exactly as expressive as a specific fragment of linear temporal logic that contains only a single temporal operator: the past past operator. We further connect this fragment to established classes in formal language theory, automata theory, and algebra, yielding a unified framework for understanding transformer expressivity under this idealization. Finally, we present empirical results that align closely with our theory: transformers trained on languages within their characterized expressive capacity generalize reliably across sequence lengths, while they consistently fail to generalize on languages beyond it. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "UBRFn7YKMe@OpenReview",
      "index": 277,
      "title": "Distributional Training Data Attribution: What do Influence Functions Sample?",
      "authors": [
        "Bruno Kacper Mlodozeniec",
        "Isaac Reid",
        "Samuel Power",
        "David Krueger",
        "Murat A Erdogdu",
        "Richard E. Turner",
        "Roger Baker Grosse"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "attribution",
        "distributional",
        "training",
        "tda",
        "ifs",
        "data",
        "influence",
        "secretly",
        "initialisation",
        "batching"
      ],
      "summary": "Randomness is an unavoidable part of training deep learning models, yet something that traditional training data attribution algorithms fail to rigorously account for. They ignore the fact that, due to stochasticity in the initialisation and batching, training on the same dataset can yield different models. In this paper, we address this shortcoming through introducing _distributional_ training data attribution (d-TDA), the goal of which is to predict how the distribution of model outputs (over training runs) depends upon the dataset. Intriguingly, we find that _influence functions_ (IFs), a popular data attribution tool, are 'secretly distributional': they emerge from our framework as the limit to unrolled differentiation, without requiring restrictive convexity assumptions. This provides a new perspective on the effectiveness of IFs in deep learning. We demonstrate the practical utility of d-TDA in experiments, including improving data pruning for vision transformers and identifying influential examples with diffusion models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UBRFn7YKMe"
        ],
        "venue": [
          "/venue/UBRFn7YKMe@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UBRFn7YKMe"
        ],
        "detail": [
          "https://openreview.net/forum?id=UBRFn7YKMe"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Distributional Training Data Attribution: What do Influence Functions Sample? [PDF ] [Copy] [Kimi ] [REL] Authors : Bruno Kacper Mlodozeniec , Isaac Reid , Samuel Power , David Krueger , Murat A Erdogdu , Richard E. Turner , Roger Baker Grosse Randomness is an unavoidable part of training deep learning models, yet something that traditional training data attribution algorithms fail to rigorously account for. They ignore the fact that, due to stochasticity in the initialisation and batching, training on the same dataset can yield different models. In this paper, we address this shortcoming through introducing _distributional_ training data attribution (d-TDA), the goal of which is to predict how the distribution of model outputs (over training runs) depends upon the dataset. Intriguingly, we find that _influence functions_ (IFs), a popular data attribution tool, are 'secretly distributional': they emerge from our framework as the limit to unrolled differentiation, without requiring restrictive convexity assumptions. This provides a new perspective on the effectiveness of IFs in deep learning. We demonstrate the practical utility of d-TDA in experiments, including improving data pruning for vision transformers and identifying influential examples with diffusion models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "roKj4IwaVT@OpenReview",
      "index": 278,
      "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
      "authors": [
        "Gleb Rodionov",
        "Roman Garipov",
        "Alina Shutova",
        "George Yakushev",
        "Erik Schultheis",
        "Vage Egiazarian",
        "Anton Sinitsin",
        "Denis Kuznedelev",
        "Dan Alistarh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hogwild",
        "parallel",
        "cache",
        "llm",
        "inference",
        "tasks",
        "concurrent",
        "llms",
        "attention",
        "concurrently"
      ],
      "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=roKj4IwaVT"
        ],
        "venue": [
          "/venue/roKj4IwaVT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=roKj4IwaVT"
        ],
        "detail": [
          "https://openreview.net/forum?id=roKj4IwaVT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Gleb Rodionov , Roman Garipov , Alina Shutova , George Yakushev , Erik Schultheis , Vage Egiazarian , Anton Sinitsin , Denis Kuznedelev , Dan Alistarh Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4FyNdd2b5S@OpenReview",
      "index": 279,
      "title": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation",
      "authors": [
        "Abdelrahman Eldesokey",
        "Aleksandar Cvejić",
        "Bernard Ghanem",
        "Peter Wonka"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "visual",
        "inconsistencies",
        "subject",
        "driven",
        "generation",
        "correspondence",
        "semantic",
        "backbones",
        "glitch",
        "features"
      ],
      "summary": "We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4FyNdd2b5S"
        ],
        "venue": [
          "/venue/4FyNdd2b5S@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4FyNdd2b5S"
        ],
        "detail": [
          "https://openreview.net/forum?id=4FyNdd2b5S"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Abdelrahman Eldesokey , Aleksandar Cvejić , Bernard Ghanem , Peter Wonka We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZYXTLo7kCi@OpenReview",
      "index": 280,
      "title": "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?",
      "authors": [
        "Denis Sutter",
        "Julian Minder",
        "Thomas Hofmann",
        "Tiago Pimentel"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "abstraction",
        "causal",
        "interpretability",
        "mechanistic",
        "maps",
        "dilemma",
        "alignment",
        "representation",
        "linearity",
        "linear"
      ],
      "summary": "The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretability papers implement these maps as linear functions, motivated by the linear representation hypothesis: the idea that features are encoded linearly in a model's representations. However, this linearity constraint is not required by the definition of causal abstraction. In this work, we critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps. In particular, we prove that under reasonable assumptions, any neural network can be mapped to any algorithm, rendering this unrestricted notion of causal abstraction trivial and uninformative. We complement these theoretical findings with empirical evidence, demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task; e.g., on an experiment using randomly initialised language models, our alignment maps reach 100\\% interchange-intervention accuracy on the indirect object identification task. This raises the non-linear representation dilemma: if we lift the linearity constraint imposed to alignment maps in causal abstraction analyses, we are left with no principled way to balance the inherent trade-off between these maps' complexity and accuracy. Together, these results suggest an answer to our title's question: causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information. Studying the connection between this information-encoding assumption and causal abstraction should lead to exciting future work.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZYXTLo7kCi"
        ],
        "venue": [
          "/venue/ZYXTLo7kCi@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZYXTLo7kCi"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZYXTLo7kCi"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability? [PDF ] [Copy] [Kimi ] [REL] Authors : Denis Sutter , Julian Minder , Thomas Hofmann , Tiago Pimentel The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretability papers implement these maps as linear functions, motivated by the linear representation hypothesis: the idea that features are encoded linearly in a model's representations. However, this linearity constraint is not required by the definition of causal abstraction. In this work, we critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps. In particular, we prove that under reasonable assumptions, any neural network can be mapped to any algorithm, rendering this unrestricted notion of causal abstraction trivial and uninformative. We complement these theoretical findings with empirical evidence, demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task; e.g., on an experiment using randomly initialised language models, our alignment maps reach 100\\% interchange-intervention accuracy on the indirect object identification task. This raises the non-linear representation dilemma: if we lift the linearity constraint imposed to alignment maps in causal abstraction analyses, we are left with no principled way to balance the inherent trade-off between these maps' complexity and accuracy. Together, these results suggest an answer to our title's question: causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information. Studying the connection between this information-encoding assumption and causal abstraction should lead to exciting future work. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "j1QkrVjNVF@OpenReview",
      "index": 281,
      "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS",
      "authors": [
        "Tao Wang",
        "Mengyu Li",
        "Geduo Zeng",
        "Cheng Meng",
        "Qiong Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "3dgs",
        "gaussian",
        "compaction",
        "rendering",
        "pens",
        "gaussians",
        "herding",
        "transport",
        "global",
        "primitives"
      ],
      "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10\\% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=j1QkrVjNVF"
        ],
        "venue": [
          "/venue/j1QkrVjNVF@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=j1QkrVjNVF"
        ],
        "detail": [
          "https://openreview.net/forum?id=j1QkrVjNVF"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Tao Wang , Mengyu Li , Geduo Zeng , Cheng Meng , Qiong Zhang 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10\\% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "jVwIfsJLvh@OpenReview",
      "index": 282,
      "title": "Generalized Top-k Mallows Model for Ranked Choices",
      "authors": [
        "Shahrzad Haddadan",
        "Sara Ahmadian"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mallows",
        "top",
        "model",
        "choices",
        "generalized",
        "ranked",
        "indifferent",
        "contributions",
        "choice",
        "buyer"
      ],
      "summary": "The classic Mallows model is a foundational tool for modeling user preferences. However, it has limitations in capturing real-world scenarios, where users often focus only on a limited set of preferred items and are indifferent to the rest. To address this, extensions such as the top- k k Mallows model have been proposed, aligning better with practical applications. In this paper, we address several challenges related to the generalized top- k k Mallows model, with a focus on analyzing buyer choices. Our key contributions are: (1) a novel sampling scheme tailored to generalized top- k k Mallows models, (2) an efficient algorithm for computing choice probabilities under this model, and (3) an active learning algorithm for estimating the model parameters from observed choice data. These contributions provide new tools for analysis and prediction in critical decision-making scenarios. We present a rigorous mathematical analysis for the performance of our algorithms. Furthermore, through extensive experiments on synthetic data and real-world data, we demonstrate the scalability and accuracy of our proposed methods, and we compare the predictive power of Mallows model for top- k k lists compared to the simpler Multinomial Logit model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jVwIfsJLvh"
        ],
        "venue": [
          "/venue/jVwIfsJLvh@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jVwIfsJLvh"
        ],
        "detail": [
          "https://openreview.net/forum?id=jVwIfsJLvh"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Generalized Top-k Mallows Model for Ranked Choices [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Shahrzad Haddadan , Sara Ahmadian The classic Mallows model is a foundational tool for modeling user preferences. However, it has limitations in capturing real-world scenarios, where users often focus only on a limited set of preferred items and are indifferent to the rest. To address this, extensions such as the top- k k Mallows model have been proposed, aligning better with practical applications. In this paper, we address several challenges related to the generalized top- k k Mallows model, with a focus on analyzing buyer choices. Our key contributions are: (1) a novel sampling scheme tailored to generalized top- k k Mallows models, (2) an efficient algorithm for computing choice probabilities under this model, and (3) an active learning algorithm for estimating the model parameters from observed choice data. These contributions provide new tools for analysis and prediction in critical decision-making scenarios. We present a rigorous mathematical analysis for the performance of our algorithms. Furthermore, through extensive experiments on synthetic data and real-world data, we demonstrate the scalability and accuracy of our proposed methods, and we compare the predictive power of Mallows model for top- k k lists compared to the simpler Multinomial Logit model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "jlJaRXDzCE@OpenReview",
      "index": 283,
      "title": "Improving Bilinear RNN with Closed-loop Control",
      "authors": [
        "Jiaxi Hu",
        "Yongqi Pan",
        "Jusen Du",
        "Disen Lan",
        "Xiaqiang Tang",
        "Qingsong Wen",
        "Yuxuan Liang",
        "Weigao Sun"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bilinear",
        "comba",
        "rnn",
        "gated",
        "deltanet",
        "340m",
        "loop",
        "rwkv",
        "gla",
        "closed"
      ],
      "summary": "Recent efficient sequence modeling methods, such as Gated DeltaNet, TTT, and RWKV-7, have achieved performance improvements by supervising the recurrent memory management through the Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, resulting in a bilinear recursive structure. In this paper, we first introduce the concept of Bilinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then based on the closed-loop control theory, we propose a novel Bilinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on a large-scale corpus. Comba demonstrates its superior performance and computation efficiency on both language modeling and vision tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jlJaRXDzCE"
        ],
        "venue": [
          "/venue/jlJaRXDzCE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jlJaRXDzCE"
        ],
        "detail": [
          "https://openreview.net/forum?id=jlJaRXDzCE"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Improving Bilinear RNN with Closed-loop Control [PDF ] [Copy] [Kimi ] [REL] Authors : Jiaxi Hu , Yongqi Pan , Jusen Du , Disen Lan , Xiaqiang Tang , Qingsong Wen , Yuxuan Liang , Weigao Sun Recent efficient sequence modeling methods, such as Gated DeltaNet, TTT, and RWKV-7, have achieved performance improvements by supervising the recurrent memory management through the Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, resulting in a bilinear recursive structure. In this paper, we first introduce the concept of Bilinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then based on the closed-loop control theory, we propose a novel Bilinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on a large-scale corpus. Comba demonstrates its superior performance and computation efficiency on both language modeling and vision tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RnXS7aK4rK@OpenReview",
      "index": 284,
      "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
      "authors": [
        "Diankun Wu",
        "Fangfu Liu",
        "Yi-Hsin Hung",
        "Yueqi Duan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "spatial",
        "mllm",
        "visual",
        "mllms",
        "reasoning",
        "encoder",
        "grpo",
        "intelligence",
        "understanding",
        "frames"
      ],
      "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a 3D spatial encoder—initialized from the backbone of the visual geometry model—to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct a training dataset from multiple sources and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that Spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RnXS7aK4rK"
        ],
        "venue": [
          "/venue/RnXS7aK4rK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RnXS7aK4rK"
        ],
        "detail": [
          "https://openreview.net/forum?id=RnXS7aK4rK"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 4
      },
      "raw_excerpt": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence [PDF ] [Copy] [Kimi 4 ] [REL] Authors : Diankun Wu , Fangfu Liu , Yi-Hsin Hung , Yueqi Duan Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a 3D spatial encoder—initialized from the backbone of the visual geometry model—to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct a training dataset from multiple sources and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that Spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "o8r3gOFTQo@OpenReview",
      "index": 285,
      "title": "SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation",
      "authors": [
        "Claudia Cuttano",
        "Gabriele Trivigno",
        "Giuseppe Averta",
        "Carlo Masone"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sansa",
        "sam2",
        "segmentation",
        "shot",
        "segment",
        "anything",
        "semantically",
        "unleashing",
        "repurposes",
        "impairs"
      ],
      "summary": "Few-shot segmentation aims to segment unseen categories from just a handful of annotated examples. This requires mechanisms to identify semantically related objects across images and accurately produce masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, provides strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art on few-shot segmentation benchmarks designed to assess generalization and outperforms generalist methods in the popular in-context setting. Additionally, it supports flexible promptable interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code at: https://github.com/ClaudiaCuttano/SANSA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=o8r3gOFTQo"
        ],
        "venue": [
          "/venue/o8r3gOFTQo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=o8r3gOFTQo"
        ],
        "detail": [
          "https://openreview.net/forum?id=o8r3gOFTQo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Claudia Cuttano , Gabriele Trivigno , Giuseppe Averta , Carlo Masone Few-shot segmentation aims to segment unseen categories from just a handful of annotated examples. This requires mechanisms to identify semantically related objects across images and accurately produce masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, provides strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art on few-shot segmentation benchmarks designed to assess generalization and outperforms generalist methods in the popular in-context setting. Additionally, it supports flexible promptable interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code at: https://github.com/ClaudiaCuttano/SANSA. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4Qe2Hga43N@OpenReview",
      "index": 286,
      "title": "Cost-Aware Contrastive Routing for LLMs",
      "authors": [
        "Reza Shirkavand",
        "Shangqian Gao",
        "Peiran Yu",
        "Heng Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cscr",
        "routing",
        "cost",
        "contrastive",
        "aware",
        "llms",
        "prompts",
        "expert",
        "faiss",
        "cheapest"
      ],
      "summary": "We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single k k ‑NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy–cost tradeoff by up to 25\\%, while generalizing robustly to unseen LLMs and out-of-distribution prompts.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4Qe2Hga43N"
        ],
        "venue": [
          "/venue/4Qe2Hga43N@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4Qe2Hga43N"
        ],
        "detail": [
          "https://openreview.net/forum?id=4Qe2Hga43N"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Cost-Aware Contrastive Routing for LLMs [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Reza Shirkavand , Shangqian Gao , Peiran Yu , Heng Huang We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single k k ‑NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy–cost tradeoff by up to 25\\%, while generalizing robustly to unseen LLMs and out-of-distribution prompts. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Tk5nQnTGmP@OpenReview",
      "index": 287,
      "title": "Is Grokking a Computational Glass Relaxation?",
      "authors": [
        "Xiaotian Zhang",
        "Yue Shang",
        "Entao Yang",
        "Ge Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "grokking",
        "nns",
        "generalizability",
        "relaxation",
        "memorization",
        "entropy",
        "glass",
        "optimizer",
        "wand",
        "goldilocks"
      ],
      "summary": "Understanding neural network' (NN) generalizability remains a central question in deep learning research. The special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability. Here we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. This mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. Our experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition. We identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. Inspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. This provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Tk5nQnTGmP"
        ],
        "venue": [
          "/venue/Tk5nQnTGmP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Tk5nQnTGmP"
        ],
        "detail": [
          "https://openreview.net/forum?id=Tk5nQnTGmP"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Is Grokking a Computational Glass Relaxation? [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Xiaotian Zhang , Yue Shang , Entao Yang , Ge Zhang Understanding neural network' (NN) generalizability remains a central question in deep learning research. The special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability. Here we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. This mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. Our experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition. We identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. Inspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. This provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "q06YjUj0FB@OpenReview",
      "index": 288,
      "title": "LoRATv2: Enabling Low-Cost Temporal Modeling in One-Stream Trackers",
      "authors": [
        "Liting Lin",
        "Heng Fan",
        "Zhipeng Zhang",
        "Yuqing Huang",
        "Yaowei Wang",
        "Yong Xu",
        "Haibin Ling"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "loratv2",
        "lorat",
        "stream",
        "frame",
        "lora",
        "trackers",
        "tracking",
        "ssla",
        "attention",
        "causal"
      ],
      "summary": "Transformer-based algorithms, such as LoRAT, have significantly enhanced object-tracking performance. However, these approaches rely on a standard attention mechanism, which incurs quadratic token complexity, making real-time inference computationally expensive. In this paper, we introduce LoRATv2, a novel tracking framework that addresses these limitations with three main contributions. First, LoRATv2 integrates frame-wise causal attention, which ensures full self-attention within each frame while enabling causal dependencies across frames, significantly reducing computational overhead. Moreover, key-value (KV) caching is employed to efficiently reuse past embeddings for further speedup. Second, building on LoRAT's parameter-efficient fine-tuning, we propose Stream-Specific LoRA Adapters (SSLA). As frame-wise causal attention introduces asymmetry in how streams access temporal information, SSLA assigns dedicated LoRA modules to the template and each search stream, with the main ViT backbone remaining frozen. This allows specialized adaptation for each stream's role in temporal tracking. Third, we introduce a two-phase progressive training strategy, which first trains a single-search-frame tracker and then gradually extends it to multi-search-frame inputs by introducing additional LoRA modules. This curriculum-based learning paradigm improves long-term tracking while maintaining training efficiency. In extensive experiments on multiple benchmarks, LoRATv2 achieves state-of-the-art performance, substantially improved efficiency, and a superior performance-to-FLOPs ratio over state-of-the-art trackers. The code is available at https://github.com/LitingLin/LoRATv2.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=q06YjUj0FB"
        ],
        "venue": [
          "/venue/q06YjUj0FB@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=q06YjUj0FB"
        ],
        "detail": [
          "https://openreview.net/forum?id=q06YjUj0FB"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "LoRATv2: Enabling Low-Cost Temporal Modeling in One-Stream Trackers [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Liting Lin , Heng Fan , Zhipeng Zhang , Yuqing Huang , Yaowei Wang , Yong Xu , Haibin Ling Transformer-based algorithms, such as LoRAT, have significantly enhanced object-tracking performance. However, these approaches rely on a standard attention mechanism, which incurs quadratic token complexity, making real-time inference computationally expensive. In this paper, we introduce LoRATv2, a novel tracking framework that addresses these limitations with three main contributions. First, LoRATv2 integrates frame-wise causal attention, which ensures full self-attention within each frame while enabling causal dependencies across frames, significantly reducing computational overhead. Moreover, key-value (KV) caching is employed to efficiently reuse past embeddings for further speedup. Second, building on LoRAT's parameter-efficient fine-tuning, we propose Stream-Specific LoRA Adapters (SSLA). As frame-wise causal attention introduces asymmetry in how streams access temporal information, SSLA assigns dedicated LoRA modules to the template and each search stream, with the main ViT backbone remaining frozen. This allows specialized adaptation for each stream's role in temporal tracking. Third, we introduce a two-phase progressive training strategy, which first trains a single-search-frame tracker and then gradually extends it to multi-search-frame inputs by introducing additional LoRA modules. This curriculum-based learning paradigm improves long-term tracking while maintaining training efficiency. In extensive experiments on multiple benchmarks, LoRATv2 achieves state-of-the-art performance, substantially improved efficiency, and a superior performance-to-FLOPs ratio over state-of-the-art trackers. The code is available at https://github.com/LitingLin/LoRATv2. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ONc9vWkwCp@OpenReview",
      "index": 289,
      "title": "On the necessity of adaptive regularisation: Optimal anytime online learning on ℓ p ℓ p -balls",
      "authors": [
        "Emmeran Johnson",
        "David Martínez-Rubio",
        "Ciara Pike-Burke",
        "Patrick Rebeschini"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "regularisation",
        "anytime",
        "balls",
        "regulariser",
        "ell",
        "dimension",
        "ftrl",
        "adaptive",
        "optimal",
        "online"
      ],
      "summary": "We study online convex optimization on ℓ p ℓ p -balls in R d R d for p > 2 p > 2 . While always sub-linear, the optimal regret exhibits a shift between the high-dimensional setting ( d > T d > T ), when the dimension d d is greater than the time horizon T T and the low-dimensional setting ( d ≤ T d ≤ T ). We show that Follow-the-Regularised-Leader (FTRL) with time-varying regularisation which is adaptive to the dimension regime is anytime optimal for all dimension regimes. Motivated by this, we ask whether it is possible to obtain anytime optimality of FTRL with fixed non-adaptive regularisation. Our main result establishes that for separable regularisers, adaptivity in the regulariser is necessary, and that any fixed regulariser will be sub-optimal in one of the two dimension regimes. Finally, we provide lower bounds which rule out sub-linear regret bounds for the linear bandit problem in sufficiently high-dimension for all ℓ p ℓ p -balls with p ≥ 1 p ≥ 1 .",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ONc9vWkwCp"
        ],
        "venue": [
          "/venue/ONc9vWkwCp@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ONc9vWkwCp"
        ],
        "detail": [
          "https://openreview.net/forum?id=ONc9vWkwCp"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "On the necessity of adaptive regularisation: Optimal anytime online learning on ℓ p ℓ p -balls [PDF ] [Copy] [Kimi ] [REL] Authors : Emmeran Johnson , David Martínez-Rubio , Ciara Pike-Burke , Patrick Rebeschini We study online convex optimization on ℓ p ℓ p -balls in R d R d for p > 2 p > 2 . While always sub-linear, the optimal regret exhibits a shift between the high-dimensional setting ( d > T d > T ), when the dimension d d is greater than the time horizon T T and the low-dimensional setting ( d ≤ T d ≤ T ). We show that Follow-the-Regularised-Leader (FTRL) with time-varying regularisation which is adaptive to the dimension regime is anytime optimal for all dimension regimes. Motivated by this, we ask whether it is possible to obtain anytime optimality of FTRL with fixed non-adaptive regularisation. Our main result establishes that for separable regularisers, adaptivity in the regulariser is necessary, and that any fixed regulariser will be sub-optimal in one of the two dimension regimes. Finally, we provide lower bounds which rule out sub-linear regret bounds for the linear bandit problem in sufficiently high-dimension for all ℓ p ℓ p -balls with p ≥ 1 p ≥ 1 . Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ESELaMThLN@OpenReview",
      "index": 290,
      "title": "Controlling Thinking Speed in Reasoning Models",
      "authors": [
        "Zhengkai Lin",
        "Zhihang Fu",
        "Ze Chen",
        "Chao Chen",
        "Liang Xie",
        "Wenxiao Wang",
        "Deng Cai",
        "Zheng Wang",
        "Jieping Ye"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lrms",
        "thinking",
        "reasoning",
        "fast",
        "speed",
        "vllm",
        "theorized",
        "slow",
        "controlling",
        "system"
      ],
      "summary": "Human cognition is theorized to operate in two modes: fast, intuitive System 1 thinking and slow, deliberate System 2 thinking. While current Large Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform fast thinking leads to high computational overhead and latency. In this work, we enable LRMs to approximate human intelligence through dynamic thinking speed adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses two key questions: (1) how to control thinking speed in LRMs, and (2) when to adjust it for optimal performance. For the first question, we identify the steering vector that governs slow-fast thinking transitions in LRMs' representation space. Using this vector, we achieve the first representation editing-based test-time scaling effect, outperforming existing prompt-based scaling methods. For the second question, we apply real-time difficulty estimation to signal reasoning segments of varying complexity. Combining these techniques, we propose the first reasoning strategy that enables fast processing of easy steps and deeper analysis for complex reasoning. Without any training or additional cost, our plug-and-play method yields an average +1.3\\% accuracy with -8.6\\% token usage across leading LRMs and advanced reasoning benchmarks. All of our algorithms are implemented based on vLLM and are expected to support broader applications and inspire future research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ESELaMThLN"
        ],
        "venue": [
          "/venue/ESELaMThLN@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ESELaMThLN"
        ],
        "detail": [
          "https://openreview.net/forum?id=ESELaMThLN"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Controlling Thinking Speed in Reasoning Models [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Zhengkai Lin , Zhihang Fu , Ze Chen , Chao Chen , Liang Xie , Wenxiao Wang , Deng Cai , Zheng Wang , Jieping Ye Human cognition is theorized to operate in two modes: fast, intuitive System 1 thinking and slow, deliberate System 2 thinking. While current Large Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform fast thinking leads to high computational overhead and latency. In this work, we enable LRMs to approximate human intelligence through dynamic thinking speed adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses two key questions: (1) how to control thinking speed in LRMs, and (2) when to adjust it for optimal performance. For the first question, we identify the steering vector that governs slow-fast thinking transitions in LRMs' representation space. Using this vector, we achieve the first representation editing-based test-time scaling effect, outperforming existing prompt-based scaling methods. For the second question, we apply real-time difficulty estimation to signal reasoning segments of varying complexity. Combining these techniques, we propose the first reasoning strategy that enables fast processing of easy steps and deeper analysis for complex reasoning. Without any training or additional cost, our plug-and-play method yields an average +1.3\\% accuracy with -8.6\\% token usage across leading LRMs and advanced reasoning benchmarks. All of our algorithms are implemented based on vLLM and are expected to support broader applications and inspire future research. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "M44RvNMZs4@OpenReview",
      "index": 291,
      "title": "Vector Quantization in the Brain: Grid-like Codes in World Models",
      "authors": [
        "Xiangyuan Peng",
        "Xingsi Dong",
        "Si Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gcq",
        "grid",
        "quantization",
        "brain",
        "attractor",
        "like",
        "codes",
        "vector",
        "world",
        "compressing"
      ],
      "summary": "We propose Grid-like Code Quantization (GCQ), a brain-inspired method for compressing observation-action sequences into discrete representations using grid-like patterns in attractor dynamics. Unlike conventional vector quantization approaches that operate on static inputs, GCQ performs spatiotemporal compression through an action-conditioned codebook, where codewords are derived from continuous attractor neural networks and dynamically selected based on actions. This enables GCQ to jointly compress space and time, serving as a unified world model. The resulting representation supports long-horizon prediction, goal-directed planning, and inverse modeling. Experiments across diverse tasks demonstrate GCQ's effectiveness in compact encoding and downstream performance. Our work offers both a computational tool for efficient sequence modeling and a theoretical perspective on the formation of grid-like codes in neural systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M44RvNMZs4"
        ],
        "venue": [
          "/venue/M44RvNMZs4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M44RvNMZs4"
        ],
        "detail": [
          "https://openreview.net/forum?id=M44RvNMZs4"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Vector Quantization in the Brain: Grid-like Codes in World Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xiangyuan Peng , Xingsi Dong , Si Wu We propose Grid-like Code Quantization (GCQ), a brain-inspired method for compressing observation-action sequences into discrete representations using grid-like patterns in attractor dynamics. Unlike conventional vector quantization approaches that operate on static inputs, GCQ performs spatiotemporal compression through an action-conditioned codebook, where codewords are derived from continuous attractor neural networks and dynamically selected based on actions. This enables GCQ to jointly compress space and time, serving as a unified world model. The resulting representation supports long-horizon prediction, goal-directed planning, and inverse modeling. Experiments across diverse tasks demonstrate GCQ's effectiveness in compact encoding and downstream performance. Our work offers both a computational tool for efficient sequence modeling and a theoretical perspective on the formation of grid-like codes in neural systems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Cggdvyt8ik@OpenReview",
      "index": 292,
      "title": "IA-GGAD: Zero-shot Generalist Graph Anomaly Detection via Invariant and Affinity Learning",
      "authors": [
        "Xiong Zhang",
        "Zhenli He",
        "Changlong Fu",
        "Cheng Xie"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ggad",
        "anomaly",
        "affinity",
        "generalist",
        "graph",
        "gss",
        "fss",
        "module",
        "invariant",
        "domain"
      ],
      "summary": "Generalist Graph Anomaly Detection (GGAD) extends traditional Graph Anomaly Detection (GAD) from one-for-one to one-for-all scenarios, posing significant challenges due to Feature Space Shift (FSS) and Graph Structure Shift (GSS). This paper first formalizes these challenges and proposes quantitative metrics to measure their severity. To tackle FSS, we develop an anomaly-driven graph invariant learning module that learns domain-invariant node representations. To address GSS, a novel structure-insensitive affinity learning module is introduced, capturing cross-domain structural correspondences via affinity-based features. Our unified framework, IA-GGAD, integrates these modules, enabling anomaly prediction on unseen graphs without target-domain retraining or fine-tuning. Extensive experiments on benchmark datasets from varied domains demonstrate IA-GGAD’s superior performance, significantly outperforming state-of-the-art methods (e.g., achieving up to +12.28\\% AUROC over ARC on ACM). Ablation studies further confirm the effectiveness of each proposed module. The code is available at \\url{https://github.com/kg-cc/IA-GGAD/}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Cggdvyt8ik"
        ],
        "venue": [
          "/venue/Cggdvyt8ik@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Cggdvyt8ik"
        ],
        "detail": [
          "https://openreview.net/forum?id=Cggdvyt8ik"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "IA-GGAD: Zero-shot Generalist Graph Anomaly Detection via Invariant and Affinity Learning [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Xiong Zhang , Zhenli He , Changlong Fu , Cheng Xie Generalist Graph Anomaly Detection (GGAD) extends traditional Graph Anomaly Detection (GAD) from one-for-one to one-for-all scenarios, posing significant challenges due to Feature Space Shift (FSS) and Graph Structure Shift (GSS). This paper first formalizes these challenges and proposes quantitative metrics to measure their severity. To tackle FSS, we develop an anomaly-driven graph invariant learning module that learns domain-invariant node representations. To address GSS, a novel structure-insensitive affinity learning module is introduced, capturing cross-domain structural correspondences via affinity-based features. Our unified framework, IA-GGAD, integrates these modules, enabling anomaly prediction on unseen graphs without target-domain retraining or fine-tuning. Extensive experiments on benchmark datasets from varied domains demonstrate IA-GGAD’s superior performance, significantly outperforming state-of-the-art methods (e.g., achieving up to +12.28\\% AUROC over ARC on ACM). Ablation studies further confirm the effectiveness of each proposed module. The code is available at \\url{https://github.com/kg-cc/IA-GGAD/}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wvcYIEaD5X@OpenReview",
      "index": 293,
      "title": "Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws",
      "authors": [
        "Lin Guo",
        "Xiaoqing Luo",
        "Wei Xie",
        "Zhancheng Zhang",
        "Hui Li",
        "Rui Wang",
        "Zhenhua Feng",
        "Xiaoning Song"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fusion",
        "generative",
        "visible",
        "hclfuse",
        "infrared",
        "laws",
        "image",
        "cognitive",
        "modal",
        "information"
      ],
      "summary": "Existing infrared and visible image fusion methods often face the dilemma of balancing modal information. Generative fusion methods reconstruct fused images by learning from data distributions, but their generative capabilities remain limited. Moreover, the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios. This manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method, termed HCLFuse. First, HCLFuse investigates the quantification theory of information mapping in unsupervised fusion networks, which leads to the design of a multi-scale mask-regulated variational bottleneck encoder. This encoder applies posterior probability modeling and information decomposition to extract accurate and concise low-level modal information, thereby supporting the generation of high-fidelity structural details. Furthermore, the probabilistic generative capability of the diffusion model is integrated with physical laws, forming a time-varying physical guidance mechanism that adaptively regulates the generation process at different stages, thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality. Experimental results show that the proposed method achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics. This fully demonstrates the advantages of this generative image fusion method, drawing inspiration from human cognition, in enhancing structural consistency and detail quality.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wvcYIEaD5X"
        ],
        "venue": [
          "/venue/wvcYIEaD5X@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wvcYIEaD5X"
        ],
        "detail": [
          "https://openreview.net/forum?id=wvcYIEaD5X"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Lin Guo , Xiaoqing Luo , Wei Xie , Zhancheng Zhang , Hui Li , Rui Wang , Zhenhua Feng , Xiaoning Song Existing infrared and visible image fusion methods often face the dilemma of balancing modal information. Generative fusion methods reconstruct fused images by learning from data distributions, but their generative capabilities remain limited. Moreover, the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios. This manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method, termed HCLFuse. First, HCLFuse investigates the quantification theory of information mapping in unsupervised fusion networks, which leads to the design of a multi-scale mask-regulated variational bottleneck encoder. This encoder applies posterior probability modeling and information decomposition to extract accurate and concise low-level modal information, thereby supporting the generation of high-fidelity structural details. Furthermore, the probabilistic generative capability of the diffusion model is integrated with physical laws, forming a time-varying physical guidance mechanism that adaptively regulates the generation process at different stages, thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality. Experimental results show that the proposed method achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics. This fully demonstrates the advantages of this generative image fusion method, drawing inspiration from human cognition, in enhancing structural consistency and detail quality. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wZzC5rpDY1@OpenReview",
      "index": 294,
      "title": "MonoLift: Learning 3D Manipulation Policies from Monocular RGB via Distillation",
      "authors": [
        "Ziru Wang",
        "Mengmeng Wang",
        "Guang Dai",
        "Yongliu Long",
        "Jingdong Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "monolift",
        "monocular",
        "rgb",
        "manipulation",
        "policies",
        "distillation",
        "deployment",
        "student",
        "depth",
        "aware"
      ],
      "summary": "Although learning 3D manipulation policies from monocular RGB images is lightweight and deployment-friendly, the lack of structural information often leads to inaccurate action estimation. While explicit 3D inputs can mitigate this issue, they typically require additional sensors and introduce data acquisition overhead. An intuitive alternative is to incorporate a pre-trained depth estimator; however, this often incurs substantial inference-time cost. To address this, we propose MonoLift, a tri-level knowledge distillation framework that transfers spatial, temporal, and action-level knowledge from a depth-guided teacher to a monocular RGB student. By jointly distilling geometry-aware features, temporal dynamics, and policy behaviors during training, MonoLift enables the student model to perform 3D-aware reasoning and precise control at deployment using only monocular RGB input. Extensive experiments on both simulated and real-world manipulation tasks show that MonoLift not only outperforms existing monocular approaches but even surpasses several methods that rely on explicit 3D input, offering a resource-efficient and effective solution for vision-based robotic control. The video demonstration is available on our project page: https://robotasy.github.io/MonoLift/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wZzC5rpDY1"
        ],
        "venue": [
          "/venue/wZzC5rpDY1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wZzC5rpDY1"
        ],
        "detail": [
          "https://openreview.net/forum?id=wZzC5rpDY1"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "MonoLift: Learning 3D Manipulation Policies from Monocular RGB via Distillation [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ziru Wang , Mengmeng Wang , Guang Dai , Yongliu Long , Jingdong Wang Although learning 3D manipulation policies from monocular RGB images is lightweight and deployment-friendly, the lack of structural information often leads to inaccurate action estimation. While explicit 3D inputs can mitigate this issue, they typically require additional sensors and introduce data acquisition overhead. An intuitive alternative is to incorporate a pre-trained depth estimator; however, this often incurs substantial inference-time cost. To address this, we propose MonoLift, a tri-level knowledge distillation framework that transfers spatial, temporal, and action-level knowledge from a depth-guided teacher to a monocular RGB student. By jointly distilling geometry-aware features, temporal dynamics, and policy behaviors during training, MonoLift enables the student model to perform 3D-aware reasoning and precise control at deployment using only monocular RGB input. Extensive experiments on both simulated and real-world manipulation tasks show that MonoLift not only outperforms existing monocular approaches but even surpasses several methods that rely on explicit 3D input, offering a resource-efficient and effective solution for vision-based robotic control. The video demonstration is available on our project page: https://robotasy.github.io/MonoLift/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kmv7yg6QXv@OpenReview",
      "index": 295,
      "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
      "authors": [
        "Zekun Qi",
        "Wenyao Zhang",
        "Yufei Ding",
        "Runpei Dong",
        "XinQiang Yu",
        "Jingwen Li",
        "Lingyun Xu",
        "Baoyu Li",
        "Xialin He",
        "Guofan Fan",
        "Jiazhao Zhang",
        "Jiawei He",
        "Jiayuan Gu",
        "Xin Jin",
        "Kaisheng Ma",
        "Zhizheng Zhang",
        "He Wang",
        "Li Yi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sofar",
        "orientation",
        "semantic",
        "object",
        "reasoning",
        "manipulation",
        "shot",
        "spatial",
        "dof",
        "orientations"
      ],
      "summary": "While spatial reasoning has made progress in object localization relationships, it often overlooks object orientation—a key factor in 6-DoF fine-grained manipulation. Traditional pose representations rely on pre-defined frames or templates, limiting generalization and semantic grounding. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a cup). To support this, we construct OrienText300K, a large-scale dataset of 3D objects annotated with semantic orientations, and develop PointSO, a general model for zero-shot semantic orientation prediction. By integrating semantic orientation into VLM agents, our SoFar framework enables 6-DoF spatial reasoning and generates robotic actions. Extensive experiments demonstrated the effectiveness and generalization of our SoFar, e.g., zero-shot 48.7\\% successful rate on Open6DOR and zero-shot 74.9\\% successful rate on SIMPLER-Env.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kmv7yg6QXv"
        ],
        "venue": [
          "/venue/kmv7yg6QXv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kmv7yg6QXv"
        ],
        "detail": [
          "https://openreview.net/forum?id=kmv7yg6QXv"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Zekun Qi , Wenyao Zhang , Yufei Ding , Runpei Dong , XinQiang Yu , Jingwen Li , Lingyun Xu , Baoyu Li , Xialin He , Guofan Fan , Jiazhao Zhang , Jiawei He , Jiayuan Gu , Xin Jin , Kaisheng Ma , Zhizheng Zhang , He Wang , Li Yi While spatial reasoning has made progress in object localization relationships, it often overlooks object orientation—a key factor in 6-DoF fine-grained manipulation. Traditional pose representations rely on pre-defined frames or templates, limiting generalization and semantic grounding. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a cup). To support this, we construct OrienText300K, a large-scale dataset of 3D objects annotated with semantic orientations, and develop PointSO, a general model for zero-shot semantic orientation prediction. By integrating semantic orientation into VLM agents, our SoFar framework enables 6-DoF spatial reasoning and generates robotic actions. Extensive experiments demonstrated the effectiveness and generalization of our SoFar, e.g., zero-shot 48.7\\% successful rate on Open6DOR and zero-shot 74.9\\% successful rate on SIMPLER-Env. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "F1wDPNLvTb@OpenReview",
      "index": 296,
      "title": "Conditional Representation Learning for Customized Tasks",
      "authors": [
        "Honglin Liu",
        "Chao Sun",
        "Peng Hu",
        "Yunfan Li",
        "Xi Peng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "crl",
        "customized",
        "representation",
        "semantics",
        "conditional",
        "tasks",
        "descriptive",
        "xlearning",
        "captures",
        "specified"
      ],
      "summary": "Conventional representation learning methods learn a universal representation that primarily captures dominant semantics, which may not always align with customized downstream tasks. For instance, in animal habitat analysis, researchers prioritize scene-related features, whereas universal embeddings emphasize categorical semantics, leading to suboptimal results. As a solution, existing approaches resort to supervised fine-tuning, which however incurs high computational and annotation costs. In this paper, we propose Conditional Representation Learning (CRL), aiming to extract representations tailored to arbitrary user-specified criteria. Specifically, we reveal that the semantics of a space are determined by its basis, thereby enabling a set of descriptive words to approximate the basis for a customized feature space. Building upon this insight, given a user-specified criterion, CRL first employs a large language model (LLM) to generate descriptive texts to construct the semantic basis, then projects the image representation into this conditional feature space leveraging a vision-language model (VLM). The conditional representation better captures semantics for the specific criterion, which could be utilized for multiple customized tasks. Extensive experiments on classification and retrieval tasks demonstrate the superiority and generality of the proposed CRL. The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=F1wDPNLvTb"
        ],
        "venue": [
          "/venue/F1wDPNLvTb@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=F1wDPNLvTb"
        ],
        "detail": [
          "https://openreview.net/forum?id=F1wDPNLvTb"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Conditional Representation Learning for Customized Tasks [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Honglin Liu , Chao Sun , Peng Hu , Yunfan Li , Xi Peng Conventional representation learning methods learn a universal representation that primarily captures dominant semantics, which may not always align with customized downstream tasks. For instance, in animal habitat analysis, researchers prioritize scene-related features, whereas universal embeddings emphasize categorical semantics, leading to suboptimal results. As a solution, existing approaches resort to supervised fine-tuning, which however incurs high computational and annotation costs. In this paper, we propose Conditional Representation Learning (CRL), aiming to extract representations tailored to arbitrary user-specified criteria. Specifically, we reveal that the semantics of a space are determined by its basis, thereby enabling a set of descriptive words to approximate the basis for a customized feature space. Building upon this insight, given a user-specified criterion, CRL first employs a large language model (LLM) to generate descriptive texts to construct the semantic basis, then projects the image representation into this conditional feature space leveraging a vision-language model (VLM). The conditional representation better captures semantics for the specific criterion, which could be utilized for multiple customized tasks. Extensive experiments on classification and retrieval tasks demonstrate the superiority and generality of the proposed CRL. The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "dGi2d5yDs4@OpenReview",
      "index": 297,
      "title": "Angular Steering: Behavior Control via Rotation in Activation Space",
      "authors": [
        "Hieu M. Vu",
        "Tan Minh Nguyen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "steering",
        "angular",
        "activation",
        "refusal",
        "rotation",
        "behavior",
        "control",
        "activations",
        "subspace",
        "behaviors"
      ],
      "summary": "Controlling specific behaviors in large language models while preserving their general capabilities is a central challenge for safe and reliable artificial intelligence deployment. Current steering methods, such as vector addition and directional ablation, are constrained within a two-dimensional subspace defined by the activation and feature direction, making them sensitive to chosen parameters and potentially affecting unrelated features due to unintended interactions in activation space. We introduce Angular Steering, a novel and flexible method for behavior modulation that operates by rotating activations within a fixed two-dimensional subspace. By formulating steering as a geometric rotation toward or away from a target behavior direction, Angular Steering provides continuous, fine-grained control over behaviors such as refusal and compliance. We demonstrate this method using refusal steering emotion steering as use cases. Additionally, we propose Adaptive Angular Steering, a selective variant that rotates only activations aligned with the target feature, further enhancing stability and coherence. Angular Steering generalizes existing addition and orthogonalization techniques under a unified geometric rotation framework, simplifying parameter selection and maintaining model stability across a broader range of adjustments. Experiments across multiple model families and sizes show that Angular Steering achieves robust behavioral control while maintaining general language modeling performance, underscoring its flexibility, generalization, and robustness compared to prior approaches. Code and artifacts are available at \\url{https://github.com/lone17/angular-steering/}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dGi2d5yDs4"
        ],
        "venue": [
          "/venue/dGi2d5yDs4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dGi2d5yDs4"
        ],
        "detail": [
          "https://openreview.net/forum?id=dGi2d5yDs4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Angular Steering: Behavior Control via Rotation in Activation Space [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hieu M. Vu , Tan Minh Nguyen Controlling specific behaviors in large language models while preserving their general capabilities is a central challenge for safe and reliable artificial intelligence deployment. Current steering methods, such as vector addition and directional ablation, are constrained within a two-dimensional subspace defined by the activation and feature direction, making them sensitive to chosen parameters and potentially affecting unrelated features due to unintended interactions in activation space. We introduce Angular Steering, a novel and flexible method for behavior modulation that operates by rotating activations within a fixed two-dimensional subspace. By formulating steering as a geometric rotation toward or away from a target behavior direction, Angular Steering provides continuous, fine-grained control over behaviors such as refusal and compliance. We demonstrate this method using refusal steering emotion steering as use cases. Additionally, we propose Adaptive Angular Steering, a selective variant that rotates only activations aligned with the target feature, further enhancing stability and coherence. Angular Steering generalizes existing addition and orthogonalization techniques under a unified geometric rotation framework, simplifying parameter selection and maintaining model stability across a broader range of adjustments. Experiments across multiple model families and sizes show that Angular Steering achieves robust behavioral control while maintaining general language modeling performance, underscoring its flexibility, generalization, and robustness compared to prior approaches. Code and artifacts are available at \\url{https://github.com/lone17/angular-steering/}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wOSZVnYH5w@OpenReview",
      "index": 298,
      "title": "COOPERA: Continual Open-Ended Human-Robot Assistance",
      "authors": [
        "Chenyang Ma",
        "Kai Lu",
        "Ruta Desai",
        "Xavier Puig",
        "Andrew Markham",
        "Niki Trigoni"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "coopera",
        "human",
        "ended",
        "robot",
        "traits",
        "hrc",
        "open",
        "humans",
        "continual",
        "assistance"
      ],
      "summary": "To understand and collaborate with humans, robots must account for individual human traits, habits, and activities over time. However, most robotic assistants lack these abilities, as they primarily focus on predefined tasks in structured environments and lack a human model to learn from. This work introduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot Assistance, where simulated humans, driven by psychological traits and long-term intentions, interact with robots in complex environments. By integrating continuous human feedback, our framework, for the first time, enables the study of long-term, open-ended human-robot collaboration (HRC) in different collaborative tasks across various time-scales. Within COOPERA, we introduce a benchmark and an approach to personalize the robot's collaborative actions by learning human traits and context-dependent intents. Experiments validate the extent to which our simulated humans reflect realistic human behaviors and demonstrate the value of inferring and personalizing to human intents for open-ended and long-term HRC.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wOSZVnYH5w"
        ],
        "venue": [
          "/venue/wOSZVnYH5w@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wOSZVnYH5w"
        ],
        "detail": [
          "https://openreview.net/forum?id=wOSZVnYH5w"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "COOPERA: Continual Open-Ended Human-Robot Assistance [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Chenyang Ma , Kai Lu , Ruta Desai , Xavier Puig , Andrew Markham , Niki Trigoni To understand and collaborate with humans, robots must account for individual human traits, habits, and activities over time. However, most robotic assistants lack these abilities, as they primarily focus on predefined tasks in structured environments and lack a human model to learn from. This work introduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot Assistance, where simulated humans, driven by psychological traits and long-term intentions, interact with robots in complex environments. By integrating continuous human feedback, our framework, for the first time, enables the study of long-term, open-ended human-robot collaboration (HRC) in different collaborative tasks across various time-scales. Within COOPERA, we introduce a benchmark and an approach to personalize the robot's collaborative actions by learning human traits and context-dependent intents. Experiments validate the extent to which our simulated humans reflect realistic human behaviors and demonstrate the value of inferring and personalizing to human intents for open-ended and long-term HRC. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VanvIu0KZU@OpenReview",
      "index": 299,
      "title": "Joint Hierarchical Representation Learning of Samples and Features via Informed Tree-Wasserstein Distance",
      "authors": [
        "Ya-Wei Eileen Lin",
        "Ronald R. Coifman",
        "Gal Mishne",
        "Ronen Talmon"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "twd",
        "hierarchical",
        "tree",
        "wasserstein",
        "mode",
        "twds",
        "samples",
        "distance",
        "method",
        "features"
      ],
      "summary": "High-dimensional data often exhibit hierarchical structures in both modes: samples and features. Yet, most existing approaches for hierarchical representation learning consider only one mode at a time. In this work, we propose an unsupervised method for jointly learning hierarchical representations of samples and features via Tree-Wasserstein Distance (TWD). Our method alternates between the two data modes. It first constructs a tree for one mode, then computes a TWD for the other mode based on that tree, and finally uses the resulting TWD to build the second mode’s tree. By repeatedly alternating through these steps, the method gradually refines both trees and the corresponding TWDs, capturing meaningful hierarchical representations of the data. We provide a theoretical analysis showing that our method converges. We show that our method can be integrated into hyperbolic graph convolutional networks as a pre-processing technique, improving performance in link prediction and node classification tasks. In addition, our method outperforms baselines in sparse approximation and unsupervised Wasserstein distance learning tasks on word-document and single-cell RNA-sequencing datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VanvIu0KZU"
        ],
        "venue": [
          "/venue/VanvIu0KZU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VanvIu0KZU"
        ],
        "detail": [
          "https://openreview.net/forum?id=VanvIu0KZU"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Joint Hierarchical Representation Learning of Samples and Features via Informed Tree-Wasserstein Distance [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ya-Wei Eileen Lin , Ronald R. Coifman , Gal Mishne , Ronen Talmon High-dimensional data often exhibit hierarchical structures in both modes: samples and features. Yet, most existing approaches for hierarchical representation learning consider only one mode at a time. In this work, we propose an unsupervised method for jointly learning hierarchical representations of samples and features via Tree-Wasserstein Distance (TWD). Our method alternates between the two data modes. It first constructs a tree for one mode, then computes a TWD for the other mode based on that tree, and finally uses the resulting TWD to build the second mode’s tree. By repeatedly alternating through these steps, the method gradually refines both trees and the corresponding TWDs, capturing meaningful hierarchical representations of the data. We provide a theoretical analysis showing that our method converges. We show that our method can be integrated into hyperbolic graph convolutional networks as a pre-processing technique, improving performance in link prediction and node classification tasks. In addition, our method outperforms baselines in sparse approximation and unsupervised Wasserstein distance learning tasks on word-document and single-cell RNA-sequencing datasets. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "27xTIAFbc6@OpenReview",
      "index": 300,
      "title": "Detecting Generated Images by Fitting Natural Image Distributions",
      "authors": [
        "Yonggang Zhang",
        "Jun Nie",
        "Xinmei Tian",
        "Mingming Gong",
        "Kun Zhang",
        "Bo Han"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "generated",
        "images",
        "natural",
        "manifold",
        "outputs",
        "extruding",
        "image",
        "differences",
        "misuse",
        "necessitating"
      ],
      "summary": "The increasing realism of generated images has raised significant concerns about their potential misuse, necessitating robust detection methods. Current approaches mainly rely on training binary classifiers, which depend heavily on the quantity and quality of available generated images. In this work, we propose a novel framework that exploits geometric differences between the data manifolds of natural and generated images. To exploit this difference, we employ a pair of functions engineered to yield consistent outputs for natural images but divergent outputs for generated ones, leveraging the property that their gradients reside in mutually orthogonal subspaces. This design enables a simple yet effective detection method: an image is identified as generated if a transformation along its data manifold induces a significant change in the loss value of a self-supervised model pre-trained on natural images. Further more, to address diminishing manifold disparities in advanced generative models, we leverage normalizing flows to amplify detectable differences by extruding generated images away from the natural image manifold. Extensive experiments demonstrate the efficacy of this method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=27xTIAFbc6"
        ],
        "venue": [
          "/venue/27xTIAFbc6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=27xTIAFbc6"
        ],
        "detail": [
          "https://openreview.net/forum?id=27xTIAFbc6"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Detecting Generated Images by Fitting Natural Image Distributions [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yonggang Zhang , Jun Nie , Xinmei Tian , Mingming Gong , Kun Zhang , Bo Han The increasing realism of generated images has raised significant concerns about their potential misuse, necessitating robust detection methods. Current approaches mainly rely on training binary classifiers, which depend heavily on the quantity and quality of available generated images. In this work, we propose a novel framework that exploits geometric differences between the data manifolds of natural and generated images. To exploit this difference, we employ a pair of functions engineered to yield consistent outputs for natural images but divergent outputs for generated ones, leveraging the property that their gradients reside in mutually orthogonal subspaces. This design enables a simple yet effective detection method: an image is identified as generated if a transformation along its data manifold induces a significant change in the loss value of a self-supervised model pre-trained on natural images. Further more, to address diminishing manifold disparities in advanced generative models, we leverage normalizing flows to amplify detectable differences by extruding generated images away from the natural image manifold. Extensive experiments demonstrate the efficacy of this method. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "K3xaVpSHkV@OpenReview",
      "index": 301,
      "title": "Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness",
      "authors": [
        "Bogdan Chornomaz",
        "Yonatan Koren",
        "Shay Moran",
        "Tom Waknine"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hanneke",
        "poisoning",
        "eta",
        "adversary",
        "error",
        "agnostic",
        "targeted",
        "learners",
        "karbasi",
        "mahmoody"
      ],
      "summary": "We study the problem of learning in the presence of an adversary that can corrupt an η η fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work established that the optimal error under such instance-targeted poisoning attacks scales as Θ ( d η ) Θ ( d η ) , where d d is the VC dimension of the hypothesis class [Hanneke, Karbasi, Mahmoody, Mehalel, and Moran (NeurIPS 2022)]. In this work, we resolve the corresponding question in the agnostic setting. We show that the optimal excess error is Θ ˜ ( d η − − √ ) Θ ~ ( d η ) , answering one of the main open problems left by Hanneke et al. To achieve this rate, it is necessary to use randomized learners: Hanneke et al.\\ showed that deterministic learners can be forced to suffer error close to 1 1 even under small amounts of poisoning. Perhaps surprisingly, our upper bound remains valid even when the learner’s random bits are fully visible to the adversary. In the other direction, our lower bound is stronger than standard PAC-style bounds: instead of tailoring a hard distribution separately for each sample size, we exhibit a single fixed distribution under which the adversary can enforce an excess error of Ω ( d η − − √ ) Ω ( d η ) infinitely often.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=K3xaVpSHkV"
        ],
        "venue": [
          "/venue/K3xaVpSHkV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=K3xaVpSHkV"
        ],
        "detail": [
          "https://openreview.net/forum?id=K3xaVpSHkV"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness [PDF ] [Copy] [Kimi ] [REL] Authors : Bogdan Chornomaz , Yonatan Koren , Shay Moran , Tom Waknine We study the problem of learning in the presence of an adversary that can corrupt an η η fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work established that the optimal error under such instance-targeted poisoning attacks scales as Θ ( d η ) Θ ( d η ) , where d d is the VC dimension of the hypothesis class [Hanneke, Karbasi, Mahmoody, Mehalel, and Moran (NeurIPS 2022)]. In this work, we resolve the corresponding question in the agnostic setting. We show that the optimal excess error is Θ ˜ ( d η − − √ ) Θ ~ ( d η ) , answering one of the main open problems left by Hanneke et al. To achieve this rate, it is necessary to use randomized learners: Hanneke et al.\\ showed that deterministic learners can be forced to suffer error close to 1 1 even under small amounts of poisoning. Perhaps surprisingly, our upper bound remains valid even when the learner’s random bits are fully visible to the adversary. In the other direction, our lower bound is stronger than standard PAC-style bounds: instead of tailoring a hard distribution separately for each sample size, we exhibit a single fixed distribution under which the adversary can enforce an excess error of Ω ( d η − − √ ) Ω ( d η ) infinitely often. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "y7ahj9RoXQ@OpenReview",
      "index": 302,
      "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints",
      "authors": [
        "Rui Xu",
        "Dakuan Lu",
        "Zicheng Zhao",
        "Xiaoyu Tan",
        "Xintao Wang",
        "Siyu Yuan",
        "Jiangjie Chen",
        "Xu Yinghui"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "mllms",
        "spatial",
        "origamispace",
        "step",
        "mathematical",
        "multimodal",
        "pattern",
        "constraints",
        "multi"
      ],
      "summary": "Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models (MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances, each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=y7ahj9RoXQ"
        ],
        "venue": [
          "/venue/y7ahj9RoXQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=y7ahj9RoXQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=y7ahj9RoXQ"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints [PDF ] [Copy] [Kimi ] [REL] Authors : Rui Xu , Dakuan Lu , Zicheng Zhao , Xiaoyu Tan , Xintao Wang , Siyu Yuan , Jiangjie Chen , Xu Yinghui Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models (MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances, each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "A4Xx9irvpp@OpenReview",
      "index": 303,
      "title": "Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel–Young Losses",
      "authors": [
        "Yuzhou Cao",
        "Han Bao",
        "Lei Feng",
        "Bo An"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "surrogate",
        "regret",
        "losses",
        "infimal",
        "convex",
        "smooth",
        "negentropy",
        "fenchel",
        "convolution",
        "linear"
      ],
      "summary": "Surrogate regret bounds, also known as excess risk bounds, bridge the gap between the convergence rates of surrogate and target losses. The regret transfer is lossless if the surrogate regret bound is linear. While convex smooth surrogate losses are appealing in particular due to the efficient estimation and optimization, the existence of a trade-off between the loss smoothness and linear regret bound has been believed in the community. Under this scenario, the better optimization and estimation properties of convex smooth surrogate losses may inevitably deteriorate after undergoing the regret transfer onto a target loss. We overcome this dilemma for arbitrary discrete target losses by constructing a convex smooth surrogate loss, which entails a linear surrogate regret bound composed with a tailored prediction link. The construction is based on Fenchel--Young losses generated by the *convolutional negentropy*, which are equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk. Consequently, the infimal convolution enables us to derive a smooth loss while maintaining the surrogate regret bound linear. We additionally benefit from the infimal convolution to have a consistent estimator of the underlying class probability. Our results are overall a novel demonstration of how convex analysis penetrates into optimization and statistical efficiency in risk minimization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=A4Xx9irvpp"
        ],
        "venue": [
          "/venue/A4Xx9irvpp@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=A4Xx9irvpp"
        ],
        "detail": [
          "https://openreview.net/forum?id=A4Xx9irvpp"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel–Young Losses [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yuzhou Cao , Han Bao , Lei Feng , Bo An Surrogate regret bounds, also known as excess risk bounds, bridge the gap between the convergence rates of surrogate and target losses. The regret transfer is lossless if the surrogate regret bound is linear. While convex smooth surrogate losses are appealing in particular due to the efficient estimation and optimization, the existence of a trade-off between the loss smoothness and linear regret bound has been believed in the community. Under this scenario, the better optimization and estimation properties of convex smooth surrogate losses may inevitably deteriorate after undergoing the regret transfer onto a target loss. We overcome this dilemma for arbitrary discrete target losses by constructing a convex smooth surrogate loss, which entails a linear surrogate regret bound composed with a tailored prediction link. The construction is based on Fenchel--Young losses generated by the *convolutional negentropy*, which are equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk. Consequently, the infimal convolution enables us to derive a smooth loss while maintaining the surrogate regret bound linear. We additionally benefit from the infimal convolution to have a consistent estimator of the underlying class probability. Our results are overall a novel demonstration of how convex analysis penetrates into optimization and statistical efficiency in risk minimization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "aDa0xEFDu1@OpenReview",
      "index": 304,
      "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation",
      "authors": [
        "Jingjing Jiang",
        "Chongjie Si",
        "Jun Luo",
        "Hanwang Zhang",
        "Chao Ma"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ulms",
        "corl",
        "textbf",
        "ulm",
        "multimodal",
        "reinforcement",
        "unified",
        "optimization",
        "understanding",
        "generation"
      ],
      "summary": "This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce \\textbf{CoRL}, a \\textbf{Co}-\\textbf{R}einforcement \\textbf{L}earning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, \\textbf{ULM-R1}, achieves average improvements of 7\\% on three text-to-image generation datasets and 23\\% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefits of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at \\url{https://github.com/mm-vl/ULM-R1}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aDa0xEFDu1"
        ],
        "venue": [
          "/venue/aDa0xEFDu1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aDa0xEFDu1"
        ],
        "detail": [
          "https://openreview.net/forum?id=aDa0xEFDu1"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Jingjing Jiang , Chongjie Si , Jun Luo , Hanwang Zhang , Chao Ma This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce \\textbf{CoRL}, a \\textbf{Co}-\\textbf{R}einforcement \\textbf{L}earning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, \\textbf{ULM-R1}, achieves average improvements of 7\\% on three text-to-image generation datasets and 23\\% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefits of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at \\url{https://github.com/mm-vl/ULM-R1}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "HKDyRDzy1E@OpenReview",
      "index": 305,
      "title": "Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models",
      "authors": [
        "Benjamin Walker",
        "Lingyi Yang",
        "Nicola Muca Cirone",
        "Cristopher Salvi",
        "Terry Lyons"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "matrices",
        "diagonal",
        "slices",
        "cdes",
        "walsh",
        "structured",
        "expressivity",
        "deltanet",
        "hadamard",
        "s4d"
      ],
      "summary": "This work introduces Structured Linear Controlled Differential Equations (SLiCEs), a unifying framework for sequence models with structured, input-dependent state-transition matrices that retain the maximal expressivity of dense matrices whilst being cheaper to compute. The framework encompasses existing architectures, such as input-dependent block-diagonal linear recurrent neural networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel variants based on sparsity and the Walsh-Hadamard transform. We prove that, unlike the diagonal state-transition matrices of S4D and Mamba, SLiCEs employing block-diagonal, sparse, or Walsh-Hadamard matrices match the maximal expressivity of dense matrices. Empirically, SLiCEs solve the A 5 A 5 state-tracking benchmark with a single layer, achieve best-in-class length generalisation on regular language tasks among parallel-in-time models, and match the performance of log neural controlled differential equations on six multivariate time-series classification datasets while cutting the average time per training step by a factor of twenty.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HKDyRDzy1E"
        ],
        "venue": [
          "/venue/HKDyRDzy1E@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HKDyRDzy1E"
        ],
        "detail": [
          "https://openreview.net/forum?id=HKDyRDzy1E"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Benjamin Walker , Lingyi Yang , Nicola Muca Cirone , Cristopher Salvi , Terry Lyons This work introduces Structured Linear Controlled Differential Equations (SLiCEs), a unifying framework for sequence models with structured, input-dependent state-transition matrices that retain the maximal expressivity of dense matrices whilst being cheaper to compute. The framework encompasses existing architectures, such as input-dependent block-diagonal linear recurrent neural networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel variants based on sparsity and the Walsh-Hadamard transform. We prove that, unlike the diagonal state-transition matrices of S4D and Mamba, SLiCEs employing block-diagonal, sparse, or Walsh-Hadamard matrices match the maximal expressivity of dense matrices. Empirically, SLiCEs solve the A 5 A 5 state-tracking benchmark with a single layer, achieve best-in-class length generalisation on regular language tasks among parallel-in-time models, and match the performance of log neural controlled differential equations on six multivariate time-series classification datasets while cutting the average time per training step by a factor of twenty. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5xPvWat3IX@OpenReview",
      "index": 306,
      "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding",
      "authors": [
        "Xiaoqian Shen",
        "Wenxuan Zhang",
        "Jun Chen",
        "Mohamed Elhoseiny"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "video",
        "lvlms",
        "vgent",
        "reasoning",
        "retrieval",
        "rag",
        "long",
        "understanding",
        "augmented",
        "clips"
      ],
      "summary": "Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel \\textbf{graph-based retrieval-reasoning-augmented generation framework} to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of 3.0 % ∼ 5.4 % 3.0 % ∼ 5.4 % over base models on MLVU, and outperformed state-of-the-art video RAG methods by 8.6 % 8.6 % . Our code is publicly available at https://xiaoqian-shen.github.io/Vgent.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5xPvWat3IX"
        ],
        "venue": [
          "/venue/5xPvWat3IX@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5xPvWat3IX"
        ],
        "detail": [
          "https://openreview.net/forum?id=5xPvWat3IX"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xiaoqian Shen , Wenxuan Zhang , Jun Chen , Mohamed Elhoseiny Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel \\textbf{graph-based retrieval-reasoning-augmented generation framework} to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of 3.0 % ∼ 5.4 % 3.0 % ∼ 5.4 % over base models on MLVU, and outperformed state-of-the-art video RAG methods by 8.6 % 8.6 % . Our code is publicly available at https://xiaoqian-shen.github.io/Vgent. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MiPIjE5onj@OpenReview",
      "index": 307,
      "title": "Privacy amplification by random allocation",
      "authors": [
        "Moshe Shenfeld",
        "Vitaly Feldman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "privacy",
        "amplification",
        "allocation",
        "scheme",
        "choquette",
        "guarantees",
        "random",
        "2024a",
        "choo",
        "sampling"
      ],
      "summary": "We consider the privacy amplification properties of a sampling scheme in which a user's data is used in k k steps chosen randomly and uniformly from a sequence (or set) of t t steps. This sampling scheme has been recently applied in the context of differentially private optimization [Chua et al., 2024a, Choquette-Choo et al., 2024] and is also motivated by communication-efficient high-dimensional private aggregation [Asi et al., 2025]. Existing analyses of this scheme either rely on privacy amplification by shuffling which leads to overly conservative bounds or require Monte Carlo simulations that are computationally prohibitive in most practical scenarios. We give the first theoretical guarantees and numerical estimation algorithms for this sampling scheme. In particular, we demonstrate that the privacy guarantees of random k k -out-of- t t allocation can be upper bounded by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability ( 1 + o ( 1 ) ) k / t ( 1 + o ( 1 ) ) k / t . Further, we provide two additional analysis techniques that lead to numerical improvements in several parameter regimes. Altogether, our bounds give efficiently-computable and nearly tight numerical results for random allocation applied to Gaussian noise addition.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MiPIjE5onj"
        ],
        "venue": [
          "/venue/MiPIjE5onj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MiPIjE5onj"
        ],
        "detail": [
          "https://openreview.net/forum?id=MiPIjE5onj"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Privacy amplification by random allocation [PDF ] [Copy] [Kimi ] [REL] Authors : Moshe Shenfeld , Vitaly Feldman We consider the privacy amplification properties of a sampling scheme in which a user's data is used in k k steps chosen randomly and uniformly from a sequence (or set) of t t steps. This sampling scheme has been recently applied in the context of differentially private optimization [Chua et al., 2024a, Choquette-Choo et al., 2024] and is also motivated by communication-efficient high-dimensional private aggregation [Asi et al., 2025]. Existing analyses of this scheme either rely on privacy amplification by shuffling which leads to overly conservative bounds or require Monte Carlo simulations that are computationally prohibitive in most practical scenarios. We give the first theoretical guarantees and numerical estimation algorithms for this sampling scheme. In particular, we demonstrate that the privacy guarantees of random k k -out-of- t t allocation can be upper bounded by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability ( 1 + o ( 1 ) ) k / t ( 1 + o ( 1 ) ) k / t . Further, we provide two additional analysis techniques that lead to numerical improvements in several parameter regimes. Altogether, our bounds give efficiently-computable and nearly tight numerical results for random allocation applied to Gaussian noise addition. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ThgoX1dMeM@OpenReview",
      "index": 308,
      "title": "Tackling Biased Evaluators in Dueling Bandits",
      "authors": [
        "Ming Tang",
        "Yuxuan Zhou",
        "Chao Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "evaluators",
        "dueling",
        "bias",
        "bandits",
        "feedback",
        "regret",
        "biased",
        "estimator",
        "unbiased",
        "case"
      ],
      "summary": "In dueling bandits, an agent explores and exploits choices (i.e., arms) by learning from their stochastic feedback in the form of relative preferences. Prior related studies focused on unbiased feedback. In practice, however, the feedback provided by evaluators can be biased. For example, human users are likely to provide biased evaluation towards large language models due to their heterogeneous background. In this work, we aim to minimize the regret in dueling bandits considering evaluators’ biased feedback. We begin with a benchmark case where evaluators’ bias information is known. Solving the known-bias case is nontrivial, because the bias cannot be easily decoupled from the feedback. We overcome this challenge and propose an unbiased arm performance estimator and a bias-sensitive dueling bandits algorithm. We manage to analyze the regret, dealing with the complex form of the estimator, and show that the feedback either matching or opposing the ground-truth reduces the regret. Then, we study the case where evaluators’ bias information is unknown. The associated estimator can hardly be solved in closed-form due to the non-convexity of the estimator solving problem. We address this challenge and propose an extended bias-sensitive algorithm by incorporating block coordinate descent. This algorithm is proven to achieve the same order of regret (as in the known bias case) with a bounded error. Experiments show that when compared with baselines, our algorithms reduces the regret by up to 86.9%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ThgoX1dMeM"
        ],
        "venue": [
          "/venue/ThgoX1dMeM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ThgoX1dMeM"
        ],
        "detail": [
          "https://openreview.net/forum?id=ThgoX1dMeM"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Tackling Biased Evaluators in Dueling Bandits [PDF ] [Copy] [Kimi ] [REL] Authors : Ming Tang , Yuxuan Zhou , Chao Huang In dueling bandits, an agent explores and exploits choices (i.e., arms) by learning from their stochastic feedback in the form of relative preferences. Prior related studies focused on unbiased feedback. In practice, however, the feedback provided by evaluators can be biased. For example, human users are likely to provide biased evaluation towards large language models due to their heterogeneous background. In this work, we aim to minimize the regret in dueling bandits considering evaluators’ biased feedback. We begin with a benchmark case where evaluators’ bias information is known. Solving the known-bias case is nontrivial, because the bias cannot be easily decoupled from the feedback. We overcome this challenge and propose an unbiased arm performance estimator and a bias-sensitive dueling bandits algorithm. We manage to analyze the regret, dealing with the complex form of the estimator, and show that the feedback either matching or opposing the ground-truth reduces the regret. Then, we study the case where evaluators’ bias information is unknown. The associated estimator can hardly be solved in closed-form due to the non-convexity of the estimator solving problem. We address this challenge and propose an extended bias-sensitive algorithm by incorporating block coordinate descent. This algorithm is proven to achieve the same order of regret (as in the known bias case) with a bounded error. Experiments show that when compared with baselines, our algorithms reduces the regret by up to 86.9%. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6FHvr5hJdd@OpenReview",
      "index": 309,
      "title": "Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization",
      "authors": [
        "Cong Wang",
        "Zexuan Deng",
        "Zhiwei Jiang",
        "Yafeng Yin",
        "Fei Shen",
        "Zifeng Cheng",
        "Shiping Ge",
        "Shiwei Gan",
        "Qing Gu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "signvip",
        "video",
        "condition",
        "slvg",
        "fsq",
        "sign",
        "language",
        "tokens",
        "generation",
        "spoken"
      ],
      "summary": "Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (e.g., skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporate multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (i.e., fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6FHvr5hJdd"
        ],
        "venue": [
          "/venue/6FHvr5hJdd@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6FHvr5hJdd"
        ],
        "detail": [
          "https://openreview.net/forum?id=6FHvr5hJdd"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Cong Wang , Zexuan Deng , Zhiwei Jiang , Yafeng Yin , Fei Shen , Zifeng Cheng , Shiping Ge , Shiwei Gan , Qing Gu Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (e.g., skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporate multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (i.e., fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "JIpKkzSqly@OpenReview",
      "index": 310,
      "title": "Transstratal Adversarial Attack: Compromising Multi-Layered Defenses in Text-to-Image Models",
      "authors": [
        "Chunlong Xie",
        "Kangjie Chen",
        "Shangwei Guo",
        "Shudong Zhang",
        "Tianwei Zhang",
        "Tao Xiang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "t2i",
        "transstratal",
        "adversarial",
        "layered",
        "defenses",
        "defense",
        "layers",
        "taa",
        "attack",
        "multi"
      ],
      "summary": "Modern Text-to-Image (T2I) models deploy multi-layered defenses to block Not-Safe-For-Work (NSFW) content generation. These defenses typically include sequential layers such as prompt filters, concept erasers and image filters. While existing adversarial attacks have demonstrated vulnerabilities in isolated defense layers, they prove largely ineffective against multi-layered defenses deployed in real-world T2I systems. In this paper, we demonstrate that exploiting overlapping vulnerabilities across these distinct defense layers enables adversaries to systematically bypass the entire safeguard of T2I systems. We propose Transstratal Adversarial Attack (TAA, a novel black-box framework to compromise T2I models with multi-layered protection. It generates transstratal adversarial prompts to evade all defense layers simultaneously. This is accomplished through transstratal adversarial candidate generation using LLMs to fulfill implicit and subjective adversarial requirements against different defense layers, combined with adversarial genetic optimization for efficient black-box search to maximize the bypass rates and generated image harmfulness. Evaluated across 14 T2I models (e.g., Stable Diffusion, DALL·E, and Midjourney) and 17 safety modules, our attack achieves an average attack success rate of 85.6\\%, surpassing state-of-the-art methods by 73.5\\%. Our findings challenge the isolated design of safety mechanisms and establish the first benchmark for holistic robustness evaluation in multi-layered safeguarded T2I models. The code can be found in https://github.com/Bluedask/TAA-T2I.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JIpKkzSqly"
        ],
        "venue": [
          "/venue/JIpKkzSqly@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JIpKkzSqly"
        ],
        "detail": [
          "https://openreview.net/forum?id=JIpKkzSqly"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Transstratal Adversarial Attack: Compromising Multi-Layered Defenses in Text-to-Image Models [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Chunlong Xie , Kangjie Chen , Shangwei Guo , Shudong Zhang , Tianwei Zhang , Tao Xiang Modern Text-to-Image (T2I) models deploy multi-layered defenses to block Not-Safe-For-Work (NSFW) content generation. These defenses typically include sequential layers such as prompt filters, concept erasers and image filters. While existing adversarial attacks have demonstrated vulnerabilities in isolated defense layers, they prove largely ineffective against multi-layered defenses deployed in real-world T2I systems. In this paper, we demonstrate that exploiting overlapping vulnerabilities across these distinct defense layers enables adversaries to systematically bypass the entire safeguard of T2I systems. We propose Transstratal Adversarial Attack (TAA, a novel black-box framework to compromise T2I models with multi-layered protection. It generates transstratal adversarial prompts to evade all defense layers simultaneously. This is accomplished through transstratal adversarial candidate generation using LLMs to fulfill implicit and subjective adversarial requirements against different defense layers, combined with adversarial genetic optimization for efficient black-box search to maximize the bypass rates and generated image harmfulness. Evaluated across 14 T2I models (e.g., Stable Diffusion, DALL·E, and Midjourney) and 17 safety modules, our attack achieves an average attack success rate of 85.6\\%, surpassing state-of-the-art methods by 73.5\\%. Our findings challenge the isolated design of safety mechanisms and establish the first benchmark for holistic robustness evaluation in multi-layered safeguarded T2I models. The code can be found in https://github.com/Bluedask/TAA-T2I. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Gibq7Wa7Bq@OpenReview",
      "index": 311,
      "title": "SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration",
      "authors": [
        "Elif Arslan",
        "Jacobus G. M. van der Linden",
        "Serge Hoogendoorn",
        "Marco Rinaldi",
        "Emir Demirović"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rashomon",
        "sortd",
        "trees",
        "anytime",
        "tree",
        "sets",
        "objective",
        "sparse",
        "decision",
        "sorted"
      ],
      "summary": "Sparse decision tree learning provides accurate and interpretable predictive models that are ideal for high-stakes applications by finding the single most accurate tree within a (soft) size limit. Rather than relying on a single “best” tree, Rashomon sets—trees with similar performance but varying structures—can be used to enhance variable importance analysis, enrich explanations, and enable users to choose simpler trees or those that satisfy stakeholder preferences (e.g., fairness) without hard-coding such criteria into the objective function. However, because finding the optimal tree is NP-hard, enumerating the Rashomon set is inherently challenging. Therefore, we introduce SORTD, a novel framework that improves scalability and enumerates trees in the Rashomon set in order of the objective value, thus offering anytime behavior. Our experiments show that SORTD reduces runtime by up to two orders of magnitude compared with the state of the art. Moreover, SORTD can compute Rashomon sets for any separable and totally ordered objective and supports post-evaluating the set using other separable (and partially ordered) objectives. Together, these advances make exploring Rashomon sets more practical in real-world applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Gibq7Wa7Bq"
        ],
        "venue": [
          "/venue/Gibq7Wa7Bq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Gibq7Wa7Bq"
        ],
        "detail": [
          "https://openreview.net/forum?id=Gibq7Wa7Bq"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration [PDF ] [Copy] [Kimi ] [REL] Authors : Elif Arslan , Jacobus G. M. van der Linden , Serge Hoogendoorn , Marco Rinaldi , Emir Demirović Sparse decision tree learning provides accurate and interpretable predictive models that are ideal for high-stakes applications by finding the single most accurate tree within a (soft) size limit. Rather than relying on a single “best” tree, Rashomon sets—trees with similar performance but varying structures—can be used to enhance variable importance analysis, enrich explanations, and enable users to choose simpler trees or those that satisfy stakeholder preferences (e.g., fairness) without hard-coding such criteria into the objective function. However, because finding the optimal tree is NP-hard, enumerating the Rashomon set is inherently challenging. Therefore, we introduce SORTD, a novel framework that improves scalability and enumerates trees in the Rashomon set in order of the objective value, thus offering anytime behavior. Our experiments show that SORTD reduces runtime by up to two orders of magnitude compared with the state of the art. Moreover, SORTD can compute Rashomon sets for any separable and totally ordered objective and supports post-evaluating the set using other separable (and partially ordered) objectives. Together, these advances make exploring Rashomon sets more practical in real-world applications. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "EyFrTjaYU3@OpenReview",
      "index": 312,
      "title": "TGA: True-to-Geometry Avatar Dynamic Reconstruction",
      "authors": [
        "Bo Guo",
        "Sijia Wen",
        "Ziwei Wang",
        "Yifan Zhao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tga",
        "avatar",
        "bvh",
        "gaussian",
        "reconstruction",
        "geometry",
        "dynamic",
        "pivoting",
        "perspective",
        "aware"
      ],
      "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have improved the visual fidelity of dynamic avatar reconstruction. However, existing methods often overlook the inherent chromatic similarity of human skin tones, leading to poor capture of intricate facial geometry under subtle appearance changes. This is caused by the affine approximation of Gaussian projection, which fails to be perspective-aware to depth-induced shear effects. To this end, we propose True-to-Geometry Avatar Dynamic Reconstruction (TGA), a perspective-aware 4D Gaussian avatar framework that sensitively captures fine-grained facial variations for accurate 3D geometry reconstruction. Specifically, to enable color-sensitive and geometry-consistent Gaussian representations under dynamic conditions, we introduce Perspective-Aware Gaussian Transformation that jointly models temporal deformations and spatial projection by integrating Jacobian-guided adaptive deformation into the homogeneous formulation. Furthermore, we develop Incremental BVH Tree Pivoting to enable fast frame-by-frame mesh extraction for 4D Gaussian representations. A dynamic Gaussian Bounding Volume Hierarchy (BVH) tree is used to model the topological relationships among points, where active ones are filtered out by BVH pivoting and subsequently re-triangulated for surface reconstruction. Extensive experiments demonstrate that TGA achieves superior geometric accuracy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EyFrTjaYU3"
        ],
        "venue": [
          "/venue/EyFrTjaYU3@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EyFrTjaYU3"
        ],
        "detail": [
          "https://openreview.net/forum?id=EyFrTjaYU3"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "TGA: True-to-Geometry Avatar Dynamic Reconstruction [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Bo Guo , Sijia Wen , Ziwei Wang , Yifan Zhao Recent advances in 3D Gaussian Splatting (3DGS) have improved the visual fidelity of dynamic avatar reconstruction. However, existing methods often overlook the inherent chromatic similarity of human skin tones, leading to poor capture of intricate facial geometry under subtle appearance changes. This is caused by the affine approximation of Gaussian projection, which fails to be perspective-aware to depth-induced shear effects. To this end, we propose True-to-Geometry Avatar Dynamic Reconstruction (TGA), a perspective-aware 4D Gaussian avatar framework that sensitively captures fine-grained facial variations for accurate 3D geometry reconstruction. Specifically, to enable color-sensitive and geometry-consistent Gaussian representations under dynamic conditions, we introduce Perspective-Aware Gaussian Transformation that jointly models temporal deformations and spatial projection by integrating Jacobian-guided adaptive deformation into the homogeneous formulation. Furthermore, we develop Incremental BVH Tree Pivoting to enable fast frame-by-frame mesh extraction for 4D Gaussian representations. A dynamic Gaussian Bounding Volume Hierarchy (BVH) tree is used to model the topological relationships among points, where active ones are filtered out by BVH pivoting and subsequently re-triangulated for surface reconstruction. Extensive experiments demonstrate that TGA achieves superior geometric accuracy. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Bi1udlTjMb@OpenReview",
      "index": 313,
      "title": "Neighbor-aware Contrastive Disambiguation for Cross-Modal Hashing with Redundant Annotations",
      "authors": [
        "Chao Su",
        "Likang Peng",
        "Yuan Sun",
        "Dezhong Peng",
        "Xi Peng",
        "Xu Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "redundant",
        "hashing",
        "labels",
        "annotations",
        "nacd",
        "modal",
        "noisy",
        "aware",
        "cross",
        "neighbor"
      ],
      "summary": "Cross-modal hashing aims to efficiently retrieve information across different modalities by mapping data into compact hash codes. However, most existing methods assume access to fully accurate supervision, which rarely holds in real-world scenarios. In fact, annotations are often redundant, i.e., each sample is associated with a set of candidate labels that includes both ground-truth labels and redundant noisy labels. Treating all annotated labels as equally valid introduces two critical issues: (1) the sparse presence of true labels within the label set is not explicitly addressed, leading to overfitting on redundant noisy annotations; (2) redundant noisy labels induce spurious similarities that distort semantic alignment across modalities and degrade the quality of the hash space. To address these challenges, we propose that effective cross-modal hashing requires explicitly identifying and leveraging the true label subset within all annotations. Based on this insight, we present Neighbor-aware Contrastive Disambiguation (NACD), a novel framework designed for robust learning under redundant supervision. NACD consists of two key components. The first, Neighbor-aware Confidence Reconstruction (NACR), refines label confidence by aggregating information from cross-modal neighbors to distinguish true labels from redundant noisy ones. The second, Class-aware Robust Contrastive Hashing (CRCH), constructs reliable positive and negative pairs based on label confidence scores, thereby significantly enhancing robustness against noisy supervision. Moreover, to effectively reduce the quantization error, we incorporate a quantization loss that enforces binary constraints on the learned hash representations. Extensive experiments conducted on three large-scale multimodal benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches, thereby establishing a new standard for cross-modal hashing with redundant annotations. Code is available at https://github.com/Rose-bud/NACD.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Bi1udlTjMb"
        ],
        "venue": [
          "/venue/Bi1udlTjMb@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Bi1udlTjMb"
        ],
        "detail": [
          "https://openreview.net/forum?id=Bi1udlTjMb"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Neighbor-aware Contrastive Disambiguation for Cross-Modal Hashing with Redundant Annotations [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Chao Su , Likang Peng , Yuan Sun , Dezhong Peng , Xi Peng , Xu Wang Cross-modal hashing aims to efficiently retrieve information across different modalities by mapping data into compact hash codes. However, most existing methods assume access to fully accurate supervision, which rarely holds in real-world scenarios. In fact, annotations are often redundant, i.e., each sample is associated with a set of candidate labels that includes both ground-truth labels and redundant noisy labels. Treating all annotated labels as equally valid introduces two critical issues: (1) the sparse presence of true labels within the label set is not explicitly addressed, leading to overfitting on redundant noisy annotations; (2) redundant noisy labels induce spurious similarities that distort semantic alignment across modalities and degrade the quality of the hash space. To address these challenges, we propose that effective cross-modal hashing requires explicitly identifying and leveraging the true label subset within all annotations. Based on this insight, we present Neighbor-aware Contrastive Disambiguation (NACD), a novel framework designed for robust learning under redundant supervision. NACD consists of two key components. The first, Neighbor-aware Confidence Reconstruction (NACR), refines label confidence by aggregating information from cross-modal neighbors to distinguish true labels from redundant noisy ones. The second, Class-aware Robust Contrastive Hashing (CRCH), constructs reliable positive and negative pairs based on label confidence scores, thereby significantly enhancing robustness against noisy supervision. Moreover, to effectively reduce the quantization error, we incorporate a quantization loss that enforces binary constraints on the learned hash representations. Extensive experiments conducted on three large-scale multimodal benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches, thereby establishing a new standard for cross-modal hashing with redundant annotations. Code is available at https://github.com/Rose-bud/NACD. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nKuFQhKZtt@OpenReview",
      "index": 314,
      "title": "Minimax Adaptive Online Nonparametric Regression over Besov spaces",
      "authors": [
        "Paul Liautaud",
        "Pierre Gaillard",
        "Olivier Wintenberger"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "adaptive",
        "besov",
        "minimax",
        "online",
        "regression",
        "smoothness",
        "regret",
        "spaces",
        "leq",
        "nonparametric"
      ],
      "summary": "We study online adversarial regression with convex losses against a rich class of continuous yet highly irregular competitor functions,% prediction rules, modeled by Besov spaces B s p q B p q s with general parameters 1 ≤ p , q ≤ ∞ 1 ≤ p , q ≤ ∞ and smoothness s > d p s > d p . We introduce an adaptive wavelet-based algorithm that performs sequential prediction without prior knowledge of ( s , p , q ) ( s , p , q ) , and establish minimax-optimal regret bounds against any comparator in B s p q B p q s . We further design a locally adaptive extension capable of sequentially adapting to spatially inhomogeneous smoothness. This adaptive mechanism adjusts the resolution of the predictions over both time and space, yielding refined regret bounds in terms of local regularity. Consequently, in heterogeneous environments, our adaptive guarantees can significantly surpass those obtained by standard global methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nKuFQhKZtt"
        ],
        "venue": [
          "/venue/nKuFQhKZtt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nKuFQhKZtt"
        ],
        "detail": [
          "https://openreview.net/forum?id=nKuFQhKZtt"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Minimax Adaptive Online Nonparametric Regression over Besov spaces [PDF ] [Copy] [Kimi ] [REL] Authors : Paul Liautaud , Pierre Gaillard , Olivier Wintenberger We study online adversarial regression with convex losses against a rich class of continuous yet highly irregular competitor functions,% prediction rules, modeled by Besov spaces B s p q B p q s with general parameters 1 ≤ p , q ≤ ∞ 1 ≤ p , q ≤ ∞ and smoothness s > d p s > d p . We introduce an adaptive wavelet-based algorithm that performs sequential prediction without prior knowledge of ( s , p , q ) ( s , p , q ) , and establish minimax-optimal regret bounds against any comparator in B s p q B p q s . We further design a locally adaptive extension capable of sequentially adapting to spatially inhomogeneous smoothness. This adaptive mechanism adjusts the resolution of the predictions over both time and space, yielding refined regret bounds in terms of local regularity. Consequently, in heterogeneous environments, our adaptive guarantees can significantly surpass those obtained by standard global methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "APXcX7z1Bi@OpenReview",
      "index": 315,
      "title": "UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection",
      "authors": [
        "Jigang Fan",
        "QuanLin Wu",
        "Shengjie Luo",
        "Liwei Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ligand",
        "unisite",
        "binding",
        "site",
        "end",
        "protein",
        "detection",
        "datasets",
        "prediction",
        "iou"
      ],
      "summary": "The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein–ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=APXcX7z1Bi"
        ],
        "venue": [
          "/venue/APXcX7z1Bi@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=APXcX7z1Bi"
        ],
        "detail": [
          "https://openreview.net/forum?id=APXcX7z1Bi"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jigang Fan , QuanLin Wu , Shengjie Luo , Liwei Wang The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein–ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "g3EF5XsapH@OpenReview",
      "index": 316,
      "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model",
      "authors": [
        "Zhe Li",
        "Xiang Bai",
        "Jieyu Zhang",
        "Zhuangzhe Wu",
        "Che Xu",
        "Ying Li",
        "Chengkai Hou",
        "Shanghang Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "urdf",
        "anything",
        "multimodal",
        "articulated",
        "kinematic",
        "constructing",
        "twins",
        "segmentation",
        "objects",
        "robotic"
      ],
      "summary": "Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \\textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized [SEG] token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\\% improvement), kinematic parameter prediction (average error reduction of 29\\%), and physical executability (surpassing baselines by 50\\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=g3EF5XsapH"
        ],
        "venue": [
          "/venue/g3EF5XsapH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=g3EF5XsapH"
        ],
        "detail": [
          "https://openreview.net/forum?id=g3EF5XsapH"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zhe Li , Xiang Bai , Jieyu Zhang , Zhuangzhe Wu , Che Xu , Ying Li , Chengkai Hou , Shanghang Zhang Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \\textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized [SEG] token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\\% improvement), kinematic parameter prediction (average error reduction of 29\\%), and physical executability (surpassing baselines by 50\\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "PoIhCjqzn0@OpenReview",
      "index": 317,
      "title": "L2DGCN: Learnable Enhancement and Label Selection Dynamic Graph Convolutional Networks for Mitigating Degree Bias",
      "authors": [
        "jingxiao zhang",
        "Shifei Ding",
        "Jian Zhang",
        "Lili Guo",
        "Xuan Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "l2dgcn",
        "label",
        "graph",
        "learnable",
        "enhancement",
        "nodes",
        "degree",
        "node",
        "teacher",
        "model"
      ],
      "summary": "Graph Neural Networks (GNNs) are powerful models for node classification, but their performance is heavily reliant on manually labeled data, which is often costly and results in insufficient labeling. Recent studies have shown that message-passing neural networks struggle to propagate information in low-degree nodes, negatively affecting overall performance. To address the information bias caused by degree imbalance, we propose a Learnable Enhancement and Label Selection Dynamic Graph Convolutional Network (L2DGCN). L2DGCN consists of a teacher model and a student model. The teacher model employs an improved label propagation mechanism that enables remote label information dissemination among all nodes. The student model introduces a dynamically learnable graph enhancement strategy, perturbing edges to facilitate information exchange among low-degree nodes. This approach maintains the global graph structure while learning graph representations. Additionally, we have designed a label selector to mitigate the impact of unreliable pseudo-labels on model learning. To validate the effectiveness of our proposed model with limited labeled data, we conducted comprehensive evaluations of semi-supervised node classification across various scenarios with a limited number of annotated nodes. Experimental results demonstrate that our data enhancement model significantly contributes to node classification tasks under sparse labeling conditions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PoIhCjqzn0"
        ],
        "venue": [
          "/venue/PoIhCjqzn0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PoIhCjqzn0"
        ],
        "detail": [
          "https://openreview.net/forum?id=PoIhCjqzn0"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "L2DGCN: Learnable Enhancement and Label Selection Dynamic Graph Convolutional Networks for Mitigating Degree Bias [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : jingxiao zhang , Shifei Ding , Jian Zhang , Lili Guo , Xuan Li Graph Neural Networks (GNNs) are powerful models for node classification, but their performance is heavily reliant on manually labeled data, which is often costly and results in insufficient labeling. Recent studies have shown that message-passing neural networks struggle to propagate information in low-degree nodes, negatively affecting overall performance. To address the information bias caused by degree imbalance, we propose a Learnable Enhancement and Label Selection Dynamic Graph Convolutional Network (L2DGCN). L2DGCN consists of a teacher model and a student model. The teacher model employs an improved label propagation mechanism that enables remote label information dissemination among all nodes. The student model introduces a dynamically learnable graph enhancement strategy, perturbing edges to facilitate information exchange among low-degree nodes. This approach maintains the global graph structure while learning graph representations. Additionally, we have designed a label selector to mitigate the impact of unreliable pseudo-labels on model learning. To validate the effectiveness of our proposed model with limited labeled data, we conducted comprehensive evaluations of semi-supervised node classification across various scenarios with a limited number of annotated nodes. Experimental results demonstrate that our data enhancement model significantly contributes to node classification tasks under sparse labeling conditions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "9eIntNc69t@OpenReview",
      "index": 318,
      "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
      "authors": [
        "Gunshi Gupta",
        "Karmesh Yadav",
        "Zsolt Kira",
        "Yarin Gal",
        "Rahaf Aljundi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "memo",
        "embodied",
        "memory",
        "context",
        "transformer",
        "memories",
        "training",
        "agents",
        "reinforcement",
        "transformers"
      ],
      "summary": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo’s effectiveness on a grid-world meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9eIntNc69t"
        ],
        "venue": [
          "/venue/9eIntNc69t@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9eIntNc69t"
        ],
        "detail": [
          "https://openreview.net/forum?id=9eIntNc69t"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Gunshi Gupta , Karmesh Yadav , Zsolt Kira , Yarin Gal , Rahaf Aljundi To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo’s effectiveness on a grid-world meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4p28lkk44b@OpenReview",
      "index": 319,
      "title": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data",
      "authors": [
        "Tianyi Chen",
        "Pengxiao Lin",
        "Zhiwei Wang",
        "Zhi-Qin John Xu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mamba",
        "heel",
        "symmetrical",
        "achilles",
        "synthetic",
        "tasks",
        "convolution",
        "architecture",
        "sequences",
        "limitations"
      ],
      "summary": "State Space Models (SSMs) have emerged as promising alternatives to attention mechanisms, with the Mamba architecture demonstrating impressive performance and linear complexity for processing long sequences. However, the fundamental differences between Mamba and Transformer architectures remain incompletely understood. In this work, we use carefully designed synthetic tasks to reveal Mamba's inherent limitations. Through experiments, we identify that Mamba's nonlinear convolution introduces an asymmetry bias that significantly impairs its ability to recognize symmetrical patterns and relationships. Using composite function and inverse sequence matching tasks, we demonstrate that Mamba strongly favors compositional solutions over symmetrical ones and struggles with tasks requiring the matching of reversed sequences. We show these limitations stem not from the SSM module itself but from the nonlinear convolution preceding it, which fuses token information asymmetrically. These insights provide a new understanding of Mamba's constraints and suggest concrete architectural improvements for future sequence models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4p28lkk44b"
        ],
        "venue": [
          "/venue/4p28lkk44b@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4p28lkk44b"
        ],
        "detail": [
          "https://openreview.net/forum?id=4p28lkk44b"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Tianyi Chen , Pengxiao Lin , Zhiwei Wang , Zhi-Qin John Xu State Space Models (SSMs) have emerged as promising alternatives to attention mechanisms, with the Mamba architecture demonstrating impressive performance and linear complexity for processing long sequences. However, the fundamental differences between Mamba and Transformer architectures remain incompletely understood. In this work, we use carefully designed synthetic tasks to reveal Mamba's inherent limitations. Through experiments, we identify that Mamba's nonlinear convolution introduces an asymmetry bias that significantly impairs its ability to recognize symmetrical patterns and relationships. Using composite function and inverse sequence matching tasks, we demonstrate that Mamba strongly favors compositional solutions over symmetrical ones and struggles with tasks requiring the matching of reversed sequences. We show these limitations stem not from the SSM module itself but from the nonlinear convolution preceding it, which fuses token information asymmetrically. These insights provide a new understanding of Mamba's constraints and suggest concrete architectural improvements for future sequence models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QwXpn5IPKk@OpenReview",
      "index": 320,
      "title": "RepLDM: Reprogramming Pretrained Latent Diffusion Models for High-Quality, High-Efficiency, High-Resolution Image Generation",
      "authors": [
        "Boyuan Cao",
        "Jiaxin Ye",
        "Yujie Wei",
        "Hongming Shan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "repldm",
        "reprogramming",
        "upsampling",
        "latent",
        "image",
        "quality",
        "resolution",
        "generation",
        "ldms",
        "pretrained"
      ],
      "summary": "While latent diffusion models (LDMs), such as Stable Diffusion, are designed for high-resolution image generation, they often struggle with significant structural distortions when generating images at resolutions higher than their training one. Instead of relying on extensive retraining, a more resource-efficient approach is to reprogram the pretrained model for high-resolution (HR) image generation; however, existing methods often result in poor image quality and long inference time. We introduce RepLDM, a novel reprogramming framework for pretrained LDMs that enables high-quality, high-efficiency, high-resolution image generation; see Fig. 1. RepLDM consists of two stages: (i) an attention guidance stage, which generates a latent representation of a higher-quality training-resolution image using a novel parameter-free self-attention mechanism to enhance the structural consistency; and (ii) a progressive upsampling stage, which progressively performs upsampling in pixel space to mitigate the severe artifacts caused by latent space upsampling. The effective initialization from the first stage allows for denoising at higher resolutions with significantly fewer steps, improving the efficiency. Extensive experimental results demonstrate that RepLDM significantly outperforms state-of-the-art methods in both quality and efficiency for HR image generation, underscoring its advantages for real-world applications. Codes: https://github.com/kmittle/RepLDM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QwXpn5IPKk"
        ],
        "venue": [
          "/venue/QwXpn5IPKk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QwXpn5IPKk"
        ],
        "detail": [
          "https://openreview.net/forum?id=QwXpn5IPKk"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "RepLDM: Reprogramming Pretrained Latent Diffusion Models for High-Quality, High-Efficiency, High-Resolution Image Generation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Boyuan Cao , Jiaxin Ye , Yujie Wei , Hongming Shan While latent diffusion models (LDMs), such as Stable Diffusion, are designed for high-resolution image generation, they often struggle with significant structural distortions when generating images at resolutions higher than their training one. Instead of relying on extensive retraining, a more resource-efficient approach is to reprogram the pretrained model for high-resolution (HR) image generation; however, existing methods often result in poor image quality and long inference time. We introduce RepLDM, a novel reprogramming framework for pretrained LDMs that enables high-quality, high-efficiency, high-resolution image generation; see Fig. 1. RepLDM consists of two stages: (i) an attention guidance stage, which generates a latent representation of a higher-quality training-resolution image using a novel parameter-free self-attention mechanism to enhance the structural consistency; and (ii) a progressive upsampling stage, which progressively performs upsampling in pixel space to mitigate the severe artifacts caused by latent space upsampling. The effective initialization from the first stage allows for denoising at higher resolutions with significantly fewer steps, improving the efficiency. Extensive experimental results demonstrate that RepLDM significantly outperforms state-of-the-art methods in both quality and efficiency for HR image generation, underscoring its advantages for real-world applications. Codes: https://github.com/kmittle/RepLDM. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "853SwC2dMZ@OpenReview",
      "index": 321,
      "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
      "authors": [
        "Zhixuan Pan",
        "Shaowen Wang",
        "Liao Pengfei",
        "Jian Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "behaviors",
        "compression",
        "laws",
        "knowledge",
        "llm",
        "llms",
        "scaling",
        "hallucinations",
        "acquisition",
        "principled"
      ],
      "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap’s and Zipf’s laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=853SwC2dMZ"
        ],
        "venue": [
          "/venue/853SwC2dMZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=853SwC2dMZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=853SwC2dMZ"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Zhixuan Pan , Shaowen Wang , Liao Pengfei , Jian Li Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap’s and Zipf’s laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "y8VWYf5cVI@OpenReview",
      "index": 322,
      "title": "Differentiable Hierarchical Visual Tokenization",
      "authors": [
        "Marius Aasan",
        "Martine Hjelkrem-Tan",
        "Nico Catalano",
        "Changkyu Choi",
        "Adín Ramírez Rivera"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "differentiable",
        "hierarchical",
        "tokenization",
        "retrofitting",
        "tokenizer",
        "raster",
        "end",
        "ignore",
        "adapts",
        "granularity"
      ],
      "summary": "Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=y8VWYf5cVI"
        ],
        "venue": [
          "/venue/y8VWYf5cVI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=y8VWYf5cVI"
        ],
        "detail": [
          "https://openreview.net/forum?id=y8VWYf5cVI"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Differentiable Hierarchical Visual Tokenization [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Marius Aasan , Martine Hjelkrem-Tan , Nico Catalano , Changkyu Choi , Adín Ramírez Rivera Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QtnCPZMxYg@OpenReview",
      "index": 323,
      "title": "Trajectory Graph Learning: Aligning with Long Trajectories in Reinforcement Learning Without Reward Design",
      "authors": [
        "Yunfan Li",
        "Eric Liu",
        "Lin Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "trajectories",
        "tgl",
        "expert",
        "trajectory",
        "aligning",
        "graph",
        "learning",
        "demonstrations",
        "imitation"
      ],
      "summary": "Reinforcement learning (RL) often relies on manually designed reward functions, which are difficult to specify and can lead to issues such as reward hacking and suboptimal behavior. Alternatives like inverse RL and preference-based RL attempt to infer surrogate rewards from demonstrations or preferences but suffer from ambiguity and distribution mismatch. A more direct approach, inspired by imitation learning, avoids reward modeling by leveraging expert demonstrations. However, most existing methods align actions only at individual states, failing to capture the coherence of long-horizon trajectories. In this work, we study the problem of directly aligning policies with expert-labeled trajectories to preserve long-horizon behavior without relying on reward signals. Specifically, we aim to learn a policy that maximizes the probability of generating the expert trajectories. Nevertheless, we prove that, in its general form, this trajectory alignment problem is NP-complete. To address this, we propose Trajectory Graph Learning (TGL), a framework that leverages structural assumptions commonly satisfied in practice—such as bounded realizability of expert trajectories or a tree-structured MDP. These enable a graph-based policy planning algorithm that computes optimal policies in polynomial time under known dynamics. For settings with unknown dynamics, we develop a sample-efficient algorithm based on UCB-style exploration and establish sub-linear regret. Experiments on grid-world tasks demonstrate that TGL substantially outperforms standard imitation learning methods for long-trajectory planning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QtnCPZMxYg"
        ],
        "venue": [
          "/venue/QtnCPZMxYg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QtnCPZMxYg"
        ],
        "detail": [
          "https://openreview.net/forum?id=QtnCPZMxYg"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Trajectory Graph Learning: Aligning with Long Trajectories in Reinforcement Learning Without Reward Design [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Yunfan Li , Eric Liu , Lin Yang Reinforcement learning (RL) often relies on manually designed reward functions, which are difficult to specify and can lead to issues such as reward hacking and suboptimal behavior. Alternatives like inverse RL and preference-based RL attempt to infer surrogate rewards from demonstrations or preferences but suffer from ambiguity and distribution mismatch. A more direct approach, inspired by imitation learning, avoids reward modeling by leveraging expert demonstrations. However, most existing methods align actions only at individual states, failing to capture the coherence of long-horizon trajectories. In this work, we study the problem of directly aligning policies with expert-labeled trajectories to preserve long-horizon behavior without relying on reward signals. Specifically, we aim to learn a policy that maximizes the probability of generating the expert trajectories. Nevertheless, we prove that, in its general form, this trajectory alignment problem is NP-complete. To address this, we propose Trajectory Graph Learning (TGL), a framework that leverages structural assumptions commonly satisfied in practice—such as bounded realizability of expert trajectories or a tree-structured MDP. These enable a graph-based policy planning algorithm that computes optimal policies in polynomial time under known dynamics. For settings with unknown dynamics, we develop a sample-efficient algorithm based on UCB-style exploration and establish sub-linear regret. Experiments on grid-world tasks demonstrate that TGL substantially outperforms standard imitation learning methods for long-trajectory planning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "aYRNINhNGV@OpenReview",
      "index": 324,
      "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
      "authors": [
        "Yung-Sung Chuang",
        "Yang Li",
        "Dong Wang",
        "Ching-Feng Yeh",
        "Kehan Lyu",
        "Ramya Raghavendra",
        "James R. Glass",
        "LIFEI HUANG",
        "Jason E Weston",
        "Luke Zettlemoyer",
        "Xinlei Chen",
        "Zhuang Liu",
        "Saining Xie",
        "Wen-tau Yih",
        "Shang-Wen Li",
        "Hu Xu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "clip",
        "english",
        "worldwide",
        "recipe",
        "meta",
        "cvqa",
        "metaclip",
        "xm3600",
        "image",
        "text"
      ],
      "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval. Code and model are available at https://github.com/facebookresearch/MetaCLIP.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aYRNINhNGV"
        ],
        "venue": [
          "/venue/aYRNINhNGV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aYRNINhNGV"
        ],
        "detail": [
          "https://openreview.net/forum?id=aYRNINhNGV"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Meta CLIP 2: A Worldwide Scaling Recipe [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yung-Sung Chuang , Yang Li , Dong Wang , Ching-Feng Yeh , Kehan Lyu , Ramya Raghavendra , James R. Glass , LIFEI HUANG , Jason E Weston , Luke Zettlemoyer , Xinlei Chen , Zhuang Liu , Saining Xie , Wen-tau Yih , Shang-Wen Li , Hu Xu Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval. Code and model are available at https://github.com/facebookresearch/MetaCLIP. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xmYT1JqVpj@OpenReview",
      "index": 325,
      "title": "Policy Compatible Skill Incremental Learning via Lazy Learning Interface",
      "authors": [
        "Daehee Lee",
        "Dongsu Lee",
        "TaeYoon Kwack",
        "Wonje Choi",
        "Honguk Woo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sil",
        "skill",
        "policies",
        "policy",
        "skills",
        "lazy",
        "compatibility",
        "subtask",
        "incremental",
        "downstream"
      ],
      "summary": "Skill Incremental Learning (SIL) is the process by which an embodied agent expands and refines its skill set over time by leveraging experience gained through interaction with its environment or by the integration of additional data. SIL facilitates efficient acquisition of hierarchical policies grounded in reusable skills for downstream tasks. However, as the skill repertoire evolves, it can disrupt compatibility with existing skill-based policies, limiting their reusability and generalization. In this work, we propose SIL-C, a novel framework that ensures skill-policy compatibility, allowing improvements in incrementally learned skills to enhance the performance of downstream policies without requiring policy re-training or structural adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to dynamically align the subtask space referenced by policies with the skill space decoded into agent behaviors. This enables each subtask, derived from the policy's decomposition of a complex task, to be executed by selecting an appropriate skill based on trajectory distribution similarity. We evaluate SIL-C across diverse SIL scenarios and demonstrate that it maintains compatibility between evolving skills and downstream policies while ensuring efficiency throughout the learning process.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xmYT1JqVpj"
        ],
        "venue": [
          "/venue/xmYT1JqVpj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xmYT1JqVpj"
        ],
        "detail": [
          "https://openreview.net/forum?id=xmYT1JqVpj"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Policy Compatible Skill Incremental Learning via Lazy Learning Interface [PDF ] [Copy] [Kimi ] [REL] Authors : Daehee Lee , Dongsu Lee , TaeYoon Kwack , Wonje Choi , Honguk Woo Skill Incremental Learning (SIL) is the process by which an embodied agent expands and refines its skill set over time by leveraging experience gained through interaction with its environment or by the integration of additional data. SIL facilitates efficient acquisition of hierarchical policies grounded in reusable skills for downstream tasks. However, as the skill repertoire evolves, it can disrupt compatibility with existing skill-based policies, limiting their reusability and generalization. In this work, we propose SIL-C, a novel framework that ensures skill-policy compatibility, allowing improvements in incrementally learned skills to enhance the performance of downstream policies without requiring policy re-training or structural adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to dynamically align the subtask space referenced by policies with the skill space decoded into agent behaviors. This enables each subtask, derived from the policy's decomposition of a complex task, to be executed by selecting an appropriate skill based on trajectory distribution similarity. We evaluate SIL-C across diverse SIL scenarios and demonstrate that it maintains compatibility between evolving skills and downstream policies while ensuring efficiency throughout the learning process. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "lNPo3FAMsl@OpenReview",
      "index": 326,
      "title": "Estimating cognitive biases with attention-aware inverse planning",
      "authors": [
        "Sounak Banerjee",
        "Daphne Cornelisse",
        "Deepak Edakkattil Gopinath",
        "Emily Sumner",
        "Jonathan DeCastro",
        "Guy Rosman",
        "Eugene Vinitsky",
        "Mark K Ho"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cognitive",
        "biases",
        "aware",
        "inverse",
        "planning",
        "attention",
        "attentional",
        "people",
        "estimating",
        "goal"
      ],
      "summary": "People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the \\textit{attention-aware inverse planning problem}, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lNPo3FAMsl"
        ],
        "venue": [
          "/venue/lNPo3FAMsl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lNPo3FAMsl"
        ],
        "detail": [
          "https://openreview.net/forum?id=lNPo3FAMsl"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Estimating cognitive biases with attention-aware inverse planning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Sounak Banerjee , Daphne Cornelisse , Deepak Edakkattil Gopinath , Emily Sumner , Jonathan DeCastro , Guy Rosman , Eugene Vinitsky , Mark K Ho People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the \\textit{attention-aware inverse planning problem}, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VymXLPX6Ps@OpenReview",
      "index": 327,
      "title": "An Efficient Orlicz-Sobolev Approach for Transporting Unbalanced Measures on a Graph",
      "authors": [
        "Tam Le",
        "Truyen Nguyen",
        "Hideitsu Hino",
        "Kenji Fukumizu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ept",
        "orlicz",
        "ost",
        "gst",
        "measures",
        "sobolev",
        "transport",
        "graph",
        "emph",
        "unbalanced"
      ],
      "summary": "We investigate optimal transport (OT) for measures on graph metric spaces with different total masses. To mitigate the limitations of traditional L p L p geometry, Orlicz-Wasserstein (OW) and generalized Sobolev transport (GST) employ \\emph{Orlicz geometric structure}, leveraging convex functions to capture nuanced geometric relationships and remarkably contribute to advance certain machine learning approaches. However, both OW and GST are restricted to measures with equal total mass, limiting their applicability to real-world scenarios where mass variation is common, and input measures may have noisy supports, or outliers. To address unbalanced measures, OW can either incorporate mass constraints or marginal discrepancy penalization, but this leads to a more complex two-level optimization problem. Additionally, GST provides a scalable yet rigid framework, which poses significant challenges to extend GST to accommodate nonnegative measures. To tackle these challenges, in this work we revisit the entropy partial transport (EPT) problem. By exploiting Caffarelli \\& McCann's insights, we develop a novel variant of EPT endowed with Orlicz geometric structure, called \\emph{Orlicz-EPT}. We establish theoretical background to solve Orlicz-EPT using a binary search algorithmic approach. Especially, by leveraging the dual EPT and the underlying graph structure, we formulate a novel regularization approach that leads to the proposed \\emph{Orlicz-Sobolev transport} (OST). Notably, we demonstrate that OST can be efficiently computed by simply solving a univariate optimization problem, in stark contrast to the intensive computation needed for Orlicz-EPT. Building on this, we derive geometric structures for OST and draw its connections to other transport distances. We empirically illustrate that OST is several-order faster than Orlicz-EPT. Furthermore, we show preliminary evidence on the advantages of OST for measures on a graph in document classification and topological data analysis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VymXLPX6Ps"
        ],
        "venue": [
          "/venue/VymXLPX6Ps@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VymXLPX6Ps"
        ],
        "detail": [
          "https://openreview.net/forum?id=VymXLPX6Ps"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "An Efficient Orlicz-Sobolev Approach for Transporting Unbalanced Measures on a Graph [PDF ] [Copy] [Kimi ] [REL] Authors : Tam Le , Truyen Nguyen , Hideitsu Hino , Kenji Fukumizu We investigate optimal transport (OT) for measures on graph metric spaces with different total masses. To mitigate the limitations of traditional L p L p geometry, Orlicz-Wasserstein (OW) and generalized Sobolev transport (GST) employ \\emph{Orlicz geometric structure}, leveraging convex functions to capture nuanced geometric relationships and remarkably contribute to advance certain machine learning approaches. However, both OW and GST are restricted to measures with equal total mass, limiting their applicability to real-world scenarios where mass variation is common, and input measures may have noisy supports, or outliers. To address unbalanced measures, OW can either incorporate mass constraints or marginal discrepancy penalization, but this leads to a more complex two-level optimization problem. Additionally, GST provides a scalable yet rigid framework, which poses significant challenges to extend GST to accommodate nonnegative measures. To tackle these challenges, in this work we revisit the entropy partial transport (EPT) problem. By exploiting Caffarelli \\& McCann's insights, we develop a novel variant of EPT endowed with Orlicz geometric structure, called \\emph{Orlicz-EPT}. We establish theoretical background to solve Orlicz-EPT using a binary search algorithmic approach. Especially, by leveraging the dual EPT and the underlying graph structure, we formulate a novel regularization approach that leads to the proposed \\emph{Orlicz-Sobolev transport} (OST). Notably, we demonstrate that OST can be efficiently computed by simply solving a univariate optimization problem, in stark contrast to the intensive computation needed for Orlicz-EPT. Building on this, we derive geometric structures for OST and draw its connections to other transport distances. We empirically illustrate that OST is several-order faster than Orlicz-EPT. Furthermore, we show preliminary evidence on the advantages of OST for measures on a graph in document classification and topological data analysis. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "CqLWckpTbG@OpenReview",
      "index": 328,
      "title": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning",
      "authors": [
        "Wenxuan Shi",
        "Haochen Tan",
        "Chuqiao Kuang",
        "Xiaoguang Li",
        "Hanting Chen",
        "Xiaozhe Ren",
        "Yasheng Wang",
        "Lu Hou",
        "Lifeng Shang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "deepdiver",
        "seeking",
        "textbf",
        "webpuzzle",
        "sft",
        "wiki",
        "web",
        "sis",
        "search",
        "ended"
      ],
      "summary": "Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing prompting and supervised fine-tuning (SFT) methods remain fixed by prompt rules or training corpora, and are usually benchmarked only on well-structured wiki sources, limiting real-world adaptability. We introduce WebPuzzle WebPuzzle , a 24k-sample training and 275-sample test benchmark that evaluates information seeking on the live internet, across both wiki and open-domain queries. Leveraging 7k WebPuzzle instances, we develop DeepDiver DeepDiver , a reinforcement-learning (RL) framework that cultivates Search Intensity Scaling (SIS) Search Intensity Scaling (SIS) —an emergent ability to escalate search frequency and depth instead of settling on overconfident, under-evidenced answers. With SIS, Qwen2.5-7B-Instruct and Pangu-7B-Reasoner attain performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver’s curriculum from cold-start SFT to a well designed RL procedure, and show that its seeking policy generalized from closed-ended queries to open-ended generation such as long-form writing. Our results advance adaptive information seeking in LLMs and provide a rigorous benchmark for future work.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CqLWckpTbG"
        ],
        "venue": [
          "/venue/CqLWckpTbG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CqLWckpTbG"
        ],
        "detail": [
          "https://openreview.net/forum?id=CqLWckpTbG"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Wenxuan Shi , Haochen Tan , Chuqiao Kuang , Xiaoguang Li , Hanting Chen , Xiaozhe Ren , Yasheng Wang , Lu Hou , Lifeng Shang Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing prompting and supervised fine-tuning (SFT) methods remain fixed by prompt rules or training corpora, and are usually benchmarked only on well-structured wiki sources, limiting real-world adaptability. We introduce WebPuzzle WebPuzzle , a 24k-sample training and 275-sample test benchmark that evaluates information seeking on the live internet, across both wiki and open-domain queries. Leveraging 7k WebPuzzle instances, we develop DeepDiver DeepDiver , a reinforcement-learning (RL) framework that cultivates Search Intensity Scaling (SIS) Search Intensity Scaling (SIS) —an emergent ability to escalate search frequency and depth instead of settling on overconfident, under-evidenced answers. With SIS, Qwen2.5-7B-Instruct and Pangu-7B-Reasoner attain performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver’s curriculum from cold-start SFT to a well designed RL procedure, and show that its seeking policy generalized from closed-ended queries to open-ended generation such as long-form writing. Our results advance adaptive information seeking in LLMs and provide a rigorous benchmark for future work. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "UXc87Orcri@OpenReview",
      "index": 329,
      "title": "MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D Scene Relighting",
      "authors": [
        "Yumeng He",
        "Yunbo Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "metags",
        "relighting",
        "phong",
        "ood",
        "lighting",
        "gaussian",
        "splatting",
        "meta",
        "blinn",
        "scene"
      ],
      "summary": "Out-of-distribution (OOD) 3D relighting requires novel view synthesis under unseen lighting conditions that differ significantly from the observed images. Existing relighting methods, which assume consistent light source distributions between training and testing, often degrade in OOD scenarios. We introduce **MetaGS** to tackle this challenge from two perspectives. First, we propose a meta-learning approach to train 3D Gaussian splatting, which explicitly promotes learning generalizable Gaussian geometries and appearance attributes across diverse lighting conditions, even with biased training data. Second, we embed fundamental physical priors from the *Blinn-Phong* reflection model into Gaussian splatting, which enhances the decoupling of shading components and leads to more accurate 3D scene reconstruction. Results on both synthetic and real-world datasets demonstrate the effectiveness of MetaGS in challenging OOD relighting tasks, supporting efficient point-light relighting and generalizing well to unseen environment lighting maps.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UXc87Orcri"
        ],
        "venue": [
          "/venue/UXc87Orcri@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UXc87Orcri"
        ],
        "detail": [
          "https://openreview.net/forum?id=UXc87Orcri"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D Scene Relighting [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Yumeng He , Yunbo Wang Out-of-distribution (OOD) 3D relighting requires novel view synthesis under unseen lighting conditions that differ significantly from the observed images. Existing relighting methods, which assume consistent light source distributions between training and testing, often degrade in OOD scenarios. We introduce **MetaGS** to tackle this challenge from two perspectives. First, we propose a meta-learning approach to train 3D Gaussian splatting, which explicitly promotes learning generalizable Gaussian geometries and appearance attributes across diverse lighting conditions, even with biased training data. Second, we embed fundamental physical priors from the *Blinn-Phong* reflection model into Gaussian splatting, which enhances the decoupling of shading components and leads to more accurate 3D scene reconstruction. Results on both synthetic and real-world datasets demonstrate the effectiveness of MetaGS in challenging OOD relighting tasks, supporting efficient point-light relighting and generalizing well to unseen environment lighting maps. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4WQ5Qgpl2F@OpenReview",
      "index": 330,
      "title": "Towards a Pairwise Ranking Model with Orderliness and Monotonicity for Label Enhancement",
      "authors": [
        "Yunan Lu",
        "Xixi Zhang",
        "Yaojin Lin",
        "Weiwei Li",
        "Lei Yang",
        "Xiuyi Jia"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "label",
        "orderliness",
        "prom",
        "monotonicity",
        "ranking",
        "enhancement",
        "rankings",
        "pairwise",
        "distributions",
        "readily"
      ],
      "summary": "Label distribution in recent years has been applied in a diverse array of complex decision-making tasks. To address the availability of label distributions, label enhancement has been established as an effective learning paradigm that aims to automatically infer label distributions from readily available multi-label data, e.g., logical labels. Recently, numerous works have demonstrated that the label ranking is significantly beneficial to label enhancement. However, these works still exhibit deficiencies in representing the probabilistic relationships between label distribution and label rankings, or fail to accommodate scenarios where multiple labels are equally important for a given instance. Therefore, we propose PROM, a pairwise ranking model with orderliness and monotonicity, to explain the probabilistic relationship between label distributions and label rankings. Specifically, we propose the monotonicity and orderliness assumptions for the probabilities of different ranking relationships and derive the mass functions for PROM, which are theoretically ensured to preserve the monotonicity and orderliness. Further, we propose a generative label enhancement algorithm based on PROM, which directly learns a label distribution predictor from the readily available multi-label data. Finally, extensive experiments demonstrate the efficacy of our proposed model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4WQ5Qgpl2F"
        ],
        "venue": [
          "/venue/4WQ5Qgpl2F@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4WQ5Qgpl2F"
        ],
        "detail": [
          "https://openreview.net/forum?id=4WQ5Qgpl2F"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Towards a Pairwise Ranking Model with Orderliness and Monotonicity for Label Enhancement [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yunan Lu , Xixi Zhang , Yaojin Lin , Weiwei Li , Lei Yang , Xiuyi Jia Label distribution in recent years has been applied in a diverse array of complex decision-making tasks. To address the availability of label distributions, label enhancement has been established as an effective learning paradigm that aims to automatically infer label distributions from readily available multi-label data, e.g., logical labels. Recently, numerous works have demonstrated that the label ranking is significantly beneficial to label enhancement. However, these works still exhibit deficiencies in representing the probabilistic relationships between label distribution and label rankings, or fail to accommodate scenarios where multiple labels are equally important for a given instance. Therefore, we propose PROM, a pairwise ranking model with orderliness and monotonicity, to explain the probabilistic relationship between label distributions and label rankings. Specifically, we propose the monotonicity and orderliness assumptions for the probabilities of different ranking relationships and derive the mass functions for PROM, which are theoretically ensured to preserve the monotonicity and orderliness. Further, we propose a generative label enhancement algorithm based on PROM, which directly learns a label distribution predictor from the readily available multi-label data. Finally, extensive experiments demonstrate the efficacy of our proposed model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "aqpHTPC63N@OpenReview",
      "index": 331,
      "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Bootstrapping",
      "authors": [
        "Pu Yang",
        "Yunzhen Feng",
        "Ziyuan Chen",
        "Yuhang Wu",
        "Zhuoyuan Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "policies",
        "bootstrapping",
        "exponential",
        "wisely",
        "post",
        "iterative",
        "synthetic",
        "spend",
        "iterations",
        "budget"
      ],
      "summary": "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model performance improves, raising a crucial question: How should the total budget for generation and training be allocated across iterations to maximize final performance? In this work, we develop a theoretical framework for analyzing budget allocation strategies. Specifically, we show that constant policies fail to converge with high probability, while increasing policies---particularly exponential growth policies---exhibit significant theoretical advantages. Experiments on image denoising with diffusion probabilistic models and math reasoning with large language models show that both exponential and polynomial growth policies consistently outperform constant policies, with exponential policies often providing more stable performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aqpHTPC63N"
        ],
        "venue": [
          "/venue/aqpHTPC63N@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aqpHTPC63N"
        ],
        "detail": [
          "https://openreview.net/forum?id=aqpHTPC63N"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Bootstrapping [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Pu Yang , Yunzhen Feng , Ziyuan Chen , Yuhang Wu , Zhuoyuan Li Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model performance improves, raising a crucial question: How should the total budget for generation and training be allocated across iterations to maximize final performance? In this work, we develop a theoretical framework for analyzing budget allocation strategies. Specifically, we show that constant policies fail to converge with high probability, while increasing policies---particularly exponential growth policies---exhibit significant theoretical advantages. Experiments on image denoising with diffusion probabilistic models and math reasoning with large language models show that both exponential and polynomial growth policies consistently outperform constant policies, with exponential policies often providing more stable performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MI1uT5rReV@OpenReview",
      "index": 332,
      "title": "Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning",
      "authors": [
        "Wenyi Xiao",
        "Leilei Gan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "grpo",
        "reasoning",
        "thinking",
        "slow",
        "fast",
        "length",
        "vision",
        "accuracy",
        "difficulty",
        "language"
      ],
      "summary": "When applying reinforcement learning—typically through GRPO—to large vision-language model reasoning struggles to effectively scale reasoning length or generates verbose outputs across all tasks with only marginal gains in accuracy. To address this issue, we present FAST-GRPO, a variant of GRPO that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. Inspired by these observations, we introduce two complementary metrics to estimate the difficulty of the questions, guiding the model to determine when fast or slow thinking is more appropriate. Next, we incorporate adaptive length-based rewards and difficulty-aware KL divergence into the GRPO algorithm. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10% relative improvement compared to the base model, while reducing token usage by 32.7-67.3% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MI1uT5rReV"
        ],
        "venue": [
          "/venue/MI1uT5rReV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MI1uT5rReV"
        ],
        "detail": [
          "https://openreview.net/forum?id=MI1uT5rReV"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Wenyi Xiao , Leilei Gan When applying reinforcement learning—typically through GRPO—to large vision-language model reasoning struggles to effectively scale reasoning length or generates verbose outputs across all tasks with only marginal gains in accuracy. To address this issue, we present FAST-GRPO, a variant of GRPO that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. Inspired by these observations, we introduce two complementary metrics to estimate the difficulty of the questions, guiding the model to determine when fast or slow thinking is more appropriate. Next, we incorporate adaptive length-based rewards and difficulty-aware KL divergence into the GRPO algorithm. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10% relative improvement compared to the base model, while reducing token usage by 32.7-67.3% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gAddPMjmUc@OpenReview",
      "index": 333,
      "title": "Exploration via Feature Perturbation in Contextual Bandits",
      "authors": [
        "Seouh-won Yi",
        "Min-hwan Oh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bandits",
        "contextual",
        "feature",
        "regret",
        "perturbation",
        "widetilde",
        "exploration",
        "eschews",
        "randomizing",
        "mathcal"
      ],
      "summary": "We propose *feature perturbation*, a simple yet effective exploration strategy for contextual bandits that injects randomness directly into feature inputs, instead of randomizing unknown parameters or adding noise to rewards. Remarkably, this algorithm achieves O ˜ ( d T − − √ ) O ~ ( d T ) worst-case regret bound for generalized linear contextual bandits, while avoiding the O ˜ ( d 3 / 2 T − − √ ) O ~ ( d 3 / 2 T ) regret typical of existing randomized bandit algorithms. Because our algorithm eschews parameter sampling, it is both computationally efficient and naturally extends to non-parametric or neural network models. We verify these advantages through empirical evaluations, demonstrating that feature perturbation not only surpasses existing methods but also unifies strong practical performance with the near-optimal regret guarantees.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gAddPMjmUc"
        ],
        "venue": [
          "/venue/gAddPMjmUc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gAddPMjmUc"
        ],
        "detail": [
          "https://openreview.net/forum?id=gAddPMjmUc"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Exploration via Feature Perturbation in Contextual Bandits [PDF ] [Copy] [Kimi ] [REL] Authors : Seouh-won Yi , Min-hwan Oh We propose *feature perturbation*, a simple yet effective exploration strategy for contextual bandits that injects randomness directly into feature inputs, instead of randomizing unknown parameters or adding noise to rewards. Remarkably, this algorithm achieves O ˜ ( d T − − √ ) O ~ ( d T ) worst-case regret bound for generalized linear contextual bandits, while avoiding the O ˜ ( d 3 / 2 T − − √ ) O ~ ( d 3 / 2 T ) regret typical of existing randomized bandit algorithms. Because our algorithm eschews parameter sampling, it is both computationally efficient and naturally extends to non-parametric or neural network models. We verify these advantages through empirical evaluations, demonstrating that feature perturbation not only surpasses existing methods but also unifies strong practical performance with the near-optimal regret guarantees. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "K0FbK2GOGj@OpenReview",
      "index": 334,
      "title": "Instance-Optimality for Private KL Distribution Estimation",
      "authors": [
        "Jiayuan Ye",
        "Vitaly Feldman",
        "Kunal Talwar"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "optimality",
        "private",
        "minimax",
        "instance",
        "distribution",
        "estimation",
        "divergence",
        "estimators",
        "neighborhood",
        "algorithm"
      ],
      "summary": "We study the fundamental problem of estimating an unknown discrete distribution p p over d d symbols, given n n i.i.d. samples from the distribution. We are interested in minimizing the KL divergence between the true distribution and the algorithm's estimate. We first construct minimax optimal private estimators. Minimax optimality however fails to shed light on an algorithm's performance on individual (non-worst-case) instances p p and simple minimax-optimal DP estimators can have poor empirical performance on real distributions. We then study this problem from an instance-optimality viewpoint, where the algorithm's error on p p is compared to the minimum achievable estimation error over a small local neighborhood of p p . Under natural notions of local neighborhood, we propose algorithms that achieve instance-optimality up to constant factors, with and without a differential privacy constraint. Our upper bounds rely on (private) variants of the Good-Turing estimator. Our lower bounds use additive local neighborhoods that more precisely captures the hardness of distribution estimation in KL divergence, compared to ones considered in prior works.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=K0FbK2GOGj"
        ],
        "venue": [
          "/venue/K0FbK2GOGj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=K0FbK2GOGj"
        ],
        "detail": [
          "https://openreview.net/forum?id=K0FbK2GOGj"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Instance-Optimality for Private KL Distribution Estimation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jiayuan Ye , Vitaly Feldman , Kunal Talwar We study the fundamental problem of estimating an unknown discrete distribution p p over d d symbols, given n n i.i.d. samples from the distribution. We are interested in minimizing the KL divergence between the true distribution and the algorithm's estimate. We first construct minimax optimal private estimators. Minimax optimality however fails to shed light on an algorithm's performance on individual (non-worst-case) instances p p and simple minimax-optimal DP estimators can have poor empirical performance on real distributions. We then study this problem from an instance-optimality viewpoint, where the algorithm's error on p p is compared to the minimum achievable estimation error over a small local neighborhood of p p . Under natural notions of local neighborhood, we propose algorithms that achieve instance-optimality up to constant factors, with and without a differential privacy constraint. Our upper bounds rely on (private) variants of the Good-Turing estimator. Our lower bounds use additive local neighborhoods that more precisely captures the hardness of distribution estimation in KL divergence, compared to ones considered in prior works. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Bzlt5tPFT6@OpenReview",
      "index": 335,
      "title": "DMWM: Dual-Mind World Model with Long-Term Imagination",
      "authors": [
        "Lingyi Wang",
        "Rashed Shelim",
        "Walid Saad",
        "Naren Ramakrishnan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "imagination",
        "rssm",
        "dmwm",
        "logical",
        "world",
        "term",
        "mind",
        "long",
        "dual",
        "linn"
      ],
      "summary": "Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite and robotic environment. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Bzlt5tPFT6"
        ],
        "venue": [
          "/venue/Bzlt5tPFT6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Bzlt5tPFT6"
        ],
        "detail": [
          "https://openreview.net/forum?id=Bzlt5tPFT6"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "DMWM: Dual-Mind World Model with Long-Term Imagination [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Lingyi Wang , Rashed Shelim , Walid Saad , Naren Ramakrishnan Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite and robotic environment. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "akhhwQh6UV@OpenReview",
      "index": 336,
      "title": "TimeWak: Temporal Chained-Hashing Watermark for Time Series Data",
      "authors": [
        "Zhi Wen Soi",
        "Chaoyi Zhu",
        "Fouad Abiad",
        "Aditya Shankar",
        "Jeroen M. Galjaard",
        "Huijuan Wang",
        "Lydia Y. Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "timewak",
        "watermark",
        "watermarking",
        "temporal",
        "chained",
        "series",
        "hashing",
        "data",
        "inverting",
        "synthetic"
      ],
      "summary": "Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in data space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in data space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the temporal-feature data space. The other unique feature is the ϵ ϵ -exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series while preserving robust watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against five datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the strongest state-of-the-art baseline, while remaining consistently detectable.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=akhhwQh6UV"
        ],
        "venue": [
          "/venue/akhhwQh6UV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=akhhwQh6UV"
        ],
        "detail": [
          "https://openreview.net/forum?id=akhhwQh6UV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "TimeWak: Temporal Chained-Hashing Watermark for Time Series Data [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Zhi Wen Soi , Chaoyi Zhu , Fouad Abiad , Aditya Shankar , Jeroen M. Galjaard , Huijuan Wang , Lydia Y. Chen Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in data space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in data space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the temporal-feature data space. The other unique feature is the ϵ ϵ -exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series while preserving robust watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against five datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the strongest state-of-the-art baseline, while remaining consistently detectable. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "8omLr8BtjL@OpenReview",
      "index": 337,
      "title": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface",
      "authors": [
        "Hao Tang",
        "Chen-Wei Xie",
        "Haiyang Wang",
        "Xiaoyi Bao",
        "Tingyu Weng",
        "Pandeng Li",
        "Yun Zheng",
        "Liwei Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ufo",
        "perception",
        "language",
        "segmentation",
        "tasks",
        "grained",
        "fine",
        "generalist",
        "ended",
        "interface"
      ],
      "summary": "Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present UFO, a framework that unifies fine-grained visual perception tasks through an open-ended language interface. By transforming all perception targets into the language space, UFO unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, UFO outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby achieving superior performance on the challenging reasoning segmentation. Code and models are available at https://github.com/nnnth/UFO.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8omLr8BtjL"
        ],
        "venue": [
          "/venue/8omLr8BtjL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8omLr8BtjL"
        ],
        "detail": [
          "https://openreview.net/forum?id=8omLr8BtjL"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Hao Tang , Chen-Wei Xie , Haiyang Wang , Xiaoyi Bao , Tingyu Weng , Pandeng Li , Yun Zheng , Liwei Wang Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present UFO, a framework that unifies fine-grained visual perception tasks through an open-ended language interface. By transforming all perception targets into the language space, UFO unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, UFO outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby achieving superior performance on the challenging reasoning segmentation. Code and models are available at https://github.com/nnnth/UFO. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YFa7eULIeN@OpenReview",
      "index": 338,
      "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models",
      "authors": [
        "Ziyi Wu",
        "Anil Kag",
        "Ivan Skorokhodov",
        "Willi Menapace",
        "Ashkan Mirzaei",
        "Igor Gilitschenski",
        "Sergey Tulyakov",
        "Aliaksandr Siarohin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "densedpo",
        "video",
        "dpo",
        "preference",
        "preferences",
        "annotators",
        "motion",
        "clips",
        "fine",
        "temporal"
      ],
      "summary": "Direct Preference Optimization (DPO) has recently been applied as a post‑training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one‑third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YFa7eULIeN"
        ],
        "venue": [
          "/venue/YFa7eULIeN@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YFa7eULIeN"
        ],
        "detail": [
          "https://openreview.net/forum?id=YFa7eULIeN"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Ziyi Wu , Anil Kag , Ivan Skorokhodov , Willi Menapace , Ashkan Mirzaei , Igor Gilitschenski , Sergey Tulyakov , Aliaksandr Siarohin Direct Preference Optimization (DPO) has recently been applied as a post‑training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one‑third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "A0M3apV5zI@OpenReview",
      "index": 339,
      "title": "Precise Asymptotics and Refined Regret of Variance-Aware UCB",
      "authors": [
        "Yingying Fan",
        "Yuxuan Han",
        "Jinchi Lv",
        "Xiaocong XU",
        "Zhengyuan Zhou"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ucb",
        "regret",
        "pulling",
        "variance",
        "arm",
        "bound",
        "refined",
        "canonical",
        "zeevi",
        "aware"
      ],
      "summary": "In this paper, we study the behavior of the Upper Confidence Bound-Variance (UCB-V) algorithm for the Multi-Armed Bandit (MAB) problems, a variant of the canonical Upper Confidence Bound (UCB) algorithm that incorporates variance estimates into its decision-making process. More precisely, we provide an asymptotic characterization of the arm-pulling rates for UCB-V, extending recent results for the canonical UCB in Kalvit and Zeevi (2021) and Khamaru and Zhang (2024). In an interesting contrast to the canonical UCB, our analysis reveals that the behavior of UCB-V can exhibit instability, meaning that the arm-pulling rates may not always be asymptotically deterministic. Besides the asymptotic characterization, we also provide non-asymptotic bounds for the arm-pulling rates in the high probability regime, offering insights into the regret analysis. As an application of this high probability result, we establish that UCB-V can achieve a more refined regret bound, previously unknown even for more complicate and advanced variance-aware online decision-making algorithms. A matching regret lower bound is also established, demonstrating the optimality of our result.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=A0M3apV5zI"
        ],
        "venue": [
          "/venue/A0M3apV5zI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=A0M3apV5zI"
        ],
        "detail": [
          "https://openreview.net/forum?id=A0M3apV5zI"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Precise Asymptotics and Refined Regret of Variance-Aware UCB [PDF ] [Copy] [Kimi ] [REL] Authors : Yingying Fan , Yuxuan Han , Jinchi Lv , Xiaocong XU , Zhengyuan Zhou In this paper, we study the behavior of the Upper Confidence Bound-Variance (UCB-V) algorithm for the Multi-Armed Bandit (MAB) problems, a variant of the canonical Upper Confidence Bound (UCB) algorithm that incorporates variance estimates into its decision-making process. More precisely, we provide an asymptotic characterization of the arm-pulling rates for UCB-V, extending recent results for the canonical UCB in Kalvit and Zeevi (2021) and Khamaru and Zhang (2024). In an interesting contrast to the canonical UCB, our analysis reveals that the behavior of UCB-V can exhibit instability, meaning that the arm-pulling rates may not always be asymptotically deterministic. Besides the asymptotic characterization, we also provide non-asymptotic bounds for the arm-pulling rates in the high probability regime, offering insights into the regret analysis. As an application of this high probability result, we establish that UCB-V can achieve a more refined regret bound, previously unknown even for more complicate and advanced variance-aware online decision-making algorithms. A matching regret lower bound is also established, demonstrating the optimality of our result. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6AbM9UG4aD@OpenReview",
      "index": 340,
      "title": "InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions",
      "authors": [
        "Liangjian Wen",
        "Qun Dai",
        "Jianzhuang Liu",
        "Jiangtao Zheng",
        "Yong Dai",
        "Dongkai Wang",
        "zhao kang",
        "Jun Wang",
        "Zenglin Xu",
        "Jiang Duan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "synergistic",
        "infmasking",
        "information",
        "multimodal",
        "masking",
        "interactions",
        "modality",
        "contrastive",
        "unleashing",
        "modalities"
      ],
      "summary": "In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6AbM9UG4aD"
        ],
        "venue": [
          "/venue/6AbM9UG4aD@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6AbM9UG4aD"
        ],
        "detail": [
          "https://openreview.net/forum?id=6AbM9UG4aD"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Liangjian Wen , Qun Dai , Jianzhuang Liu , Jiangtao Zheng , Yong Dai , Dongkai Wang , zhao kang , Jun Wang , Zenglin Xu , Jiang Duan In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "uvTea5Rfek@OpenReview",
      "index": 341,
      "title": "Extracting task-relevant preserved dynamics from contrastive aligned neural recordings",
      "authors": [
        "Yiqi Jiang",
        "Kaiwen Sheng",
        "Yujia Gao",
        "E. Kelly Buchanan",
        "Yu Shikano",
        "Seung Je Woo",
        "Yixiu Zhao",
        "Tony Hyun Kim",
        "Fatih Dinc",
        "Scott Linderman",
        "Mark Schnitzer"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "candy",
        "underline",
        "behavioral",
        "sessions",
        "neural",
        "recording",
        "preserved",
        "aligned",
        "text",
        "dynamics"
      ],
      "summary": "Recent work indicates that low-dimensional dynamics of neural and behavioral data are often preserved across days and subjects. However, extracting these preserved dynamics remains challenging: high-dimensional neural population activity and the recorded neuron populations vary across recording sessions. While existing modeling tools can improve alignment between neural and behavioral data, they often operate on a per-subject basis or discretize behavior into categories, disrupting its natural continuity and failing to capture the underlying dynamics. We introduce C – – C _ ontrastive A – – A _ ligned N – – N _ eural D – – D _ Y – – Y _ namics (CANDY), an end‑to‑end framework that aligns neural and behavioral data using rank-based contrastive learning, adapted for continuous behavioral variables, to project neural activity from different sessions onto a shared low-dimensional embedding space. CANDY fits a shared linear dynamical system to the aligned embeddings, enabling an interpretable model of the conserved temporal structure in the latent space. We validate CANDY on synthetic and real-world datasets spanning multiple species, behaviors, and recording modalities. Our results show that CANDY is able to learn aligned latent embeddings and preserved dynamics across neural recording sessions and subjects, and it achieves improved cross-session behavior decoding performance. We further show that the latent linear dynamical system generalizes to new sessions and subjects, achieving comparable or even superior behavior decoding performance to models trained from scratch. These advances enable robust cross‑session behavioral decoding and offer a path towards identifying shared neural dynamics that underlie behavior across individuals and recording conditions. The code and two-photon imaging data of striatal neural activity that we acquired here are available at https://github.com/schnitzer-lab/CANDY-public.git.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uvTea5Rfek"
        ],
        "venue": [
          "/venue/uvTea5Rfek@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uvTea5Rfek"
        ],
        "detail": [
          "https://openreview.net/forum?id=uvTea5Rfek"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Extracting task-relevant preserved dynamics from contrastive aligned neural recordings [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Yiqi Jiang , Kaiwen Sheng , Yujia Gao , E. Kelly Buchanan , Yu Shikano , Seung Je Woo , Yixiu Zhao , Tony Hyun Kim , Fatih Dinc , Scott Linderman , Mark Schnitzer Recent work indicates that low-dimensional dynamics of neural and behavioral data are often preserved across days and subjects. However, extracting these preserved dynamics remains challenging: high-dimensional neural population activity and the recorded neuron populations vary across recording sessions. While existing modeling tools can improve alignment between neural and behavioral data, they often operate on a per-subject basis or discretize behavior into categories, disrupting its natural continuity and failing to capture the underlying dynamics. We introduce C – – C _ ontrastive A – – A _ ligned N – – N _ eural D – – D _ Y – – Y _ namics (CANDY), an end‑to‑end framework that aligns neural and behavioral data using rank-based contrastive learning, adapted for continuous behavioral variables, to project neural activity from different sessions onto a shared low-dimensional embedding space. CANDY fits a shared linear dynamical system to the aligned embeddings, enabling an interpretable model of the conserved temporal structure in the latent space. We validate CANDY on synthetic and real-world datasets spanning multiple species, behaviors, and recording modalities. Our results show that CANDY is able to learn aligned latent embeddings and preserved dynamics across neural recording sessions and subjects, and it achieves improved cross-session behavior decoding performance. We further show that the latent linear dynamical system generalizes to new sessions and subjects, achieving comparable or even superior behavior decoding performance to models trained from scratch. These advances enable robust cross‑session behavioral decoding and offer a path towards identifying shared neural dynamics that underlie behavior across individuals and recording conditions. The code and two-photon imaging data of striatal neural activity that we acquired here are available at https://github.com/schnitzer-lab/CANDY-public.git. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Rtd6GoJcoT@OpenReview",
      "index": 342,
      "title": "Orochi: Versatile Biomedical Image Processor",
      "authors": [
        "Gaole Dai",
        "Chenghao Zhou",
        "Yu Zhou",
        "Rongyu Zhang",
        "Yuan Zhang",
        "Chengkai Hou",
        "Tiejun Huang",
        "Jianxu Chen",
        "Shanghang Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "orochi",
        "biomedical",
        "plugins",
        "biologists",
        "mamba",
        "processor",
        "versatile",
        "registration",
        "napari",
        "fiji"
      ],
      "summary": "Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce **Orochi**, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models. Our pre-trained weights and code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Rtd6GoJcoT"
        ],
        "venue": [
          "/venue/Rtd6GoJcoT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Rtd6GoJcoT"
        ],
        "detail": [
          "https://openreview.net/forum?id=Rtd6GoJcoT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Orochi: Versatile Biomedical Image Processor [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Gaole Dai , Chenghao Zhou , Yu Zhou , Rongyu Zhang , Yuan Zhang , Chengkai Hou , Tiejun Huang , Jianxu Chen , Shanghang Zhang Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce **Orochi**, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models. Our pre-trained weights and code will be released. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4jWuS5hye1@OpenReview",
      "index": 343,
      "title": "WISA: World simulator assistant for physics-aware text-to-video generation",
      "authors": [
        "Jing Wang",
        "Ao Ma",
        "Ke Cao",
        "Jun Zheng",
        "Jiasong Feng",
        "Zhanjie Zhang",
        "Wanyuan Pang",
        "Xiaodan Liang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "wisa",
        "physical",
        "t2v",
        "laws",
        "video",
        "principles",
        "imulator",
        "ssistant",
        "cogvideox",
        "wan2"
      ],
      "summary": "Recent advances in text-to-video (T2V) generation, exemplified by models such as Sora and Kling, have demonstrated strong potential for constructing world simulators. However, existing T2V models still struggle to understand abstract physical principles and to generate videos that faithfully obey physical laws. This limitation stems primarily from the lack of explicit physical guidance, caused by a significant gap between high-level physical concepts and the generative capabilities of current models. To address this challenge, we propose the **W**orld **S**imulator **A**ssistant (**WISA**), a novel framework designed to systematically decompose and integrate physical principles into T2V models. Specifically, WISA decomposes physical knowledge into three hierarchical levels: textual physical descriptions, qualitative physical categories, and quantitative physical properties. It then incorporates several carefully designed modules—such as Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier—to effectively encode these attributes and enhance the model’s adherence to physical laws during generation. In addition, most existing video datasets feature only weak or implicit representations of physical phenomena, limiting their utility for learning explicit physical principles. To bridge this gap, we present **WISA-80K**, a new dataset comprising 80,000 human-curated videos that depict 17 fundamental physical laws across three core domains of physics: dynamics, thermodynamics, and optics. Experimental results show that WISA substantially improves the alignment of T2V models (such as CogVideoX and Wan2.1) with real-world physical laws, achieving notable gains on the VideoPhy benchmark. Our data, code, and models are available in the [Project Page](https://wisav1.github.io/WISA/).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4jWuS5hye1"
        ],
        "venue": [
          "/venue/4jWuS5hye1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4jWuS5hye1"
        ],
        "detail": [
          "https://openreview.net/forum?id=4jWuS5hye1"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": null
      },
      "raw_excerpt": "WISA: World simulator assistant for physics-aware text-to-video generation [PDF 6 ] [Copy] [Kimi ] [REL] Authors : Jing Wang , Ao Ma , Ke Cao , Jun Zheng , Jiasong Feng , Zhanjie Zhang , Wanyuan Pang , Xiaodan Liang Recent advances in text-to-video (T2V) generation, exemplified by models such as Sora and Kling, have demonstrated strong potential for constructing world simulators. However, existing T2V models still struggle to understand abstract physical principles and to generate videos that faithfully obey physical laws. This limitation stems primarily from the lack of explicit physical guidance, caused by a significant gap between high-level physical concepts and the generative capabilities of current models. To address this challenge, we propose the **W**orld **S**imulator **A**ssistant (**WISA**), a novel framework designed to systematically decompose and integrate physical principles into T2V models. Specifically, WISA decomposes physical knowledge into three hierarchical levels: textual physical descriptions, qualitative physical categories, and quantitative physical properties. It then incorporates several carefully designed modules—such as Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier—to effectively encode these attributes and enhance the model’s adherence to physical laws during generation. In addition, most existing video datasets feature only weak or implicit representations of physical phenomena, limiting their utility for learning explicit physical principles. To bridge this gap, we present **WISA-80K**, a new dataset comprising 80,000 human-curated videos that depict 17 fundamental physical laws across three core domains of physics: dynamics, thermodynamics, and optics. Experimental results show that WISA substantially improves the alignment of T2V models (such as CogVideoX and Wan2.1) with real-world physical laws, achieving notable gains on the VideoPhy benchmark. Our data, code, and models are available in the [Project Page](https://wisav1.github.io/WISA/). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Zc5Ntjt5Ul@OpenReview",
      "index": 344,
      "title": "Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning",
      "authors": [
        "Dravyansh Sharma",
        "Alec Sun"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "attias",
        "learnability",
        "agents",
        "learning",
        "realizable",
        "improper",
        "online",
        "classified",
        "characterize",
        "improvements"
      ],
      "summary": "Machine learning is now ubiquitous in societal decision-making, for example in evaluating job candidates or loan applications, and it is increasingly important to take into account how classified agents will react to the learning algorithms. The majority of recent literature on strategic classification has focused on reducing and countering deceptive behaviors by the classified agents, but recent work of Attias et al. identifies surprising properties of learnability when the agents genuinely improve in order to attain the desirable classification, such as smaller generalization error than standard PAC-learning. In this paper we characterize so-called learnability with improvements across multiple new axes. We introduce an asymmetric variant of minimally consistent concept classes and use it to provide an exact characterization of proper learning with improvements in the realizable setting. While prior work studies learnability only under general, arbitrary agent improvement regions, we give positive results for more natural Euclidean ball improvement sets. In particular, we characterize improper learning under a generative assumption on the data distribution. We further show how to learn in more challenging settings, achieving lower generalization error under well-studied bounded noise models and obtaining mistake bounds in realizable and agnostic online learning. We resolve open questions posed by Attias et al. for both proper and improper learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Zc5Ntjt5Ul"
        ],
        "venue": [
          "/venue/Zc5Ntjt5Ul@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Zc5Ntjt5Ul"
        ],
        "detail": [
          "https://openreview.net/forum?id=Zc5Ntjt5Ul"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning [PDF ] [Copy] [Kimi ] [REL] Authors : Dravyansh Sharma , Alec Sun Machine learning is now ubiquitous in societal decision-making, for example in evaluating job candidates or loan applications, and it is increasingly important to take into account how classified agents will react to the learning algorithms. The majority of recent literature on strategic classification has focused on reducing and countering deceptive behaviors by the classified agents, but recent work of Attias et al. identifies surprising properties of learnability when the agents genuinely improve in order to attain the desirable classification, such as smaller generalization error than standard PAC-learning. In this paper we characterize so-called learnability with improvements across multiple new axes. We introduce an asymmetric variant of minimally consistent concept classes and use it to provide an exact characterization of proper learning with improvements in the realizable setting. While prior work studies learnability only under general, arbitrary agent improvement regions, we give positive results for more natural Euclidean ball improvement sets. In particular, we characterize improper learning under a generative assumption on the data distribution. We further show how to learn in more challenging settings, achieving lower generalization error under well-studied bounded noise models and obtaining mistake bounds in realizable and agnostic online learning. We resolve open questions posed by Attias et al. for both proper and improper learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "uAeqQePu4c@OpenReview",
      "index": 345,
      "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
      "authors": [
        "Jiajun Shi",
        "Jian Yang",
        "Jiaheng Liu",
        "Xingyuan Bu",
        "Jiangjie Chen",
        "Junting Zhou",
        "Kaijing Ma",
        "Zhoufutu Wen",
        "Bingli Wang",
        "Yancheng He",
        "Liang Song",
        "Hualei Zhu",
        "Shilong Li",
        "Xingjian Wang",
        "Wei Zhang",
        "Ruibin Yuan",
        "Yifan Yao",
        "Wenjun Yang",
        "Yunli Wang",
        "Siyuan Fang",
        "Siyu Yuan",
        "Qianyu He",
        "Xiangru Tang",
        "Yingshui Tan",
        "Wangchunshu Zhou",
        "Zhaoxiang Zhang",
        "Zhoujun Li",
        "Wenhao Huang",
        "Ge Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "korgym",
        "reasoning",
        "gymnasium",
        "llm",
        "evaluation",
        "platform",
        "kor",
        "interactive",
        "dynamic",
        "llms"
      ],
      "summary": "Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM’s general reasoning potential. To address this limitation, we introduce the **Knowledge Orthogonal Reasoning Gymnasium (KORGym)**, a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uAeqQePu4c"
        ],
        "venue": [
          "/venue/uAeqQePu4c@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uAeqQePu4c"
        ],
        "detail": [
          "https://openreview.net/forum?id=uAeqQePu4c"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jiajun Shi , Jian Yang , Jiaheng Liu , Xingyuan Bu , Jiangjie Chen , Junting Zhou , Kaijing Ma , Zhoufutu Wen , Bingli Wang , Yancheng He , Liang Song , Hualei Zhu , Shilong Li , Xingjian Wang , Wei Zhang , Ruibin Yuan , Yifan Yao , Wenjun Yang , Yunli Wang , Siyuan Fang , Siyu Yuan , Qianyu He , Xiangru Tang , Yingshui Tan , Wangchunshu Zhou , Zhaoxiang Zhang , Zhoujun Li , Wenhao Huang , Ge Zhang Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM’s general reasoning potential. To address this limitation, we introduce the **Knowledge Orthogonal Reasoning Gymnasium (KORGym)**, a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gMHLQASj11@OpenReview",
      "index": 346,
      "title": "Learnable Sampler Distillation for Discrete Diffusion Models",
      "authors": [
        "Feiyang Fu",
        "Tongxian Guo",
        "Zhaoqiang Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ddms",
        "sampler",
        "lsd",
        "steps",
        "learnable",
        "sampling",
        "distillation",
        "generation",
        "quality",
        "samplers"
      ],
      "summary": "Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gMHLQASj11"
        ],
        "venue": [
          "/venue/gMHLQASj11@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gMHLQASj11"
        ],
        "detail": [
          "https://openreview.net/forum?id=gMHLQASj11"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Learnable Sampler Distillation for Discrete Diffusion Models [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Feiyang Fu , Tongxian Guo , Zhaoqiang Liu Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0GvEaa9prl@OpenReview",
      "index": 347,
      "title": "On the Hardness of Approximating Distributions with Tractable Probabilistic Models",
      "authors": [
        "John Leland",
        "YooJung Choi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pcs",
        "probabilistic",
        "inference",
        "circuits",
        "tpms",
        "approximating",
        "tractable",
        "size",
        "hardness",
        "expressivity"
      ],
      "summary": "A fundamental challenge in probabilistic modeling is to balance expressivity and inference efficiency. Tractable probabilistic models (TPMs) aim to directly address this tradeoff by imposing constraints that guarantee efficient inference of certain queries while maintaining expressivity. In particular, probabilistic circuits (PCs) provide a unifying framework for many TPMs, by characterizing families of models as circuits satisfying different structural properties. Because the complexity of inference on PCs is a function of the circuit size, understanding the size requirements of different families of PCs is fundamental in mapping the trade-off between tractability and expressive efficiency. However, the study of expressive efficiency of circuits are often concerned with exact representations, which may not align with model learning, where we look to approximate the underlying data distribution closely by some distance measure. Moreover, due to hardness of inference tasks, exactly representing distributions while supporting tractable inference often incurs exponential size blow-ups. In this paper, we consider a natural, yet so far underexplored, question: can we avoid such size blow-up by allowing for some small approximation error? We study approximating distributions with probabilistic circuits with guarantees based on f f -divergences, and analyze which inference queries remain well-approximated under this framework. We show that approximating an arbitrary distribution with bounded f f -divergence is NP-hard for any model that can tractably compute marginals. In addition, we prove an exponential size gap for approximation between the class of decomposable PCs and that of decomposable and deterministic PCs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0GvEaa9prl"
        ],
        "venue": [
          "/venue/0GvEaa9prl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0GvEaa9prl"
        ],
        "detail": [
          "https://openreview.net/forum?id=0GvEaa9prl"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "On the Hardness of Approximating Distributions with Tractable Probabilistic Models [PDF 1 ] [Copy] [Kimi ] [REL] Authors : John Leland , YooJung Choi A fundamental challenge in probabilistic modeling is to balance expressivity and inference efficiency. Tractable probabilistic models (TPMs) aim to directly address this tradeoff by imposing constraints that guarantee efficient inference of certain queries while maintaining expressivity. In particular, probabilistic circuits (PCs) provide a unifying framework for many TPMs, by characterizing families of models as circuits satisfying different structural properties. Because the complexity of inference on PCs is a function of the circuit size, understanding the size requirements of different families of PCs is fundamental in mapping the trade-off between tractability and expressive efficiency. However, the study of expressive efficiency of circuits are often concerned with exact representations, which may not align with model learning, where we look to approximate the underlying data distribution closely by some distance measure. Moreover, due to hardness of inference tasks, exactly representing distributions while supporting tractable inference often incurs exponential size blow-ups. In this paper, we consider a natural, yet so far underexplored, question: can we avoid such size blow-up by allowing for some small approximation error? We study approximating distributions with probabilistic circuits with guarantees based on f f -divergences, and analyze which inference queries remain well-approximated under this framework. We show that approximating an arbitrary distribution with bounded f f -divergence is NP-hard for any model that can tractably compute marginals. In addition, we prove an exponential size gap for approximation between the class of decomposable PCs and that of decomposable and deterministic PCs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "FAeU7516MR@OpenReview",
      "index": 348,
      "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE",
      "authors": [
        "Zongle Huang",
        "Lei Zhu",
        "ZongYuan Zhan",
        "Ting Hu",
        "Weikai Mao",
        "Xianzhi Yu",
        "Yongpan Liu",
        "Tianyu Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "moe",
        "moesd",
        "speculative",
        "batch",
        "decoding",
        "dense",
        "acceleration",
        "acceptance",
        "57b",
        "qwen2"
      ],
      "summary": "Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FAeU7516MR"
        ],
        "venue": [
          "/venue/FAeU7516MR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FAeU7516MR"
        ],
        "detail": [
          "https://openreview.net/forum?id=FAeU7516MR"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zongle Huang , Lei Zhu , ZongYuan Zhan , Ting Hu , Weikai Mao , Xianzhi Yu , Yongpan Liu , Tianyu Zhang Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "PHu9xJeAum@OpenReview",
      "index": 349,
      "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
      "authors": [
        "Xiyao Wang",
        "Zhengyuan Yang",
        "Chao Feng",
        "Hongjin Lu",
        "Linjie Li",
        "Chung-Ching Lin",
        "Kevin Lin",
        "Furong Huang",
        "Lijuan Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "thinklite",
        "qwen2",
        "72b",
        "mcts",
        "reasoning",
        "sota",
        "rft",
        "instruct",
        "mathvista",
        "difficulty"
      ],
      "summary": "We introduce ThinkLite-VL, a family of visual reasoning models that achieve state-of-the-art (SoTA) performance using an order of magnitude fewer training samples, relying purely on reinforcement fine-tuning (RFT) self-improvement without any knowledge distillation. Our central insight is that sample difficulty critically influences RFT effectiveness: appropriately challenging examples can drive substantial reasoning improvements, even in low-data regimes. However, quantifying sample difficulty in a reliable and scalable manner remains non-trivial. To address this, we repurpose Monte Carlo Tree Search (MCTS) to measure sample difficulty via the number of reasoning iterations a vision-language model (VLM) requires to solve each instance. This MCTS-based selection procedure identifies samples that induce deeper reasoning while remaining solvable, allowing us to filter a high-quality subset from 70k open-source examples spanning math, natural image understanding, and chart comprehension. Using this approach, we select just 11k challenging samples for RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly outperform their respective base models across eight visual reasoning benchmarks. In particular, ThinkLite-VL-7B improves the average performance of Qwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark improvement of 4.42 over the open-source SOTA. These results demonstrate that MCTS-guided difficulty filtering provides a scalable and effective path toward data-efficient self-improvement in multimodal reasoning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PHu9xJeAum"
        ],
        "venue": [
          "/venue/PHu9xJeAum@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PHu9xJeAum"
        ],
        "detail": [
          "https://openreview.net/forum?id=PHu9xJeAum"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Xiyao Wang , Zhengyuan Yang , Chao Feng , Hongjin Lu , Linjie Li , Chung-Ching Lin , Kevin Lin , Furong Huang , Lijuan Wang We introduce ThinkLite-VL, a family of visual reasoning models that achieve state-of-the-art (SoTA) performance using an order of magnitude fewer training samples, relying purely on reinforcement fine-tuning (RFT) self-improvement without any knowledge distillation. Our central insight is that sample difficulty critically influences RFT effectiveness: appropriately challenging examples can drive substantial reasoning improvements, even in low-data regimes. However, quantifying sample difficulty in a reliable and scalable manner remains non-trivial. To address this, we repurpose Monte Carlo Tree Search (MCTS) to measure sample difficulty via the number of reasoning iterations a vision-language model (VLM) requires to solve each instance. This MCTS-based selection procedure identifies samples that induce deeper reasoning while remaining solvable, allowing us to filter a high-quality subset from 70k open-source examples spanning math, natural image understanding, and chart comprehension. Using this approach, we select just 11k challenging samples for RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly outperform their respective base models across eight visual reasoning benchmarks. In particular, ThinkLite-VL-7B improves the average performance of Qwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark improvement of 4.42 over the open-source SOTA. These results demonstrate that MCTS-guided difficulty filtering provides a scalable and effective path toward data-efficient self-improvement in multimodal reasoning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "36cKp4tsHF@OpenReview",
      "index": 350,
      "title": "Deno-IF: Unsupervised Noisy Visible and Infrared Image Fusion Method",
      "authors": [
        "Han Xu",
        "Yuyang Li",
        "Yunfei Deng",
        "Jiayi Ma",
        "Guangcan Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fusion",
        "deno",
        "unsupervised",
        "noisy",
        "noise",
        "visible",
        "convolutional",
        "image",
        "infrared",
        "generalization"
      ],
      "summary": "Most image fusion methods are designed for ideal scenarios and struggle to handle noise. Existing noise-aware fusion methods are supervised and heavily rely on constructed paired data, limiting performance and generalization. This paper proposes a novel unsupervised noisy visible and infrared image fusion method, comprising two key modules. First, when only noisy source images are available, a convolutional low-rank optimization module decomposes clean components based on convolutional low-rank priors, guiding subsequent optimization. The unsupervised approach eliminates data dependency and enhances generalization across various and variable noise. Second, a unified network jointly realizes denoising and fusion. It consists of both intra-modal recovery and inter-modal recovery and fusion, also with a convolutional low-rankness loss for regularization. By exploiting the commonalities of denoising and fusion, the joint framework significantly reduces network complexity while expanding functionality. Extensive experiments validate the effectiveness and generalization of the proposed method for image fusion under various and variable noise conditions. The code is publicly available at https://github.com/hanna-xu/Deno-IF.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=36cKp4tsHF"
        ],
        "venue": [
          "/venue/36cKp4tsHF@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=36cKp4tsHF"
        ],
        "detail": [
          "https://openreview.net/forum?id=36cKp4tsHF"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Deno-IF: Unsupervised Noisy Visible and Infrared Image Fusion Method [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Han Xu , Yuyang Li , Yunfei Deng , Jiayi Ma , Guangcan Liu Most image fusion methods are designed for ideal scenarios and struggle to handle noise. Existing noise-aware fusion methods are supervised and heavily rely on constructed paired data, limiting performance and generalization. This paper proposes a novel unsupervised noisy visible and infrared image fusion method, comprising two key modules. First, when only noisy source images are available, a convolutional low-rank optimization module decomposes clean components based on convolutional low-rank priors, guiding subsequent optimization. The unsupervised approach eliminates data dependency and enhances generalization across various and variable noise. Second, a unified network jointly realizes denoising and fusion. It consists of both intra-modal recovery and inter-modal recovery and fusion, also with a convolutional low-rankness loss for regularization. By exploiting the commonalities of denoising and fusion, the joint framework significantly reduces network complexity while expanding functionality. Extensive experiments validate the effectiveness and generalization of the proposed method for image fusion under various and variable noise conditions. The code is publicly available at https://github.com/hanna-xu/Deno-IF. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2zjH76SmiF@OpenReview",
      "index": 351,
      "title": "ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection",
      "authors": [
        "Haowei Zhu",
        "Tianxiang Pan",
        "Rui Qin",
        "Jun-Hai Yong",
        "Bin Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "recon",
        "augmentation",
        "rectification",
        "controllable",
        "generative",
        "region",
        "alignment",
        "datasets",
        "perception",
        "semantic"
      ],
      "summary": "The scale and quality of datasets are crucial for training robust perception models. However, obtaining large-scale annotated data is both costly and time-consuming. Generative models have emerged as a powerful tool for data augmentation by synthesizing samples that adhere to desired distributions. However, current generative approaches often rely on complex post-processing or extensive fine-tuning on massive datasets to achieve satisfactory results, and they remain prone to content–position mismatches and semantic leakage. To overcome these limitations, we introduce ReCon, a novel augmentation framework that enhances the capacity of structure-controllable generative models for object detection. ReCon integrates region-guided rectification into the diffusion sampling process, using feedback from a pre-trained perception model to rectify misgenerated regions within diffusion sampling process. We further propose region-aligned cross-attention to enforce spatial–semantic alignment between image regions and their textual cues, thereby improving both semantic consistency and overall image fidelity. Extensive experiments demonstrate that ReCon substantially improve the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2zjH76SmiF"
        ],
        "venue": [
          "/venue/2zjH76SmiF@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2zjH76SmiF"
        ],
        "detail": [
          "https://openreview.net/forum?id=2zjH76SmiF"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Haowei Zhu , Tianxiang Pan , Rui Qin , Jun-Hai Yong , Bin Wang The scale and quality of datasets are crucial for training robust perception models. However, obtaining large-scale annotated data is both costly and time-consuming. Generative models have emerged as a powerful tool for data augmentation by synthesizing samples that adhere to desired distributions. However, current generative approaches often rely on complex post-processing or extensive fine-tuning on massive datasets to achieve satisfactory results, and they remain prone to content–position mismatches and semantic leakage. To overcome these limitations, we introduce ReCon, a novel augmentation framework that enhances the capacity of structure-controllable generative models for object detection. ReCon integrates region-guided rectification into the diffusion sampling process, using feedback from a pre-trained perception model to rectify misgenerated regions within diffusion sampling process. We further propose region-aligned cross-attention to enforce spatial–semantic alignment between image regions and their textual cues, thereby improving both semantic consistency and overall image fidelity. Extensive experiments demonstrate that ReCon substantially improve the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "imO353Gyrl@OpenReview",
      "index": 352,
      "title": "Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues",
      "authors": [
        "Chinmay Talegaonkar",
        "Nikhil Gandudi Suresh",
        "Zachary Novack",
        "Yash Belhe",
        "Priyanka Nagasamudra",
        "Nicholas Antipa"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "marigold",
        "defocus",
        "depth",
        "blur",
        "mmde",
        "cues",
        "shot",
        "metric",
        "zero",
        "estimation"
      ],
      "summary": "Recent monocular metric depth estimation (MMDE) methods have made notable progress towards zero-shot generalization. However, they still exhibit a significant performance drop on out-of-distribution datasets. We address this limitation by injecting defocus blur cues at inference time into Marigold, a \\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular depth estimation (MDE). Our method effectively turns Marigold into a metric depth predictor in a training-free manner. To incorporate defocus cues, we capture two images with a small and a large aperture from the same viewpoint. To recover metric depth, we then optimize the metric depth scaling parameters and the noise latents of Marigold at inference time using gradients from a loss function based on the defocus-blur image formation model. We compare our method against existing state-of-the-art zero-shot MMDE methods on a self-collected real dataset, showing quantitative and qualitative improvements.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=imO353Gyrl"
        ],
        "venue": [
          "/venue/imO353Gyrl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=imO353Gyrl"
        ],
        "detail": [
          "https://openreview.net/forum?id=imO353Gyrl"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Chinmay Talegaonkar , Nikhil Gandudi Suresh , Zachary Novack , Yash Belhe , Priyanka Nagasamudra , Nicholas Antipa Recent monocular metric depth estimation (MMDE) methods have made notable progress towards zero-shot generalization. However, they still exhibit a significant performance drop on out-of-distribution datasets. We address this limitation by injecting defocus blur cues at inference time into Marigold, a \\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular depth estimation (MDE). Our method effectively turns Marigold into a metric depth predictor in a training-free manner. To incorporate defocus cues, we capture two images with a small and a large aperture from the same viewpoint. To recover metric depth, we then optimize the metric depth scaling parameters and the noise latents of Marigold at inference time using gradients from a loss function based on the defocus-blur image formation model. We compare our method against existing state-of-the-art zero-shot MMDE methods on a self-collected real dataset, showing quantitative and qualitative improvements. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yjLew3Nd7z@OpenReview",
      "index": 353,
      "title": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding",
      "authors": [
        "Ansel Blume",
        "Jeonghwan Kim",
        "Hyeonjeong Ha",
        "Elen Chatikyan",
        "Xiaomeng Jin",
        "Khanh Duy Nguyen",
        "Nanyun Peng",
        "Kai-Wei Chang",
        "Derek Hoiem",
        "Heng Ji"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lmms",
        "partonomy",
        "plum",
        "segmenting",
        "part",
        "parts",
        "segmentation",
        "lmm",
        "visual",
        "segmentations"
      ],
      "summary": "Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning—yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 parts and 5346 objects for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY utilizes highly technical concepts and challenges models to compare objects’ parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that utilizes span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM dominates existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yjLew3Nd7z"
        ],
        "venue": [
          "/venue/yjLew3Nd7z@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yjLew3Nd7z"
        ],
        "detail": [
          "https://openreview.net/forum?id=yjLew3Nd7z"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Ansel Blume , Jeonghwan Kim , Hyeonjeong Ha , Elen Chatikyan , Xiaomeng Jin , Khanh Duy Nguyen , Nanyun Peng , Kai-Wei Chang , Derek Hoiem , Heng Ji Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning—yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 parts and 5346 objects for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY utilizes highly technical concepts and challenges models to compare objects’ parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that utilizes span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM dominates existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7cPDOBWTbM@OpenReview",
      "index": 354,
      "title": "Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data",
      "authors": [
        "Lingkai Kong",
        "Haichuan Wang",
        "Tonghan Wang",
        "GUOJUN XIONG",
        "Milind Tambe"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dynamics",
        "source",
        "target",
        "gap",
        "compflow",
        "flow",
        "composite",
        "shifted",
        "principled",
        "matching"
      ],
      "summary": "Incorporating pre-collected offline data from a source environment can significantly improve the sample efficiency of reinforcement learning (RL), but this benefit is often challenged by discrepancies between the transition dynamics of the source and target environments. Existing methods typically address this issue by penalizing or filtering out source transitions in high dynamics-gap regions. However, their estimation of the dynamics gap often relies on KL divergence or mutual information, which can be ill-defined when the source and target dynamics have disjoint support. To overcome these limitations, we propose CompFlow, a method grounded in the theoretical connection between flow matching and optimal transport. Specifically, we model the target dynamics as a conditional flow built upon the output distribution of the source-domain flow, rather than learning it directly from a Gaussian prior. This composite structure offers two key advantages: (1) improved generalization for learning target dynamics, and (2) a principled estimation of the dynamics gap via the Wasserstein distance between source and target transitions. Leveraging our principled estimation of the dynamics gap, we further introduce an optimistic active data collection strategy that prioritizes exploration in regions of high dynamics gap, and theoretically prove that it reduces the performance disparity with the optimal policy. Empirically, CompFlow outperforms strong baselines across several RL benchmarks with shifted dynamics.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7cPDOBWTbM"
        ],
        "venue": [
          "/venue/7cPDOBWTbM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7cPDOBWTbM"
        ],
        "detail": [
          "https://openreview.net/forum?id=7cPDOBWTbM"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Lingkai Kong , Haichuan Wang , Tonghan Wang , GUOJUN XIONG , Milind Tambe Incorporating pre-collected offline data from a source environment can significantly improve the sample efficiency of reinforcement learning (RL), but this benefit is often challenged by discrepancies between the transition dynamics of the source and target environments. Existing methods typically address this issue by penalizing or filtering out source transitions in high dynamics-gap regions. However, their estimation of the dynamics gap often relies on KL divergence or mutual information, which can be ill-defined when the source and target dynamics have disjoint support. To overcome these limitations, we propose CompFlow, a method grounded in the theoretical connection between flow matching and optimal transport. Specifically, we model the target dynamics as a conditional flow built upon the output distribution of the source-domain flow, rather than learning it directly from a Gaussian prior. This composite structure offers two key advantages: (1) improved generalization for learning target dynamics, and (2) a principled estimation of the dynamics gap via the Wasserstein distance between source and target transitions. Leveraging our principled estimation of the dynamics gap, we further introduce an optimistic active data collection strategy that prioritizes exploration in regions of high dynamics gap, and theoretically prove that it reduces the performance disparity with the optimal policy. Empirically, CompFlow outperforms strong baselines across several RL benchmarks with shifted dynamics. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "9CzRx5MZct@OpenReview",
      "index": 355,
      "title": "Asymmetric Duos: Sidekicks Improve Uncertainty",
      "authors": [
        "Tim G. Zhou",
        "Evan Shelhamer",
        "Geoff Pleiss"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "duos",
        "sidekick",
        "uncertainty",
        "sidekicks",
        "asymmetric",
        "decisions",
        "fine",
        "quantification",
        "soups",
        "improve"
      ],
      "summary": "The go-to strategy to apply deep networks in settings where uncertainty informs decisions—ensembling multiple training runs with random initializations—is ill-suited for the extremely large-scale models and practical fine-tuning workflows of today. We introduce a new cost-effective strategy for improving the uncertainty quantification and downstream decisions of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate but much smaller \"sidekick\" (e.g. a fine-tuned ResNet-34) with a fraction of the computational cost. We propose aggregating the predictions of this *Asymmetric Duo* by simple learned weighted averaging. Surprisingly, despite their inherent asymmetry, the sidekick model almost never harms the performance of the larger model. In fact, across five image classification benchmarks, and a variety of model architectures and training schemes (including soups), Asymmetric Duos significantly improve accuracy, uncertainty quantification, and selective classification metrics with only ∼ 10 − 20 ∼ 10 − 20 % more computation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9CzRx5MZct"
        ],
        "venue": [
          "/venue/9CzRx5MZct@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9CzRx5MZct"
        ],
        "detail": [
          "https://openreview.net/forum?id=9CzRx5MZct"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Asymmetric Duos: Sidekicks Improve Uncertainty [PDF ] [Copy] [Kimi ] [REL] Authors : Tim G. Zhou , Evan Shelhamer , Geoff Pleiss The go-to strategy to apply deep networks in settings where uncertainty informs decisions—ensembling multiple training runs with random initializations—is ill-suited for the extremely large-scale models and practical fine-tuning workflows of today. We introduce a new cost-effective strategy for improving the uncertainty quantification and downstream decisions of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate but much smaller \"sidekick\" (e.g. a fine-tuned ResNet-34) with a fraction of the computational cost. We propose aggregating the predictions of this *Asymmetric Duo* by simple learned weighted averaging. Surprisingly, despite their inherent asymmetry, the sidekick model almost never harms the performance of the larger model. In fact, across five image classification benchmarks, and a variety of model architectures and training schemes (including soups), Asymmetric Duos significantly improve accuracy, uncertainty quantification, and selective classification metrics with only ∼ 10 − 20 ∼ 10 − 20 % more computation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZrCQGVpQrl@OpenReview",
      "index": 356,
      "title": "Robust learning of halfspaces under log-concave marginals",
      "authors": [
        "Jane Lange",
        "Arsen Vasilyan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "volume",
        "adversarially",
        "varepsilon",
        "quad",
        "boundary",
        "regression",
        "robust",
        "concave",
        "polynomial",
        "threshold"
      ],
      "summary": "We say that a classifier is \\emph{adversarially robust} \\emph{adversarially robust} to perturbations of norm r r if, with high probability over a point x x drawn from the input distribution, there is no point within distance ≤ r ≤ r from x x that is classified differently. The \\emph{boundary volume} \\emph{boundary volume} is the probability that a point falls within distance r r of a point with a different label. This work studies the task of learning a hypothesis with small boundary volume, where the input is distributed as a subgaussian isotropic log-concave distribution over R d R d . Linear threshold functions are adversarially robust; they have boundary volume proportional to r r . Such concept classes are efficiently learnable by polynomial regression, which produces a polynomial threshold function (PTF), but PTFs in general may have boundary volume Ω ( 1 ) Ω ( 1 ) , even for r ≪ 1 r ≪ 1 . We give an algorithm that agnostically learns linear threshold functions and returns a classfier with boundary volume O ( r + ε ) O ( r + ε ) at radius of perturbation r r . The time and sample complexity of d O ~ ( 1 / ε 2 ) d O ~ ( 1 / ε 2 ) matches the complexity of polynomial regression. Our algorithm augments the classic approach of polynomial regression with three additional steps:\\ a) performing the ℓ 1 ℓ 1 -error regression under ℓ 1 ℓ 1 noise sensitivity constraints,\\ b) a structured partitioning and rounding step that returns a Boolean classifier with error o p t + O ( ε ) o p t + O ( ε ) and noise sensitivity O ( r + ε ) O ( r + ε ) simultaneously, and \\ c ) c ) a local corrector that ``smooths'' a function with low noise sensitivity into a function that is adversarially robust.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZrCQGVpQrl"
        ],
        "venue": [
          "/venue/ZrCQGVpQrl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZrCQGVpQrl"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZrCQGVpQrl"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Robust learning of halfspaces under log-concave marginals [PDF ] [Copy] [Kimi ] [REL] Authors : Jane Lange , Arsen Vasilyan We say that a classifier is \\emph{adversarially robust} \\emph{adversarially robust} to perturbations of norm r r if, with high probability over a point x x drawn from the input distribution, there is no point within distance ≤ r ≤ r from x x that is classified differently. The \\emph{boundary volume} \\emph{boundary volume} is the probability that a point falls within distance r r of a point with a different label. This work studies the task of learning a hypothesis with small boundary volume, where the input is distributed as a subgaussian isotropic log-concave distribution over R d R d . Linear threshold functions are adversarially robust; they have boundary volume proportional to r r . Such concept classes are efficiently learnable by polynomial regression, which produces a polynomial threshold function (PTF), but PTFs in general may have boundary volume Ω ( 1 ) Ω ( 1 ) , even for r ≪ 1 r ≪ 1 . We give an algorithm that agnostically learns linear threshold functions and returns a classfier with boundary volume O ( r + ε ) O ( r + ε ) at radius of perturbation r r . The time and sample complexity of d O ~ ( 1 / ε 2 ) d O ~ ( 1 / ε 2 ) matches the complexity of polynomial regression. Our algorithm augments the classic approach of polynomial regression with three additional steps:\\ a) performing the ℓ 1 ℓ 1 -error regression under ℓ 1 ℓ 1 noise sensitivity constraints,\\ b) a structured partitioning and rounding step that returns a Boolean classifier with error o p t + O ( ε ) o p t + O ( ε ) and noise sensitivity O ( r + ε ) O ( r + ε ) simultaneously, and \\ c ) c ) a local corrector that ``smooths'' a function with low noise sensitivity into a function that is adversarially robust. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Zn2ajV1kTQ@OpenReview",
      "index": 357,
      "title": "Implicit Bias of Spectral Descent and Muon on Multiclass Separable Data",
      "authors": [
        "Chen Fan",
        "Mark Schmidt",
        "Christos Thrampoulidis"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "norm",
        "descent",
        "max",
        "nmd",
        "nsd",
        "converge",
        "implicit",
        "steepest",
        "muon",
        "spectral"
      ],
      "summary": "Different gradient-based methods for optimizing overparameterized models can all achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. We provide the first complete characterization of implicit optimization bias for p-norm normalized steepest descent (NSD) and momentum steepest descent (NMD) algorithms in multi-class linear classification with cross-entropy loss. Our key theoretical contribution is proving that these algorithms converge to solutions maximizing the margin with respect to the classifier matrix's p-norm, with established convergence rates. These results encompass important special cases including Spectral Descent and Muon, which we show converge to max-margin solutions with respect to the spectral norm. A key insight of our contribution is that the analysis of general entry-wise and Schatten p-norms can be reduced to the analysis of NSD/NMD with max-norm by exploiting a natural ordering property between all p-norms relative to the max-norm and its dual sum-norm. For the specific case of descent with respect to the max-norm, we further extend our analysis to include preconditioning, showing that Adam converges to the matrix's max-norm solution. Our results demonstrate that the multi-class linear setting, which is inherently richer than the binary counterpart, provides the most transparent framework for studying implicit biases of matrix-parameter optimization algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Zn2ajV1kTQ"
        ],
        "venue": [
          "/venue/Zn2ajV1kTQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Zn2ajV1kTQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=Zn2ajV1kTQ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Implicit Bias of Spectral Descent and Muon on Multiclass Separable Data [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Chen Fan , Mark Schmidt , Christos Thrampoulidis Different gradient-based methods for optimizing overparameterized models can all achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. We provide the first complete characterization of implicit optimization bias for p-norm normalized steepest descent (NSD) and momentum steepest descent (NMD) algorithms in multi-class linear classification with cross-entropy loss. Our key theoretical contribution is proving that these algorithms converge to solutions maximizing the margin with respect to the classifier matrix's p-norm, with established convergence rates. These results encompass important special cases including Spectral Descent and Muon, which we show converge to max-margin solutions with respect to the spectral norm. A key insight of our contribution is that the analysis of general entry-wise and Schatten p-norms can be reduced to the analysis of NSD/NMD with max-norm by exploiting a natural ordering property between all p-norms relative to the max-norm and its dual sum-norm. For the specific case of descent with respect to the max-norm, we further extend our analysis to include preconditioning, showing that Adam converges to the matrix's max-norm solution. Our results demonstrate that the multi-class linear setting, which is inherently richer than the binary counterpart, provides the most transparent framework for studying implicit biases of matrix-parameter optimization algorithms. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MaJ3ASZ0NI@OpenReview",
      "index": 358,
      "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally",
      "authors": [
        "Tobias Schnabel",
        "Kiran Tomlinson",
        "Adith Swaminathan",
        "Jennifer Neville"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bapo",
        "llms",
        "bapos",
        "bandwidth",
        "cot",
        "hard",
        "failures",
        "fail",
        "tasks",
        "lost"
      ],
      "summary": "Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MaJ3ASZ0NI"
        ],
        "venue": [
          "/venue/MaJ3ASZ0NI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MaJ3ASZ0NI"
        ],
        "detail": [
          "https://openreview.net/forum?id=MaJ3ASZ0NI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "Lost in Transmission: When and Why LLMs Fail to Reason Globally [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Tobias Schnabel , Kiran Tomlinson , Adith Swaminathan , Jennifer Neville Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Iic9I2nHdZ@OpenReview",
      "index": 359,
      "title": "On Transferring Transferability: Towards a Theory for Size Generalization",
      "authors": [
        "Eitan Levin",
        "Yuxin Ma",
        "Mateo Diaz Diaz",
        "Soledad Villar"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "transferability",
        "inputs",
        "transferring",
        "architectures",
        "instantiate",
        "transferable",
        "towards",
        "size",
        "generalization",
        "work"
      ],
      "summary": "Many modern learning tasks require models that can take inputs of varying sizes. Consequently, dimension-independent architectures have been proposed for domains where the inputs are graphs, sets, and point clouds. Recent work on graph neural networks has explored whether a model trained on low-dimensional data can transfer its performance to higher-dimensional inputs. We extend this body of work by introducing a general framework for transferability across dimensions. We show that transferability corresponds precisely to continuity in a limit space formed by identifying small problem instances with equivalent large ones. This identification is driven by the data and the learning task. We instantiate our framework on existing architectures, and implement the necessary changes to ensure their transferability. Finally, we provide design principles for designing new transferable models. Numerical experiments support our findings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Iic9I2nHdZ"
        ],
        "venue": [
          "/venue/Iic9I2nHdZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Iic9I2nHdZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=Iic9I2nHdZ"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "On Transferring Transferability: Towards a Theory for Size Generalization [PDF ] [Copy] [Kimi ] [REL] Authors : Eitan Levin , Yuxin Ma , Mateo Diaz Diaz , Soledad Villar Many modern learning tasks require models that can take inputs of varying sizes. Consequently, dimension-independent architectures have been proposed for domains where the inputs are graphs, sets, and point clouds. Recent work on graph neural networks has explored whether a model trained on low-dimensional data can transfer its performance to higher-dimensional inputs. We extend this body of work by introducing a general framework for transferability across dimensions. We show that transferability corresponds precisely to continuity in a limit space formed by identifying small problem instances with equivalent large ones. This identification is driven by the data and the learning task. We instantiate our framework on existing architectures, and implement the necessary changes to ensure their transferability. Finally, we provide design principles for designing new transferable models. Numerical experiments support our findings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "o8hWyJIgAV@OpenReview",
      "index": 360,
      "title": "When Worse is Better: Navigating the Compression Generation Trade-off In Visual Tokenization",
      "authors": [
        "Vivek Ramanujan",
        "Kushal Tirumala",
        "Armen Aghajanyan",
        "Luke Zettlemoyer",
        "Ali Farhadi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "stage",
        "generation",
        "trade",
        "tokenization",
        "crt",
        "latents",
        "worse",
        "latent",
        "775m",
        "compress"
      ],
      "summary": "Current image generation methods are based on a two-stage training approach. In stage 1, an auto-encoder is trained to compress an image into a latent space; in stage 2, a generative model is trained to learn a distribution over that latent space. This reveals a fundamental trade-off, do we compress more aggressively to make the latent distribution easier for the stage 2 model to learn even if it makes reconstruction worse? We study this problem in the context of discrete, auto-regressive image generation. Through the lens of scaling laws, we show that smaller stage 2 models can benefit from more compressed stage 1 latents even if reconstruction performance worsens, demonstrating that generation modeling capacity plays a role in this trade-off. Diving deeper, we rigorously study the connection between compute scaling and the stage 1 rate-distortion trade-off. Next, we introduce Causally Regularized Tokenization (CRT), which uses knowledge of the stage 2 generation modeling procedure to embed useful inductive biases in stage 1 latents. This regularization improves stage 2 generation performance better by making the tokens easier to model without affecting the stage 1 compression rate and marginally affecting distortion: we are able to improve compute efficiency 2-3 × × over baseline. Finally, we use CRT with further optimizations to the visual tokenizer setup to result in a generative pipeline that matches LlamaGen-3B generation performance (2.18 FID) with half the tokens per image (256 vs. 576) and a fourth the total model parameters (775M vs. 3.1B) while using the same architecture and inference procedure.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=o8hWyJIgAV"
        ],
        "venue": [
          "/venue/o8hWyJIgAV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=o8hWyJIgAV"
        ],
        "detail": [
          "https://openreview.net/forum?id=o8hWyJIgAV"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "When Worse is Better: Navigating the Compression Generation Trade-off In Visual Tokenization [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Vivek Ramanujan , Kushal Tirumala , Armen Aghajanyan , Luke Zettlemoyer , Ali Farhadi Current image generation methods are based on a two-stage training approach. In stage 1, an auto-encoder is trained to compress an image into a latent space; in stage 2, a generative model is trained to learn a distribution over that latent space. This reveals a fundamental trade-off, do we compress more aggressively to make the latent distribution easier for the stage 2 model to learn even if it makes reconstruction worse? We study this problem in the context of discrete, auto-regressive image generation. Through the lens of scaling laws, we show that smaller stage 2 models can benefit from more compressed stage 1 latents even if reconstruction performance worsens, demonstrating that generation modeling capacity plays a role in this trade-off. Diving deeper, we rigorously study the connection between compute scaling and the stage 1 rate-distortion trade-off. Next, we introduce Causally Regularized Tokenization (CRT), which uses knowledge of the stage 2 generation modeling procedure to embed useful inductive biases in stage 1 latents. This regularization improves stage 2 generation performance better by making the tokens easier to model without affecting the stage 1 compression rate and marginally affecting distortion: we are able to improve compute efficiency 2-3 × × over baseline. Finally, we use CRT with further optimizations to the visual tokenizer setup to result in a generative pipeline that matches LlamaGen-3B generation performance (2.18 FID) with half the tokens per image (256 vs. 576) and a fourth the total model parameters (775M vs. 3.1B) while using the same architecture and inference procedure. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gZsmYwFHci@OpenReview",
      "index": 361,
      "title": "FlowFeat: Pixel-Dense Embedding of Motion Profiles",
      "authors": [
        "Nikita Araslanov",
        "Anna Ribic",
        "Daniel Cremers"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "flowfeat",
        "dense",
        "motion",
        "profiles",
        "versatile",
        "apparent",
        "flow",
        "video",
        "segmentation",
        "pixel"
      ],
      "summary": "Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present *FlowFeat*, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or *motion profiles*. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gZsmYwFHci"
        ],
        "venue": [
          "/venue/gZsmYwFHci@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gZsmYwFHci"
        ],
        "detail": [
          "https://openreview.net/forum?id=gZsmYwFHci"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "FlowFeat: Pixel-Dense Embedding of Motion Profiles [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Nikita Araslanov , Anna Ribic , Daniel Cremers Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present *FlowFeat*, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or *motion profiles*. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5X6PL4906S@OpenReview",
      "index": 362,
      "title": "The Generative Leap: Tight Sample Complexity for Efficiently Learning Gaussian Multi-Index Models",
      "authors": [
        "Alex Damian",
        "Jason D. Lee",
        "Joan Bruna"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "leap",
        "generative",
        "index",
        "gaussian",
        "damian",
        "subspace",
        "exponent",
        "multi",
        "sample",
        "complexity"
      ],
      "summary": "In this work we consider generic Gaussian Multi-index models, in which the labels only depend on the (Gaussian) d d -dimensional inputs through their projection onto a low-dimensional r = O d ( 1 ) r = O d ( 1 ) subspace, and we study efficient agnostic estimation procedures for this hidden subspace. We introduce the *generative leap* exponent, a natural extension of the generative exponent from Damian et al. 2024 to the multi-index setting. We show that a sample complexity of n = Θ ( d 1 ∨ k ⋆ / 2 ) n = Θ ( d 1 ∨ k ⋆ / 2 ) is necessary in the class of algorithms captured by the Low-Degree-Polynomial framework; and also sufficient, by giving a sequential estimation procedure based on a spectral U-statistic over appropriate Hermite tensors.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5X6PL4906S"
        ],
        "venue": [
          "/venue/5X6PL4906S@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5X6PL4906S"
        ],
        "detail": [
          "https://openreview.net/forum?id=5X6PL4906S"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "The Generative Leap: Tight Sample Complexity for Efficiently Learning Gaussian Multi-Index Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Alex Damian , Jason D. Lee , Joan Bruna In this work we consider generic Gaussian Multi-index models, in which the labels only depend on the (Gaussian) d d -dimensional inputs through their projection onto a low-dimensional r = O d ( 1 ) r = O d ( 1 ) subspace, and we study efficient agnostic estimation procedures for this hidden subspace. We introduce the *generative leap* exponent, a natural extension of the generative exponent from Damian et al. 2024 to the multi-index setting. We show that a sample complexity of n = Θ ( d 1 ∨ k ⋆ / 2 ) n = Θ ( d 1 ∨ k ⋆ / 2 ) is necessary in the class of algorithms captured by the Low-Degree-Polynomial framework; and also sufficient, by giving a sequential estimation procedure based on a spectral U-statistic over appropriate Hermite tensors. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bTssV4Cnjn@OpenReview",
      "index": 363,
      "title": "Incremental Sequence Classification with Temporal Consistency",
      "authors": [
        "Lucas Maystre",
        "Gabriel Barello",
        "Tudor Berariu",
        "Aleix Cambray",
        "Rares Dolga",
        "Alvaro Ortega Gonzalez",
        "Andrei Cristian Nica",
        "David Barber"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "incremental",
        "sequence",
        "classification",
        "temporal",
        "generations",
        "consistency",
        "unpromising",
        "predictions",
        "condition",
        "loss"
      ],
      "summary": "We address the problem of incremental sequence classification, where predictions are updated as new elements in the sequence are revealed. Drawing on temporal-difference learning from reinforcement learning, we identify a temporal-consistency condition that successive predictions should satisfy. We leverage this condition to develop a novel loss function for training incremental sequence classifiers. Through a concrete example, we demonstrate that optimizing this loss can offer substantial gains in data efficiency. We apply our method to text classification tasks and show that it improves predictive accuracy over competing approaches on several benchmark datasets. We further evaluate our approach on the task of verifying large language model generations for correctness in grade-school math problems. Our results show that models trained with our method are better able to distinguish promising generations from unpromising ones after observing only a few tokens.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bTssV4Cnjn"
        ],
        "venue": [
          "/venue/bTssV4Cnjn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bTssV4Cnjn"
        ],
        "detail": [
          "https://openreview.net/forum?id=bTssV4Cnjn"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Incremental Sequence Classification with Temporal Consistency [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Lucas Maystre , Gabriel Barello , Tudor Berariu , Aleix Cambray , Rares Dolga , Alvaro Ortega Gonzalez , Andrei Cristian Nica , David Barber We address the problem of incremental sequence classification, where predictions are updated as new elements in the sequence are revealed. Drawing on temporal-difference learning from reinforcement learning, we identify a temporal-consistency condition that successive predictions should satisfy. We leverage this condition to develop a novel loss function for training incremental sequence classifiers. Through a concrete example, we demonstrate that optimizing this loss can offer substantial gains in data efficiency. We apply our method to text classification tasks and show that it improves predictive accuracy over competing approaches on several benchmark datasets. We further evaluate our approach on the task of verifying large language model generations for correctness in grade-school math problems. Our results show that models trained with our method are better able to distinguish promising generations from unpromising ones after observing only a few tokens. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Iqu63cYI3z@OpenReview",
      "index": 364,
      "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering",
      "authors": [
        "Jonas Kulhanek",
        "Marie-Julie Rakotosaona",
        "Fabian Manhardt",
        "Christina Tsalicoglou",
        "Michael Niemeyer",
        "Torsten Sattler",
        "Songyou Peng",
        "Federico Tombari"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lod",
        "rendering",
        "splatting",
        "lodge",
        "memory",
        "gaussians",
        "detail",
        "level",
        "gaussian",
        "hierarchical"
      ],
      "summary": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Iqu63cYI3z"
        ],
        "venue": [
          "/venue/Iqu63cYI3z@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Iqu63cYI3z"
        ],
        "detail": [
          "https://openreview.net/forum?id=Iqu63cYI3z"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Jonas Kulhanek , Marie-Julie Rakotosaona , Fabian Manhardt , Christina Tsalicoglou , Michael Niemeyer , Torsten Sattler , Songyou Peng , Federico Tombari In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5xdbWUdM87@OpenReview",
      "index": 365,
      "title": "Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics",
      "authors": [
        "Deep Patel",
        "Emmanouil-Vasileios Vlatakis-Gkaragkounis"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "games",
        "min",
        "max",
        "overparameterization",
        "initialization",
        "convex",
        "hidden",
        "concave",
        "involve",
        "emph"
      ],
      "summary": "Many emerging applications—such as adversarial training, AI alignment, and robust optimization—can be framed as zero-sum games between neural nets, with von Neumann–Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of \\emph{hidden convexity} and \\emph{overparameterization}. We identify sufficient conditions spanning initialization, training dynamics, and network width—that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for alternating gradient-descent-ascent scheme in min-max games; and (b) we show that games with hidden convex–concave geometry reduce to settings satisfying two-sided Polyak–Łojasiewicz (PL) and smoothness conditions, which hold with high probability under overparameterization, using tools from random matrix theory.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5xdbWUdM87"
        ],
        "venue": [
          "/venue/5xdbWUdM87@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5xdbWUdM87"
        ],
        "detail": [
          "https://openreview.net/forum?id=5xdbWUdM87"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Deep Patel , Emmanouil-Vasileios Vlatakis-Gkaragkounis Many emerging applications—such as adversarial training, AI alignment, and robust optimization—can be framed as zero-sum games between neural nets, with von Neumann–Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of \\emph{hidden convexity} and \\emph{overparameterization}. We identify sufficient conditions spanning initialization, training dynamics, and network width—that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for alternating gradient-descent-ascent scheme in min-max games; and (b) we show that games with hidden convex–concave geometry reduce to settings satisfying two-sided Polyak–Łojasiewicz (PL) and smoothness conditions, which hold with high probability under overparameterization, using tools from random matrix theory. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "1ffIkWo0yq@OpenReview",
      "index": 366,
      "title": "Provable Gradient Editing of Deep Neural Networks",
      "authors": [
        "Zhe Tao",
        "Aditya V. Thakur"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "prograd",
        "mathrm",
        "constraints",
        "dnn",
        "gradients",
        "hard",
        "editing",
        "gradient",
        "theta",
        "provable"
      ],
      "summary": "In explainable AI, DNN gradients are used to interpret the prediction; in safety-critical control systems, gradients could encode safety constraints; in scientific-computing applications, gradients could encode physical invariants. While recent work on provable editing of DNNs has focused on input-output constraints, the problem of enforcing hard constraints on DNN gradients remains unaddressed. We present ProGrad, the first efficient approach for editing the parameters of a DNN to provably enforce hard constraints on the DNN gradients. Given a DNN N N with parameters θ θ , and a set S S of pairs ( x , Q ) ( x , Q ) of input x x and corresponding linear gradient constraints Q Q , ProGrad finds new parameters θ ′ θ ′ such that ⋀ ( x , Q ) ∈ S ∂ ∂ x N ( x ; θ ′ ) ∈ Q ⋀ ( x , Q ) ∈ S ∂ ∂ x N ( x ; θ ′ ) ∈ Q while minimizing the changes ∥ θ ′ − θ ∥ ‖ θ ′ − θ ‖ . The key contribution is a novel *conditional variable gradient* of DNNs, which relaxes the NP-hard provable gradient editing problem to a linear program (LP), enabling ProGrad to use an LP solver to efficiently and effectively enforce the gradient constraints. We experimentally evaluated ProGrad via enforcing (i) hard Grad-CAM constraints on ImageNet ResNet DNNs; (ii) hard Integrated Gradients constraints on Llama 3 and Qwen 3 LLMs; (iii) hard gradient constraints in training a DNN to approximate a target function as a proxy for safety constraints in control systems and physical invariants in scientific applications. The results highlight the unique capability of ProGrad in enforcing hard constraints on DNN gradients.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1ffIkWo0yq"
        ],
        "venue": [
          "/venue/1ffIkWo0yq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1ffIkWo0yq"
        ],
        "detail": [
          "https://openreview.net/forum?id=1ffIkWo0yq"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "Provable Gradient Editing of Deep Neural Networks [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Zhe Tao , Aditya V. Thakur In explainable AI, DNN gradients are used to interpret the prediction; in safety-critical control systems, gradients could encode safety constraints; in scientific-computing applications, gradients could encode physical invariants. While recent work on provable editing of DNNs has focused on input-output constraints, the problem of enforcing hard constraints on DNN gradients remains unaddressed. We present ProGrad, the first efficient approach for editing the parameters of a DNN to provably enforce hard constraints on the DNN gradients. Given a DNN N N with parameters θ θ , and a set S S of pairs ( x , Q ) ( x , Q ) of input x x and corresponding linear gradient constraints Q Q , ProGrad finds new parameters θ ′ θ ′ such that ⋀ ( x , Q ) ∈ S ∂ ∂ x N ( x ; θ ′ ) ∈ Q ⋀ ( x , Q ) ∈ S ∂ ∂ x N ( x ; θ ′ ) ∈ Q while minimizing the changes ∥ θ ′ − θ ∥ ‖ θ ′ − θ ‖ . The key contribution is a novel *conditional variable gradient* of DNNs, which relaxes the NP-hard provable gradient editing problem to a linear program (LP), enabling ProGrad to use an LP solver to efficiently and effectively enforce the gradient constraints. We experimentally evaluated ProGrad via enforcing (i) hard Grad-CAM constraints on ImageNet ResNet DNNs; (ii) hard Integrated Gradients constraints on Llama 3 and Qwen 3 LLMs; (iii) hard gradient constraints in training a DNN to approximate a target function as a proxy for safety constraints in control systems and physical invariants in scientific applications. The results highlight the unique capability of ProGrad in enforcing hard constraints on DNN gradients. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "FjByDpDVIO@OpenReview",
      "index": 367,
      "title": "Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models",
      "authors": [
        "Sreyan Ghosh",
        "Arushi Goel",
        "Jaehyeon Kim",
        "Sonal Kumar",
        "Zhifeng Kong",
        "Sang-gil Lee",
        "Chao-Han Huck Yang",
        "Ramani Duraiswami",
        "Dinesh Manocha",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "audio",
        "af3",
        "flamingo",
        "reasoning",
        "open",
        "chat",
        "speech",
        "music",
        "sota",
        "understanding"
      ],
      "summary": "We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large audio-language model that advances reasoning and understanding across speech, sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder trained using a novel strategy for joint representation learning across all 3 modalities of speech, sound, and music; (ii) flexible, on-demand thinking, allowing the model to do chain-of-thought-type reasoning before answering; (iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning (including speech) up to 10 minutes; and (v) voice-to-voice interaction. To enable these capabilities, we propose several large-scale training datasets curated using novel strategies, including AudioSkills-XL, LongAudio-XL, AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based training strategy. Trained on only open-source audio data, AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FjByDpDVIO"
        ],
        "venue": [
          "/venue/FjByDpDVIO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FjByDpDVIO"
        ],
        "detail": [
          "https://openreview.net/forum?id=FjByDpDVIO"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Sreyan Ghosh , Arushi Goel , Jaehyeon Kim , Sonal Kumar , Zhifeng Kong , Sang-gil Lee , Chao-Han Huck Yang , Ramani Duraiswami , Dinesh Manocha , Rafael Valle , Bryan Catanzaro We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large audio-language model that advances reasoning and understanding across speech, sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder trained using a novel strategy for joint representation learning across all 3 modalities of speech, sound, and music; (ii) flexible, on-demand thinking, allowing the model to do chain-of-thought-type reasoning before answering; (iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning (including speech) up to 10 minutes; and (v) voice-to-voice interaction. To enable these capabilities, we propose several large-scale training datasets curated using novel strategies, including AudioSkills-XL, LongAudio-XL, AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based training strategy. Trained on only open-source audio data, AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0fZoqVoc0o@OpenReview",
      "index": 368,
      "title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes",
      "authors": [
        "Fang Li",
        "Hao Zhang",
        "Narendra Ahuja"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "camera",
        "rgb",
        "scenes",
        "optimization",
        "video",
        "dynamic",
        "reliance",
        "masks",
        "parameter",
        "motion"
      ],
      "summary": "Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video, dubbed \\textit{ROS-Cam} \\textit{ROS-Cam} . Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0fZoqVoc0o"
        ],
        "venue": [
          "/venue/0fZoqVoc0o@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0fZoqVoc0o"
        ],
        "detail": [
          "https://openreview.net/forum?id=0fZoqVoc0o"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Fang Li , Hao Zhang , Narendra Ahuja Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video, dubbed \\textit{ROS-Cam} \\textit{ROS-Cam} . Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "k4jg1QCw0e@OpenReview",
      "index": 369,
      "title": "Integration Matters for Learning PDEs with Backwards SDEs",
      "authors": [
        "Sungje Park",
        "Stephen Tu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bsde",
        "integration",
        "pinns",
        "pdes",
        "heun",
        "bias",
        "backwards",
        "sdes",
        "stochastic",
        "based"
      ],
      "summary": "Backward stochastic differential equation (BSDE)-based deep learning methods provide an alternative to Physics-Informed Neural Networks (PINNs) for solving high-dimensional partial differential equations (PDEs), offering potential algorithmic advantages in settings such as stochastic optimal control, where the PDEs of interest are tied to an underlying dynamical system. However, standard BSDE-based solvers have empirically been shown to underperform relative to PINNs in the literature. In this paper, we identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme applied to one-step self-consistency BSDE losses, which shifts the optimization landscape off target. We find that this bias cannot be satisfactorily addressed through finer step-sizes or multi-step self-consistency losses. To properly handle this issue, we propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration. We show that our proposed approach completely eliminates the bias issues faced by EM integration. Furthermore, our empirical results show that our Heun-based BSDE method consistently outperforms EM-based variants and achieves competitive results with PINNs across multiple high-dimensional benchmarks. Our findings highlight the critical role of integration schemes in BSDE-based PDE solvers, an algorithmic detail that has received little attention thus far in the literature.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=k4jg1QCw0e"
        ],
        "venue": [
          "/venue/k4jg1QCw0e@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=k4jg1QCw0e"
        ],
        "detail": [
          "https://openreview.net/forum?id=k4jg1QCw0e"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Integration Matters for Learning PDEs with Backwards SDEs [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Sungje Park , Stephen Tu Backward stochastic differential equation (BSDE)-based deep learning methods provide an alternative to Physics-Informed Neural Networks (PINNs) for solving high-dimensional partial differential equations (PDEs), offering potential algorithmic advantages in settings such as stochastic optimal control, where the PDEs of interest are tied to an underlying dynamical system. However, standard BSDE-based solvers have empirically been shown to underperform relative to PINNs in the literature. In this paper, we identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme applied to one-step self-consistency BSDE losses, which shifts the optimization landscape off target. We find that this bias cannot be satisfactorily addressed through finer step-sizes or multi-step self-consistency losses. To properly handle this issue, we propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration. We show that our proposed approach completely eliminates the bias issues faced by EM integration. Furthermore, our empirical results show that our Heun-based BSDE method consistently outperforms EM-based variants and achieves competitive results with PINNs across multiple high-dimensional benchmarks. Our findings highlight the critical role of integration schemes in BSDE-based PDE solvers, an algorithmic detail that has received little attention thus far in the literature. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2ptM76yNzZ@OpenReview",
      "index": 370,
      "title": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding",
      "authors": [
        "Jingyang Lin",
        "Jialian Wu",
        "Ximeng Sun",
        "Ze Wang",
        "Jiang Liu",
        "Yusheng Su",
        "Xiaodong Yu",
        "Hao Chen",
        "Jiebo Luo",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "video",
        "hour",
        "videomarathon",
        "long",
        "llava",
        "language",
        "lmms",
        "unleashing",
        "training",
        "videos"
      ],
      "summary": "Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2ptM76yNzZ"
        ],
        "venue": [
          "/venue/2ptM76yNzZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2ptM76yNzZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=2ptM76yNzZ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Jingyang Lin , Jialian Wu , Ximeng Sun , Ze Wang , Jiang Liu , Yusheng Su , Xiaodong Yu , Hao Chen , Jiebo Luo , Zicheng Liu , Emad Barsoum Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "f6AYwCvynr@OpenReview",
      "index": 371,
      "title": "Neural Entropy",
      "authors": [
        "Akhil Premkumar"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "entropy",
        "neural",
        "diffusion",
        "information",
        "reinstating",
        "structured",
        "imperfectly",
        "erased",
        "diffused",
        "noise"
      ],
      "summary": "We explore the connection between deep learning and information theory through the paradigm of diffusion models. A diffusion model converts noise into structured data by reinstating, imperfectly, information that is erased when data was diffused to noise. This information is stored in a neural network during training. We quantify this information by introducing a measure called \\textit{neural entropy}, which is related to the total entropy produced by diffusion. Neural entropy is a function of not just the data distribution, but also the diffusive process itself. Measurements of neural entropy on a few simple image diffusion models reveal that they are extremely efficient at compressing large ensembles of structured data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=f6AYwCvynr"
        ],
        "venue": [
          "/venue/f6AYwCvynr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=f6AYwCvynr"
        ],
        "detail": [
          "https://openreview.net/forum?id=f6AYwCvynr"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Neural Entropy [PDF 3 ] [Copy] [Kimi 1 ] [REL] Author : Akhil Premkumar We explore the connection between deep learning and information theory through the paradigm of diffusion models. A diffusion model converts noise into structured data by reinstating, imperfectly, information that is erased when data was diffused to noise. This information is stored in a neural network during training. We quantify this information by introducing a measure called \\textit{neural entropy}, which is related to the total entropy produced by diffusion. Neural entropy is a function of not just the data distribution, but also the diffusive process itself. Measurements of neural entropy on a few simple image diffusion models reveal that they are extremely efficient at compressing large ensembles of structured data. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "skunuOdavO@OpenReview",
      "index": 372,
      "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
      "authors": [
        "Artem Lukoianov",
        "Chenyang Yuan",
        "Justin Solomon",
        "Vincent Sitzmann"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "locality",
        "diffusion",
        "pixel",
        "denoiser",
        "image",
        "emerges",
        "work",
        "neural",
        "patterns",
        "models"
      ],
      "summary": "Recent work has shown that the generalization ability of image diffusion models arises from the locality properties of the trained neural network. In particular, when denoising a particular pixel, the model relies on a limited neighborhood of the input image around that pixel, which, according to the previous work, is tightly related to the ability of these models to produce novel images. Since locality is central to generalization, it is crucial to understand why diffusion models learn local behavior in the first place, as well as the factors that govern the properties of locality patterns. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset and is not due to the inductive bias of convolutional neural networks, as suggested in previous work. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to deep neural denoisers. We show, both theoretically and experimentally, that this locality arises directly from pixel correlations present in the image datasets. Moreover, locality patterns are drastically different on specialized datasets, approximating principal components of the data’s covariance. We use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than prior expert-crafted alternatives. Our key takeaway is that while neural network architectures influence generation quality, their primary role is to capture locality patterns inherent in the data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=skunuOdavO"
        ],
        "venue": [
          "/venue/skunuOdavO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=skunuOdavO"
        ],
        "detail": [
          "https://openreview.net/forum?id=skunuOdavO"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Locality in Image Diffusion Models Emerges from Data Statistics [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Artem Lukoianov , Chenyang Yuan , Justin Solomon , Vincent Sitzmann Recent work has shown that the generalization ability of image diffusion models arises from the locality properties of the trained neural network. In particular, when denoising a particular pixel, the model relies on a limited neighborhood of the input image around that pixel, which, according to the previous work, is tightly related to the ability of these models to produce novel images. Since locality is central to generalization, it is crucial to understand why diffusion models learn local behavior in the first place, as well as the factors that govern the properties of locality patterns. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset and is not due to the inductive bias of convolutional neural networks, as suggested in previous work. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to deep neural denoisers. We show, both theoretically and experimentally, that this locality arises directly from pixel correlations present in the image datasets. Moreover, locality patterns are drastically different on specialized datasets, approximating principal components of the data’s covariance. We use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than prior expert-crafted alternatives. Our key takeaway is that while neural network architectures influence generation quality, their primary role is to capture locality patterns inherent in the data. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TjQP5hc3WC@OpenReview",
      "index": 373,
      "title": "Regularized least squares learning with heavy-tailed noise is minimax optimal",
      "authors": [
        "Mattes Mollenhauer",
        "Nicole Mücke",
        "Dimitri Meunier",
        "Arthur Gretton"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "subgaussian",
        "noise",
        "tailed",
        "regularized",
        "squares",
        "heavy",
        "fuk",
        "nagaev",
        "minimax",
        "least"
      ],
      "summary": "This paper examines the performance of ridge regression in reproducing kernel Hilbert spaces in the presence of noise that exhibits a finite number of higher moments. We establish excess risk bounds consisting of subgaussian and polynomial terms based on the well known integral operator framework. The dominant subgaussian component allows to achieve convergence rates that have previously only been derived under subexponential noise—a prevalent assumption in related work from the last two decades. These rates are optimal under standard eigenvalue decay conditions, demonstrating the asymptotic robustness of regularized least squares against heavy- tailed noise. Our derivations are based on a Fuk–Nagaev inequality for Hilbert-space valued random variables.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TjQP5hc3WC"
        ],
        "venue": [
          "/venue/TjQP5hc3WC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TjQP5hc3WC"
        ],
        "detail": [
          "https://openreview.net/forum?id=TjQP5hc3WC"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Regularized least squares learning with heavy-tailed noise is minimax optimal [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Mattes Mollenhauer , Nicole Mücke , Dimitri Meunier , Arthur Gretton This paper examines the performance of ridge regression in reproducing kernel Hilbert spaces in the presence of noise that exhibits a finite number of higher moments. We establish excess risk bounds consisting of subgaussian and polynomial terms based on the well known integral operator framework. The dominant subgaussian component allows to achieve convergence rates that have previously only been derived under subexponential noise—a prevalent assumption in related work from the last two decades. These rates are optimal under standard eigenvalue decay conditions, demonstrating the asymptotic robustness of regularized least squares against heavy- tailed noise. Our derivations are based on a Fuk–Nagaev inequality for Hilbert-space valued random variables. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qu6mRbSnUs@OpenReview",
      "index": 374,
      "title": "On Feasible Rewards in Multi-Agent Inverse Reinforcement Learning",
      "authors": [
        "Till Freihaut",
        "Giorgia Ramponi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "agent",
        "mairl",
        "equilibrium",
        "feasible",
        "inverse",
        "reinforcement",
        "rewards",
        "multi",
        "markov"
      ],
      "summary": "Multi-agent inverse reinforcement learning (MAIRL) aims to recover agent reward functions from expert demonstrations. We characterize the feasible reward set in Markov games, identifying all reward functions that rationalize a given equilibrium. However, equilibrium-based observations are often ambiguous: a single Nash equilibrium can correspond to many reward structures, potentially changing the game's nature in multi-agent systems. We address this by introducing entropy-regularized Markov games, which yield a unique equilibrium while preserving strategic incentives. For this setting, we provide a sample complexity analysis detailing how errors affect learned policy performance. Our work establishes theoretical foundations and practical insights for MAIRL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qu6mRbSnUs"
        ],
        "venue": [
          "/venue/qu6mRbSnUs@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qu6mRbSnUs"
        ],
        "detail": [
          "https://openreview.net/forum?id=qu6mRbSnUs"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 5
      },
      "raw_excerpt": "On Feasible Rewards in Multi-Agent Inverse Reinforcement Learning [PDF 3 ] [Copy] [Kimi 5 ] [REL] Authors : Till Freihaut , Giorgia Ramponi Multi-agent inverse reinforcement learning (MAIRL) aims to recover agent reward functions from expert demonstrations. We characterize the feasible reward set in Markov games, identifying all reward functions that rationalize a given equilibrium. However, equilibrium-based observations are often ambiguous: a single Nash equilibrium can correspond to many reward structures, potentially changing the game's nature in multi-agent systems. We address this by introducing entropy-regularized Markov games, which yield a unique equilibrium while preserving strategic incentives. For this setting, we provide a sample complexity analysis detailing how errors affect learned policy performance. Our work establishes theoretical foundations and practical insights for MAIRL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XfHfTqeXfZ@OpenReview",
      "index": 375,
      "title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention",
      "authors": [
        "Can Yaras",
        "Alec S Xu",
        "Pierre Abillama",
        "Changwoo Lee",
        "Laura Balzano"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "monarchattention",
        "attention",
        "monarch",
        "softmax",
        "matrices",
        "sequences",
        "hardware",
        "structured",
        "complexity",
        "times"
      ],
      "summary": "Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention -- a novel approach to sub-quadratic attention approximation via Monarch matrices, an expressive class of structured matrices. Based on the variational form of softmax, we describe an efficient optimization-based algorithm to compute an approximate projection of softmax attention onto the class of Monarch matrices with Θ ( N N − − √ d ) Θ ( N N d ) computational complexity and Θ ( N d ) Θ ( N d ) memory/IO complexity. Unlike previous approaches, MonarchAttention is both (1) transferable, yielding minimal performance loss with no additional training, even when replacing every attention layer of the transformer, and (2) hardware-efficient, utilizing the highest-throughput tensor core units on modern GPUs. With optimized kernels, MonarchAttention achieves substantial speed-ups in wall-time over FlashAttention-2: 1.4 × 1.4 × for shorter sequences ( N = 256 ) ( N = 256 ) , 4.5 × 4.5 × for medium-length sequences ( N = 4 K ) ( N = 4 K ) , and 8.2 × 8.2 × for longer sequences ( N = 16 K ) ( N = 16 K ) . We demonstrate the quality of MonarchAttention on diverse tasks and architectures in vision and language problems, showing that it flexibly and accurately approximates softmax attention in a variety of contexts.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XfHfTqeXfZ"
        ],
        "venue": [
          "/venue/XfHfTqeXfZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XfHfTqeXfZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=XfHfTqeXfZ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Can Yaras , Alec S Xu , Pierre Abillama , Changwoo Lee , Laura Balzano Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention -- a novel approach to sub-quadratic attention approximation via Monarch matrices, an expressive class of structured matrices. Based on the variational form of softmax, we describe an efficient optimization-based algorithm to compute an approximate projection of softmax attention onto the class of Monarch matrices with Θ ( N N − − √ d ) Θ ( N N d ) computational complexity and Θ ( N d ) Θ ( N d ) memory/IO complexity. Unlike previous approaches, MonarchAttention is both (1) transferable, yielding minimal performance loss with no additional training, even when replacing every attention layer of the transformer, and (2) hardware-efficient, utilizing the highest-throughput tensor core units on modern GPUs. With optimized kernels, MonarchAttention achieves substantial speed-ups in wall-time over FlashAttention-2: 1.4 × 1.4 × for shorter sequences ( N = 256 ) ( N = 256 ) , 4.5 × 4.5 × for medium-length sequences ( N = 4 K ) ( N = 4 K ) , and 8.2 × 8.2 × for longer sequences ( N = 16 K ) ( N = 16 K ) . We demonstrate the quality of MonarchAttention on diverse tasks and architectures in vision and language problems, showing that it flexibly and accurately approximates softmax attention in a variety of contexts. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ccPts3Df2q@OpenReview",
      "index": 376,
      "title": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test Awareness",
      "authors": [
        "Sahar Abdelnabi",
        "Ahmed Salem"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "awareness",
        "test",
        "hawthorne",
        "reasoning",
        "safety",
        "harmful",
        "impacts",
        "steering",
        "llms",
        "steers"
      ],
      "summary": "Reasoning-focused LLMs sometimes alter their behavior when they detect that they are being evaluated—which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such “test awareness” impacts model behavior, particularly its performance on safety-related tasks. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-weight reasoning LLMs across both realistic and hypothetical tasks (denoting tests or simulations). Our results demonstrate that test awareness significantly impacts safety alignment (such as compliance with harmful requests and conforming to stereotypes) with effects varying in both magnitude and direction across models. By providing control over this latent effect, our work aims to provide a stress-test mechanism and increase trust in how we perform safety evaluations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ccPts3Df2q"
        ],
        "venue": [
          "/venue/ccPts3Df2q@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ccPts3Df2q"
        ],
        "detail": [
          "https://openreview.net/forum?id=ccPts3Df2q"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test Awareness [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Sahar Abdelnabi , Ahmed Salem Reasoning-focused LLMs sometimes alter their behavior when they detect that they are being evaluated—which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such “test awareness” impacts model behavior, particularly its performance on safety-related tasks. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-weight reasoning LLMs across both realistic and hypothetical tasks (denoting tests or simulations). Our results demonstrate that test awareness significantly impacts safety alignment (such as compliance with harmful requests and conforming to stereotypes) with effects varying in both magnitude and direction across models. By providing control over this latent effect, our work aims to provide a stress-test mechanism and increase trust in how we perform safety evaluations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "R0LqbSgZjP@OpenReview",
      "index": 377,
      "title": "Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks",
      "authors": [
        "Ann Huang",
        "Satpreet Harcharan Singh",
        "Flavio Martinelli",
        "Kanaka Rajan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "degeneracy",
        "rnns",
        "task",
        "trained",
        "neural",
        "controlling",
        "neuroscience",
        "solution",
        "networks",
        "contravariance"
      ],
      "summary": "Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions, a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks—flip-flop memory, sine wave generation, delayed discrimination, and path integration—while systematically varying task complexity, learning regime, network size, and regularization. We find that increased task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers seeking to tune the variability of RNN solutions, either to uncover shared neural mechanisms or to model the individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=R0LqbSgZjP"
        ],
        "venue": [
          "/venue/R0LqbSgZjP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=R0LqbSgZjP"
        ],
        "detail": [
          "https://openreview.net/forum?id=R0LqbSgZjP"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ann Huang , Satpreet Harcharan Singh , Flavio Martinelli , Kanaka Rajan Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions, a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks—flip-flop memory, sine wave generation, delayed discrimination, and path integration—while systematically varying task complexity, learning regime, network size, and regularization. We find that increased task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers seeking to tune the variability of RNN solutions, either to uncover shared neural mechanisms or to model the individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "DYuPwwDy9n@OpenReview",
      "index": 378,
      "title": "Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach",
      "authors": [
        "Chenbei Lu",
        "Zaiwei Chen",
        "Tongxin Li",
        "Chenye Wu",
        "Adam Wierman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "predictions",
        "bola",
        "imperfect",
        "mdps",
        "jensen",
        "step",
        "bayesian",
        "bellman",
        "value",
        "transition"
      ],
      "summary": "Traditional reinforcement learning (RL) assumes the agents make decisions based on Markov decision processes (MDPs) with one-step transition models. In many real-world applications, such as energy management and stock investment, agents can access multi-step predictions of future states, which provide additional advantages for decision making. However, multi-step predictions are inherently high-dimensional: naively embedding these predictions into an MDP leads to an exponential blow-up in state space and the curse of dimensionality. Moreover, existing RL theory provides few tools to analyze prediction-augmented MDPs, as it typically works on one-step transition kernels and cannot accommodate multi-step predictions with errors or partial action-coverage. We address these challenges with three key innovations: First, we propose the \\emph{Bayesian value function} to characterize the optimal prediction-aware policy tractably. Second, we develop a novel \\emph{Bellman–Jensen Gap} analysis on the Bayesian value function, which enables characterizing the value of imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with Online Adaptation), a two-stage model-based RL algorithm that separates offline Bayesian value learning from lightweight online adaptation to real-time predictions. We prove that BOLA remains sample-efficient even under imperfect predictions. We validate our theory and algorithm on synthetic MDPs and a real-world wind energy storage control problem.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DYuPwwDy9n"
        ],
        "venue": [
          "/venue/DYuPwwDy9n@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DYuPwwDy9n"
        ],
        "detail": [
          "https://openreview.net/forum?id=DYuPwwDy9n"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Chenbei Lu , Zaiwei Chen , Tongxin Li , Chenye Wu , Adam Wierman Traditional reinforcement learning (RL) assumes the agents make decisions based on Markov decision processes (MDPs) with one-step transition models. In many real-world applications, such as energy management and stock investment, agents can access multi-step predictions of future states, which provide additional advantages for decision making. However, multi-step predictions are inherently high-dimensional: naively embedding these predictions into an MDP leads to an exponential blow-up in state space and the curse of dimensionality. Moreover, existing RL theory provides few tools to analyze prediction-augmented MDPs, as it typically works on one-step transition kernels and cannot accommodate multi-step predictions with errors or partial action-coverage. We address these challenges with three key innovations: First, we propose the \\emph{Bayesian value function} to characterize the optimal prediction-aware policy tractably. Second, we develop a novel \\emph{Bellman–Jensen Gap} analysis on the Bayesian value function, which enables characterizing the value of imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with Online Adaptation), a two-stage model-based RL algorithm that separates offline Bayesian value learning from lightweight online adaptation to real-time predictions. We prove that BOLA remains sample-efficient even under imperfect predictions. We validate our theory and algorithm on synthetic MDPs and a real-world wind energy storage control problem. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RwfrdKSgCE@OpenReview",
      "index": 379,
      "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench",
      "authors": [
        "Edan Toledo",
        "Karen Hambardzumyan",
        "Martin Josifoski",
        "RISHI HAZRA",
        "Nicolas Baldwin",
        "Alexis Audran-Reiss",
        "Michael Kuchnik",
        "Despoina Magka",
        "Minqi Jiang",
        "Alisia Maria Lupidi",
        "Andrei Lupu",
        "Roberta Raileanu",
        "Tatiana Shavrina",
        "Kelvin Niu",
        "Jean-Christophe Gagnon-Audet",
        "Michael Shvartsman",
        "Shagun Sodhani",
        "Alexander H Miller",
        "Abhishek Charnalia",
        "Derek Dunfield",
        "Carole-Jean Wu",
        "Pontus Stenetorp",
        "Nicola Cancedda",
        "Jakob Nicolaus Foerster",
        "Yoram Bachrach"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mle",
        "bench",
        "agents",
        "search",
        "machine",
        "kaggle",
        "research",
        "policies",
        "operator",
        "medal"
      ],
      "summary": "AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RwfrdKSgCE"
        ],
        "venue": [
          "/venue/RwfrdKSgCE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RwfrdKSgCE"
        ],
        "detail": [
          "https://openreview.net/forum?id=RwfrdKSgCE"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Edan Toledo , Karen Hambardzumyan , Martin Josifoski , RISHI HAZRA , Nicolas Baldwin , Alexis Audran-Reiss , Michael Kuchnik , Despoina Magka , Minqi Jiang , Alisia Maria Lupidi , Andrei Lupu , Roberta Raileanu , Tatiana Shavrina , Kelvin Niu , Jean-Christophe Gagnon-Audet , Michael Shvartsman , Shagun Sodhani , Alexander H Miller , Abhishek Charnalia , Derek Dunfield , Carole-Jean Wu , Pontus Stenetorp , Nicola Cancedda , Jakob Nicolaus Foerster , Yoram Bachrach AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "95z3psF9zJ@OpenReview",
      "index": 380,
      "title": "Differentiable Cyclic Causal Discovery Under Unmeasured Confounders",
      "authors": [
        "Muralikrishnna Guruswamy Sethuraman",
        "Faramarz Fekri"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "causal",
        "dccd",
        "confounders",
        "unmeasured",
        "conf",
        "confounder",
        "differentiable",
        "cyclic",
        "discovery",
        "graph"
      ],
      "summary": "Understanding causal relationships between variables is fundamental across scientific disciplines. Most causal discovery algorithms rely on two key assumptions: (i) all variables are observed, and (ii) the underlying causal graph is acyclic. While these assumptions simplify theoretical analysis, they are often violated in real-world systems, such as biological networks. Existing methods that account for confounders either assume linearity or struggle with scalability. To address these limitations, we propose DCCD-CONF, a novel framework for differentiable learning of nonlinear cyclic causal graphs in the presence of unmeasured confounders using interventional data. Our approach alternates between optimizing the graph structure and estimating the confounder distribution by maximizing the log-likelihood of the data. Through experiments on synthetic data and real-world gene perturbation datasets, we show that DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery and confounder identification. Additionally, we provide consistency guarantees for our framework, reinforcing its theoretical soundness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=95z3psF9zJ"
        ],
        "venue": [
          "/venue/95z3psF9zJ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=95z3psF9zJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=95z3psF9zJ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Differentiable Cyclic Causal Discovery Under Unmeasured Confounders [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Muralikrishnna Guruswamy Sethuraman , Faramarz Fekri Understanding causal relationships between variables is fundamental across scientific disciplines. Most causal discovery algorithms rely on two key assumptions: (i) all variables are observed, and (ii) the underlying causal graph is acyclic. While these assumptions simplify theoretical analysis, they are often violated in real-world systems, such as biological networks. Existing methods that account for confounders either assume linearity or struggle with scalability. To address these limitations, we propose DCCD-CONF, a novel framework for differentiable learning of nonlinear cyclic causal graphs in the presence of unmeasured confounders using interventional data. Our approach alternates between optimizing the graph structure and estimating the confounder distribution by maximizing the log-likelihood of the data. Through experiments on synthetic data and real-world gene perturbation datasets, we show that DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery and confounder identification. Additionally, we provide consistency guarantees for our framework, reinforcing its theoretical soundness. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Rz9ISbLxf0@OpenReview",
      "index": 381,
      "title": "ELECTRA: A Cartesian Network for 3D Charge Density Prediction with Floating Orbitals",
      "authors": [
        "Jonas Elsborg",
        "Luca Thiede",
        "Alan Aspuru-Guzik",
        "Tejs Vegge",
        "Arghya Bhowmik"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "orbitals",
        "electra",
        "floating",
        "cartesian",
        "densities",
        "charge",
        "tensor",
        "predicting",
        "position",
        "orbital"
      ],
      "summary": "We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an equivariant model for predicting electronic charge densities using floating orbitals. Floating orbitals are a long-standing concept in the quantum chemistry community that promises more compact and accurate representations by placing orbitals freely in space, as opposed to centering all orbitals at the position of atoms. Finding the ideal placement of these orbitals requires extensive domain knowledge, though, which thus far has prevented widespread adoption. We solve this in a data-driven manner by training a Cartesian tensor network to predict the orbital positions along with orbital coefficients. This is made possible through a symmetry-breaking mechanism that is used to learn position displacements with lower symmetry than the input molecule while preserving the rotation equivariance of the charge density itself. Inspired by recent successes of Gaussian Splatting in representing densities in space, we are using Gaussian orbitals and predicting their weights and covariance matrices. Our method achieves a state-of-the-art balance between computational efficiency and predictive accuracy on established benchmarks. Furthermore, ELECTRA is able to lower the compute time required to arrive at converged DFT solutions - initializing calculations using our predicted densities yields an average 50.72 % reduction in self-consistent field (SCF) iterations on unseen molecules.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Rz9ISbLxf0"
        ],
        "venue": [
          "/venue/Rz9ISbLxf0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Rz9ISbLxf0"
        ],
        "detail": [
          "https://openreview.net/forum?id=Rz9ISbLxf0"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ELECTRA: A Cartesian Network for 3D Charge Density Prediction with Floating Orbitals [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jonas Elsborg , Luca Thiede , Alan Aspuru-Guzik , Tejs Vegge , Arghya Bhowmik We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an equivariant model for predicting electronic charge densities using floating orbitals. Floating orbitals are a long-standing concept in the quantum chemistry community that promises more compact and accurate representations by placing orbitals freely in space, as opposed to centering all orbitals at the position of atoms. Finding the ideal placement of these orbitals requires extensive domain knowledge, though, which thus far has prevented widespread adoption. We solve this in a data-driven manner by training a Cartesian tensor network to predict the orbital positions along with orbital coefficients. This is made possible through a symmetry-breaking mechanism that is used to learn position displacements with lower symmetry than the input molecule while preserving the rotation equivariance of the charge density itself. Inspired by recent successes of Gaussian Splatting in representing densities in space, we are using Gaussian orbitals and predicting their weights and covariance matrices. Our method achieves a state-of-the-art balance between computational efficiency and predictive accuracy on established benchmarks. Furthermore, ELECTRA is able to lower the compute time required to arrive at converged DFT solutions - initializing calculations using our predicted densities yields an average 50.72 % reduction in self-consistent field (SCF) iterations on unseen molecules. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WbpXT0WL9S@OpenReview",
      "index": 382,
      "title": "Compositional Neural Network Verification via Assume-Guarantee Reasoning",
      "authors": [
        "Hai Duong",
        "David Shriver",
        "ThanhVu Nguyen",
        "Matthew B. Dwyer"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "covenn",
        "neuralsat",
        "verification",
        "verifiers",
        "compositional",
        "guarantee",
        "neural",
        "assume",
        "dynaroars",
        "networks"
      ],
      "summary": "Verifying the behavior of neural networks is necessary if developers are to confidently deploy them as parts of mission-critical systems. Toward this end, researchers have been actively developing a range of increasingly sophisticated and scalable neural network verifiers. However, scaling verification to large networks is challenging, at least in part due to the significant memory requirements of verification algorithms. In this paper, we propose an assume-guarantee compositional framework, CoVeNN, that is parameterized by an underlying verifier to generate a sequence of verification sub-problems to address this challenge. We present an iterative refinement-based strategy for computing assumptions that allow sub-problems to retain sufficient accuracy. An evaluation using 7 neural networks and a total of 140 property specifications demonstrates that CoVeNN can verify nearly 7 times more problems than state-of-the-art verifiers. CoVeNN is part of the NeuralSAT verification project: https://github.com/dynaroars/neuralsat.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WbpXT0WL9S"
        ],
        "venue": [
          "/venue/WbpXT0WL9S@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WbpXT0WL9S"
        ],
        "detail": [
          "https://openreview.net/forum?id=WbpXT0WL9S"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Compositional Neural Network Verification via Assume-Guarantee Reasoning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hai Duong , David Shriver , ThanhVu Nguyen , Matthew B. Dwyer Verifying the behavior of neural networks is necessary if developers are to confidently deploy them as parts of mission-critical systems. Toward this end, researchers have been actively developing a range of increasingly sophisticated and scalable neural network verifiers. However, scaling verification to large networks is challenging, at least in part due to the significant memory requirements of verification algorithms. In this paper, we propose an assume-guarantee compositional framework, CoVeNN, that is parameterized by an underlying verifier to generate a sequence of verification sub-problems to address this challenge. We present an iterative refinement-based strategy for computing assumptions that allow sub-problems to retain sufficient accuracy. An evaluation using 7 neural networks and a total of 140 property specifications demonstrates that CoVeNN can verify nearly 7 times more problems than state-of-the-art verifiers. CoVeNN is part of the NeuralSAT verification project: https://github.com/dynaroars/neuralsat. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "31CaYYw1Xz@OpenReview",
      "index": 383,
      "title": "A machine learning approach that beats Rubik's cubes",
      "authors": [
        "Alexander Chervov",
        "Kirill Khoruzhii",
        "Nikita Bukhal",
        "Jalal Naghiyev",
        "Vladislav Zamkovoy",
        "Ivan Koltsov",
        "Lyudmila Cheldieva",
        "Arsenii Sychev",
        "Arsenii Lenin",
        "Mark Obozov",
        "Egor Urvanov",
        "Alexey M. Romanov"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rubik",
        "3x3x3",
        "cubes",
        "pathfinding",
        "machine",
        "5x5x5",
        "deepcubea",
        "4x4x4",
        "beats",
        "outperforming"
      ],
      "summary": "The paper proposes a novel machine learning-based approach to the pathfinding problem on extremely large graphs. This method leverages diffusion distance estimation via a neural network and uses beam search for pathfinding. We demonstrate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik's cubes with unprecedentedly short solution lengths, outperforming all available solvers and introducing the first machine learning solver beyond the 3x3x3 case. In particular, it surpasses every single case of the combined best results in the Kaggle Santa 2023 challenge, which involved over 1,000 teams. For the 3x3x3 Rubik's cube, our approach achieves an optimality rate exceeding 98%, matching the performance of task-specific solvers and significantly outperforming prior solutions such as DeepCubeA (60.3%) and EfficientCube (69.6%). Our solution in its current implementation is approximately 25.6 times faster in solving 3x3x3 Rubik's cubes while requiring up to 8.5 times less model training time than the most efficient state-of-the-art competitor. Finally, it is demonstrated that even a single agent trained using a relatively small number of examples can robustly solve a broad range of puzzles represented by Cayley graphs of size up to 10 145 10 145 , confirming the generality of the proposed method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=31CaYYw1Xz"
        ],
        "venue": [
          "/venue/31CaYYw1Xz@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=31CaYYw1Xz"
        ],
        "detail": [
          "https://openreview.net/forum?id=31CaYYw1Xz"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "A machine learning approach that beats Rubik's cubes [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Alexander Chervov , Kirill Khoruzhii , Nikita Bukhal , Jalal Naghiyev , Vladislav Zamkovoy , Ivan Koltsov , Lyudmila Cheldieva , Arsenii Sychev , Arsenii Lenin , Mark Obozov , Egor Urvanov , Alexey M. Romanov The paper proposes a novel machine learning-based approach to the pathfinding problem on extremely large graphs. This method leverages diffusion distance estimation via a neural network and uses beam search for pathfinding. We demonstrate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik's cubes with unprecedentedly short solution lengths, outperforming all available solvers and introducing the first machine learning solver beyond the 3x3x3 case. In particular, it surpasses every single case of the combined best results in the Kaggle Santa 2023 challenge, which involved over 1,000 teams. For the 3x3x3 Rubik's cube, our approach achieves an optimality rate exceeding 98%, matching the performance of task-specific solvers and significantly outperforming prior solutions such as DeepCubeA (60.3%) and EfficientCube (69.6%). Our solution in its current implementation is approximately 25.6 times faster in solving 3x3x3 Rubik's cubes while requiring up to 8.5 times less model training time than the most efficient state-of-the-art competitor. Finally, it is demonstrated that even a single agent trained using a relatively small number of examples can robustly solve a broad range of puzzles represented by Cayley graphs of size up to 10 145 10 145 , confirming the generality of the proposed method. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3CYXSMFv55@OpenReview",
      "index": 384,
      "title": "ENMA: Tokenwise Autoregression for Continuous Neural PDE Operators",
      "authors": [
        "Armand Kassaï Koupaï",
        "Lise Le Boudec",
        "Louis Serrano",
        "Patrick Gallinari"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "enma",
        "tokenwise",
        "pde",
        "generative",
        "autoregression",
        "pdes",
        "dynamics",
        "spatio",
        "compressed",
        "neural"
      ],
      "summary": "Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete—as is often the case—a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3CYXSMFv55"
        ],
        "venue": [
          "/venue/3CYXSMFv55@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3CYXSMFv55"
        ],
        "detail": [
          "https://openreview.net/forum?id=3CYXSMFv55"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "ENMA: Tokenwise Autoregression for Continuous Neural PDE Operators [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Armand Kassaï Koupaï , Lise Le Boudec , Louis Serrano , Patrick Gallinari Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete—as is often the case—a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "XUKUx7Xu89@OpenReview",
      "index": 385,
      "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training",
      "authors": [
        "William Merrill",
        "Shane Arora",
        "Dirk Groeneveld",
        "Hannaneh Hajishirzi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cbs",
        "batch",
        "size",
        "training",
        "olmo",
        "warmup",
        "language",
        "train",
        "gradient",
        "runs"
      ],
      "summary": "The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is *too large* will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a *critical batch size* (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to *directly* measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate *batch size warmup* as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XUKUx7Xu89"
        ],
        "venue": [
          "/venue/XUKUx7Xu89@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XUKUx7Xu89"
        ],
        "detail": [
          "https://openreview.net/forum?id=XUKUx7Xu89"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : William Merrill , Shane Arora , Dirk Groeneveld , Hannaneh Hajishirzi The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is *too large* will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a *critical batch size* (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to *directly* measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate *batch size warmup* as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "loVvFhfsDf@OpenReview",
      "index": 386,
      "title": "Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning",
      "authors": [
        "Bastien Dubail",
        "Stefan Stojanovic",
        "Alexandre Proutiere"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "successor",
        "rank",
        "shift",
        "measure",
        "low",
        "shifted",
        "reinforcement",
        "conditioned",
        "estimation",
        "approximation"
      ],
      "summary": "Low-rank structure is a common implicit assumption in many modern reinforcement learning (RL) algorithms. For instance, reward-free and goal-conditioned RL methods often presume that the successor measure admits a low-rank representation. In this work, we challenge this assumption by first remarking that the successor measure itself is not approximately low-rank. Instead, we demonstrate that a low-rank structure naturally emerges in the shifted successor measure, which captures the system dynamics after bypassing a few initial transitions. We provide finite-sample performance guarantees for the entry-wise estimation of a low-rank approximation of the shifted successor measure from sampled entries. Our analysis reveals that both the approximation and estimation errors are primarily governed by a newly introduced quantitity: the spectral recoverability of the corresponding matrix. To bound this parameter, we derive a new class of functional inequalities for Markov chains that we call Type II Poincaré inequalities and from which we can quantify the amount of shift needed for effective low-rank approximation and estimation. This analysis shows in particular that the required shift depends on decay of the high-order singular values of the shifted successor measure and is hence typically small in practice. Additionally, we establish a connection between the necessary shift and the local mixing properties of the underlying dynamical system, which provides a natural way of selecting the shift. Finally, we validate our theoretical findings with experiments, and demonstrate that shifting the successor measure indeed leads to improved performance in goal-conditioned RL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=loVvFhfsDf"
        ],
        "venue": [
          "/venue/loVvFhfsDf@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=loVvFhfsDf"
        ],
        "detail": [
          "https://openreview.net/forum?id=loVvFhfsDf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Bastien Dubail , Stefan Stojanovic , Alexandre Proutiere Low-rank structure is a common implicit assumption in many modern reinforcement learning (RL) algorithms. For instance, reward-free and goal-conditioned RL methods often presume that the successor measure admits a low-rank representation. In this work, we challenge this assumption by first remarking that the successor measure itself is not approximately low-rank. Instead, we demonstrate that a low-rank structure naturally emerges in the shifted successor measure, which captures the system dynamics after bypassing a few initial transitions. We provide finite-sample performance guarantees for the entry-wise estimation of a low-rank approximation of the shifted successor measure from sampled entries. Our analysis reveals that both the approximation and estimation errors are primarily governed by a newly introduced quantitity: the spectral recoverability of the corresponding matrix. To bound this parameter, we derive a new class of functional inequalities for Markov chains that we call Type II Poincaré inequalities and from which we can quantify the amount of shift needed for effective low-rank approximation and estimation. This analysis shows in particular that the required shift depends on decay of the high-order singular values of the shifted successor measure and is hence typically small in practice. Additionally, we establish a connection between the necessary shift and the local mixing properties of the underlying dynamical system, which provides a natural way of selecting the shift. Finally, we validate our theoretical findings with experiments, and demonstrate that shifting the successor measure indeed leads to improved performance in goal-conditioned RL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fGfl6dqQVf@OpenReview",
      "index": 387,
      "title": "Stochastic Optimization in Semi-Discrete Optimal Transport: Convergence Analysis and Minimax Rate",
      "authors": [
        "Ferdinand Genans",
        "Antoine Godichon-Baggioni",
        "François-Xavier Vialard",
        "Olivier Wintenberger"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sgd",
        "minimax",
        "measure",
        "discrete",
        "convergence",
        "whic",
        "source",
        "map",
        "mtw",
        "stochastic"
      ],
      "summary": "We investigate the semi-discrete Optimal Transport (OT) problem, where a continuous source measure μ μ is transported to a discrete target measure ν ν , with particular attention to the OT map approximation. In this setting, Stochastic Gradient Descent (SGD) based solvers have demonstrated strong empirical performance in recent machine learning applications, yet their theoretical guarantee to approximate the OT map is an open question. In this work, we answer it positively by providing both computational and statistical convergence guarantees of SGD. Specifically, we show that SGD methods can estimate the OT map with a minimax convergence rate of O ( 1 / n − − √ ) O ( 1 / n ) , where n n is the number of samples drawn from μ μ . To establish this result, we study the averaged projected SGD algorithm, and identify a suitable projection set that contains a minimizer of the objective, even when the source measure is not compactly supported. Our analysis holds under mild assumptions on the source measure and applies to MTW cost functions,whic include ∥ ⋅ ∥ p ‖ ⋅ ‖ p for p ∈ ( 1 , ∞ ) p ∈ ( 1 , ∞ ) . We finally provide numerical evidence for our theoretical results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fGfl6dqQVf"
        ],
        "venue": [
          "/venue/fGfl6dqQVf@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fGfl6dqQVf"
        ],
        "detail": [
          "https://openreview.net/forum?id=fGfl6dqQVf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Stochastic Optimization in Semi-Discrete Optimal Transport: Convergence Analysis and Minimax Rate [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Ferdinand Genans , Antoine Godichon-Baggioni , François-Xavier Vialard , Olivier Wintenberger We investigate the semi-discrete Optimal Transport (OT) problem, where a continuous source measure μ μ is transported to a discrete target measure ν ν , with particular attention to the OT map approximation. In this setting, Stochastic Gradient Descent (SGD) based solvers have demonstrated strong empirical performance in recent machine learning applications, yet their theoretical guarantee to approximate the OT map is an open question. In this work, we answer it positively by providing both computational and statistical convergence guarantees of SGD. Specifically, we show that SGD methods can estimate the OT map with a minimax convergence rate of O ( 1 / n − − √ ) O ( 1 / n ) , where n n is the number of samples drawn from μ μ . To establish this result, we study the averaged projected SGD algorithm, and identify a suitable projection set that contains a minimizer of the objective, even when the source measure is not compactly supported. Our analysis holds under mild assumptions on the source measure and applies to MTW cost functions,whic include ∥ ⋅ ∥ p ‖ ⋅ ‖ p for p ∈ ( 1 , ∞ ) p ∈ ( 1 , ∞ ) . We finally provide numerical evidence for our theoretical results. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yb5JOOmfxA@OpenReview",
      "index": 388,
      "title": "Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models",
      "authors": [
        "Tomas Soucek",
        "Sylvestre-Alvise Rebuffi",
        "Pierre Fernandez",
        "Nikola Jovanović",
        "Hady Elsahar",
        "Valeriu Lacatusu",
        "Tuan A. Tran",
        "Alexandre Mourachko"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "watermarking",
        "watermarks",
        "forging",
        "watermark",
        "content",
        "watermarked",
        "forge",
        "image",
        "preference",
        "hoc"
      ],
      "summary": "Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yb5JOOmfxA"
        ],
        "venue": [
          "/venue/yb5JOOmfxA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yb5JOOmfxA"
        ],
        "detail": [
          "https://openreview.net/forum?id=yb5JOOmfxA"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Tomas Soucek , Sylvestre-Alvise Rebuffi , Pierre Fernandez , Nikola Jovanović , Hady Elsahar , Valeriu Lacatusu , Tuan A. Tran , Alexandre Mourachko Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "UwtFuWbW6B@OpenReview",
      "index": 389,
      "title": "Non-Asymptotic Analysis Of Data Augmentation For Precision Matrix Estimation",
      "authors": [
        "Lucas Morisset",
        "Adrien Hardy",
        "Alain Oliviero Durmus"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "estimators",
        "matrix",
        "augmentation",
        "precision",
        "samples",
        "estimation",
        "artificial",
        "classes",
        "enriching",
        "random"
      ],
      "summary": "This paper addresses the problem of inverse covariance (also known as precision matrix) estimation in high-dimensional settings. Specifically, we focus on two classes of estimators: linear shrinkage estimators with a target proportional to the identity matrix, and estimators derived from data augmentation (DA). Here, DA refers to the common practice of enriching a dataset with artificial samples—typically generated via a generative model or through random transformations of the original data—prior to model fitting. For both classes of estimators, we derive estimators and provide concentration bounds for their quadratic error. This allows for both method comparison and hyperparameter tuning, such as selecting the optimal proportion of artificial samples. On the technical side, our analysis relies on tools from random matrix theory. We introduce a novel deterministic equivalent for generalized resolvent matrices, accommodating dependent samples with specific structure. We support our theoretical results with numerical experiments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UwtFuWbW6B"
        ],
        "venue": [
          "/venue/UwtFuWbW6B@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UwtFuWbW6B"
        ],
        "detail": [
          "https://openreview.net/forum?id=UwtFuWbW6B"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Non-Asymptotic Analysis Of Data Augmentation For Precision Matrix Estimation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Lucas Morisset , Adrien Hardy , Alain Oliviero Durmus This paper addresses the problem of inverse covariance (also known as precision matrix) estimation in high-dimensional settings. Specifically, we focus on two classes of estimators: linear shrinkage estimators with a target proportional to the identity matrix, and estimators derived from data augmentation (DA). Here, DA refers to the common practice of enriching a dataset with artificial samples—typically generated via a generative model or through random transformations of the original data—prior to model fitting. For both classes of estimators, we derive estimators and provide concentration bounds for their quadratic error. This allows for both method comparison and hyperparameter tuning, such as selecting the optimal proportion of artificial samples. On the technical side, our analysis relies on tools from random matrix theory. We introduce a novel deterministic equivalent for generalized resolvent matrices, accommodating dependent samples with specific structure. We support our theoretical results with numerical experiments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "AjOl3iahHd@OpenReview",
      "index": 390,
      "title": "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration",
      "authors": [
        "Thomas Decker",
        "Volker Tresp",
        "Florian Buettner"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "explanations",
        "perturbation",
        "recalx",
        "uncertainty",
        "calibration",
        "specific",
        "perturbations",
        "explanation",
        "recalibrate",
        "miscalibration"
      ],
      "summary": "Perturbation-based explanations are widely utilized to enhance the transparency of machine-learning models in practice. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved explanations while preserving their original predictions. Empirical evaluations across diverse models and datasets demonstrate that ReCalX consistently reduces perturbation-specific miscalibration most effectively while enhancing explanation robustness and the identification of globally important input features.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AjOl3iahHd"
        ],
        "venue": [
          "/venue/AjOl3iahHd@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AjOl3iahHd"
        ],
        "detail": [
          "https://openreview.net/forum?id=AjOl3iahHd"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Thomas Decker , Volker Tresp , Florian Buettner Perturbation-based explanations are widely utilized to enhance the transparency of machine-learning models in practice. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved explanations while preserving their original predictions. Empirical evaluations across diverse models and datasets demonstrate that ReCalX consistently reduces perturbation-specific miscalibration most effectively while enhancing explanation robustness and the identification of globally important input features. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kGMRb4jbTP@OpenReview",
      "index": 391,
      "title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner",
      "authors": [
        "Marco Spinaci",
        "Marek Polewczyk",
        "Maximilian Schambach",
        "Sam Thelin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tabular",
        "contexttab",
        "icl",
        "world",
        "semantics",
        "tabicl",
        "context",
        "native",
        "sota",
        "tabpfn"
      ],
      "summary": "Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: https://github.com/SAP-samples/contexttab .",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kGMRb4jbTP"
        ],
        "venue": [
          "/venue/kGMRb4jbTP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kGMRb4jbTP"
        ],
        "detail": [
          "https://openreview.net/forum?id=kGMRb4jbTP"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ConTextTab: A Semantics-Aware Tabular In-Context Learner [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Marco Spinaci , Marek Polewczyk , Maximilian Schambach , Sam Thelin Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: https://github.com/SAP-samples/contexttab . Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "s3WyfnHw6B@OpenReview",
      "index": 392,
      "title": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness",
      "authors": [
        "Mojtaba Kolahdouzi",
        "Hatice Gunes",
        "Ali Etemad"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "optimizers",
        "fairness",
        "rmsprop",
        "sgd",
        "fairer",
        "equal",
        "optimization",
        "adaptive",
        "group",
        "fairface"
      ],
      "summary": "We study whether and how the choice of optimization algorithm can impact group fairness in deep neural networks. Through stochastic differential equation analysis of optimization dynamics in an analytically tractable setup, we demonstrate that the choice of optimization algorithm indeed influences fairness outcomes, particularly under severe imbalance. Furthermore, we show that when comparing two categories of optimizers, adaptive methods and stochastic methods, RMSProp (from the adaptive category) has a higher likelihood of converging to fairer minima than SGD (from the stochastic category). Building on this insight, we derive two new theoretical guarantees showing that, under appropriate conditions, RMSProp exhibits fairer parameter updates and improved fairness in a single optimization step compared to SGD. We then validate these findings through extensive experiments on three publicly available datasets, namely CelebA, FairFace, and MS-COCO, across different tasks as facial expression recognition, gender classification, and multi-label classification, using various backbones. Considering multiple fairness definitions including equalized odds, equal opportunity, and demographic parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD in terms of group fairness, while maintaining comparable predictive accuracy. Our results highlight the role of adaptive updates as a crucial yet overlooked mechanism for promoting fair outcomes. We release the source code at: https://github.com/Mkolahdoozi/Some-Optimizers-Are-More-Equal.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=s3WyfnHw6B"
        ],
        "venue": [
          "/venue/s3WyfnHw6B@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=s3WyfnHw6B"
        ],
        "detail": [
          "https://openreview.net/forum?id=s3WyfnHw6B"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Mojtaba Kolahdouzi , Hatice Gunes , Ali Etemad We study whether and how the choice of optimization algorithm can impact group fairness in deep neural networks. Through stochastic differential equation analysis of optimization dynamics in an analytically tractable setup, we demonstrate that the choice of optimization algorithm indeed influences fairness outcomes, particularly under severe imbalance. Furthermore, we show that when comparing two categories of optimizers, adaptive methods and stochastic methods, RMSProp (from the adaptive category) has a higher likelihood of converging to fairer minima than SGD (from the stochastic category). Building on this insight, we derive two new theoretical guarantees showing that, under appropriate conditions, RMSProp exhibits fairer parameter updates and improved fairness in a single optimization step compared to SGD. We then validate these findings through extensive experiments on three publicly available datasets, namely CelebA, FairFace, and MS-COCO, across different tasks as facial expression recognition, gender classification, and multi-label classification, using various backbones. Considering multiple fairness definitions including equalized odds, equal opportunity, and demographic parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD in terms of group fairness, while maintaining comparable predictive accuracy. Our results highlight the role of adaptive updates as a crucial yet overlooked mechanism for promoting fair outcomes. We release the source code at: https://github.com/Mkolahdoozi/Some-Optimizers-Are-More-Equal. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "R2ZJSjLDJC@OpenReview",
      "index": 393,
      "title": "Less is More: Improving LLM Alignment via Preference Data Selection",
      "authors": [
        "Xun Deng",
        "Han Zhong",
        "Rui Ai",
        "Fuli Feng",
        "Zheng Wang",
        "Xiangnan He"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dpo",
        "preference",
        "data",
        "selection",
        "alpacaeval2",
        "ultrafeedback",
        "aspect",
        "margin",
        "qwen",
        "approach"
      ],
      "summary": "Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To further mitigate the noise in different reward models, we propose a Bayesian Aggregation approach that unifies multiple margin sources (external and implicit) into a single preference probability. Extensive experiments in diverse settings demonstrate the consistently high data efficiency of our approach. Remarkably, by using just 10\\% of the Ultrafeedback dataset, our approach achieves 3\\% to 8\\% improvements across various Llama, Mistral, and Qwen models on the AlpacaEval2 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data, revealing the high redundancy in this presumed high-quality data construction manner. These results highlight the potential of data selection strategies for advancing preference optimization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=R2ZJSjLDJC"
        ],
        "venue": [
          "/venue/R2ZJSjLDJC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=R2ZJSjLDJC"
        ],
        "detail": [
          "https://openreview.net/forum?id=R2ZJSjLDJC"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Less is More: Improving LLM Alignment via Preference Data Selection [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Xun Deng , Han Zhong , Rui Ai , Fuli Feng , Zheng Wang , Xiangnan He Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To further mitigate the noise in different reward models, we propose a Bayesian Aggregation approach that unifies multiple margin sources (external and implicit) into a single preference probability. Extensive experiments in diverse settings demonstrate the consistently high data efficiency of our approach. Remarkably, by using just 10\\% of the Ultrafeedback dataset, our approach achieves 3\\% to 8\\% improvements across various Llama, Mistral, and Qwen models on the AlpacaEval2 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data, revealing the high redundancy in this presumed high-quality data construction manner. These results highlight the potential of data selection strategies for advancing preference optimization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Vqj65VeDOu@OpenReview",
      "index": 394,
      "title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning",
      "authors": [
        "Roger Creus Castanyer",
        "Johan Obando-Ceron",
        "Lu Li",
        "Pierre-Luc Bacon",
        "Glen Berseth",
        "Aaron Courville",
        "Pablo Samuel Castro"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "interventions",
        "reinforcement",
        "stable",
        "causes",
        "gradients",
        "performance",
        "suites",
        "learning",
        "often",
        "series"
      ],
      "summary": "Scaling deep reinforcement learning networks is challenging and often results in degraded performance, yet the root causes of this failure mode remain poorly understood. Several recent works have proposed mechanisms to address this, but they are often complex and fail to highlight the causes underlying this difficulty. In this work, we conduct a series of empirical analyses which suggest that the combination of non-stationarity with gradient pathologies, due to suboptimal architectural choices, underlie the challenges of scale. We propose a series of direct interventions that stabilize gradient flow, enabling robust performance across a range of network depths and widths. Our interventions are simple to implement and compatible with well-established algorithms, and result in an effective mechanism that enables strong performance even at large scales. We validate our findings on a variety of agents and suites of environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Vqj65VeDOu"
        ],
        "venue": [
          "/venue/Vqj65VeDOu@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Vqj65VeDOu"
        ],
        "detail": [
          "https://openreview.net/forum?id=Vqj65VeDOu"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Roger Creus Castanyer , Johan Obando-Ceron , Lu Li , Pierre-Luc Bacon , Glen Berseth , Aaron Courville , Pablo Samuel Castro Scaling deep reinforcement learning networks is challenging and often results in degraded performance, yet the root causes of this failure mode remain poorly understood. Several recent works have proposed mechanisms to address this, but they are often complex and fail to highlight the causes underlying this difficulty. In this work, we conduct a series of empirical analyses which suggest that the combination of non-stationarity with gradient pathologies, due to suboptimal architectural choices, underlie the challenges of scale. We propose a series of direct interventions that stabilize gradient flow, enabling robust performance across a range of network depths and widths. Our interventions are simple to implement and compatible with well-established algorithms, and result in an effective mechanism that enables strong performance even at large scales. We validate our findings on a variety of agents and suites of environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2sG4ebgqBd@OpenReview",
      "index": 395,
      "title": "SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding",
      "authors": [
        "Thomas Walton",
        "Darin Tsui",
        "Aryan Musharaf",
        "Amirali Aghazadeh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "specmer",
        "protein",
        "speculative",
        "decoding",
        "mer",
        "generation",
        "draft",
        "sequence",
        "sequences",
        "autoregressive"
      ],
      "summary": "Autoregressive models have transformed protein engineering by enabling the generation of novel protein sequences beyond those found in nature. However, their sequential inference introduces significant latency, limiting their utility in high-throughput protein screening. Speculative decoding accelerates generation by employing a lightweight draft model to sample tokens, which a larger target model then verifies and refines. Yet in protein sequence generation, draft models are typically agnostic to the structural and functional constraints of the target protein, leading to biologically implausible outputs and a shift in the likelihood distribution of generated sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a novel framework that incorporates biological, structural, and functional priors using k-mer motifs extracted from multiple sequence alignments. By scoring candidate sequences in parallel and selecting those most consistent with known biological patterns, SpecMER significantly improves sequence plausibility while retaining the efficiency of speculative decoding. SpecMER achieves 24–32% speedup over standard autoregressive decoding, along with higher acceptance rates and improved sequence likelihoods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2sG4ebgqBd"
        ],
        "venue": [
          "/venue/2sG4ebgqBd@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2sG4ebgqBd"
        ],
        "detail": [
          "https://openreview.net/forum?id=2sG4ebgqBd"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Thomas Walton , Darin Tsui , Aryan Musharaf , Amirali Aghazadeh Autoregressive models have transformed protein engineering by enabling the generation of novel protein sequences beyond those found in nature. However, their sequential inference introduces significant latency, limiting their utility in high-throughput protein screening. Speculative decoding accelerates generation by employing a lightweight draft model to sample tokens, which a larger target model then verifies and refines. Yet in protein sequence generation, draft models are typically agnostic to the structural and functional constraints of the target protein, leading to biologically implausible outputs and a shift in the likelihood distribution of generated sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a novel framework that incorporates biological, structural, and functional priors using k-mer motifs extracted from multiple sequence alignments. By scoring candidate sequences in parallel and selecting those most consistent with known biological patterns, SpecMER significantly improves sequence plausibility while retaining the efficiency of speculative decoding. SpecMER achieves 24–32% speedup over standard autoregressive decoding, along with higher acceptance rates and improved sequence likelihoods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4QVLKwgg3S@OpenReview",
      "index": 396,
      "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs",
      "authors": [
        "Jinwoo Park",
        "Seunggeun Cho",
        "Dongsu Han"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "specedge",
        "server",
        "edge",
        "serving",
        "token",
        "assisted",
        "gpus",
        "scalable",
        "llms",
        "llm"
      ],
      "summary": "Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by **1.91×** through achieving **2.22×** server throughput, and reduces inter token latency by **11.24\\%** compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4QVLKwgg3S"
        ],
        "venue": [
          "/venue/4QVLKwgg3S@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4QVLKwgg3S"
        ],
        "detail": [
          "https://openreview.net/forum?id=4QVLKwgg3S"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 5
      },
      "raw_excerpt": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs [PDF 1 ] [Copy] [Kimi 5 ] [REL] Authors : Jinwoo Park , Seunggeun Cho , Dongsu Han Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by **1.91×** through achieving **2.22×** server throughput, and reduces inter token latency by **11.24\\%** compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "baBhSzaSHI@OpenReview",
      "index": 397,
      "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
      "authors": [
        "Simone Carnemolla",
        "Matteo Pennisi",
        "Sarinda Samarasinghe",
        "Giovanni Bellitto",
        "Simone Palazzo",
        "Daniela Giordano",
        "Mubarak Shah",
        "Concetto Spampinato"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dexter",
        "explanations",
        "textual",
        "explanation",
        "perceivelab",
        "language",
        "classifiers",
        "waterbirds",
        "diffusion",
        "classifier"
      ],
      "summary": "Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks—activation maximization, slice discovery and debiasing, and bias explanation—each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=baBhSzaSHI"
        ],
        "venue": [
          "/venue/baBhSzaSHI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=baBhSzaSHI"
        ],
        "detail": [
          "https://openreview.net/forum?id=baBhSzaSHI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Simone Carnemolla , Matteo Pennisi , Sarinda Samarasinghe , Giovanni Bellitto , Simone Palazzo , Daniela Giordano , Mubarak Shah , Concetto Spampinato Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks—activation maximization, slice discovery and debiasing, and bias explanation—each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nYg6Qzm5xS@OpenReview",
      "index": 398,
      "title": "What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains",
      "authors": [
        "Chanakya Ekbote",
        "Ashok Vardhan Makkuva",
        "Marco Bondaschi",
        "Nived Rajaraman",
        "Michael Gastpar",
        "Jason D. Lee",
        "Paul Pu Liang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "icl",
        "markov",
        "layer",
        "transformer",
        "induction",
        "heads",
        "represent",
        "transformers",
        "head",
        "order"
      ],
      "summary": "In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k k -grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1 1 -gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: *can a two-layer single-head transformer represent any k th k th -order Markov process?* In this paper, we precisely address this and theoretically show that a two-layer transformer with one head per layer can indeed represent any conditional k k -gram. Thus, our result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, we further analyze the learning dynamics of our two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen our current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nYg6Qzm5xS"
        ],
        "venue": [
          "/venue/nYg6Qzm5xS@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nYg6Qzm5xS"
        ],
        "detail": [
          "https://openreview.net/forum?id=nYg6Qzm5xS"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Chanakya Ekbote , Ashok Vardhan Makkuva , Marco Bondaschi , Nived Rajaraman , Michael Gastpar , Jason D. Lee , Paul Pu Liang In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k k -grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1 1 -gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: *can a two-layer single-head transformer represent any k th k th -order Markov process?* In this paper, we precisely address this and theoretically show that a two-layer transformer with one head per layer can indeed represent any conditional k k -gram. Thus, our result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, we further analyze the learning dynamics of our two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen our current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gaHjGx1cMh@OpenReview",
      "index": 399,
      "title": "Sample-Adaptivity Tradeoff in On-Demand Sampling",
      "authors": [
        "Nika Haghtalab",
        "Omar Montasser",
        "Mingda Qiao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "oods",
        "round",
        "mdl",
        "tradeoff",
        "complexity",
        "sample",
        "widetilde",
        "adaptivity",
        "demand",
        "sampling"
      ],
      "summary": "We study the tradeoff between sample complexity and round complexity in *on-demand sampling*, where the learning algorithm adaptively samples from k k distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an r r -round algorithm scales approximately as d k Θ ( 1 / r ) / ϵ d k Θ ( 1 / r ) / ϵ . For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of O ˜ ( ( d + k ) / ϵ 2 ) O ~ ( ( d + k ) / ϵ 2 ) within O ˜ ( k − − √ ) O ~ ( k ) rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the O ˜ ( k − − √ ) O ~ ( k ) -round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gaHjGx1cMh"
        ],
        "venue": [
          "/venue/gaHjGx1cMh@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gaHjGx1cMh"
        ],
        "detail": [
          "https://openreview.net/forum?id=gaHjGx1cMh"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Sample-Adaptivity Tradeoff in On-Demand Sampling [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Nika Haghtalab , Omar Montasser , Mingda Qiao We study the tradeoff between sample complexity and round complexity in *on-demand sampling*, where the learning algorithm adaptively samples from k k distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an r r -round algorithm scales approximately as d k Θ ( 1 / r ) / ϵ d k Θ ( 1 / r ) / ϵ . For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of O ˜ ( ( d + k ) / ϵ 2 ) O ~ ( ( d + k ) / ϵ 2 ) within O ˜ ( k − − √ ) O ~ ( k ) rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the O ˜ ( k − − √ ) O ~ ( k ) -round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Oo9KXaM6Mu@OpenReview",
      "index": 400,
      "title": "Online Strategic Classification With Noise and Partial Feedback",
      "authors": [
        "Tianrun Zhao",
        "Xiaojie Mao",
        "Yong Liang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "strategic",
        "label",
        "manipulations",
        "feature",
        "principal",
        "agents",
        "noise",
        "online",
        "classified",
        "announces"
      ],
      "summary": "In this paper, we study an online strategic classification problem, where a principal aims to learn an accurate binary linear classifier from sequentially arriving agents. For each agent, the principal announces a classifier. The agent can strategically exercise costly manipulations on his features to be classified as the favorable positive class. The principal is unaware of the true feature-label distribution, but observes all reported features and only labels of positively classified agents. We assume that the true feature-label distribution is given by a halfspace model subject to arbitrary feature-dependent bounded noise (i.e., Massart Noise). This problem faces the combined challenges of agents' strategic feature manipulations, partial label observations, and label noises. We tackle these challenges by a novel learning algorithm. We show that the proposed algorithm yields classifiers that converge to the clairvoyant optimal one and attains a regret rate of O ( T − − √ ) O ( T ) up to poly-logarithmic and constant factors over T T cycles.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Oo9KXaM6Mu"
        ],
        "venue": [
          "/venue/Oo9KXaM6Mu@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Oo9KXaM6Mu"
        ],
        "detail": [
          "https://openreview.net/forum?id=Oo9KXaM6Mu"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Online Strategic Classification With Noise and Partial Feedback [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Tianrun Zhao , Xiaojie Mao , Yong Liang In this paper, we study an online strategic classification problem, where a principal aims to learn an accurate binary linear classifier from sequentially arriving agents. For each agent, the principal announces a classifier. The agent can strategically exercise costly manipulations on his features to be classified as the favorable positive class. The principal is unaware of the true feature-label distribution, but observes all reported features and only labels of positively classified agents. We assume that the true feature-label distribution is given by a halfspace model subject to arbitrary feature-dependent bounded noise (i.e., Massart Noise). This problem faces the combined challenges of agents' strategic feature manipulations, partial label observations, and label noises. We tackle these challenges by a novel learning algorithm. We show that the proposed algorithm yields classifiers that converge to the clairvoyant optimal one and attains a regret rate of O ( T − − √ ) O ( T ) up to poly-logarithmic and constant factors over T T cycles. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "T6RkYsuoMW@OpenReview",
      "index": 401,
      "title": "Partition-Then-Adapt: Combating Prediction Bias for Reliable Multi-Modal Test-Time Adaptation",
      "authors": [
        "Guowei Wang",
        "Fan Lyu",
        "Changxing Ding"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pta",
        "pdr",
        "aga",
        "combating",
        "bias",
        "reliable",
        "modal",
        "tta",
        "shifts",
        "partition"
      ],
      "summary": "Existing test-time adaptation (TTA) methods primarily focus on scenarios involving domain shifts in a single modality. However, they often prove ineffective when multiple modalities simultaneously undergo domain shifts, as they struggle to identify and utilize reliable samples within testing batches amid severe prediction bias. To address this problem, we propose Partition-Then-Adapt (PTA), a novel approach combating prediction bias for TTA with multi-modal domain shifts. PTA comprises two key components: Partition and Debiased Reweighting (PDR) and multi-modal Attention-Guided Alignment (AGA). Specifically, PDR evaluates each sample’s predicted label frequency relative to the batch average, partitioning the batch into potential reliable and unreliable subsets. It then reweights each sample by jointly assessing its bias and confidence levels through a quantile-based approach. By applying weighted entropy loss, PTA simultaneously promotes learning from reliable subsets and discourages reliance on unreliable ones. Moreover, AGA regularizes PDR to focus on semantically meaningful multi-modal cues. Extensive experiments validate the effectiveness of PTA, surpassing state-of-the-art method by 6.1\\% on Kinetics50-MC and 5.8\\% on VGGSound-MC, respectively. Code of this paper is available at https://github.com/MPI-Lab/PTA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=T6RkYsuoMW"
        ],
        "venue": [
          "/venue/T6RkYsuoMW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=T6RkYsuoMW"
        ],
        "detail": [
          "https://openreview.net/forum?id=T6RkYsuoMW"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 4
      },
      "raw_excerpt": "Partition-Then-Adapt: Combating Prediction Bias for Reliable Multi-Modal Test-Time Adaptation [PDF 2 ] [Copy] [Kimi 4 ] [REL] Authors : Guowei Wang , Fan Lyu , Changxing Ding Existing test-time adaptation (TTA) methods primarily focus on scenarios involving domain shifts in a single modality. However, they often prove ineffective when multiple modalities simultaneously undergo domain shifts, as they struggle to identify and utilize reliable samples within testing batches amid severe prediction bias. To address this problem, we propose Partition-Then-Adapt (PTA), a novel approach combating prediction bias for TTA with multi-modal domain shifts. PTA comprises two key components: Partition and Debiased Reweighting (PDR) and multi-modal Attention-Guided Alignment (AGA). Specifically, PDR evaluates each sample’s predicted label frequency relative to the batch average, partitioning the batch into potential reliable and unreliable subsets. It then reweights each sample by jointly assessing its bias and confidence levels through a quantile-based approach. By applying weighted entropy loss, PTA simultaneously promotes learning from reliable subsets and discourages reliance on unreliable ones. Moreover, AGA regularizes PDR to focus on semantically meaningful multi-modal cues. Extensive experiments validate the effectiveness of PTA, surpassing state-of-the-art method by 6.1\\% on Kinetics50-MC and 5.8\\% on VGGSound-MC, respectively. Code of this paper is available at https://github.com/MPI-Lab/PTA. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "px1OlCcqkj@OpenReview",
      "index": 402,
      "title": "Strategic Hypothesis Testing",
      "authors": [
        "Yatong Chen",
        "Safwan Hossain",
        "Yiling Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "principal",
        "strategic",
        "hypothesis",
        "testing",
        "agent",
        "threshold",
        "value",
        "false",
        "submits",
        "approvals"
      ],
      "summary": "We examine hypothesis testing within a principal-agent framework, where a strategic agent, holding private beliefs about the effectiveness of a product, submits data to a principal who decides on approval. The principal employs a hypothesis testing rule, aiming to pick a p-value threshold that balances false positives and false negatives while anticipating the agent’s incentive to maximize expected profitability. Building on prior work, we develop a game-theoretic model that captures how the agent’s participation and reporting behavior respond to the principal’s statistical decision rule. Despite the complexity of the interaction, we show that the principal's errors exhibit clear monotonic behavior when segmented by an efficiently computable critical p-value threshold, leading to an interpretable characterization of their optimal p-value threshold. We empirically validate our model and these insights using publicly available data on drug approvals. Overall, our work offers a comprehensive perspective on strategic interactions within the hypothesis testing framework, providing technical and regulatory insights.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=px1OlCcqkj"
        ],
        "venue": [
          "/venue/px1OlCcqkj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=px1OlCcqkj"
        ],
        "detail": [
          "https://openreview.net/forum?id=px1OlCcqkj"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Strategic Hypothesis Testing [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Yatong Chen , Safwan Hossain , Yiling Chen We examine hypothesis testing within a principal-agent framework, where a strategic agent, holding private beliefs about the effectiveness of a product, submits data to a principal who decides on approval. The principal employs a hypothesis testing rule, aiming to pick a p-value threshold that balances false positives and false negatives while anticipating the agent’s incentive to maximize expected profitability. Building on prior work, we develop a game-theoretic model that captures how the agent’s participation and reporting behavior respond to the principal’s statistical decision rule. Despite the complexity of the interaction, we show that the principal's errors exhibit clear monotonic behavior when segmented by an efficiently computable critical p-value threshold, leading to an interpretable characterization of their optimal p-value threshold. We empirically validate our model and these insights using publicly available data on drug approvals. Overall, our work offers a comprehensive perspective on strategic interactions within the hypothesis testing framework, providing technical and regulatory insights. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "L51U5RSFBo@OpenReview",
      "index": 403,
      "title": "Accelerating Optimization via Differentiable Stopping Time",
      "authors": [
        "Zhonglin Xie",
        "Yiman Fong",
        "Haoran Yuan",
        "Zaiwen Wen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "differentiable",
        "accelerating",
        "stopping",
        "optimization",
        "time",
        "algorithms",
        "loss",
        "effectiveness",
        "hyperparameters",
        "justify"
      ],
      "summary": "A common approach for accelerating optimization algorithms is to minimize the loss achieved in a fixed time, which enables a differentiable framework with respect to the algorithm's hyperparameters. In contrast, the complementary objective of minimizing the time to reach a target loss is traditionally considered non-differentiable. To address this limitation, we propose a differentiable discrete stopping time and theoretically justify it based on its connection to continuous differential equations. We design an efficient algorithm to compute its sensitivities, thereby enabling a new differentiable formulation for directly accelerating algorithms. We demonstrate its effectiveness in applications such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=L51U5RSFBo"
        ],
        "venue": [
          "/venue/L51U5RSFBo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=L51U5RSFBo"
        ],
        "detail": [
          "https://openreview.net/forum?id=L51U5RSFBo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Accelerating Optimization via Differentiable Stopping Time [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Zhonglin Xie , Yiman Fong , Haoran Yuan , Zaiwen Wen A common approach for accelerating optimization algorithms is to minimize the loss achieved in a fixed time, which enables a differentiable framework with respect to the algorithm's hyperparameters. In contrast, the complementary objective of minimizing the time to reach a target loss is traditionally considered non-differentiable. To address this limitation, we propose a differentiable discrete stopping time and theoretically justify it based on its connection to continuous differential equations. We design an efficient algorithm to compute its sensitivities, thereby enabling a new differentiable formulation for directly accelerating algorithms. We demonstrate its effectiveness in applications such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wAq0ZLxrGq@OpenReview",
      "index": 404,
      "title": "Tight Generalization Bounds for Large-Margin Halfspaces",
      "authors": [
        "Kasper Green Larsen",
        "Natascha Schalburg"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "margin",
        "halfspaces",
        "tight",
        "generalization",
        "points",
        "tradeoff",
        "bounds",
        "training",
        "failure",
        "large"
      ],
      "summary": "We prove the first generalization bound for large-margin halfspaces that is asymptotically tight in the tradeoff between the margin, the fraction of training points with the given margin, the failure probability and the number of training points.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wAq0ZLxrGq"
        ],
        "venue": [
          "/venue/wAq0ZLxrGq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wAq0ZLxrGq"
        ],
        "detail": [
          "https://openreview.net/forum?id=wAq0ZLxrGq"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Tight Generalization Bounds for Large-Margin Halfspaces [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Kasper Green Larsen , Natascha Schalburg We prove the first generalization bound for large-margin halfspaces that is asymptotically tight in the tradeoff between the margin, the fraction of training points with the given margin, the failure probability and the number of training points. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SYcggdxX6W@OpenReview",
      "index": 405,
      "title": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis",
      "authors": [
        "tianrui wang",
        "Haoyu Wang",
        "Meng Ge",
        "Cheng Gong",
        "Chunyu Qiang",
        "Ziyang Ma",
        "Zikang Huang",
        "Guanrou Yang",
        "Xiaobao Wang",
        "EngSiong Chng",
        "Xie Chen",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "emotional",
        "word",
        "tts",
        "control",
        "level",
        "wescon",
        "expressive",
        "emotion",
        "expression",
        "shot"
      ],
      "summary": "While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation. In this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions. Our method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner. Experimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SYcggdxX6W"
        ],
        "venue": [
          "/venue/SYcggdxX6W@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SYcggdxX6W"
        ],
        "detail": [
          "https://openreview.net/forum?id=SYcggdxX6W"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : tianrui wang , Haoyu Wang , Meng Ge , Cheng Gong , Chunyu Qiang , Ziyang Ma , Zikang Huang , Guanrou Yang , Xiaobao Wang , EngSiong Chng , Xie Chen , Longbiao Wang , Jianwu Dang While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation. In this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions. Our method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner. Experimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2Z0OFReqkT@OpenReview",
      "index": 406,
      "title": "UMoE: Unifying Attention and FFN with Shared Experts",
      "authors": [
        "Yuanhang Yang",
        "Chaozheng Wang",
        "Jing Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ffn",
        "moe",
        "umoe",
        "attention",
        "layers",
        "experts",
        "unifying",
        "performance",
        "shared",
        "feed"
      ],
      "summary": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, that reveals an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2Z0OFReqkT"
        ],
        "venue": [
          "/venue/2Z0OFReqkT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2Z0OFReqkT"
        ],
        "detail": [
          "https://openreview.net/forum?id=2Z0OFReqkT"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "UMoE: Unifying Attention and FFN with Shared Experts [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Yuanhang Yang , Chaozheng Wang , Jing Li Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, that reveals an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "dZ94ZS410X@OpenReview",
      "index": 407,
      "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search",
      "authors": [
        "Ning Gao",
        "Xiuhui Zhang",
        "Xingyu Jiang",
        "Mukang You",
        "Mohan Zhang",
        "Yue Deng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "agent",
        "search",
        "language",
        "tree",
        "functions",
        "control",
        "tasks",
        "historical",
        "contextual"
      ],
      "summary": "Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLM. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dZ94ZS410X"
        ],
        "venue": [
          "/venue/dZ94ZS410X@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dZ94ZS410X"
        ],
        "detail": [
          "https://openreview.net/forum?id=dZ94ZS410X"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 4
      },
      "raw_excerpt": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search [PDF 1 ] [Copy] [Kimi 4 ] [REL] Authors : Ning Gao , Xiuhui Zhang , Xingyu Jiang , Mukang You , Mohan Zhang , Yue Deng Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLM. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "8wvOMQ2Olw@OpenReview",
      "index": 408,
      "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
      "authors": [
        "Yeonjoon Jung",
        "Daehyun Ahn",
        "Hyungjun Kim",
        "Taesu Kim",
        "Eunhyeok Park"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gralora",
        "lora",
        "rank",
        "peft",
        "granular",
        "adaptation",
        "tuning",
        "fft",
        "fine",
        "ranks"
      ],
      "summary": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32–64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA’s structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA’s limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation, commonsense reasoning, mathematical reasoning, general language understanding, and image generation benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5\\% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8wvOMQ2Olw"
        ],
        "venue": [
          "/venue/8wvOMQ2Olw@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8wvOMQ2Olw"
        ],
        "detail": [
          "https://openreview.net/forum?id=8wvOMQ2Olw"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Yeonjoon Jung , Daehyun Ahn , Hyungjun Kim , Taesu Kim , Eunhyeok Park Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32–64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA’s structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA’s limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation, commonsense reasoning, mathematical reasoning, general language understanding, and image generation benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5\\% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3a18D8IeQ1@OpenReview",
      "index": 409,
      "title": "Quantization-Free Autoregressive Action Transformer",
      "authors": [
        "Ziyad Sheebaelhamd",
        "Michael Tschannen",
        "Michael Muehlebach",
        "Claire Vernade"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "autoregressive",
        "quantization",
        "transformer",
        "action",
        "imitation",
        "givt",
        "transformers",
        "generative",
        "policy",
        "outs"
      ],
      "summary": "Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3a18D8IeQ1"
        ],
        "venue": [
          "/venue/3a18D8IeQ1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3a18D8IeQ1"
        ],
        "detail": [
          "https://openreview.net/forum?id=3a18D8IeQ1"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Quantization-Free Autoregressive Action Transformer [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Ziyad Sheebaelhamd , Michael Tschannen , Michael Muehlebach , Claire Vernade Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VA5P0rUZPx@OpenReview",
      "index": 410,
      "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models",
      "authors": [
        "Qianyue Hao",
        "Yiwen Song",
        "Qingmin Liao",
        "Jian Yuan",
        "Yong Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "policy",
        "exploration",
        "llm",
        "explorer",
        "plug",
        "task",
        "agent",
        "learning",
        "reinforcement",
        "stochastic"
      ],
      "summary": "Policy exploration is critical in reinforcement learning (RL), where existing approaches include ϵ ϵ -greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design **LLM-Explorer** to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://github.com/tsinghua-fib-lab/LLM-Explorer for reproducibility.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VA5P0rUZPx"
        ],
        "venue": [
          "/venue/VA5P0rUZPx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VA5P0rUZPx"
        ],
        "detail": [
          "https://openreview.net/forum?id=VA5P0rUZPx"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Qianyue Hao , Yiwen Song , Qingmin Liao , Jian Yuan , Yong Li Policy exploration is critical in reinforcement learning (RL), where existing approaches include ϵ ϵ -greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design **LLM-Explorer** to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://github.com/tsinghua-fib-lab/LLM-Explorer for reproducibility. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "FEugj28qhC@OpenReview",
      "index": 411,
      "title": "BayeSQP: Bayesian Optimization through Sequential Quadratic Programming",
      "authors": [
        "Paul Brunzema",
        "Sebastian Trimpe"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bayesqp",
        "optimization",
        "subproblem",
        "bayesian",
        "sequential",
        "quadratic",
        "programming",
        "box",
        "uncertainty",
        "order"
      ],
      "summary": "We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show that BayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FEugj28qhC"
        ],
        "venue": [
          "/venue/FEugj28qhC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FEugj28qhC"
        ],
        "detail": [
          "https://openreview.net/forum?id=FEugj28qhC"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "BayeSQP: Bayesian Optimization through Sequential Quadratic Programming [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Paul Brunzema , Sebastian Trimpe We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show that BayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "aTiMLVePXi@OpenReview",
      "index": 412,
      "title": "Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference",
      "authors": [
        "Álvaro Parafita",
        "Tomas Garriga",
        "Axel Brando",
        "Francisco J. Cazorla"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shap",
        "estimand",
        "explanations",
        "agnostic",
        "causal",
        "shapley",
        "practical",
        "overlooks",
        "interventional",
        "estimands"
      ],
      "summary": "Among explainability techniques, SHAP stands out as one of the most popular, but often overlooks the causal structure of the problem. In response, do-SHAP employs interventional queries, but its reliance on estimands hinders its practical application. To address this problem, we propose the use of estimand-agnostic approaches, which allow for the estimation of any identifiable query from a single model, making do-SHAP feasible on complex graphs. We also develop a novel algorithm to significantly accelerate its computation at a negligible cost, as well as a method to explain inaccessible Data Generating Processes. We demonstrate the estimation and computational performance of our approach, and validate it on two real-world datasets, highlighting its potential in obtaining reliable explanations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aTiMLVePXi"
        ],
        "venue": [
          "/venue/aTiMLVePXi@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aTiMLVePXi"
        ],
        "detail": [
          "https://openreview.net/forum?id=aTiMLVePXi"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Álvaro Parafita , Tomas Garriga , Axel Brando , Francisco J. Cazorla Among explainability techniques, SHAP stands out as one of the most popular, but often overlooks the causal structure of the problem. In response, do-SHAP employs interventional queries, but its reliance on estimands hinders its practical application. To address this problem, we propose the use of estimand-agnostic approaches, which allow for the estimation of any identifiable query from a single model, making do-SHAP feasible on complex graphs. We also develop a novel algorithm to significantly accelerate its computation at a negligible cost, as well as a method to explain inaccessible Data Generating Processes. We demonstrate the estimation and computational performance of our approach, and validate it on two real-world datasets, highlighting its potential in obtaining reliable explanations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IMmkDMqFMU@OpenReview",
      "index": 413,
      "title": "AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws",
      "authors": [
        "Oren Neumann",
        "Claudius Gros"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "zipf",
        "scaling",
        "law",
        "alphazero",
        "laws",
        "quanta",
        "descending",
        "game",
        "states",
        "tale"
      ],
      "summary": "Neural scaling laws are observed in a range of domains, to date with no universal understanding of why they occur. Recent theories suggest that loss power laws arise from Zipf's law, a power law observed in domains like natural language. One theory suggests that language scaling laws emerge when Zipf-distributed task quanta are learned in descending order of frequency. In this paper we examine power-law scaling in AlphaZero, a reinforcement learning algorithm, using a model of language-model scaling. We find that game states in training and inference data scale with Zipf's law, which is known to arise from the tree structure of the environment, and examine the correlation between scaling-law and Zipf's-law exponents. In agreement with the quanta scaling model, we find that agents optimize state loss in descending order of frequency, even though this order scales inversely with modelling complexity. We also find that inverse scaling, the failure of models to improve with size, is correlated with unusual Zipf curves where end-game states are among the most frequent states. We show evidence that larger models shift their focus to these less-important states, sacrificing their understanding of important early-game states.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IMmkDMqFMU"
        ],
        "venue": [
          "/venue/IMmkDMqFMU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IMmkDMqFMU"
        ],
        "detail": [
          "https://openreview.net/forum?id=IMmkDMqFMU"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Oren Neumann , Claudius Gros Neural scaling laws are observed in a range of domains, to date with no universal understanding of why they occur. Recent theories suggest that loss power laws arise from Zipf's law, a power law observed in domains like natural language. One theory suggests that language scaling laws emerge when Zipf-distributed task quanta are learned in descending order of frequency. In this paper we examine power-law scaling in AlphaZero, a reinforcement learning algorithm, using a model of language-model scaling. We find that game states in training and inference data scale with Zipf's law, which is known to arise from the tree structure of the environment, and examine the correlation between scaling-law and Zipf's-law exponents. In agreement with the quanta scaling model, we find that agents optimize state loss in descending order of frequency, even though this order scales inversely with modelling complexity. We also find that inverse scaling, the failure of models to improve with size, is correlated with unusual Zipf curves where end-game states are among the most frequent states. We show evidence that larger models shift their focus to these less-important states, sacrificing their understanding of important early-game states. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QBnfYm6Naa@OpenReview",
      "index": 414,
      "title": "Set Smoothness Unlocks Clarke Hyper-stationarity in Bilevel Optimization",
      "authors": [
        "He Chen",
        "Jiajin Li",
        "Anthony Man-Cho So"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hyper",
        "clarke",
        "blo",
        "stationarity",
        "smoothness",
        "objective",
        "bilevel",
        "set",
        "stationary",
        "optimization"
      ],
      "summary": "Solving bilevel optimization (BLO) problems to global optimality is generally intractable. A common surrogate is to compute a hyper-stationary point—a stationary point of the hyper-objective function obtained by minimizing or maximizing the upper-level objective over the lower-level solution set. Existing methods, however, either provide weak notions of stationarity or require restrictive assumptions to guarantee the smoothness of hyper-objective functions. In this paper, we eliminate these impractical assumptions and show that strong (Clarke) hyper-stationarity remains computable even when the hyper-objective is nonsmooth. Our key ingredient is a new structural property, called set smoothness, which captures the variational dependence of the lower-level solution set on the upper-level variable. We prove that this property holds for a broad class of BLO problems and ensures weak convexity (resp. concavity) of pessimistic (resp. optimistic) hyper-objective functions. Building on this foundation, we show that a zeroth-order algorithm that computes approximate Clarke hyper-stationary points with non-asymptotic convergence guarantees. To the best of our knowledge, this is the first computational guarantee for Clarke-type stationarity in nonsmooth BLO. Beyond this specific application, the set smoothness property emerges as a structural concept of independent interest, with potential to inform the analysis of broader classes of optimization and variational problems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QBnfYm6Naa"
        ],
        "venue": [
          "/venue/QBnfYm6Naa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QBnfYm6Naa"
        ],
        "detail": [
          "https://openreview.net/forum?id=QBnfYm6Naa"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Set Smoothness Unlocks Clarke Hyper-stationarity in Bilevel Optimization [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : He Chen , Jiajin Li , Anthony Man-Cho So Solving bilevel optimization (BLO) problems to global optimality is generally intractable. A common surrogate is to compute a hyper-stationary point—a stationary point of the hyper-objective function obtained by minimizing or maximizing the upper-level objective over the lower-level solution set. Existing methods, however, either provide weak notions of stationarity or require restrictive assumptions to guarantee the smoothness of hyper-objective functions. In this paper, we eliminate these impractical assumptions and show that strong (Clarke) hyper-stationarity remains computable even when the hyper-objective is nonsmooth. Our key ingredient is a new structural property, called set smoothness, which captures the variational dependence of the lower-level solution set on the upper-level variable. We prove that this property holds for a broad class of BLO problems and ensures weak convexity (resp. concavity) of pessimistic (resp. optimistic) hyper-objective functions. Building on this foundation, we show that a zeroth-order algorithm that computes approximate Clarke hyper-stationary points with non-asymptotic convergence guarantees. To the best of our knowledge, this is the first computational guarantee for Clarke-type stationarity in nonsmooth BLO. Beyond this specific application, the set smoothness property emerges as a structural concept of independent interest, with potential to inform the analysis of broader classes of optimization and variational problems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5JcDVsV8pf@OpenReview",
      "index": 415,
      "title": "The Computational Advantage of Depth in Learning High-Dimensional Hierarchical Targets",
      "authors": [
        "Yatin Dandi",
        "Luca Pesce",
        "Lenka Zdeborova",
        "Florent Krzakala"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shallow",
        "hierarchical",
        "targets",
        "dimensional",
        "depth",
        "networks",
        "learning",
        "dimensionalities",
        "ones",
        "deep"
      ],
      "summary": "Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge. In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD successively reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms. These findings open the way to further quantitative studies of the crucial role of depth in learning hierarchical structures with deep networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5JcDVsV8pf"
        ],
        "venue": [
          "/venue/5JcDVsV8pf@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5JcDVsV8pf"
        ],
        "detail": [
          "https://openreview.net/forum?id=5JcDVsV8pf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "The Computational Advantage of Depth in Learning High-Dimensional Hierarchical Targets [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Yatin Dandi , Luca Pesce , Lenka Zdeborova , Florent Krzakala Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge. In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD successively reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms. These findings open the way to further quantitative studies of the crucial role of depth in learning hierarchical structures with deep networks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WcUo7Z2Jnh@OpenReview",
      "index": 416,
      "title": "Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models",
      "authors": [
        "Yue Wang",
        "Qiuzhi Liu",
        "Jiahao Xu",
        "Tian Liang",
        "Xingyu Chen",
        "Zhiwei He",
        "Linfeng Song",
        "Dian Yu",
        "Juntao Li",
        "Zhuosheng Zhang",
        "Rui Wang",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "underthinking",
        "lrms",
        "reasoning",
        "thoughts",
        "incorrect",
        "challenging",
        "thought",
        "switching",
        "deepseek",
        "discourages"
      ],
      "summary": "Long reasoning models (LRMs) such as OpenAI's o1 and DeepSeek's R1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we term underthinking, where LRMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems. To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source LRMs, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy with thought switching penalty (Tip) that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path. Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning. Our findings contribute to understanding reasoning inefficiencies in LRMs and offer a practical solution to enhance their problem-solving capabilities. Our code is open-source and available at https://github.com/wangyuenlp/underthinking.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WcUo7Z2Jnh"
        ],
        "venue": [
          "/venue/WcUo7Z2Jnh@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WcUo7Z2Jnh"
        ],
        "detail": [
          "https://openreview.net/forum?id=WcUo7Z2Jnh"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yue Wang , Qiuzhi Liu , Jiahao Xu , Tian Liang , Xingyu Chen , Zhiwei He , Linfeng Song , Dian Yu , Juntao Li , Zhuosheng Zhang , Rui Wang , Zhaopeng Tu , Haitao Mi , Dong Yu Long reasoning models (LRMs) such as OpenAI's o1 and DeepSeek's R1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we term underthinking, where LRMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems. To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source LRMs, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy with thought switching penalty (Tip) that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path. Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning. Our findings contribute to understanding reasoning inefficiencies in LRMs and offer a practical solution to enhance their problem-solving capabilities. Our code is open-source and available at https://github.com/wangyuenlp/underthinking. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "24tuzE5KZc@OpenReview",
      "index": 417,
      "title": "OPTFM: A Scalable Multi-View Graph Transformer for Hierarchical Pre-Training in Combinatorial Optimization",
      "authors": [
        "Hao Yuan",
        "Wenli Ouyang",
        "Changwen Zhang",
        "Congrui Li",
        "Yong Sun"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "optfm",
        "optimization",
        "combinatorial",
        "graph",
        "scalable",
        "transformer",
        "foundation",
        "attention",
        "view",
        "pre"
      ],
      "summary": "Foundation Models (FMs) have demonstrated remarkable success in fields like computer vision and natural language processing, yet their application to combinatorial optimization remains underexplored. Optimization problems, often modeled as graphs, pose unique challenges due to their diverse structures, varying distributions, and NP-hard complexity. To address these challenges, we propose OPTFM, the first graph foundation model for general combinatorial optimization. OPTFM introduces a scalable multi-view graph transformer with hybrid self-attention and cross-attention to model large-scale heterogeneous graphs in O ( N ) O ( N ) time complexity while maintaining semantic consistency throughout the attention computation. A Dual-level pre-training framework integrates node-level graph reconstruction and instance-level contrastive learning, enabling robust and adaptable representations at multiple levels. Experimental results across diverse optimization tasks show that models trained on OPTFM embeddings without fine-tuning consistently outperform task-specific approaches, establishing a new benchmark for solving combinatorial optimization problems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=24tuzE5KZc"
        ],
        "venue": [
          "/venue/24tuzE5KZc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=24tuzE5KZc"
        ],
        "detail": [
          "https://openreview.net/forum?id=24tuzE5KZc"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "OPTFM: A Scalable Multi-View Graph Transformer for Hierarchical Pre-Training in Combinatorial Optimization [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hao Yuan , Wenli Ouyang , Changwen Zhang , Congrui Li , Yong Sun Foundation Models (FMs) have demonstrated remarkable success in fields like computer vision and natural language processing, yet their application to combinatorial optimization remains underexplored. Optimization problems, often modeled as graphs, pose unique challenges due to their diverse structures, varying distributions, and NP-hard complexity. To address these challenges, we propose OPTFM, the first graph foundation model for general combinatorial optimization. OPTFM introduces a scalable multi-view graph transformer with hybrid self-attention and cross-attention to model large-scale heterogeneous graphs in O ( N ) O ( N ) time complexity while maintaining semantic consistency throughout the attention computation. A Dual-level pre-training framework integrates node-level graph reconstruction and instance-level contrastive learning, enabling robust and adaptable representations at multiple levels. Experimental results across diverse optimization tasks show that models trained on OPTFM embeddings without fine-tuning consistently outperform task-specific approaches, establishing a new benchmark for solving combinatorial optimization problems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "cjjPn1EIwq@OpenReview",
      "index": 418,
      "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation",
      "authors": [
        "Jiani Huang",
        "Amish Sethi",
        "Matthew Kuo",
        "Mayank Keoliya",
        "Neelay Velingker",
        "JungHo Jung",
        "Ser-Nam Lim",
        "Ziyang Li",
        "Mayur Naik"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "esca",
        "sgclip",
        "embodied",
        "scene",
        "mllms",
        "perception",
        "agents",
        "open",
        "contextualizing",
        "grounding"
      ],
      "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, existing MLLMs do not reliably capture fine-grained links between low-level visual features and high-level textual semantics, leading to weak grounding and inaccurate perception. To overcome this challenge, we propose ESCA, a framework that contextualizes embodied agents by grounding their perception in spatial-temporal scene graphs. At its core is SGCLIP, a novel, open-domain, promptable foundation model for generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+ open-domain videos using a neurosymbolic pipeline that aligns automatically generated captions with scene graphs produced by the model itself, eliminating the need for human-labeled annotations. We demonstrate that SGCLIP excels in both prompt-based inference and task-specific fine-tuning, achieving state-of-the-art results on scene graph generation and action localization benchmarks. ESCA with SGCLIP improves perception for embodied agents based on both open-source and commercial MLLMs, achieving state of-the-art performance across two embodied environments. Notably, ESCA significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines. We release the source code for SGCLIP model training at https://github.com/video-fm/LASER and for the embodied agent at https://github.com/video-fm/ESCA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cjjPn1EIwq"
        ],
        "venue": [
          "/venue/cjjPn1EIwq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cjjPn1EIwq"
        ],
        "detail": [
          "https://openreview.net/forum?id=cjjPn1EIwq"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Jiani Huang , Amish Sethi , Matthew Kuo , Mayank Keoliya , Neelay Velingker , JungHo Jung , Ser-Nam Lim , Ziyang Li , Mayur Naik Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, existing MLLMs do not reliably capture fine-grained links between low-level visual features and high-level textual semantics, leading to weak grounding and inaccurate perception. To overcome this challenge, we propose ESCA, a framework that contextualizes embodied agents by grounding their perception in spatial-temporal scene graphs. At its core is SGCLIP, a novel, open-domain, promptable foundation model for generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+ open-domain videos using a neurosymbolic pipeline that aligns automatically generated captions with scene graphs produced by the model itself, eliminating the need for human-labeled annotations. We demonstrate that SGCLIP excels in both prompt-based inference and task-specific fine-tuning, achieving state-of-the-art results on scene graph generation and action localization benchmarks. ESCA with SGCLIP improves perception for embodied agents based on both open-source and commercial MLLMs, achieving state of-the-art performance across two embodied environments. Notably, ESCA significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines. We release the source code for SGCLIP model training at https://github.com/video-fm/LASER and for the embodied agent at https://github.com/video-fm/ESCA. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "McPNQEDEZE@OpenReview",
      "index": 419,
      "title": "Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback",
      "authors": [
        "Orin Levy",
        "Liad Erez",
        "Alon Cohen",
        "Yishay Mansour"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mathcal",
        "sqrt",
        "regret",
        "log",
        "oracle",
        "max",
        "contextual",
        "bound",
        "delayed",
        "ktr"
      ],
      "summary": "We present regret minimization algorithms for the contextual multi-armed bandit (CMAB) problem over K K actions in the presence of delayed feedback, a scenario where loss observations arrive with delays chosen by an adversary. As a preliminary result, assuming direct access to a finite policy class Π Π we establish an optimal expected regret bound of O ( K T log | Π | − − − − − − − − √ + D log | Π | ) − − − − − − − − √ O ( K T log ⁡ | Π | + D log ⁡ | Π | ) where D D is the sum of delays. For our main contribution, we study the general function approximation setting over a (possibly infinite) contextual loss function class F F with access to an online least-square regression oracle O O over F F . In this setting, we achieve an expected regret bound of O ( K T R T ( O ) − − − − − − − − √ + d max D β − − − − − − √ ) O ( K T R T ( O ) + d max D β ) assuming FIFO order, where d max d max is the maximal delay, R T ( O ) R T ( O ) is an upper bound on the oracle's regret and β β is a stability parameter associated with the oracle. We complement this general result by presenting a novel stability analysis of a Hedge-based version of Vovk's aggregating forecaster as an oracle implementation for least-square regression over a finite function class F F and show that its stability parameter β β is bounded by log | F | log ⁡ | F | , resulting in an expected regret bound of O ( K T log | F | − − − − − − − − − √ + d max D log | F | − − − − − − − − − − − √ ) O ( K T log ⁡ | F | + d max D log ⁡ | F | ) which is a d max − − − − √ d max factor away from the lower bound of Ω ( K T log | F | − − − − − − − − − √ + D log | F | − − − − − − − √ ) Ω ( K T log ⁡ | F | + D log ⁡ | F | ) that we also present.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=McPNQEDEZE"
        ],
        "venue": [
          "/venue/McPNQEDEZE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=McPNQEDEZE"
        ],
        "detail": [
          "https://openreview.net/forum?id=McPNQEDEZE"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Orin Levy , Liad Erez , Alon Cohen , Yishay Mansour We present regret minimization algorithms for the contextual multi-armed bandit (CMAB) problem over K K actions in the presence of delayed feedback, a scenario where loss observations arrive with delays chosen by an adversary. As a preliminary result, assuming direct access to a finite policy class Π Π we establish an optimal expected regret bound of O ( K T log | Π | − − − − − − − − √ + D log | Π | ) − − − − − − − − √ O ( K T log ⁡ | Π | + D log ⁡ | Π | ) where D D is the sum of delays. For our main contribution, we study the general function approximation setting over a (possibly infinite) contextual loss function class F F with access to an online least-square regression oracle O O over F F . In this setting, we achieve an expected regret bound of O ( K T R T ( O ) − − − − − − − − √ + d max D β − − − − − − √ ) O ( K T R T ( O ) + d max D β ) assuming FIFO order, where d max d max is the maximal delay, R T ( O ) R T ( O ) is an upper bound on the oracle's regret and β β is a stability parameter associated with the oracle. We complement this general result by presenting a novel stability analysis of a Hedge-based version of Vovk's aggregating forecaster as an oracle implementation for least-square regression over a finite function class F F and show that its stability parameter β β is bounded by log | F | log ⁡ | F | , resulting in an expected regret bound of O ( K T log | F | − − − − − − − − − √ + d max D log | F | − − − − − − − − − − − √ ) O ( K T log ⁡ | F | + d max D log ⁡ | F | ) which is a d max − − − − √ d max factor away from the lower bound of Ω ( K T log | F | − − − − − − − − − √ + D log | F | − − − − − − − √ ) Ω ( K T log ⁡ | F | + D log ⁡ | F | ) that we also present. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "KlzLtQV64O@OpenReview",
      "index": 420,
      "title": "Towards Multi-Table Learning: A Novel Paradigm for Complementarity Quantification and Integration",
      "authors": [
        "Junyu Zhang",
        "Lizhong Ding",
        "MinghongZhang",
        "Ye Yuan",
        "Xingcan Li",
        "Pengqi Li",
        "Tihang Xi",
        "Guoren Wang",
        "Changsheng Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "table",
        "complementarity",
        "tables",
        "multi",
        "atca",
        "paradigm",
        "complementary",
        "quantifies",
        "learning",
        "integration"
      ],
      "summary": "Multi-table data integrate various entities and attributes, with potential interconnections between them. However, existing tabular learning methods often struggle to describe and leverage the underlying complementarity across distinct tables. To address this limitation, we propose the first unified paradigm for multi-table learning that systematically quantifies and integrates complementary information across tables. Specifically, we introduce a metric called complementarity strength (CS), which captures inter-table complementarity by incorporating relevance, similarity, and informativeness. For the first time, we systematically formulate the paradigm towards multi-table learning by establishing formal definitions of tasks and loss functions. Correspondingly, we present a network for multi-table learning that combines Adaptive Table encoder and Cross table Attention mechanism (ATCA-Net), achieving the simultaneous integration of complementary information from distinct tables. Extensive experiments show that ATCA-Net effectively leverages complementary information and that the CS metric accurately quantifies the richness of complementarity across multiple tables. To the best of our knowledge, this is the first work to establish theoretical and practical foundations for multi-table learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KlzLtQV64O"
        ],
        "venue": [
          "/venue/KlzLtQV64O@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KlzLtQV64O"
        ],
        "detail": [
          "https://openreview.net/forum?id=KlzLtQV64O"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Towards Multi-Table Learning: A Novel Paradigm for Complementarity Quantification and Integration [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Junyu Zhang , Lizhong Ding , MinghongZhang , Ye Yuan , Xingcan Li , Pengqi Li , Tihang Xi , Guoren Wang , Changsheng Li Multi-table data integrate various entities and attributes, with potential interconnections between them. However, existing tabular learning methods often struggle to describe and leverage the underlying complementarity across distinct tables. To address this limitation, we propose the first unified paradigm for multi-table learning that systematically quantifies and integrates complementary information across tables. Specifically, we introduce a metric called complementarity strength (CS), which captures inter-table complementarity by incorporating relevance, similarity, and informativeness. For the first time, we systematically formulate the paradigm towards multi-table learning by establishing formal definitions of tasks and loss functions. Correspondingly, we present a network for multi-table learning that combines Adaptive Table encoder and Cross table Attention mechanism (ATCA-Net), achieving the simultaneous integration of complementary information from distinct tables. Extensive experiments show that ATCA-Net effectively leverages complementary information and that the CS metric accurately quantifies the richness of complementarity across multiple tables. To the best of our knowledge, this is the first work to establish theoretical and practical foundations for multi-table learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "CrBWOjZoKc@OpenReview",
      "index": 421,
      "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
      "authors": [
        "Wanlong Liu",
        "Junxiao Xu",
        "Fei Yu",
        "Yukang Lin",
        "Ke Ji",
        "Wenyu Chen",
        "Lifeng Shang",
        "Yasheng Wang",
        "Yan Xu",
        "Benyou Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cot",
        "qfft",
        "reasoning",
        "patterns",
        "tuning",
        "fine",
        "sft",
        "short",
        "long",
        "question"
      ],
      "summary": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CrBWOjZoKc"
        ],
        "venue": [
          "/venue/CrBWOjZoKc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CrBWOjZoKc"
        ],
        "detail": [
          "https://openreview.net/forum?id=CrBWOjZoKc"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Wanlong Liu , Junxiao Xu , Fei Yu , Yukang Lin , Ke Ji , Wenyu Chen , Lifeng Shang , Yasheng Wang , Yan Xu , Benyou Wang Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gfXBNBKx02@OpenReview",
      "index": 422,
      "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning",
      "authors": [
        "Hongjoon Ahn",
        "Heewoong Choi",
        "Jisu Han",
        "Taesup Moon"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ota",
        "gcrl",
        "offline",
        "policy",
        "option",
        "abstracted",
        "value",
        "horizon",
        "aware",
        "temporally"
      ],
      "summary": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm in which goal-reaching policies are trained from abundant state–action trajectory datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. Identifying the root cause of this challenge, we observe the following insight. Firstly, performance bottlenecks mainly stem from the high-level policy’s inability to generate appropriate subgoals. Secondly, when learning the high-level policy in the long-horizon regime, the sign of the advantage estimate frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage estimate for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: _**Option-aware Temporally Abstracted**_ value learning, dubbed **OTA**, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be _option-aware_, our approach contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy learned using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments. Our code is available at https://github.com/ota-v/ota-v",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gfXBNBKx02"
        ],
        "venue": [
          "/venue/gfXBNBKx02@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gfXBNBKx02"
        ],
        "detail": [
          "https://openreview.net/forum?id=gfXBNBKx02"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hongjoon Ahn , Heewoong Choi , Jisu Han , Taesup Moon Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm in which goal-reaching policies are trained from abundant state–action trajectory datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. Identifying the root cause of this challenge, we observe the following insight. Firstly, performance bottlenecks mainly stem from the high-level policy’s inability to generate appropriate subgoals. Secondly, when learning the high-level policy in the long-horizon regime, the sign of the advantage estimate frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage estimate for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: _**Option-aware Temporally Abstracted**_ value learning, dubbed **OTA**, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be _option-aware_, our approach contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy learned using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments. Our code is available at https://github.com/ota-v/ota-v Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "dpllevHMbc@OpenReview",
      "index": 423,
      "title": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning Rate Schedules",
      "authors": [
        "Binghui Li",
        "Fengling Chen",
        "Zixun Huang",
        "Lean Wang",
        "Lei Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lrss",
        "loss",
        "schedules",
        "wsd",
        "scaling",
        "fsl",
        "laws",
        "decay",
        "functional",
        "schedule"
      ],
      "summary": "Scaling laws have emerged as a unifying lens for understanding and guiding the training of large language models (LLMs). However, existing studies predominantly focus on the final-step loss, leaving open whether the entire loss dynamics loss dynamics obey similar laws and, crucially, how the learning rate schedule learning rate schedule (LRS) shapes them. We address these gaps in a controlled theoretical setting by analyzing stochastic gradient descent (SGD) on a power-law kernel regression model. The key insight is a novel intrinsic-time intrinsic-time viewpoint, which captures the training progress more faithfully than iteration count. We then establish a Functional Scaling Law (FSL) Functional Scaling Law (FSL) that captures the full loss trajectory under arbitrary LRSs, with the schedule’s influence entering through a simple convolutional functional. We further instantiate the theory for three representative LRSs---constant, exponential decay, and warmup–stable–decay (WSD)---and derive explicit scaling relations in both data- and compute-limited regimes. These comparisons explain key empirical phenomena: (i) higher-capacity models are more data- and compute-efficient; (ii) learning-rate decay improves training efficiency; and (iii) WSD-type schedules outperform pure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters demonstrate the practical relevance of FSL as a surrogate model for fitting and predicting loss trajectories in large-scale pre-training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dpllevHMbc"
        ],
        "venue": [
          "/venue/dpllevHMbc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dpllevHMbc"
        ],
        "detail": [
          "https://openreview.net/forum?id=dpllevHMbc"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning Rate Schedules [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Binghui Li , Fengling Chen , Zixun Huang , Lean Wang , Lei Wu Scaling laws have emerged as a unifying lens for understanding and guiding the training of large language models (LLMs). However, existing studies predominantly focus on the final-step loss, leaving open whether the entire loss dynamics loss dynamics obey similar laws and, crucially, how the learning rate schedule learning rate schedule (LRS) shapes them. We address these gaps in a controlled theoretical setting by analyzing stochastic gradient descent (SGD) on a power-law kernel regression model. The key insight is a novel intrinsic-time intrinsic-time viewpoint, which captures the training progress more faithfully than iteration count. We then establish a Functional Scaling Law (FSL) Functional Scaling Law (FSL) that captures the full loss trajectory under arbitrary LRSs, with the schedule’s influence entering through a simple convolutional functional. We further instantiate the theory for three representative LRSs---constant, exponential decay, and warmup–stable–decay (WSD)---and derive explicit scaling relations in both data- and compute-limited regimes. These comparisons explain key empirical phenomena: (i) higher-capacity models are more data- and compute-efficient; (ii) learning-rate decay improves training efficiency; and (iii) WSD-type schedules outperform pure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters demonstrate the practical relevance of FSL as a surrogate model for fitting and predicting loss trajectories in large-scale pre-training. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "E3oNDQ8e9r@OpenReview",
      "index": 424,
      "title": "GSRF: Complex-Valued 3D Gaussian Splatting for Efficient Radio-Frequency Data Synthesis",
      "authors": [
        "Kang Yang",
        "Gaofeng Dong",
        "Sijie JI",
        "Wan Du",
        "Mani Srivastava"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gsrf",
        "splatting",
        "synthesis",
        "valued",
        "gaussian",
        "radiance",
        "complex",
        "data",
        "radio",
        "latency"
      ],
      "summary": "Synthesizing radio-frequency (RF) data given the transmitter and receiver positions, e.g., received signal strength indicator (RSSI), is critical for wireless networking and sensing applications, such as indoor localization. However, it remains challenging due to complex propagation interactions, including reflection, diffraction, and scattering. State-of-the-art neural radiance field (NeRF)-based methods achieve high-fidelity RF data synthesis but are limited by long training times and high inference latency. We introduce GSRF, a framework that extends 3D Gaussian Splatting (3DGS) from the optical domain to the RF domain, enabling efficient RF data synthesis. GSRF realizes this adaptation through three key innovations: First, it introduces complex-valued 3D Gaussians with a hybrid Fourier–Legendre basis to model directional and phase-dependent radiance. Second, it employs orthographic splatting for efficient ray–Gaussian intersection identification. Third, it incorporates a complex-valued ray tracing algorithm, executed on RF-customized CUDA kernels and grounded in wavefront propagation principles, to synthesize RF data in real time. Evaluated across various RF technologies, GSRF preserves high-fidelity RF data synthesis while achieving significant improvements in training efficiency, shorter training time, and reduced inference latency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=E3oNDQ8e9r"
        ],
        "venue": [
          "/venue/E3oNDQ8e9r@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=E3oNDQ8e9r"
        ],
        "detail": [
          "https://openreview.net/forum?id=E3oNDQ8e9r"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "GSRF: Complex-Valued 3D Gaussian Splatting for Efficient Radio-Frequency Data Synthesis [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Kang Yang , Gaofeng Dong , Sijie JI , Wan Du , Mani Srivastava Synthesizing radio-frequency (RF) data given the transmitter and receiver positions, e.g., received signal strength indicator (RSSI), is critical for wireless networking and sensing applications, such as indoor localization. However, it remains challenging due to complex propagation interactions, including reflection, diffraction, and scattering. State-of-the-art neural radiance field (NeRF)-based methods achieve high-fidelity RF data synthesis but are limited by long training times and high inference latency. We introduce GSRF, a framework that extends 3D Gaussian Splatting (3DGS) from the optical domain to the RF domain, enabling efficient RF data synthesis. GSRF realizes this adaptation through three key innovations: First, it introduces complex-valued 3D Gaussians with a hybrid Fourier–Legendre basis to model directional and phase-dependent radiance. Second, it employs orthographic splatting for efficient ray–Gaussian intersection identification. Third, it incorporates a complex-valued ray tracing algorithm, executed on RF-customized CUDA kernels and grounded in wavefront propagation principles, to synthesize RF data in real time. Evaluated across various RF technologies, GSRF preserves high-fidelity RF data synthesis while achieving significant improvements in training efficiency, shorter training time, and reduced inference latency. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "b7waOsMnq8@OpenReview",
      "index": 425,
      "title": "Sharp Gaussian approximations for Decentralized Federated Learning",
      "authors": [
        "SOHAM BONNERJEE",
        "Sayar Karmakar",
        "Wei Biao Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sgd",
        "gaussian",
        "approximations",
        "decentralized",
        "federated",
        "bootstrap",
        "local",
        "uniform",
        "esseen",
        "traction"
      ],
      "summary": "Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=b7waOsMnq8"
        ],
        "venue": [
          "/venue/b7waOsMnq8@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=b7waOsMnq8"
        ],
        "detail": [
          "https://openreview.net/forum?id=b7waOsMnq8"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Sharp Gaussian approximations for Decentralized Federated Learning [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : SOHAM BONNERJEE , Sayar Karmakar , Wei Biao Wu Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "77eEDRhPkQ@OpenReview",
      "index": 426,
      "title": "DAPO : Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage-Based Policy Optimization",
      "authors": [
        "Jiacai Liu",
        "Chaojie Wang",
        "Chris Yuhao Liu",
        "Liang Zeng",
        "Rui Yan",
        "Yiwen Sun",
        "Yang Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dapo",
        "critic",
        "reasoning",
        "policy",
        "optimization",
        "abilities",
        "llms",
        "grpo",
        "step",
        "actor"
      ],
      "summary": "The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of LLMs. One key challenge is the sparse reward, which introduces more training variance in policy optimization and makes it difficult to obtain a good estimation for value function in Actor-Critic (AC) methods. To address these issues, we introduce Direct Advantage-Based Policy Optimization (DAPO), a novel step-level offline RL algorithm with theoretical guarantees for enhancing the reasoning abilities of LLMs. Unlike response-level methods (such as DPO and GRPO) that the update directions of all reasoning steps are governed by the outcome reward uniformly, DAPO employs a critic function to provide step-level dense signals for policy optimization. Additionally, the actor and critic in DAPO are trained independently, ensuring that critic is a good estimation of true state value function and avoiding the co-training instability observed in standard AC methods. We train DAPO on mathematical and code problems and then evaluate its performance on multiple benchmarks. Our results show that DAPO can effectively enhance the mathematical and code capabilities on both SFT models and RL models, demonstrating the effectiveness of DAPO.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=77eEDRhPkQ"
        ],
        "venue": [
          "/venue/77eEDRhPkQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=77eEDRhPkQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=77eEDRhPkQ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "DAPO : Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage-Based Policy Optimization [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Jiacai Liu , Chaojie Wang , Chris Yuhao Liu , Liang Zeng , Rui Yan , Yiwen Sun , Yang Liu The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of LLMs. One key challenge is the sparse reward, which introduces more training variance in policy optimization and makes it difficult to obtain a good estimation for value function in Actor-Critic (AC) methods. To address these issues, we introduce Direct Advantage-Based Policy Optimization (DAPO), a novel step-level offline RL algorithm with theoretical guarantees for enhancing the reasoning abilities of LLMs. Unlike response-level methods (such as DPO and GRPO) that the update directions of all reasoning steps are governed by the outcome reward uniformly, DAPO employs a critic function to provide step-level dense signals for policy optimization. Additionally, the actor and critic in DAPO are trained independently, ensuring that critic is a good estimation of true state value function and avoiding the co-training instability observed in standard AC methods. We train DAPO on mathematical and code problems and then evaluate its performance on multiple benchmarks. Our results show that DAPO can effectively enhance the mathematical and code capabilities on both SFT models and RL models, demonstrating the effectiveness of DAPO. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TDFSKAspoQ@OpenReview",
      "index": 427,
      "title": "MGUP: A Momentum-Gradient Alignment Update Policy for Stochastic Optimization",
      "authors": [
        "Da Chang",
        "Ganzhao Yuan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mgup",
        "textbf",
        "adamw",
        "optimizers",
        "lion",
        "selective",
        "updates",
        "optimization",
        "pretraining",
        "intra"
      ],
      "summary": "Efficient optimization is essential for training large language models. Although intra-layer selective updates have been explored, a general mechanism that enables fine-grained control while ensuring convergence guarantees is still lacking. To bridge this gap, we propose \\textbf{MGUP}, a novel mechanism for selective updates. \\textbf{MGUP} augments standard momentum-based optimizers by applying larger step-sizes to a selected fixed proportion of parameters in each iteration, while applying smaller, non-zero step-sizes to the rest. As a nearly {plug-and-play} module, \\textbf{MGUP} seamlessly integrates with optimizers such as AdamW, Lion, and Muon. This yields powerful variants such as \\textbf{MGUP-AdamW}, \\textbf{MGUP-Lion}, and \\textbf{MGUP-Muon}. Under standard assumptions, we provide theoretical convergence guarantees for \\textbf{MGUP-AdamW} (without weight decay) in stochastic optimization. Extensive experiments across diverse tasks, including MAE pretraining, LLM pretraining, and downstream fine-tuning, demonstrate that our \\textbf{MGUP}-enhanced optimizers achieve superior or more stable performance compared to their original base optimizers. We offer a principled, versatile, and theoretically grounded strategy for efficient intra-layer selective updates, accelerating and stabilizing the training of large-scale models. The code is publicly available at https://github.com/MaeChd/MGUP.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TDFSKAspoQ"
        ],
        "venue": [
          "/venue/TDFSKAspoQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TDFSKAspoQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=TDFSKAspoQ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "MGUP: A Momentum-Gradient Alignment Update Policy for Stochastic Optimization [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Da Chang , Ganzhao Yuan Efficient optimization is essential for training large language models. Although intra-layer selective updates have been explored, a general mechanism that enables fine-grained control while ensuring convergence guarantees is still lacking. To bridge this gap, we propose \\textbf{MGUP}, a novel mechanism for selective updates. \\textbf{MGUP} augments standard momentum-based optimizers by applying larger step-sizes to a selected fixed proportion of parameters in each iteration, while applying smaller, non-zero step-sizes to the rest. As a nearly {plug-and-play} module, \\textbf{MGUP} seamlessly integrates with optimizers such as AdamW, Lion, and Muon. This yields powerful variants such as \\textbf{MGUP-AdamW}, \\textbf{MGUP-Lion}, and \\textbf{MGUP-Muon}. Under standard assumptions, we provide theoretical convergence guarantees for \\textbf{MGUP-AdamW} (without weight decay) in stochastic optimization. Extensive experiments across diverse tasks, including MAE pretraining, LLM pretraining, and downstream fine-tuning, demonstrate that our \\textbf{MGUP}-enhanced optimizers achieve superior or more stable performance compared to their original base optimizers. We offer a principled, versatile, and theoretically grounded strategy for efficient intra-layer selective updates, accelerating and stabilizing the training of large-scale models. The code is publicly available at https://github.com/MaeChd/MGUP. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "V8dGVO5Xpg@OpenReview",
      "index": 428,
      "title": "Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training",
      "authors": [
        "Qinglun Li",
        "Yingqi Liu",
        "Miao Zhang",
        "Xiaochun Cao",
        "Quanjun Yin",
        "Li Shen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mgs",
        "centralized",
        "decentralized",
        "beta",
        "error",
        "gossip",
        "generalization",
        "training",
        "gap",
        "frac"
      ],
      "summary": "Decentralized training removes the centralized server, making it a communication-efficient approach that can significantly improve training efficiency, but it often suffers from degraded performance compared to centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective bridge between decentralized and centralized training, significantly reducing experiment performance gaps. However, the theoretical reasons for its effectiveness and whether this gap can be fully eliminated by MGS remain open questions. In this paper, we derive upper bounds on the generalization error and excess error of MGS using stability analysis, systematically answering these two key questions. 1). Optimization Error Reduction: MGS reduces the optimization error bound at an exponential rate, thereby exponentially tightening the generalization error bound and enabling convergence to better solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a non-negligible gap in generalization error remains compared to centralized mini-batch SGD ( O ( T c β c β + 1 / n m ) O ( T c β c β + 1 / n m ) in centralized and O ( T 2 c β 2 c β + 2 / n m 1 2 c β + 2 ) O ( T 2 c β 2 c β + 2 / n m 1 2 c β + 2 ) in decentralized). Furthermore, we provide the first unified analysis of how factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact the generalization of MGS under non-convex settings without the bounded gradients assumption, filling a critical theoretical gap in decentralized training. Finally, promising experiments on CIFAR datasets support our theoretical findings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=V8dGVO5Xpg"
        ],
        "venue": [
          "/venue/V8dGVO5Xpg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=V8dGVO5Xpg"
        ],
        "detail": [
          "https://openreview.net/forum?id=V8dGVO5Xpg"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Qinglun Li , Yingqi Liu , Miao Zhang , Xiaochun Cao , Quanjun Yin , Li Shen Decentralized training removes the centralized server, making it a communication-efficient approach that can significantly improve training efficiency, but it often suffers from degraded performance compared to centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective bridge between decentralized and centralized training, significantly reducing experiment performance gaps. However, the theoretical reasons for its effectiveness and whether this gap can be fully eliminated by MGS remain open questions. In this paper, we derive upper bounds on the generalization error and excess error of MGS using stability analysis, systematically answering these two key questions. 1). Optimization Error Reduction: MGS reduces the optimization error bound at an exponential rate, thereby exponentially tightening the generalization error bound and enabling convergence to better solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a non-negligible gap in generalization error remains compared to centralized mini-batch SGD ( O ( T c β c β + 1 / n m ) O ( T c β c β + 1 / n m ) in centralized and O ( T 2 c β 2 c β + 2 / n m 1 2 c β + 2 ) O ( T 2 c β 2 c β + 2 / n m 1 2 c β + 2 ) in decentralized). Furthermore, we provide the first unified analysis of how factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact the generalization of MGS under non-convex settings without the bounded gradients assumption, filling a critical theoretical gap in decentralized training. Finally, promising experiments on CIFAR datasets support our theoretical findings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oUf6lhK52B@OpenReview",
      "index": 429,
      "title": "Enhancing Contrastive Learning with Variable Similarity",
      "authors": [
        "Haowen Cui",
        "Shuo Chen",
        "Jun Li",
        "Jian Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "similarity",
        "augmentation",
        "contrastive",
        "augmented",
        "views",
        "variable",
        "clvs",
        "imagenet",
        "learning",
        "invariance"
      ],
      "summary": "Contrastive learning has achieved remarkable success in self-supervised learning by pretraining a generalizable feature representation based on the augmentation invariance. Most existing approaches assume that different augmented views of the same instance (i.e., the *positive pairs*) remain semantically invariant. However, the augmentation results with *varying extent* may introduce semantic discrepancies or even content distortion, and thus the conventional (pseudo) supervision from augmentation invariance may lead to misguided learning objectives. In this paper, we propose a novel method called Contrastive Learning with Variable Similarity (CLVS) to accurately characterize the intrinsic similarity relationships between different augmented views. Our method dynamically adjusts the similarity based on the augmentation extent, and it ensures that strongly augmented views are always assigned lower similarity scores than weakly augmented ones. We provide a theoretical analysis to guarantee the effectiveness of the variable similarity in improving model generalizability. Extensive experiments demonstrate the superiority of our approach, achieving gains of 2.1\\% on ImageNet-100 and 1.4\\% on ImageNet-1k compared with the state-of-the-art methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oUf6lhK52B"
        ],
        "venue": [
          "/venue/oUf6lhK52B@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oUf6lhK52B"
        ],
        "detail": [
          "https://openreview.net/forum?id=oUf6lhK52B"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Enhancing Contrastive Learning with Variable Similarity [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Haowen Cui , Shuo Chen , Jun Li , Jian Yang Contrastive learning has achieved remarkable success in self-supervised learning by pretraining a generalizable feature representation based on the augmentation invariance. Most existing approaches assume that different augmented views of the same instance (i.e., the *positive pairs*) remain semantically invariant. However, the augmentation results with *varying extent* may introduce semantic discrepancies or even content distortion, and thus the conventional (pseudo) supervision from augmentation invariance may lead to misguided learning objectives. In this paper, we propose a novel method called Contrastive Learning with Variable Similarity (CLVS) to accurately characterize the intrinsic similarity relationships between different augmented views. Our method dynamically adjusts the similarity based on the augmentation extent, and it ensures that strongly augmented views are always assigned lower similarity scores than weakly augmented ones. We provide a theoretical analysis to guarantee the effectiveness of the variable similarity in improving model generalizability. Extensive experiments demonstrate the superiority of our approach, achieving gains of 2.1\\% on ImageNet-100 and 1.4\\% on ImageNet-1k compared with the state-of-the-art methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "n3armuTFit@OpenReview",
      "index": 430,
      "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
      "authors": [
        "Zehan Wang",
        "Ziang Zhang",
        "Jiayang Xu",
        "Jialei Wang",
        "Tianyu Pang",
        "Chao Du",
        "Hengshuang Zhao",
        "Zhou Zhao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "orient",
        "anything",
        "orientation",
        "object",
        "front",
        "rotations",
        "rotation",
        "rotational",
        "symmetry",
        "estimation"
      ],
      "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=n3armuTFit"
        ],
        "venue": [
          "/venue/n3armuTFit@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=n3armuTFit"
        ],
        "detail": [
          "https://openreview.net/forum?id=n3armuTFit"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Orient Anything V2: Unifying Orientation and Rotation Understanding [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Zehan Wang , Ziang Zhang , Jiayang Xu , Jialei Wang , Tianyu Pang , Chao Du , Hengshuang Zhao , Zhou Zhao This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7FhWZFoVem@OpenReview",
      "index": 431,
      "title": "A Closer Look at Graph Transformers: Cross-Aggregation and Beyond",
      "authors": [
        "Jiaming Zhuo",
        "Ziyi Ma",
        "Yintong Lu",
        "Yuwei Liu",
        "Kun Fu",
        "Di Jin",
        "Chuan Wang",
        "Wenning Wu",
        "Zhen Wang",
        "Xiaochun Cao",
        "Liang Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "graph",
        "aggregation",
        "gts",
        "ugcformer",
        "cross",
        "attributes",
        "transformers",
        "topology",
        "node",
        "capture"
      ],
      "summary": "Graph Transformers (GTs), which effectively capture long-range dependencies and structural biases simultaneously, have recently emerged as promising alternatives to traditional Graph Neural Networks (GNNs). Advanced approaches for GTs to leverage topology information involve integrating GNN modules or modulating node attributes using positional encodings. Unfortunately, the underlying mechanism driving their effectiveness remains insufficiently understood. In this paper, we revisit these strategies and uncover a shared underlying mechanism—Cross Aggregation—that effectively captures the interaction between graph topology and node attributes. Building on this insight, we propose the Universal Graph Cross-attention Transformer (UGCFormer), a universal GT framework with linear computational complexity. The idea is to interactively learn the representations of graph topology and node attributes through a linearized Dual Cross-attention (DCA) module. In theory, this module can adaptively capture interactions between these two types of graph information, thereby achieving effective aggregation. To alleviate overfitting arising from the dual-channel design, we introduce a consistency constraint that enforces representational alignment. Extensive evaluations on multiple benchmark datasets demonstrate the effectiveness and efficiency of UGCFormer.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7FhWZFoVem"
        ],
        "venue": [
          "/venue/7FhWZFoVem@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7FhWZFoVem"
        ],
        "detail": [
          "https://openreview.net/forum?id=7FhWZFoVem"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "A Closer Look at Graph Transformers: Cross-Aggregation and Beyond [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jiaming Zhuo , Ziyi Ma , Yintong Lu , Yuwei Liu , Kun Fu , Di Jin , Chuan Wang , Wenning Wu , Zhen Wang , Xiaochun Cao , Liang Yang Graph Transformers (GTs), which effectively capture long-range dependencies and structural biases simultaneously, have recently emerged as promising alternatives to traditional Graph Neural Networks (GNNs). Advanced approaches for GTs to leverage topology information involve integrating GNN modules or modulating node attributes using positional encodings. Unfortunately, the underlying mechanism driving their effectiveness remains insufficiently understood. In this paper, we revisit these strategies and uncover a shared underlying mechanism—Cross Aggregation—that effectively captures the interaction between graph topology and node attributes. Building on this insight, we propose the Universal Graph Cross-attention Transformer (UGCFormer), a universal GT framework with linear computational complexity. The idea is to interactively learn the representations of graph topology and node attributes through a linearized Dual Cross-attention (DCA) module. In theory, this module can adaptively capture interactions between these two types of graph information, thereby achieving effective aggregation. To alleviate overfitting arising from the dual-channel design, we introduce a consistency constraint that enforces representational alignment. Extensive evaluations on multiple benchmark datasets demonstrate the effectiveness and efficiency of UGCFormer. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "FUd016XD4d@OpenReview",
      "index": 432,
      "title": "Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution",
      "authors": [
        "Zhanyi Sun",
        "Shuran Song"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "expert",
        "visuomotor",
        "policy",
        "barrier",
        "lpb",
        "latent",
        "distribution",
        "policies",
        "imitation",
        "ood"
      ],
      "summary": "Visuomotor policies trained via behavior cloning are vulnerable to covariate shift, where small deviations from expert trajectories can compound into failure. Common strategies to mitigate this issue involve expanding the training distribution through human-in-the-loop corrections or synthetic data augmentation. However, these approaches are often labor-intensive, rely on strong task assumptions, or compromise the quality of imitation. We introduce Latent Policy Barrier, a framework for robust visuomotor policy learning. Inspired by Control Barrier Functions, LPB treats the latent embeddings of expert demonstrations as an implicit barrier separating safe, in-distribution states from unsafe, out-of-distribution (OOD) ones. Our approach decouples the role of precise expert imitation and OOD recovery into two separate modules: a base diffusion policy solely on expert data, and a dynamics model trained on both expert and suboptimal policy rollout data. At inference time, the dynamics model predicts future latent states and optimizes them to stay within the expert distribution. Both simulated and real-world experiments show that LPB improves both policy robustness and data efficiency, enabling reliable manipulation from limited expert data and without additional human correction or annotation. More details are on our anonymous project website https://latentpolicybarrier.github.io.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FUd016XD4d"
        ],
        "venue": [
          "/venue/FUd016XD4d@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FUd016XD4d"
        ],
        "detail": [
          "https://openreview.net/forum?id=FUd016XD4d"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Zhanyi Sun , Shuran Song Visuomotor policies trained via behavior cloning are vulnerable to covariate shift, where small deviations from expert trajectories can compound into failure. Common strategies to mitigate this issue involve expanding the training distribution through human-in-the-loop corrections or synthetic data augmentation. However, these approaches are often labor-intensive, rely on strong task assumptions, or compromise the quality of imitation. We introduce Latent Policy Barrier, a framework for robust visuomotor policy learning. Inspired by Control Barrier Functions, LPB treats the latent embeddings of expert demonstrations as an implicit barrier separating safe, in-distribution states from unsafe, out-of-distribution (OOD) ones. Our approach decouples the role of precise expert imitation and OOD recovery into two separate modules: a base diffusion policy solely on expert data, and a dynamics model trained on both expert and suboptimal policy rollout data. At inference time, the dynamics model predicts future latent states and optimizes them to stay within the expert distribution. Both simulated and real-world experiments show that LPB improves both policy robustness and data efficiency, enabling reliable manipulation from limited expert data and without additional human correction or annotation. More details are on our anonymous project website https://latentpolicybarrier.github.io. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0TD3eO46gk@OpenReview",
      "index": 433,
      "title": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models",
      "authors": [
        "Tyler A. Chang",
        "Ben Bergen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "subnetworks",
        "bigram",
        "token",
        "next",
        "language",
        "predictions",
        "transformer",
        "activations",
        "current",
        "model"
      ],
      "summary": "In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying more complex language model circuits by building up from a minimal circuit.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0TD3eO46gk"
        ],
        "venue": [
          "/venue/0TD3eO46gk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0TD3eO46gk"
        ],
        "detail": [
          "https://openreview.net/forum?id=0TD3eO46gk"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Tyler A. Chang , Ben Bergen In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying more complex language model circuits by building up from a minimal circuit. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QudbVyFaTu@OpenReview",
      "index": 434,
      "title": "Minimax-Optimal Univariate Function Selection in Sparse Additive Models: Rates, Adaptation, and the Estimation-Selection Gap",
      "authors": [
        "Shixiang Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "selection",
        "spam",
        "minimax",
        "univariate",
        "sparse",
        "additive",
        "estimation",
        "function",
        "adaptation",
        "recovery"
      ],
      "summary": "The sparse additive model (SpAM) offers a trade-off between interpretability and flexibility, and hence is a powerful model for high-dimensional research. This paper focuses on the variable selection, i.e., the univariate function selection problem in SpAM. We establish the minimax separation rates from both the perspectives of sparse multiple testing (FDR + FNR control) and support recovery (wrong recovery probability control). We further study how adaptation to unknown smoothness affects the minimax separation rate, and propose an adaptive selection procedure. Finally, we discuss the difference between estimation and selection in SpAM: Procedures achieving optimal function estimation may fail to achieve optimal univariate function selection.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QudbVyFaTu"
        ],
        "venue": [
          "/venue/QudbVyFaTu@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QudbVyFaTu"
        ],
        "detail": [
          "https://openreview.net/forum?id=QudbVyFaTu"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Minimax-Optimal Univariate Function Selection in Sparse Additive Models: Rates, Adaptation, and the Estimation-Selection Gap [PDF 3 ] [Copy] [Kimi 2 ] [REL] Author : Shixiang Liu The sparse additive model (SpAM) offers a trade-off between interpretability and flexibility, and hence is a powerful model for high-dimensional research. This paper focuses on the variable selection, i.e., the univariate function selection problem in SpAM. We establish the minimax separation rates from both the perspectives of sparse multiple testing (FDR + FNR control) and support recovery (wrong recovery probability control). We further study how adaptation to unknown smoothness affects the minimax separation rate, and propose an adaptive selection procedure. Finally, we discuss the difference between estimation and selection in SpAM: Procedures achieving optimal function estimation may fail to achieve optimal univariate function selection. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "y0wDflmpLk@OpenReview",
      "index": 435,
      "title": "Continuous Thought Machines",
      "authors": [
        "Luke Nicholas Darlow",
        "Ciaran Regan",
        "Sebastian Risi",
        "Jeffrey Seely",
        "Llion Jones"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ctm",
        "neuron",
        "pub",
        "sakana",
        "neural",
        "brains",
        "thought",
        "innovations",
        "synchronization",
        "tasks"
      ],
      "summary": "Biological brains demonstrate complex neural activity, where neural dynamics are critical to how brains process information. Most artificial neural networks ignore the complexity of individual neurons. We challenge that paradigm. By incorporating neuron-level processing and synchronization, we reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two innovations: (1) neuron-level temporal processing}, where each neuron uses unique weight parameters to process incoming histories; and (2) neural synchronization as a latent representation. The CTM aims to strike a balance between neuron abstractions and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable. We demonstrate the CTM's performance and versatility across a range of tasks, including solving 2D mazes, ImageNet-1K classification, parity computation, and more. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems. We provide an accompanying [interactive online demonstration](https://pub.sakana.ai/ctm/) and an [extended technical report](https://pub.sakana.ai/ctm/paper).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=y0wDflmpLk"
        ],
        "venue": [
          "/venue/y0wDflmpLk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=y0wDflmpLk"
        ],
        "detail": [
          "https://openreview.net/forum?id=y0wDflmpLk"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Continuous Thought Machines [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Luke Nicholas Darlow , Ciaran Regan , Sebastian Risi , Jeffrey Seely , Llion Jones Biological brains demonstrate complex neural activity, where neural dynamics are critical to how brains process information. Most artificial neural networks ignore the complexity of individual neurons. We challenge that paradigm. By incorporating neuron-level processing and synchronization, we reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two innovations: (1) neuron-level temporal processing}, where each neuron uses unique weight parameters to process incoming histories; and (2) neural synchronization as a latent representation. The CTM aims to strike a balance between neuron abstractions and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable. We demonstrate the CTM's performance and versatility across a range of tasks, including solving 2D mazes, ImageNet-1K classification, parity computation, and more. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems. We provide an accompanying [interactive online demonstration](https://pub.sakana.ai/ctm/) and an [extended technical report](https://pub.sakana.ai/ctm/paper). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QYJt0pX0zJ@OpenReview",
      "index": 436,
      "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Xuan Liu",
        "Yanyun Wang",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "poisoning",
        "synthetic",
        "attacks",
        "virus",
        "attack",
        "infection",
        "llms",
        "data",
        "backdoor",
        "via"
      ],
      "summary": "Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective “shell” and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QYJt0pX0zJ"
        ],
        "venue": [
          "/venue/QYJt0pX0zJ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QYJt0pX0zJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=QYJt0pX0zJ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Zi Liang , Qingqing Ye , Xuan Liu , Yanyun Wang , Jianliang Xu , Haibo Hu Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective “shell” and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "jPaM3AiFLq@OpenReview",
      "index": 437,
      "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting",
      "authors": [
        "Chengqi Li",
        "Zhihao Shi",
        "Yangdi Lu",
        "Wenbo He",
        "Xiangyu Xu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "splatting",
        "3dgs",
        "artifacts",
        "ema",
        "wild",
        "asymmetric",
        "masking",
        "proxy",
        "inconsistent",
        "training"
      ],
      "summary": "3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jPaM3AiFLq"
        ],
        "venue": [
          "/venue/jPaM3AiFLq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jPaM3AiFLq"
        ],
        "detail": [
          "https://openreview.net/forum?id=jPaM3AiFLq"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Chengqi Li , Zhihao Shi , Yangdi Lu , Wenbo He , Xiangyu Xu 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "J8JCF64aEn@OpenReview",
      "index": 438,
      "title": "Frame Context Packing and Drift Prevention in Next-Frame-Prediction Video Diffusion Models",
      "authors": [
        "Lvmin Zhang",
        "Shengqu Cai",
        "Muyang Li",
        "Gordon Wetzstein",
        "Maneesh Agrawala"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "frame",
        "framepack",
        "video",
        "packing",
        "prevention",
        "frames",
        "drift",
        "next",
        "contexts",
        "directional"
      ],
      "summary": "We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. FramePack compresses input frame contexts with frame-wise importance so that more frames can be encoded within a fixed context length, with more important frames having longer contexts. The frame importance can be measured using time proximity, feature similarity, or hybrid metrics. The packing method allows for inference with thousands of frames and training with relatively large batch sizes. We also present drift prevention methods to address observation bias (error accumulation), including early-established endpoints, adjusted sampling orders, and discrete history representation. Ablation studies validate the effectiveness of the anti-drifting methods in both single-directional video streaming and bi-directional video generation. Finally, we show that existing video diffusion models can be finetuned with FramePack, and analyze the differences between different packing schedules.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=J8JCF64aEn"
        ],
        "venue": [
          "/venue/J8JCF64aEn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=J8JCF64aEn"
        ],
        "detail": [
          "https://openreview.net/forum?id=J8JCF64aEn"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Frame Context Packing and Drift Prevention in Next-Frame-Prediction Video Diffusion Models [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Lvmin Zhang , Shengqu Cai , Muyang Li , Gordon Wetzstein , Maneesh Agrawala We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. FramePack compresses input frame contexts with frame-wise importance so that more frames can be encoded within a fixed context length, with more important frames having longer contexts. The frame importance can be measured using time proximity, feature similarity, or hybrid metrics. The packing method allows for inference with thousands of frames and training with relatively large batch sizes. We also present drift prevention methods to address observation bias (error accumulation), including early-established endpoints, adjusted sampling orders, and discrete history representation. Ablation studies validate the effectiveness of the anti-drifting methods in both single-directional video streaming and bi-directional video generation. Finally, we show that existing video diffusion models can be finetuned with FramePack, and analyze the differences between different packing schedules. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "761hggw1Wx@OpenReview",
      "index": 439,
      "title": "CoLT: The conditional localization test for assessing the accuracy of neural posterior estimates",
      "authors": [
        "Tianyu Chen",
        "Vansh Bansal",
        "James G. Scott"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "colt",
        "posterior",
        "theta",
        "mid",
        "true",
        "localization",
        "neural",
        "assessing",
        "validating",
        "conditioning"
      ],
      "summary": "We consider the problem of validating whether a neural posterior estimate q ( θ ∣ x ) q ( θ ∣ x ) is an accurate approximation to the true, unknown true posterior p ( θ ∣ x ) p ( θ ∣ x ) . Existing methods for evaluating the quality of an NPE estimate are largely derived from classifier-based tests or divergence measures, but these suffer from several practical drawbacks. As an alternative, we introduce the *Conditional Localization Test* (**CoLT**), a principled method designed to detect discrepancies between p ( θ ∣ x ) p ( θ ∣ x ) and q ( θ ∣ x ) q ( θ ∣ x ) across the full range of conditioning inputs. Rather than relying on exhaustive comparisons or density estimation at every x x , CoLT learns a localization function that adaptively selects points θ l ( x ) θ l ( x ) where the neural posterior q q deviates most strongly from the true posterior p p for that x x . This approach is particularly advantageous in typical simulation-based inference settings, where only a single draw θ ∼ p ( θ ∣ x ) θ ∼ p ( θ ∣ x ) from the true posterior is observed for each conditioning input, but where the neural posterior q ( θ ∣ x ) q ( θ ∣ x ) can be sampled an arbitrary number of times. Our theoretical results establish necessary and sufficient conditions for assessing distributional equality across all x x , offering both rigorous guarantees and practical scalability. Empirically, we demonstrate that CoLT not only performs better than existing methods at comparing p p and q q , but also pinpoints regions of significant divergence, providing actionable insights for model refinement. These properties position CoLT as a state-of-the-art solution for validating neural posterior estimates.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=761hggw1Wx"
        ],
        "venue": [
          "/venue/761hggw1Wx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=761hggw1Wx"
        ],
        "detail": [
          "https://openreview.net/forum?id=761hggw1Wx"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "CoLT: The conditional localization test for assessing the accuracy of neural posterior estimates [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Tianyu Chen , Vansh Bansal , James G. Scott We consider the problem of validating whether a neural posterior estimate q ( θ ∣ x ) q ( θ ∣ x ) is an accurate approximation to the true, unknown true posterior p ( θ ∣ x ) p ( θ ∣ x ) . Existing methods for evaluating the quality of an NPE estimate are largely derived from classifier-based tests or divergence measures, but these suffer from several practical drawbacks. As an alternative, we introduce the *Conditional Localization Test* (**CoLT**), a principled method designed to detect discrepancies between p ( θ ∣ x ) p ( θ ∣ x ) and q ( θ ∣ x ) q ( θ ∣ x ) across the full range of conditioning inputs. Rather than relying on exhaustive comparisons or density estimation at every x x , CoLT learns a localization function that adaptively selects points θ l ( x ) θ l ( x ) where the neural posterior q q deviates most strongly from the true posterior p p for that x x . This approach is particularly advantageous in typical simulation-based inference settings, where only a single draw θ ∼ p ( θ ∣ x ) θ ∼ p ( θ ∣ x ) from the true posterior is observed for each conditioning input, but where the neural posterior q ( θ ∣ x ) q ( θ ∣ x ) can be sampled an arbitrary number of times. Our theoretical results establish necessary and sufficient conditions for assessing distributional equality across all x x , offering both rigorous guarantees and practical scalability. Empirically, we demonstrate that CoLT not only performs better than existing methods at comparing p p and q q , but also pinpoints regions of significant divergence, providing actionable insights for model refinement. These properties position CoLT as a state-of-the-art solution for validating neural posterior estimates. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ErEaq1UNaQ@OpenReview",
      "index": 440,
      "title": "Predictive Preference Learning from Human Interventions",
      "authors": [
        "Haoyuan Cai",
        "Zhenghao Peng",
        "Bolei Zhou"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "preference",
        "human",
        "ppl",
        "interventions",
        "agent",
        "future",
        "horizon",
        "intervention",
        "metadriverse",
        "predictive"
      ],
      "summary": "Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent’s action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ErEaq1UNaQ"
        ],
        "venue": [
          "/venue/ErEaq1UNaQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ErEaq1UNaQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=ErEaq1UNaQ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Predictive Preference Learning from Human Interventions [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Haoyuan Cai , Zhenghao Peng , Bolei Zhou Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent’s action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MtwsRjPZhf@OpenReview",
      "index": 441,
      "title": "Transformers for Mixed-type Event Sequences",
      "authors": [
        "Felix Draxler",
        "Yang Meng",
        "Kai Nelson",
        "Lukas Laskowski",
        "Yibo Yang",
        "Theofanis Karaletsos",
        "Stephan Mandt"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "event",
        "sequences",
        "mixed",
        "continuous",
        "czi",
        "modeling",
        "discrete",
        "mtpp",
        "tokenize",
        "type"
      ],
      "summary": "Event sequences appear widely in domains such as medicine, finance, and remote sensing, yet modeling them is challenging due to their heterogeneity: sequences often contain multiple event types with diverse structures—for example, electronic health records that mix discrete events like medical procedures with continuous lab measurements. Existing approaches either tokenize all entries, violating natural inductive biases, or ignore parts of the data to enforce a consistent structure. In this work, we propose a simple yet powerful Marked Temporal Point Process (MTPP) framework for modeling event sequences with flexible structure, using a single unified model. Our approach employs a single autoregressive transformer with discrete and continuous prediction heads, capable of modeling variable-length, mixed-type event sequences. The continuous head leverages an expressive normalizing flow to model continuous event attributes, avoiding the numerical integration required for inter-event times in most competing methods. Empirically, our model excels on both discrete-only and mixed-type sequences, improving prediction quality and enabling interpretable uncertainty quantification. We make our code public at https://github.com/czi-ai/FlexTPP.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MtwsRjPZhf"
        ],
        "venue": [
          "/venue/MtwsRjPZhf@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MtwsRjPZhf"
        ],
        "detail": [
          "https://openreview.net/forum?id=MtwsRjPZhf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Transformers for Mixed-type Event Sequences [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Felix Draxler , Yang Meng , Kai Nelson , Lukas Laskowski , Yibo Yang , Theofanis Karaletsos , Stephan Mandt Event sequences appear widely in domains such as medicine, finance, and remote sensing, yet modeling them is challenging due to their heterogeneity: sequences often contain multiple event types with diverse structures—for example, electronic health records that mix discrete events like medical procedures with continuous lab measurements. Existing approaches either tokenize all entries, violating natural inductive biases, or ignore parts of the data to enforce a consistent structure. In this work, we propose a simple yet powerful Marked Temporal Point Process (MTPP) framework for modeling event sequences with flexible structure, using a single unified model. Our approach employs a single autoregressive transformer with discrete and continuous prediction heads, capable of modeling variable-length, mixed-type event sequences. The continuous head leverages an expressive normalizing flow to model continuous event attributes, avoiding the numerical integration required for inter-event times in most competing methods. Empirically, our model excels on both discrete-only and mixed-type sequences, improving prediction quality and enabling interpretable uncertainty quantification. We make our code public at https://github.com/czi-ai/FlexTPP. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VRwcEVRcC9@OpenReview",
      "index": 442,
      "title": "ROGR: Relightable 3D Objects using Generative Relighting",
      "authors": [
        "Jiapeng Tang",
        "Matthew Jacob Levine",
        "Dor Verbin",
        "Stephan J. Garbin",
        "Matthias Nießner",
        "Ricardo Martin Brualla",
        "Pratul P. Srinivasan",
        "Philipp Henzler"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lighting",
        "relighting",
        "rogr",
        "relightable",
        "nerf",
        "object",
        "conditioned",
        "tensoir",
        "generative",
        "specularities"
      ],
      "summary": "We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VRwcEVRcC9"
        ],
        "venue": [
          "/venue/VRwcEVRcC9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VRwcEVRcC9"
        ],
        "detail": [
          "https://openreview.net/forum?id=VRwcEVRcC9"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "ROGR: Relightable 3D Objects using Generative Relighting [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Jiapeng Tang , Matthew Jacob Levine , Dor Verbin , Stephan J. Garbin , Matthias Nießner , Ricardo Martin Brualla , Pratul P. Srinivasan , Philipp Henzler We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nW6SMcfDq3@OpenReview",
      "index": 443,
      "title": "Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time",
      "authors": [
        "Qiang Fu",
        "Andre Wibisono"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rhgd",
        "accelerated",
        "hamiltonian",
        "descent",
        "opt",
        "integration",
        "rates",
        "rhf",
        "agd",
        "randomized"
      ],
      "summary": "We study the Hamiltonian flow for optimization (HF-opt), which simulates the Hamiltonian dynamics for some integration time and resets the velocity to 0 0 to decrease the objective function; this is the optimization analogue of the Hamiltonian Monte Carlo algorithm for sampling. For short integration time, HF-opt has the same convergence rates as gradient descent for minimizing strongly and weakly convex functions. We show that by randomizing the integration time in HF-opt, the resulting randomized Hamiltonian flow (RHF) achieves accelerated convergence rates in continuous time, similar to the rates for accelerated gradient flow. We study a discrete-time implementation of RHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove that RHGD achieves the same accelerated convergence rates as Nesterov's accelerated gradient descent (AGD) for minimizing smooth strongly and weakly convex functions. We provide numerical experiments to demonstrate that RHGD is competitive with classical accelerated methods such as AGD across all settings and outperforms them in certain regimes.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nW6SMcfDq3"
        ],
        "venue": [
          "/venue/nW6SMcfDq3@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nW6SMcfDq3"
        ],
        "detail": [
          "https://openreview.net/forum?id=nW6SMcfDq3"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Qiang Fu , Andre Wibisono We study the Hamiltonian flow for optimization (HF-opt), which simulates the Hamiltonian dynamics for some integration time and resets the velocity to 0 0 to decrease the objective function; this is the optimization analogue of the Hamiltonian Monte Carlo algorithm for sampling. For short integration time, HF-opt has the same convergence rates as gradient descent for minimizing strongly and weakly convex functions. We show that by randomizing the integration time in HF-opt, the resulting randomized Hamiltonian flow (RHF) achieves accelerated convergence rates in continuous time, similar to the rates for accelerated gradient flow. We study a discrete-time implementation of RHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove that RHGD achieves the same accelerated convergence rates as Nesterov's accelerated gradient descent (AGD) for minimizing smooth strongly and weakly convex functions. We provide numerical experiments to demonstrate that RHGD is competitive with classical accelerated methods such as AGD across all settings and outperforms them in certain regimes. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3AdTRYA2uJ@OpenReview",
      "index": 444,
      "title": "Cost-aware LLM-based Online Dataset Annotation",
      "authors": [
        "Eray Can Elumar",
        "Cem Tekin",
        "Osman Yagan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "camvo",
        "voting",
        "labeling",
        "majority",
        "llm",
        "annotation",
        "cost",
        "llms",
        "dataset",
        "aware"
      ],
      "summary": "Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3AdTRYA2uJ"
        ],
        "venue": [
          "/venue/3AdTRYA2uJ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3AdTRYA2uJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=3AdTRYA2uJ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Cost-aware LLM-based Online Dataset Annotation [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Eray Can Elumar , Cem Tekin , Osman Yagan Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yvGL2HP7pU@OpenReview",
      "index": 445,
      "title": "Scaling Unlocks Broader Generation and Deeper Functional Understanding of Proteins",
      "authors": [
        "Aadyot Bhatnagar",
        "Sarthak Jain",
        "Joel Beazer",
        "Samuel C. Curran",
        "Alexander M. Hoffnagle",
        "Kyle Shan Ching",
        "Michael Martyn",
        "Stephen Nayfach",
        "Jeffrey A. Ruffolo",
        "Ali Madani"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "plms",
        "proteins",
        "progen3",
        "protein",
        "46b",
        "larger",
        "curated",
        "unlocks",
        "trained",
        "scaling"
      ],
      "summary": "Generative protein language models (PLMs) are powerful tools for designing proteins purpose-built to solve problems in medicine, agriculture, and industrial processes. Recent work has trained ever larger language models, but there has been little systematic study of the optimal training distributions and the influence of model scale on the sequences generated by PLMs. We introduce the ProGen3 family of sparse generative PLMs, and we develop compute-optimal scaling laws to scale up to a 46B-parameter model pre-trained on 1.5T amino acid tokens. ProGen3's pre-training data is sampled from an optimized data distribution over the PPA v1, a carefully curated dataset of 3.4B full-length proteins. We evaluate for the first time in the wet lab the influence of model scale on the sequences generated by PLMs, and we find that larger models generate viable proteins for a much wider diversity of protein families. Finally, we find both computationally and experimentally that larger models are more responsive to alignment with laboratory data, resulting in improved protein fitness prediction and sequence generation capabilities. These results indicate that larger PLMs like ProGen3-46B trained on larger, well-curated datasets are powerful foundation models that push the frontier of protein design.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yvGL2HP7pU"
        ],
        "venue": [
          "/venue/yvGL2HP7pU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yvGL2HP7pU"
        ],
        "detail": [
          "https://openreview.net/forum?id=yvGL2HP7pU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Scaling Unlocks Broader Generation and Deeper Functional Understanding of Proteins [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Aadyot Bhatnagar , Sarthak Jain , Joel Beazer , Samuel C. Curran , Alexander M. Hoffnagle , Kyle Shan Ching , Michael Martyn , Stephen Nayfach , Jeffrey A. Ruffolo , Ali Madani Generative protein language models (PLMs) are powerful tools for designing proteins purpose-built to solve problems in medicine, agriculture, and industrial processes. Recent work has trained ever larger language models, but there has been little systematic study of the optimal training distributions and the influence of model scale on the sequences generated by PLMs. We introduce the ProGen3 family of sparse generative PLMs, and we develop compute-optimal scaling laws to scale up to a 46B-parameter model pre-trained on 1.5T amino acid tokens. ProGen3's pre-training data is sampled from an optimized data distribution over the PPA v1, a carefully curated dataset of 3.4B full-length proteins. We evaluate for the first time in the wet lab the influence of model scale on the sequences generated by PLMs, and we find that larger models generate viable proteins for a much wider diversity of protein families. Finally, we find both computationally and experimentally that larger models are more responsive to alignment with laboratory data, resulting in improved protein fitness prediction and sequence generation capabilities. These results indicate that larger PLMs like ProGen3-46B trained on larger, well-curated datasets are powerful foundation models that push the frontier of protein design. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3YguS2rxdk@OpenReview",
      "index": 446,
      "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis",
      "authors": [
        "Jiatao Gu",
        "Tianrong Chen",
        "David Berthelot",
        "Huangjie Zheng",
        "Yuyang Wang",
        "Ruixiang ZHANG",
        "Laurent Dinh",
        "Miguel Ángel Bautista",
        "Joshua M. Susskind",
        "Shuangfei Zhai"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "starflow",
        "normalizing",
        "flows",
        "transformer",
        "tarflow",
        "resolution",
        "image",
        "latent",
        "synthesis",
        "autoregressive"
      ],
      "summary": "We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance on high-resolution image synthesis. STARFlow's main building block is Transformer Autoregressive Flow (TARFlow), which combines normalizing flows with Autoregressive Transformer architectures and has recently achieved impressive results in image modeling. In this work, we first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce a set of architectural and algorithmic innovations that significantly enhance the scalability: (1) a deep-shallow design where a deep Transformer block captures most of the model’s capacity, followed by a few shallow Transformer blocks that are computationally cheap yet contribute non-negligibly, (2) learning in the latent space of pretrained autoencoders, which proves far more effective than modeling pixels directly, and (3) a novel guidance algorithm that substantially improves sample quality. Crucially, our model remains a single, end-to-end normalizing flow, allowing exact maximum likelihood training in continuous space without discretization. STARFlow achieves competitive results in both class- and text-conditional image generation, with sample quality approaching that of state-of-the-art diffusion models. To our knowledge, this is the **first** successful demonstration of normalizing flows at this scale and resolution. Code and weights available at https://github.com/apple/ml-starflow.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3YguS2rxdk"
        ],
        "venue": [
          "/venue/3YguS2rxdk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3YguS2rxdk"
        ],
        "detail": [
          "https://openreview.net/forum?id=3YguS2rxdk"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Jiatao Gu , Tianrong Chen , David Berthelot , Huangjie Zheng , Yuyang Wang , Ruixiang ZHANG , Laurent Dinh , Miguel Ángel Bautista , Joshua M. Susskind , Shuangfei Zhai We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance on high-resolution image synthesis. STARFlow's main building block is Transformer Autoregressive Flow (TARFlow), which combines normalizing flows with Autoregressive Transformer architectures and has recently achieved impressive results in image modeling. In this work, we first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce a set of architectural and algorithmic innovations that significantly enhance the scalability: (1) a deep-shallow design where a deep Transformer block captures most of the model’s capacity, followed by a few shallow Transformer blocks that are computationally cheap yet contribute non-negligibly, (2) learning in the latent space of pretrained autoencoders, which proves far more effective than modeling pixels directly, and (3) a novel guidance algorithm that substantially improves sample quality. Crucially, our model remains a single, end-to-end normalizing flow, allowing exact maximum likelihood training in continuous space without discretization. STARFlow achieves competitive results in both class- and text-conditional image generation, with sample quality approaching that of state-of-the-art diffusion models. To our knowledge, this is the **first** successful demonstration of normalizing flows at this scale and resolution. Code and weights available at https://github.com/apple/ml-starflow. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "N1KPOlcN6P@OpenReview",
      "index": 447,
      "title": "Flow Equivariant Recurrent Neural Networks",
      "authors": [
        "T. Anderson Keller"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "equivariant",
        "transformations",
        "equivariance",
        "sequence",
        "rnns",
        "symmetries",
        "flow",
        "stimuli",
        "recurrent",
        "generalization"
      ],
      "summary": "Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of 'flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=N1KPOlcN6P"
        ],
        "venue": [
          "/venue/N1KPOlcN6P@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=N1KPOlcN6P"
        ],
        "detail": [
          "https://openreview.net/forum?id=N1KPOlcN6P"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Flow Equivariant Recurrent Neural Networks [PDF 1 ] [Copy] [Kimi 1 ] [REL] Author : T. Anderson Keller Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of 'flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VjjJlJ5qik@OpenReview",
      "index": 448,
      "title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking",
      "authors": [
        "Xiangqi Wang",
        "Yue Huang",
        "Yanbo Wang",
        "Xiaonan Luo",
        "Kehan Guo",
        "Yujun Zhou",
        "Xiangliang Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "adareasoner",
        "reasoning",
        "configurations",
        "tasks",
        "thinking",
        "llm",
        "joke",
        "adaptive",
        "llms",
        "requiring"
      ],
      "summary": "LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work “well enough” across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VjjJlJ5qik"
        ],
        "venue": [
          "/venue/VjjJlJ5qik@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VjjJlJ5qik"
        ],
        "detail": [
          "https://openreview.net/forum?id=VjjJlJ5qik"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xiangqi Wang , Yue Huang , Yanbo Wang , Xiaonan Luo , Kehan Guo , Yujun Zhou , Xiangliang Zhang LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work “well enough” across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "CABcYH1wKM@OpenReview",
      "index": 449,
      "title": "LABridge: Text–Image Latent Alignment Framework via Mean-Conditioned OU Process",
      "authors": [
        "Huiyang Shao",
        "Xin Xia",
        "Yuxi Ren",
        "XING WANG",
        "Xuefeng Xiao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "labridge",
        "text",
        "alignment",
        "image",
        "latent",
        "conditioned",
        "latents",
        "denoising",
        "aligned",
        "priors"
      ],
      "summary": "Diffusion models have emerged as state‑of‑the‑art in image synthesis.However, it often suffer from semantic instability and slow iterative denoising. We introduce Latent Alignment Framework (LABridge), a novel Text–Image Latent Alignment Framework via an Ornstein–Uhlenbeck (OU) Process, which explicitly preserves and aligns textual and visual semantics in an aligned latent space. LABridge employs a Text-Image Alignment Encoder (TIAE) to encode text prompts into structured priors that are directly aligned with image latents. Instead of a homogeneous Gaussian, Mean-Conditioned OU process smoothly interpolates between these text‑conditioned priors and image latents, improving stability and reducing the number of denoising steps. Extensive experiments on standard text-to-image benchmarks show that LABridge achieves better text–image alignment metric and competitive FID scores compared to leading diffusion baselines. By unifying text and image representations through principled latent alignment, LABridge paves the way for more efficient, semantically consistent, and high‑fidelity text to image generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CABcYH1wKM"
        ],
        "venue": [
          "/venue/CABcYH1wKM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CABcYH1wKM"
        ],
        "detail": [
          "https://openreview.net/forum?id=CABcYH1wKM"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "LABridge: Text–Image Latent Alignment Framework via Mean-Conditioned OU Process [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Huiyang Shao , Xin Xia , Yuxi Ren , XING WANG , Xuefeng Xiao Diffusion models have emerged as state‑of‑the‑art in image synthesis.However, it often suffer from semantic instability and slow iterative denoising. We introduce Latent Alignment Framework (LABridge), a novel Text–Image Latent Alignment Framework via an Ornstein–Uhlenbeck (OU) Process, which explicitly preserves and aligns textual and visual semantics in an aligned latent space. LABridge employs a Text-Image Alignment Encoder (TIAE) to encode text prompts into structured priors that are directly aligned with image latents. Instead of a homogeneous Gaussian, Mean-Conditioned OU process smoothly interpolates between these text‑conditioned priors and image latents, improving stability and reducing the number of denoising steps. Extensive experiments on standard text-to-image benchmarks show that LABridge achieves better text–image alignment metric and competitive FID scores compared to leading diffusion baselines. By unifying text and image representations through principled latent alignment, LABridge paves the way for more efficient, semantically consistent, and high‑fidelity text to image generation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TQNlIQIrcK@OpenReview",
      "index": 450,
      "title": "Achieving O ~ ( 1 / N ) O ~ ( 1 / N ) Optimality Gap in Restless Bandits through Gaussian Approximation",
      "authors": [
        "Chen YAN",
        "Weina Wang",
        "Lei Ying"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rmabs",
        "optimality",
        "rmab",
        "tilde",
        "restless",
        "gap",
        "approximation",
        "mathcal",
        "degenerate",
        "based"
      ],
      "summary": "We study the finite-horizon Restless Multi-Armed Bandit (RMAB) problem with N N homogeneous arms. Prior work has shown that when an RMAB satisfies a non-degeneracy condition, Linear-Programming-based (LP-based) policies derived from the fluid approximation, which captures the mean dynamics of the system, achieve an exponentially small optimality gap. However, it is common for RMABs to be degenerate, in which case LP-based policies can result in a Θ ( 1 / N − − √ ) Θ ( 1 / N ) optimality gap per arm. In this paper, we propose a novel Stochastic-Programming-based (SP-based) policy that, under a uniqueness assumption, achieves an O ~ ( 1 / N ) O ~ ( 1 / N ) optimality gap for degenerate RMABs. Our approach is based on the construction of a Gaussian stochastic system that captures not only the mean but also the variance of the RMAB dynamics, resulting in a more accurate approximation than the fluid approximation. We then solve a stochastic program for this system to obtain our policy. This is the first result to establish an O ~ ( 1 / N ) O ~ ( 1 / N ) optimality gap for degenerate RMABs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TQNlIQIrcK"
        ],
        "venue": [
          "/venue/TQNlIQIrcK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TQNlIQIrcK"
        ],
        "detail": [
          "https://openreview.net/forum?id=TQNlIQIrcK"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Achieving O ~ ( 1 / N ) O ~ ( 1 / N ) Optimality Gap in Restless Bandits through Gaussian Approximation [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Chen YAN , Weina Wang , Lei Ying We study the finite-horizon Restless Multi-Armed Bandit (RMAB) problem with N N homogeneous arms. Prior work has shown that when an RMAB satisfies a non-degeneracy condition, Linear-Programming-based (LP-based) policies derived from the fluid approximation, which captures the mean dynamics of the system, achieve an exponentially small optimality gap. However, it is common for RMABs to be degenerate, in which case LP-based policies can result in a Θ ( 1 / N − − √ ) Θ ( 1 / N ) optimality gap per arm. In this paper, we propose a novel Stochastic-Programming-based (SP-based) policy that, under a uniqueness assumption, achieves an O ~ ( 1 / N ) O ~ ( 1 / N ) optimality gap for degenerate RMABs. Our approach is based on the construction of a Gaussian stochastic system that captures not only the mean but also the variance of the RMAB dynamics, resulting in a more accurate approximation than the fluid approximation. We then solve a stochastic program for this system to obtain our policy. This is the first result to establish an O ~ ( 1 / N ) O ~ ( 1 / N ) optimality gap for degenerate RMABs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hTxnm6H93P@OpenReview",
      "index": 451,
      "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling",
      "authors": [
        "Moritz Haas",
        "Sebastian Bordt",
        "Ulrike von Luxburg",
        "Leena Chennuru Vankadara"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "width",
        "regime",
        "learning",
        "rates",
        "divergence",
        "stable",
        "unstable",
        "controlled",
        "scaling",
        "initialization"
      ],
      "summary": "Scaling limits, such as infinite-width limits, serve as promising theoretical tools to study large-scale models. However, it is widely believed that existing infinite-width theory does not faithfully explain the behavior of practical networks, especially those trained in *standard parameterization* (SP) meaning He initialization with a global learning rate. For instance, existing theory for SP predicts instability at large learning rates and vanishing feature learning at stable ones. In practice, however, optimal learning rates decay slower than theoretically predicted and networks exhibit both stable training and non-trivial feature learning, even at very large widths. Here, we show that this discrepancy is not fully explained by finite-width phenomena. Instead, we find a resolution through a finer-grained analysis of the regime previously considered unstable and therefore uninteresting. In particular, we show that, under the cross-entropy (CE) loss, the unstable regime comprises two distinct sub-regimes: a catastrophically unstable regime and a more benign controlled divergence regime, where logits diverge but gradients and activations remain stable. Moreover, under large learning rates at the edge of the controlled divergence regime, there exists a well-defined infinite width limit where features continue to evolve in all the hidden layers. In experiments across optimizers, architectures, and data modalities, we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically maximal stable learning rate exponents which provide useful guidance on optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scalings for standard initialization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hTxnm6H93P"
        ],
        "venue": [
          "/venue/hTxnm6H93P@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hTxnm6H93P"
        ],
        "detail": [
          "https://openreview.net/forum?id=hTxnm6H93P"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Moritz Haas , Sebastian Bordt , Ulrike von Luxburg , Leena Chennuru Vankadara Scaling limits, such as infinite-width limits, serve as promising theoretical tools to study large-scale models. However, it is widely believed that existing infinite-width theory does not faithfully explain the behavior of practical networks, especially those trained in *standard parameterization* (SP) meaning He initialization with a global learning rate. For instance, existing theory for SP predicts instability at large learning rates and vanishing feature learning at stable ones. In practice, however, optimal learning rates decay slower than theoretically predicted and networks exhibit both stable training and non-trivial feature learning, even at very large widths. Here, we show that this discrepancy is not fully explained by finite-width phenomena. Instead, we find a resolution through a finer-grained analysis of the regime previously considered unstable and therefore uninteresting. In particular, we show that, under the cross-entropy (CE) loss, the unstable regime comprises two distinct sub-regimes: a catastrophically unstable regime and a more benign controlled divergence regime, where logits diverge but gradients and activations remain stable. Moreover, under large learning rates at the edge of the controlled divergence regime, there exists a well-defined infinite width limit where features continue to evolve in all the hidden layers. In experiments across optimizers, architectures, and data modalities, we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically maximal stable learning rate exponents which provide useful guidance on optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scalings for standard initialization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gKsG5qR3Bt@OpenReview",
      "index": 452,
      "title": "Flash Invariant Point Attention",
      "authors": [
        "Andrew Liu",
        "Axel Elaldi",
        "Nicholas T Franklin",
        "Nathan Russell",
        "Gurinder S. Atwal",
        "Yih-En Andrew Ban",
        "Olivia Viessmann"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ipa",
        "flashipa",
        "flash",
        "length",
        "flashattention",
        "invariant",
        "attention",
        "flagshippioneering",
        "unattainable",
        "sequence"
      ],
      "summary": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gKsG5qR3Bt"
        ],
        "venue": [
          "/venue/gKsG5qR3Bt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gKsG5qR3Bt"
        ],
        "detail": [
          "https://openreview.net/forum?id=gKsG5qR3Bt"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Flash Invariant Point Attention [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Andrew Liu , Axel Elaldi , Nicholas T Franklin , Nathan Russell , Gurinder S. Atwal , Yih-En Andrew Ban , Olivia Viessmann Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0BVrpXMr5Y@OpenReview",
      "index": 453,
      "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference",
      "authors": [
        "Yi Zhao",
        "Yajuan Peng",
        "Nguyen Cam-Tu",
        "Zuchao Li",
        "Wang Xiaoliang",
        "hai zhao",
        "Xiaoming Fu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "smallkv",
        "eviction",
        "compensation",
        "cache",
        "tokens",
        "attention",
        "marginal",
        "compression",
        "assisted",
        "llm"
      ],
      "summary": "KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens uniformly, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs with different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model’s attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0BVrpXMr5Y"
        ],
        "venue": [
          "/venue/0BVrpXMr5Y@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0BVrpXMr5Y"
        ],
        "detail": [
          "https://openreview.net/forum?id=0BVrpXMr5Y"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Yi Zhao , Yajuan Peng , Nguyen Cam-Tu , Zuchao Li , Wang Xiaoliang , hai zhao , Xiaoming Fu KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens uniformly, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs with different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model’s attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0r4yzkvt9j@OpenReview",
      "index": 454,
      "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs",
      "authors": [
        "Jan Hagnberger",
        "Daniel Musekamp",
        "Mathias Niepert"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "calm",
        "pde",
        "pdes",
        "latent",
        "irregularly",
        "discretized",
        "space",
        "compressed",
        "sampled",
        "memory"
      ],
      "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engineering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical space often incurs significant computational costs. To address this issue, several neural surrogate models have been developed that operate in a compressed latent space to solve the PDE. While these approaches reduce computational complexity, they often use Transformer-based attention mechanisms to handle irregularly sampled domains, resulting in increased memory consumption. In contrast, convolutional neural networks allow memory-efficient encoding and decoding but are limited to regular discretizations. Motivated by these considerations, we propose CALM-PDE, a model class that efficiently solves arbitrarily discretized PDEs in a compressed latent space. We introduce a novel continuous convolution-based encoder-decoder architecture that uses an epsilon-neighborhood-constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. We demonstrate the effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and irregularly sampled spatial domains. CALM-PDE is competitive with or outperforms existing baseline methods while offering significant improvements in memory and inference time efficiency compared to Transformer-based methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0r4yzkvt9j"
        ],
        "venue": [
          "/venue/0r4yzkvt9j@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0r4yzkvt9j"
        ],
        "detail": [
          "https://openreview.net/forum?id=0r4yzkvt9j"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Jan Hagnberger , Daniel Musekamp , Mathias Niepert Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engineering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical space often incurs significant computational costs. To address this issue, several neural surrogate models have been developed that operate in a compressed latent space to solve the PDE. While these approaches reduce computational complexity, they often use Transformer-based attention mechanisms to handle irregularly sampled domains, resulting in increased memory consumption. In contrast, convolutional neural networks allow memory-efficient encoding and decoding but are limited to regular discretizations. Motivated by these considerations, we propose CALM-PDE, a model class that efficiently solves arbitrarily discretized PDEs in a compressed latent space. We introduce a novel continuous convolution-based encoder-decoder architecture that uses an epsilon-neighborhood-constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. We demonstrate the effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and irregularly sampled spatial domains. CALM-PDE is competitive with or outperforms existing baseline methods while offering significant improvements in memory and inference time efficiency compared to Transformer-based methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YES7VDXPV8@OpenReview",
      "index": 455,
      "title": "On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection",
      "authors": [
        "Weiqing He",
        "Xiang Li",
        "Tianqi Shang",
        "Li Shen",
        "Weijie J Su",
        "Qi Long"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gof",
        "watermark",
        "tests",
        "goodness",
        "text",
        "detection",
        "llms",
        "fit",
        "underused",
        "content"
      ],
      "summary": "Large language models (LLMs) raise concerns about content authenticity and integrity because they can generate human-like text at scale. Text watermarks, which embed detectable statistical signals into generated text, offer a provable way to verify content origin. Many detection methods rely on pivotal statistics that are i.i.d. under human-written text, making goodness-of-fit (GoF) tests a natural tool for watermark detection. However, GoF tests remain largely underexplored in this setting. In this paper, we systematically evaluate eight GoF tests across three popular watermarking schemes, using three open-source LLMs, two datasets, various generation temperatures, and multiple post-editing methods. We find that general GoF tests can improve both the detection power and robustness of watermark detectors. Notably, we observe that text repetition, common in low-temperature settings, gives GoF tests a unique advantage not exploited by existing methods. Our results highlight that classic GoF tests are a simple yet powerful and underused tool for watermark detection in LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YES7VDXPV8"
        ],
        "venue": [
          "/venue/YES7VDXPV8@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YES7VDXPV8"
        ],
        "detail": [
          "https://openreview.net/forum?id=YES7VDXPV8"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Weiqing He , Xiang Li , Tianqi Shang , Li Shen , Weijie J Su , Qi Long Large language models (LLMs) raise concerns about content authenticity and integrity because they can generate human-like text at scale. Text watermarks, which embed detectable statistical signals into generated text, offer a provable way to verify content origin. Many detection methods rely on pivotal statistics that are i.i.d. under human-written text, making goodness-of-fit (GoF) tests a natural tool for watermark detection. However, GoF tests remain largely underexplored in this setting. In this paper, we systematically evaluate eight GoF tests across three popular watermarking schemes, using three open-source LLMs, two datasets, various generation temperatures, and multiple post-editing methods. We find that general GoF tests can improve both the detection power and robustness of watermark detectors. Notably, we observe that text repetition, common in low-temperature settings, gives GoF tests a unique advantage not exploited by existing methods. Our results highlight that classic GoF tests are a simple yet powerful and underused tool for watermark detection in LLMs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "auiURbhoYx@OpenReview",
      "index": 456,
      "title": "Balancing Multimodal Training Through Game-Theoretic Regularization",
      "authors": [
        "Konstantinos Kontras",
        "Thomas Strypsteen",
        "Christos Chatzichristos",
        "Paul Pu Liang",
        "Matthew B. Blaschko",
        "Maarten De Vos"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "multimodal",
        "mcr",
        "modalities",
        "training",
        "competition",
        "game",
        "theoretic",
        "across",
        "modality",
        "extraction"
      ],
      "summary": "Multimodal learning holds the promise for richer information extraction by capturing dependencies across data sources. Yet, current training methods often underperform due to modality competition, a phenomenon where modalities contend for training resources, leaving some underoptimized. This raises a pivotal question: how can we address training imbalances, ensure adequate optimization across all modalities, and achieve consistent performance improvements as we transition from unimodal to multimodal data? This paper proposes the Multimodal Competition Regularizer (MCR), inspired by a mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) A game-theoretic framework that adaptively balances modality contributions by encouraging each to maximize its informative role in the final prediction. 2) Refining lower and upper bounds for each MI term to enhance the extraction of both task-relevant unique and shared information across modalities. 3) Proposing latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and simple baselines, demonstrating that training modalities jointly lead to important performance gains on synthetic and large real-world datasets. We release our code and models at https://github.com/kkontras/MCR.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=auiURbhoYx"
        ],
        "venue": [
          "/venue/auiURbhoYx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=auiURbhoYx"
        ],
        "detail": [
          "https://openreview.net/forum?id=auiURbhoYx"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Balancing Multimodal Training Through Game-Theoretic Regularization [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Konstantinos Kontras , Thomas Strypsteen , Christos Chatzichristos , Paul Pu Liang , Matthew B. Blaschko , Maarten De Vos Multimodal learning holds the promise for richer information extraction by capturing dependencies across data sources. Yet, current training methods often underperform due to modality competition, a phenomenon where modalities contend for training resources, leaving some underoptimized. This raises a pivotal question: how can we address training imbalances, ensure adequate optimization across all modalities, and achieve consistent performance improvements as we transition from unimodal to multimodal data? This paper proposes the Multimodal Competition Regularizer (MCR), inspired by a mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) A game-theoretic framework that adaptively balances modality contributions by encouraging each to maximize its informative role in the final prediction. 2) Refining lower and upper bounds for each MI term to enhance the extraction of both task-relevant unique and shared information across modalities. 3) Proposing latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and simple baselines, demonstrating that training modalities jointly lead to important performance gains on synthetic and large real-world datasets. We release our code and models at https://github.com/kkontras/MCR. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nG45z7lJ7D@OpenReview",
      "index": 457,
      "title": "Bipolar Self-attention for Spiking Transformers",
      "authors": [
        "Shuai Wang",
        "Malu Zhang",
        "Jingya Wang",
        "Dehao Zhang",
        "Yimeng Shan",
        "Jieyuan Zhang",
        "Yichen Xiao",
        "Honglin Cao",
        "Haonan Zhang",
        "Zeyu Ma",
        "Yang Yang",
        "Haizhou Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "spiking",
        "attention",
        "bsa",
        "transformers",
        "ssa",
        "bipolar",
        "stochasticity",
        "shiftmax",
        "self",
        "spike"
      ],
      "summary": "Harnessing the event-driven characteristic, Spiking Neural Networks (SNNs) present a promising avenue toward energy-efficient Transformer architectures. However, existing Spiking Transformers still suffer significant performance gaps compared to their Artificial Neural Network counterparts. Through comprehensive analysis, we attribute this gap to these two factors. First, the binary nature of spike trains limits Spiking Self-attention (SSA)’s capacity to capture negative–negative and positive–negative membrane potential interactions on Querys and Keys. Second, SSA typically omits Softmax functions to avoid energy-intensive multiply-accumulate operations, thereby failing to maintain row-stochasticity constraints on attention scores. To address these issues, we propose a Bipolar Self-attention (BSA) paradigm, effectively modeling multi-polar membrane potential interactions with a fully spike-driven characteristic. Specifically, we demonstrate that ternary matrix multiplication provides a closer approximation to real-valued computation on both distribution and local correlation, enabling clear differentiation between homopolar and heteropolar interactions. Moreover, we propose a shift-based Softmax approximation named Shiftmax, which efficiently achieves low-entropy activation and partly maintains row-stochasticity without non-linear operation, enabling precise attention allocation. Extensive experiments show that BSA achieves substantial performance improvements across various tasks, including image classification, semantic segmentation, and event-based tracking. These results establish its potential as a fundamental building block for energy-efficient Spiking Transformers.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nG45z7lJ7D"
        ],
        "venue": [
          "/venue/nG45z7lJ7D@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nG45z7lJ7D"
        ],
        "detail": [
          "https://openreview.net/forum?id=nG45z7lJ7D"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Bipolar Self-attention for Spiking Transformers [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Shuai Wang , Malu Zhang , Jingya Wang , Dehao Zhang , Yimeng Shan , Jieyuan Zhang , Yichen Xiao , Honglin Cao , Haonan Zhang , Zeyu Ma , Yang Yang , Haizhou Li Harnessing the event-driven characteristic, Spiking Neural Networks (SNNs) present a promising avenue toward energy-efficient Transformer architectures. However, existing Spiking Transformers still suffer significant performance gaps compared to their Artificial Neural Network counterparts. Through comprehensive analysis, we attribute this gap to these two factors. First, the binary nature of spike trains limits Spiking Self-attention (SSA)’s capacity to capture negative–negative and positive–negative membrane potential interactions on Querys and Keys. Second, SSA typically omits Softmax functions to avoid energy-intensive multiply-accumulate operations, thereby failing to maintain row-stochasticity constraints on attention scores. To address these issues, we propose a Bipolar Self-attention (BSA) paradigm, effectively modeling multi-polar membrane potential interactions with a fully spike-driven characteristic. Specifically, we demonstrate that ternary matrix multiplication provides a closer approximation to real-valued computation on both distribution and local correlation, enabling clear differentiation between homopolar and heteropolar interactions. Moreover, we propose a shift-based Softmax approximation named Shiftmax, which efficiently achieves low-entropy activation and partly maintains row-stochasticity without non-linear operation, enabling precise attention allocation. Extensive experiments show that BSA achieves substantial performance improvements across various tasks, including image classification, semantic segmentation, and event-based tracking. These results establish its potential as a fundamental building block for energy-efficient Spiking Transformers. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RPuTB28HsK@OpenReview",
      "index": 458,
      "title": "Online Functional Tensor Decomposition via Continual Learning for Streaming Data Completion",
      "authors": [
        "Xi Zhang",
        "Yanyi Li",
        "Yisi Luo",
        "Qi Xie",
        "Deyu Meng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tensor",
        "streaming",
        "online",
        "continual",
        "decomposition",
        "oftd",
        "inrs",
        "inr",
        "data",
        "completion"
      ],
      "summary": "Online tensor decompositions are powerful and proven techniques that address the challenges in processing high-velocity streaming tensor data, such as traffic flow and weather system. The main aim of this work is to propose a novel online functional tensor decomposition (OFTD) framework, which represents a spatial-temporal continuous function using the CP tensor decomposition parameterized by coordinate-based implicit neural representations (INRs). The INRs allow for natural characterization of continually expanded streaming data by simply adding new coordinates into the network. Particularly, our method transforms the classical online tensor decomposition algorithm into a more dynamic continual learning paradigm of updating the INR weights to fit the new data without forgetting the previous tensor knowledge. To this end, we introduce a long-tail memory replay method that adapts to the local continuity property of INR. Extensive experiments for streaming tensor completion using traffic, weather, user-item, and video data verify the effectiveness of the OFTD approach for streaming data analysis. This endeavor serves as a pivotal inspiration for future research to connect classical online tensor tools with continual learning paradigms to better explore knowledge underlying streaming tensor data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RPuTB28HsK"
        ],
        "venue": [
          "/venue/RPuTB28HsK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RPuTB28HsK"
        ],
        "detail": [
          "https://openreview.net/forum?id=RPuTB28HsK"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Online Functional Tensor Decomposition via Continual Learning for Streaming Data Completion [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Xi Zhang , Yanyi Li , Yisi Luo , Qi Xie , Deyu Meng Online tensor decompositions are powerful and proven techniques that address the challenges in processing high-velocity streaming tensor data, such as traffic flow and weather system. The main aim of this work is to propose a novel online functional tensor decomposition (OFTD) framework, which represents a spatial-temporal continuous function using the CP tensor decomposition parameterized by coordinate-based implicit neural representations (INRs). The INRs allow for natural characterization of continually expanded streaming data by simply adding new coordinates into the network. Particularly, our method transforms the classical online tensor decomposition algorithm into a more dynamic continual learning paradigm of updating the INR weights to fit the new data without forgetting the previous tensor knowledge. To this end, we introduce a long-tail memory replay method that adapts to the local continuity property of INR. Extensive experiments for streaming tensor completion using traffic, weather, user-item, and video data verify the effectiveness of the OFTD approach for streaming data analysis. This endeavor serves as a pivotal inspiration for future research to connect classical online tensor tools with continual learning paradigms to better explore knowledge underlying streaming tensor data. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "pRYGjhirkY@OpenReview",
      "index": 459,
      "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model",
      "authors": [
        "Yichen Liu",
        "Yan Lin",
        "Shengnan Guo",
        "Zeyu Zhou",
        "Youfang Lin",
        "Huaiyu Wan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "trajmamba",
        "travel",
        "trajectory",
        "vehicle",
        "trajectories",
        "purposes",
        "pre",
        "gps",
        "rich",
        "semantics"
      ],
      "summary": "Vehicle GPS trajectories record how vehicles move over time, storing valuable travel semantics, including movement patterns and travel purposes. Learning travel semantics effectively and efficiently is crucial for real-world applications of trajectory data, which is hindered by two major challenges. First, travel purposes are tied to the functions of the roads and points-of-interest (POIs) involved in a trip. Such information is encoded in textual addresses and descriptions and introduces heavy computational burden to modeling. Second, real-world trajectories often contain redundant points, which harm both computational efficiency and trajectory embedding quality. To address these challenges, we propose TrajMamba, a novel approach for efficient and semantically rich vehicle trajectory learning. TrajMamba introduces a Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS and road perspectives of trajectories, enabling robust representations of continuous travel behaviors. It also incorporates a Travel Purpose-aware Pre-training procedure to integrate travel purposes into the learned embeddings without introducing extra overhead to embedding calculation. To reduce redundancy in trajectories, TrajMamba features a Knowledge Distillation Pre-training scheme to identify key trajectory points through a learnable mask generator and obtain effective compressed trajectory embeddings. Extensive experiments on two real-world datasets and three downstream tasks show that TrajMamba outperforms state-of-the-art baselines in both efficiency and accuracy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pRYGjhirkY"
        ],
        "venue": [
          "/venue/pRYGjhirkY@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pRYGjhirkY"
        ],
        "detail": [
          "https://openreview.net/forum?id=pRYGjhirkY"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yichen Liu , Yan Lin , Shengnan Guo , Zeyu Zhou , Youfang Lin , Huaiyu Wan Vehicle GPS trajectories record how vehicles move over time, storing valuable travel semantics, including movement patterns and travel purposes. Learning travel semantics effectively and efficiently is crucial for real-world applications of trajectory data, which is hindered by two major challenges. First, travel purposes are tied to the functions of the roads and points-of-interest (POIs) involved in a trip. Such information is encoded in textual addresses and descriptions and introduces heavy computational burden to modeling. Second, real-world trajectories often contain redundant points, which harm both computational efficiency and trajectory embedding quality. To address these challenges, we propose TrajMamba, a novel approach for efficient and semantically rich vehicle trajectory learning. TrajMamba introduces a Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS and road perspectives of trajectories, enabling robust representations of continuous travel behaviors. It also incorporates a Travel Purpose-aware Pre-training procedure to integrate travel purposes into the learned embeddings without introducing extra overhead to embedding calculation. To reduce redundancy in trajectories, TrajMamba features a Knowledge Distillation Pre-training scheme to identify key trajectory points through a learnable mask generator and obtain effective compressed trajectory embeddings. Extensive experiments on two real-world datasets and three downstream tasks show that TrajMamba outperforms state-of-the-art baselines in both efficiency and accuracy. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ECTxVRFhUa@OpenReview",
      "index": 460,
      "title": "Tensor Product Attention Is All You Need",
      "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Huizhuo Yuan",
        "Zhen Qin",
        "Yang Yuan",
        "Quanquan Gu",
        "Andrew C Yao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tpa",
        "attention",
        "tensor",
        "product",
        "language",
        "memory",
        "efficiency",
        "head",
        "mha",
        "mqa"
      ],
      "summary": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ECTxVRFhUa"
        ],
        "venue": [
          "/venue/ECTxVRFhUa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ECTxVRFhUa"
        ],
        "detail": [
          "https://openreview.net/forum?id=ECTxVRFhUa"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Tensor Product Attention Is All You Need [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Yifan Zhang , Yifeng Liu , Huizhuo Yuan , Zhen Qin , Yang Yuan , Quanquan Gu , Andrew C Yao Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wmweEDugTZ@OpenReview",
      "index": 461,
      "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
      "authors": [
        "Sheng Wang",
        "Pengan CHEN",
        "Jingqi Zhou",
        "Qintong Li",
        "Jingwei Dong",
        "Jiahui Gao",
        "Boyang XUE",
        "Jiyue Jiang",
        "Lingpeng Kong",
        "Chuan Wu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "treesynth",
        "subspace",
        "partitioning",
        "tree",
        "data",
        "datasets",
        "synthesis",
        "synthesizing",
        "diverse",
        "diversity"
      ],
      "summary": "Model customization necessitates high-quality and diverse datasets, but acquiring such data remains time-consuming and labor-intensive. Despite the great potential of large language models (LLMs) for data synthesis, current approaches are constrained by limited seed data, model biases and low-variation prompts, resulting in limited diversity and biased distribution with the increase of data scales. To tackle this challenge, we introduce TreeSynth, a tree-guided subspace-based data synthesis approach inspired by decision trees. It constructs a spatial partitioning tree to recursively divide a task-specific full data space (i.e., root node) into numerous atomic subspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes to ensure both distinctiveness and comprehensiveness, before synthesizing samples within each atomic subspace. This globally divide-and-synthesize method finally collects subspace samples into a comprehensive dataset, effectively circumventing repetition and space collapse to ensure the diversity of large-scale data synthesis. Furthermore, the spatial partitioning tree enables sample allocation into atomic subspaces, allowing the re-balancing of existing datasets for more balanced and comprehensive distributions. Empirically, extensive experiments across diverse benchmarks consistently validates the superior data diversity, model performance, and robust scalability of TreeSynth compared to both human-crafted datasets and peer data synthesis methods, with the average performance gain reaching 10%. Besides, the consistent improvements of TreeSynth-balanced datasets highlight its efficacious application to redistribute existing datasets for more comprehensive coverage and the induced performance enhancement. The code is available at https://github.com/cpa2001/TreeSynth.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wmweEDugTZ"
        ],
        "venue": [
          "/venue/wmweEDugTZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wmweEDugTZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=wmweEDugTZ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Sheng Wang , Pengan CHEN , Jingqi Zhou , Qintong Li , Jingwei Dong , Jiahui Gao , Boyang XUE , Jiyue Jiang , Lingpeng Kong , Chuan Wu Model customization necessitates high-quality and diverse datasets, but acquiring such data remains time-consuming and labor-intensive. Despite the great potential of large language models (LLMs) for data synthesis, current approaches are constrained by limited seed data, model biases and low-variation prompts, resulting in limited diversity and biased distribution with the increase of data scales. To tackle this challenge, we introduce TreeSynth, a tree-guided subspace-based data synthesis approach inspired by decision trees. It constructs a spatial partitioning tree to recursively divide a task-specific full data space (i.e., root node) into numerous atomic subspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes to ensure both distinctiveness and comprehensiveness, before synthesizing samples within each atomic subspace. This globally divide-and-synthesize method finally collects subspace samples into a comprehensive dataset, effectively circumventing repetition and space collapse to ensure the diversity of large-scale data synthesis. Furthermore, the spatial partitioning tree enables sample allocation into atomic subspaces, allowing the re-balancing of existing datasets for more balanced and comprehensive distributions. Empirically, extensive experiments across diverse benchmarks consistently validates the superior data diversity, model performance, and robust scalability of TreeSynth compared to both human-crafted datasets and peer data synthesis methods, with the average performance gain reaching 10%. Besides, the consistent improvements of TreeSynth-balanced datasets highlight its efficacious application to redistribute existing datasets for more comprehensive coverage and the induced performance enhancement. The code is available at https://github.com/cpa2001/TreeSynth. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "mOYGK7Hw9Y@OpenReview",
      "index": 462,
      "title": "DeCaFlow: A deconfounding causal generative model",
      "authors": [
        "Alejandro Almodóvar",
        "Adrián Javaloy",
        "Juan Parras",
        "Santiago Zazo",
        "Isabel Valera"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "decaflow",
        "causal",
        "deconfounding",
        "identifiable",
        "queries",
        "confounders",
        "hidden",
        "generative",
        "variables",
        "calculus"
      ],
      "summary": "We introduce DeCaFlow, a deconfounding causal generative model. Training once per dataset using just observational data and the underlying causal graph, DeCaFlow enables accurate causal inference on continuous variables under the presence of hidden confounders. Specifically, we extend previous results on causal estimation under hidden confounding to show that a single instance of DeCaFlow provides correct estimates for all causal queries identifiable with do-calculus, leveraging proxy variables to adjust for the causal effects when do-calculus alone is insufficient. Moreover, we show that counterfactual queries are identifiable as long as their interventional counterparts are identifiable, and thus are also correctly estimated by DeCaFlow. Our empirical results on diverse settings—including the Ecoli70 dataset, with 3 independent hidden confounders, tens of observed variables and hundreds of causal queries—show that DeCaFlow outperforms existing approaches, while demonstrating its out-of-the-box applicability to any given causal graph.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mOYGK7Hw9Y"
        ],
        "venue": [
          "/venue/mOYGK7Hw9Y@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mOYGK7Hw9Y"
        ],
        "detail": [
          "https://openreview.net/forum?id=mOYGK7Hw9Y"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "DeCaFlow: A deconfounding causal generative model [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Alejandro Almodóvar , Adrián Javaloy , Juan Parras , Santiago Zazo , Isabel Valera We introduce DeCaFlow, a deconfounding causal generative model. Training once per dataset using just observational data and the underlying causal graph, DeCaFlow enables accurate causal inference on continuous variables under the presence of hidden confounders. Specifically, we extend previous results on causal estimation under hidden confounding to show that a single instance of DeCaFlow provides correct estimates for all causal queries identifiable with do-calculus, leveraging proxy variables to adjust for the causal effects when do-calculus alone is insufficient. Moreover, we show that counterfactual queries are identifiable as long as their interventional counterparts are identifiable, and thus are also correctly estimated by DeCaFlow. Our empirical results on diverse settings—including the Ecoli70 dataset, with 3 independent hidden confounders, tens of observed variables and hundreds of causal queries—show that DeCaFlow outperforms existing approaches, while demonstrating its out-of-the-box applicability to any given causal graph. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "arXNS7T90z@OpenReview",
      "index": 463,
      "title": "Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression",
      "authors": [
        "Yeichan Kim",
        "Ilmun Kim",
        "Seyoung Park"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mni",
        "transfer",
        "benign",
        "overfitting",
        "mnis",
        "regression",
        "informative",
        "leveraging",
        "interpolator",
        "operationalize"
      ],
      "summary": "Transfer learning is a key component of modern machine learning, enhancing the performance of target tasks by leveraging diverse data sources. Simultaneously, overparameterized models such as the minimum- ℓ 2 ℓ 2 -norm interpolator (MNI) in high-dimensional linear regression have garnered significant attention for their remarkable generalization capabilities, a property known as *benign overfitting*. Despite their individual importance, the intersection of transfer learning and MNI remains largely unexplored. Our research bridges this gap by proposing a novel two-step Transfer MNI approach and analyzing its trade-offs. We characterize its non-asymptotic excess risk and identify conditions under which it outperforms the target-only MNI. Our analysis reveals *free-lunch* covariate shift regimes, where leveraging heterogeneous data yields the benefit of knowledge transfer at limited cost. To operationalize our findings, we develop a data-driven procedure to detect *informative* sources and introduce an ensemble method incorporating multiple informative Transfer MNIs. Finite-sample experiments demonstrate the robustness of our methods to model and data heterogeneity, confirming their advantage.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=arXNS7T90z"
        ],
        "venue": [
          "/venue/arXNS7T90z@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=arXNS7T90z"
        ],
        "detail": [
          "https://openreview.net/forum?id=arXNS7T90z"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yeichan Kim , Ilmun Kim , Seyoung Park Transfer learning is a key component of modern machine learning, enhancing the performance of target tasks by leveraging diverse data sources. Simultaneously, overparameterized models such as the minimum- ℓ 2 ℓ 2 -norm interpolator (MNI) in high-dimensional linear regression have garnered significant attention for their remarkable generalization capabilities, a property known as *benign overfitting*. Despite their individual importance, the intersection of transfer learning and MNI remains largely unexplored. Our research bridges this gap by proposing a novel two-step Transfer MNI approach and analyzing its trade-offs. We characterize its non-asymptotic excess risk and identify conditions under which it outperforms the target-only MNI. Our analysis reveals *free-lunch* covariate shift regimes, where leveraging heterogeneous data yields the benefit of knowledge transfer at limited cost. To operationalize our findings, we develop a data-driven procedure to detect *informative* sources and introduce an ensemble method incorporating multiple informative Transfer MNIs. Finite-sample experiments demonstrate the robustness of our methods to model and data heterogeneity, confirming their advantage. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SRDF3RV0KP@OpenReview",
      "index": 464,
      "title": "LLM Meeting Decision Trees on Tabular Data",
      "authors": [
        "Hangting Ye",
        "Jinmeng Li",
        "He Zhao",
        "Dandan Guo",
        "Yi Chang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tabular",
        "llm",
        "decision",
        "data",
        "llms",
        "serialization",
        "tree",
        "rule",
        "trees",
        "methods"
      ],
      "summary": "Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data through logical decision tree rules as intermediaries, proposing a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SRDF3RV0KP"
        ],
        "venue": [
          "/venue/SRDF3RV0KP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SRDF3RV0KP"
        ],
        "detail": [
          "https://openreview.net/forum?id=SRDF3RV0KP"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "LLM Meeting Decision Trees on Tabular Data [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Hangting Ye , Jinmeng Li , He Zhao , Dandan Guo , Yi Chang Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data through logical decision tree rules as intermediaries, proposing a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "mlU9KqdZUS@OpenReview",
      "index": 465,
      "title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement",
      "authors": [
        "J Rosser",
        "Jakob Nicolaus Foerster"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "scaffolds",
        "agentbreeder",
        "safety",
        "scaffolding",
        "agent",
        "mitigating",
        "risks",
        "multi",
        "self",
        "improving"
      ],
      "summary": "Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In \"blue\" mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In \"red\" mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at \\url{https://github.com/jrosseruk/AgentBreeder}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mlU9KqdZUS"
        ],
        "venue": [
          "/venue/mlU9KqdZUS@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mlU9KqdZUS"
        ],
        "detail": [
          "https://openreview.net/forum?id=mlU9KqdZUS"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : J Rosser , Jakob Nicolaus Foerster Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In \"blue\" mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In \"red\" mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at \\url{https://github.com/jrosseruk/AgentBreeder}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "zL4ifL17bU@OpenReview",
      "index": 466,
      "title": "Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing",
      "authors": [
        "Manchao Bao",
        "Shengjiang Fang",
        "Tao Yue",
        "Xuemei Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tof",
        "encodable",
        "depth",
        "burst",
        "flight",
        "imaging",
        "itof",
        "distance",
        "learnable",
        "sensing"
      ],
      "summary": "Long-distance depth imaging holds great promise for applications such as autonomous driving and robotics. Direct time-of-flight (dToF) imaging offers high-precision, long-distance depth sensing, yet demands ultra-short pulse light sources and high-resolution time-to-digital converters. In contrast, indirect time-of-flight (iToF) imaging often suffers from phase wrapping and low signal-to-noise ratio (SNR) as the sensing distance increases. In this paper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable Time-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth imaging. Specifically, the BE-ToF system emits light pulses in burst mode and estimates the phase delay of the reflected signal over the entire burst period, thereby effectively avoiding the phase wrapping inherent to conventional iToF systems. Moreover, to address the low SNR caused by light attenuation over increasing distances, we propose an end-to-end learnable framework that jointly optimizes the coding functions and the depth reconstruction network. A specialized double well function and first-order difference term are incorporated into the framework to ensure the hardware implementability of the coding functions. The proposed approach is rigorously validated through comprehensive simulations and real-world prototype experiments, demonstrating its effectiveness and practical applicability. The code is available at: https://github.com/ComputationalPerceptionLab/BE-ToF.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zL4ifL17bU"
        ],
        "venue": [
          "/venue/zL4ifL17bU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zL4ifL17bU"
        ],
        "detail": [
          "https://openreview.net/forum?id=zL4ifL17bU"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Manchao Bao , Shengjiang Fang , Tao Yue , Xuemei Hu Long-distance depth imaging holds great promise for applications such as autonomous driving and robotics. Direct time-of-flight (dToF) imaging offers high-precision, long-distance depth sensing, yet demands ultra-short pulse light sources and high-resolution time-to-digital converters. In contrast, indirect time-of-flight (iToF) imaging often suffers from phase wrapping and low signal-to-noise ratio (SNR) as the sensing distance increases. In this paper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable Time-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth imaging. Specifically, the BE-ToF system emits light pulses in burst mode and estimates the phase delay of the reflected signal over the entire burst period, thereby effectively avoiding the phase wrapping inherent to conventional iToF systems. Moreover, to address the low SNR caused by light attenuation over increasing distances, we propose an end-to-end learnable framework that jointly optimizes the coding functions and the depth reconstruction network. A specialized double well function and first-order difference term are incorporated into the framework to ensure the hardware implementability of the coding functions. The proposed approach is rigorously validated through comprehensive simulations and real-world prototype experiments, demonstrating its effectiveness and practical applicability. The code is available at: https://github.com/ComputationalPerceptionLab/BE-ToF. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7Z25QbOv4a@OpenReview",
      "index": 467,
      "title": "MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions",
      "authors": [
        "Pucheng Dang",
        "Di Huang",
        "Dong Li",
        "Kang Chen",
        "Yuanbo Wen",
        "Qi Guo",
        "Xing Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "kernel",
        "miggpt",
        "migration",
        "tree",
        "patches",
        "linux",
        "versions",
        "llms",
        "across",
        "patch"
      ],
      "summary": "Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkable progress across various domains, suggesting their potential for automating out-of-tree kernel patch migration. However, our findings reveal that LLMs, while promising, struggle with incomplete code context understanding and inaccurate migration point identification. In this work, we propose MigGPT, a framework that employs a novel code fingerprint structure to retain code snippet information and incorporates three meticulously designed modules to improve the migration accuracy and efficiency of out-of-tree kernel patches. Furthermore, we establish a robust benchmark using real-world out-of-tree kernel patch projects to evaluate LLM capabilities. Evaluations show that MigGPT significantly outperforms the direct application of vanilla LLMs, achieving an average completion rate of 72.59\\% ( ↑ 50.74 % ↑ 50.74 % ) for migration tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7Z25QbOv4a"
        ],
        "venue": [
          "/venue/7Z25QbOv4a@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7Z25QbOv4a"
        ],
        "detail": [
          "https://openreview.net/forum?id=7Z25QbOv4a"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Pucheng Dang , Di Huang , Dong Li , Kang Chen , Yuanbo Wen , Qi Guo , Xing Hu Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkable progress across various domains, suggesting their potential for automating out-of-tree kernel patch migration. However, our findings reveal that LLMs, while promising, struggle with incomplete code context understanding and inaccurate migration point identification. In this work, we propose MigGPT, a framework that employs a novel code fingerprint structure to retain code snippet information and incorporates three meticulously designed modules to improve the migration accuracy and efficiency of out-of-tree kernel patches. Furthermore, we establish a robust benchmark using real-world out-of-tree kernel patch projects to evaluate LLM capabilities. Evaluations show that MigGPT significantly outperforms the direct application of vanilla LLMs, achieving an average completion rate of 72.59\\% ( ↑ 50.74 % ↑ 50.74 % ) for migration tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kuzye4EPLR@OpenReview",
      "index": 468,
      "title": "FP4 All the Way: Fully Quantized Training of Large Language Models",
      "authors": [
        "Brian Chmiel",
        "Maxim Fishman",
        "Ron Banner",
        "Daniel Soudry"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fp4",
        "quantized",
        "training",
        "rounding",
        "e2m1",
        "gaudi2",
        "anonymous1252022",
        "billion",
        "e4m3",
        "fqt"
      ],
      "summary": "We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately 3 − − √ 3 times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kuzye4EPLR"
        ],
        "venue": [
          "/venue/kuzye4EPLR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kuzye4EPLR"
        ],
        "detail": [
          "https://openreview.net/forum?id=kuzye4EPLR"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "FP4 All the Way: Fully Quantized Training of Large Language Models [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Brian Chmiel , Maxim Fishman , Ron Banner , Daniel Soudry We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately 3 − − √ 3 times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZWOe1kkufx@OpenReview",
      "index": 469,
      "title": "Efficient Knowledge Transfer in Federated Recommendation for Joint Venture Ecosystem",
      "authors": [
        "Yichen Li",
        "Yijing Shan",
        "YI LIU",
        "Haozhao Wang",
        "Cheng Wang",
        "wangshi.ww",
        "Yi Wang",
        "Ruixuan Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "venture",
        "fedrs",
        "jve",
        "federated",
        "recommendation",
        "ecosystem",
        "user",
        "rating",
        "preference",
        "client"
      ],
      "summary": "The current Federated Recommendation System (FedRS) focuses on personalized recommendation services and assumes clients are personalized IoT devices (e.g., Mobile phones). In this paper, we deeply dive into new but practical FedRS applications within the joint venture ecosystem. Subsidiaries engage as participants with their users and items. However, in such a situation, merely exchanging item embedding is insufficient, as user bases always exhibit both overlaps and exclusive segments, demonstrating the complexity of user information. Meanwhile, directly uploading user information is a violation of privacy and unacceptable. To tackle the above challenges, we propose an efficient and privacy-enhanced federated recommendation for the joint venture ecosystem (FR-JVE) that each client transfers more common knowledge from other clients with a distilled user's \\textit{rating preference} from the local dataset. More specifically, we first transform the local data into a new format and apply model inversion techniques to distill the rating preference with frozen user gradients before the federated training. Then, a bridge function is employed on each client side to align the local rating preference and aggregated global preference in a privacy-friendly manner. Finally, each client matches similar users to make a better prediction for overlapped users. From a theoretical perspective, we analyze how effectively FR-JVE can guarantee user privacy. Empirically, we show that FR-JVE achieves superior performance compared to state-of-the-art methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZWOe1kkufx"
        ],
        "venue": [
          "/venue/ZWOe1kkufx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZWOe1kkufx"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZWOe1kkufx"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Efficient Knowledge Transfer in Federated Recommendation for Joint Venture Ecosystem [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yichen Li , Yijing Shan , YI LIU , Haozhao Wang , Cheng Wang , wangshi.ww , Yi Wang , Ruixuan Li The current Federated Recommendation System (FedRS) focuses on personalized recommendation services and assumes clients are personalized IoT devices (e.g., Mobile phones). In this paper, we deeply dive into new but practical FedRS applications within the joint venture ecosystem. Subsidiaries engage as participants with their users and items. However, in such a situation, merely exchanging item embedding is insufficient, as user bases always exhibit both overlaps and exclusive segments, demonstrating the complexity of user information. Meanwhile, directly uploading user information is a violation of privacy and unacceptable. To tackle the above challenges, we propose an efficient and privacy-enhanced federated recommendation for the joint venture ecosystem (FR-JVE) that each client transfers more common knowledge from other clients with a distilled user's \\textit{rating preference} from the local dataset. More specifically, we first transform the local data into a new format and apply model inversion techniques to distill the rating preference with frozen user gradients before the federated training. Then, a bridge function is employed on each client side to align the local rating preference and aggregated global preference in a privacy-friendly manner. Finally, each client matches similar users to make a better prediction for overlapped users. From a theoretical perspective, we analyze how effectively FR-JVE can guarantee user privacy. Empirically, we show that FR-JVE achieves superior performance compared to state-of-the-art methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4oYxzssbVg@OpenReview",
      "index": 470,
      "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning",
      "authors": [
        "Haozhe Wang",
        "Chao Qu",
        "Zuming Huang",
        "Wei Chu",
        "Fangzhen Lin",
        "Wenhu Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rethinker",
        "thinking",
        "mathvision",
        "mathverse",
        "reflection",
        "mathvista",
        "self",
        "gpt",
        "rethinking",
        "incentivizing"
      ],
      "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. We conduct comprehensive ablations and analysis to provide insights into the effectiveness of our approach.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4oYxzssbVg"
        ],
        "venue": [
          "/venue/4oYxzssbVg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4oYxzssbVg"
        ],
        "detail": [
          "https://openreview.net/forum?id=4oYxzssbVg"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Haozhe Wang , Chao Qu , Zuming Huang , Wei Chu , Fangzhen Lin , Wenhu Chen Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. We conduct comprehensive ablations and analysis to provide insights into the effectiveness of our approach. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3dnG7LcKxT@OpenReview",
      "index": 471,
      "title": "Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum",
      "authors": [
        "Snir Hordan",
        "Maya Bechler-Speicher",
        "Gur Lifshitz",
        "Nadav Dym"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sgnns",
        "graph",
        "graphs",
        "equiepnn",
        "expressive",
        "expressivity",
        "zinc",
        "incomplete",
        "spectral",
        "gnns"
      ],
      "summary": "Spectral features are widely incorporated within Graph Neural Networks (GNNs) to improve their expressive power, or their ability to distinguish among non-isomorphic graphs. One popular example is the usage of graph Laplacian eigenvectors for positional encoding in MPNNs and Graph Transformers. The expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated via the k k -WL graph isomorphism test hierarchy and homomorphism counting. Yet, these frameworks align poorly with the graph spectra, yielding limited insight into SGNNs' expressive power. In this paper, we leverage a well-studied paradigm of classifying graphs by their largest eigenvalue multiplicity to introduce an expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt rotation equivariant neural networks to the graph spectra setting, yielding equiEPNN, a novel SGNN that provably improves upon contemporary SGNNs' expressivity on simple spectrum graphs. We then demonstrate that equiEPNN achieves perfect eigenvector canonicalization on ZINC, and performs favorably on image classification on MNIST-Superpixel and graph property regression on ZINC, compared to leading spectral methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3dnG7LcKxT"
        ],
        "venue": [
          "/venue/3dnG7LcKxT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3dnG7LcKxT"
        ],
        "detail": [
          "https://openreview.net/forum?id=3dnG7LcKxT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Snir Hordan , Maya Bechler-Speicher , Gur Lifshitz , Nadav Dym Spectral features are widely incorporated within Graph Neural Networks (GNNs) to improve their expressive power, or their ability to distinguish among non-isomorphic graphs. One popular example is the usage of graph Laplacian eigenvectors for positional encoding in MPNNs and Graph Transformers. The expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated via the k k -WL graph isomorphism test hierarchy and homomorphism counting. Yet, these frameworks align poorly with the graph spectra, yielding limited insight into SGNNs' expressive power. In this paper, we leverage a well-studied paradigm of classifying graphs by their largest eigenvalue multiplicity to introduce an expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt rotation equivariant neural networks to the graph spectra setting, yielding equiEPNN, a novel SGNN that provably improves upon contemporary SGNNs' expressivity on simple spectrum graphs. We then demonstrate that equiEPNN achieves perfect eigenvector canonicalization on ZINC, and performs favorably on image classification on MNIST-Superpixel and graph property regression on ZINC, compared to leading spectral methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "muWdWcMvpW@OpenReview",
      "index": 472,
      "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding",
      "authors": [
        "Junliang Ye",
        "Zhengyi Wang",
        "Ruowen Zhao",
        "Shenghao Xie",
        "Jun Zhu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "native",
        "shapellm",
        "multimodal",
        "omni",
        "alpaca",
        "capabilities",
        "generation",
        "understanding",
        "text",
        "language"
      ],
      "summary": "Recently, the powerful text-to-image capabilities of GPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni—a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, we perform instruction-based fine-tuning of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset, equipping it with native 3D understanding and generation capabilities. Our work represents an effective step toward extending multimodal large language models with fundamental 3D intelligence, paving the way for future advances in 3D-native AI.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=muWdWcMvpW"
        ],
        "venue": [
          "/venue/muWdWcMvpW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=muWdWcMvpW"
        ],
        "detail": [
          "https://openreview.net/forum?id=muWdWcMvpW"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Junliang Ye , Zhengyi Wang , Ruowen Zhao , Shenghao Xie , Jun Zhu Recently, the powerful text-to-image capabilities of GPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni—a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, we perform instruction-based fine-tuning of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset, equipping it with native 3D understanding and generation capabilities. Our work represents an effective step toward extending multimodal large language models with fundamental 3D intelligence, paving the way for future advances in 3D-native AI. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3UaItHVjyE@OpenReview",
      "index": 473,
      "title": "The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels",
      "authors": [
        "Yonatan Slutzky",
        "Yotam Alexander",
        "Noam Razin",
        "Nadav Cohen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ssms",
        "implicit",
        "bias",
        "clean",
        "generalization",
        "poisoning",
        "training",
        "teacher",
        "poisoned",
        "structured"
      ],
      "summary": "Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs). Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e., having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, we believe that delineating their susceptibility to clean-label poisoning, and developing methods for overcoming this susceptibility, are critical research directions to pursue.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3UaItHVjyE"
        ],
        "venue": [
          "/venue/3UaItHVjyE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3UaItHVjyE"
        ],
        "detail": [
          "https://openreview.net/forum?id=3UaItHVjyE"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yonatan Slutzky , Yotam Alexander , Noam Razin , Nadav Cohen Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs). Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e., having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, we believe that delineating their susceptibility to clean-label poisoning, and developing methods for overcoming this susceptibility, are critical research directions to pursue. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "r9YDEErKXU@OpenReview",
      "index": 474,
      "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation",
      "authors": [
        "Xinyu Yang",
        "Yuwei An",
        "Hongyi Liu",
        "Tianqi Chen",
        "Beidi Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "multiverse",
        "parallel",
        "32b",
        "llms",
        "generation",
        "secretly",
        "sequential",
        "stage",
        "reasoning",
        "aime24"
      ],
      "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, we introduce Multiverse, a new generative model enabling natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, we build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. For data creation, we develop Multiverse Curator, an automated LLM-assisted pipeline that transforms sequential reasoning chains into structured training data, avoiding costly human annotations. Algorithmically, we design Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, we implement Multiverse Engine to support parallel inference. It features a dedicated interpreter that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, our Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, our budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. We have open-sourced the entire Multiverse ecosystem, including data, model weights, serving system, supporting tools, as well as data curation prompts and detailed training and evaluation recipes.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=r9YDEErKXU"
        ],
        "venue": [
          "/venue/r9YDEErKXU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=r9YDEErKXU"
        ],
        "detail": [
          "https://openreview.net/forum?id=r9YDEErKXU"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xinyu Yang , Yuwei An , Hongyi Liu , Tianqi Chen , Beidi Chen Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, we introduce Multiverse, a new generative model enabling natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, we build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. For data creation, we develop Multiverse Curator, an automated LLM-assisted pipeline that transforms sequential reasoning chains into structured training data, avoiding costly human annotations. Algorithmically, we design Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, we implement Multiverse Engine to support parallel inference. It features a dedicated interpreter that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, our Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, our budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. We have open-sourced the entire Multiverse ecosystem, including data, model weights, serving system, supporting tools, as well as data curation prompts and detailed training and evaluation recipes. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Ig5mtZ8etr@OpenReview",
      "index": 475,
      "title": "BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization",
      "authors": [
        "Qiwei Wang",
        "Wu Shaoxun",
        "Yujiao Shi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bevsplat",
        "height",
        "ambiguity",
        "view",
        "ground",
        "cross",
        "bev",
        "primitives",
        "localization",
        "pose"
      ],
      "summary": "This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird’s-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Ig5mtZ8etr"
        ],
        "venue": [
          "/venue/Ig5mtZ8etr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Ig5mtZ8etr"
        ],
        "detail": [
          "https://openreview.net/forum?id=Ig5mtZ8etr"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Qiwei Wang , Wu Shaoxun , Yujiao Shi This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird’s-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wHm5J9uanV@OpenReview",
      "index": 476,
      "title": "Conflict-Aware Knowledge Editing in the Wild: Semantic-Augmented Graph Representation for Unstructured Text",
      "authors": [
        "Zhange Zhang",
        "Zhicheng Geng",
        "Yuqing Ma",
        "Tianbo Wang",
        "Kai Lv",
        "Xianglong Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "editing",
        "knowledge",
        "unstructured",
        "conflict",
        "cake",
        "wild",
        "aware",
        "semantic",
        "graph",
        "representation"
      ],
      "summary": "Large Language Models (LLMs) have demonstrated broad applications but suffer from issues like hallucinations, erroneous outputs and outdated knowledge. Model editing emerges as an effective solution to refine knowledge in LLMs, yet existing methods typically depend on structured knowledge representations. However, real-world knowledge is primarily embedded within complex, unstructured text. Existing structured knowledge editing approaches face significant challenges when handling the entangled and intricate knowledge present in unstructured text, resulting in issues such as representation ambiguity and editing conflicts. To address these challenges, we propose a Conflict-Aware Knowledge Editing in the Wild (CAKE) framework, the first framework explicitly designed for editing knowledge extracted from wild unstructured text. CAKE comprises two core components: a Semantic-augmented Graph Representation module and a Conflict-aware Knowledge Editing strategy. The Semantic-augmented Graph Representation module enhances knowledge encoding through structural disambiguation, relational enrichment, and semantic diversification. Meanwhile, the Conflict-aware Knowledge Editing strategy utilizes a graph-theoretic coloring algorithm to disentangle conflicted edits by allocating them to orthogonal parameter subspaces, thereby effectively mitigating editing conflicts. Experimental results on the AKEW benchmark demonstrate that CAKE significantly outperforms existing methods, achieving a 15.43\\% improvement in accuracy on llama3 editing tasks. Our framework successfully bridges the gap between unstructured textual knowledge and reliable model editing, enabling more robust and scalable updates for practical LLM applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wHm5J9uanV"
        ],
        "venue": [
          "/venue/wHm5J9uanV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wHm5J9uanV"
        ],
        "detail": [
          "https://openreview.net/forum?id=wHm5J9uanV"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Conflict-Aware Knowledge Editing in the Wild: Semantic-Augmented Graph Representation for Unstructured Text [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Zhange Zhang , Zhicheng Geng , Yuqing Ma , Tianbo Wang , Kai Lv , Xianglong Liu Large Language Models (LLMs) have demonstrated broad applications but suffer from issues like hallucinations, erroneous outputs and outdated knowledge. Model editing emerges as an effective solution to refine knowledge in LLMs, yet existing methods typically depend on structured knowledge representations. However, real-world knowledge is primarily embedded within complex, unstructured text. Existing structured knowledge editing approaches face significant challenges when handling the entangled and intricate knowledge present in unstructured text, resulting in issues such as representation ambiguity and editing conflicts. To address these challenges, we propose a Conflict-Aware Knowledge Editing in the Wild (CAKE) framework, the first framework explicitly designed for editing knowledge extracted from wild unstructured text. CAKE comprises two core components: a Semantic-augmented Graph Representation module and a Conflict-aware Knowledge Editing strategy. The Semantic-augmented Graph Representation module enhances knowledge encoding through structural disambiguation, relational enrichment, and semantic diversification. Meanwhile, the Conflict-aware Knowledge Editing strategy utilizes a graph-theoretic coloring algorithm to disentangle conflicted edits by allocating them to orthogonal parameter subspaces, thereby effectively mitigating editing conflicts. Experimental results on the AKEW benchmark demonstrate that CAKE significantly outperforms existing methods, achieving a 15.43\\% improvement in accuracy on llama3 editing tasks. Our framework successfully bridges the gap between unstructured textual knowledge and reliable model editing, enabling more robust and scalable updates for practical LLM applications. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VsDsRqaMJv@OpenReview",
      "index": 477,
      "title": "Vision Transformers with Self-Distilled Registers",
      "authors": [
        "Zipeng Yan",
        "Yinjie Chen",
        "Chong Zhou",
        "Bo Dai",
        "Andrew Luo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tokens",
        "vits",
        "registers",
        "vit",
        "artifact",
        "register",
        "student",
        "teacher",
        "reg",
        "transformers"
      ],
      "summary": "Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is the addition of register tokens to ViTs, which implicitly ''absorb'' the artifact term during training. Given the availability of existing large-scale pre-trained ViTs, in this paper we seek add register tokens to existing models without needing to re-train from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (**PH-Reg**), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher’s inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VsDsRqaMJv"
        ],
        "venue": [
          "/venue/VsDsRqaMJv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VsDsRqaMJv"
        ],
        "detail": [
          "https://openreview.net/forum?id=VsDsRqaMJv"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Vision Transformers with Self-Distilled Registers [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Zipeng Yan , Yinjie Chen , Chong Zhou , Bo Dai , Andrew Luo Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is the addition of register tokens to ViTs, which implicitly ''absorb'' the artifact term during training. Given the availability of existing large-scale pre-trained ViTs, in this paper we seek add register tokens to existing models without needing to re-train from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (**PH-Reg**), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher’s inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "igB289kbej@OpenReview",
      "index": 478,
      "title": "EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment",
      "authors": [
        "Naga Sai Abhiram kusumba",
        "Maitreya Patel",
        "Kyle Min",
        "Changhoon Kim",
        "Chitta Baral",
        "Yezhou Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "eraseflow",
        "erasure",
        "gflownet",
        "concepts",
        "concept",
        "denoising",
        "hackable",
        "trajectories",
        "gflownets",
        "steers"
      ],
      "summary": "Erasing harmful or proprietary concepts from powerful text‑to‑image generators is an emerging safety requirement, yet current ``concept erasure'' techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion‑based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with a GFlowNets equipped with the trajectory‑balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model’s prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade-off between performance and prior preservation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=igB289kbej"
        ],
        "venue": [
          "/venue/igB289kbej@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=igB289kbej"
        ],
        "detail": [
          "https://openreview.net/forum?id=igB289kbej"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Naga Sai Abhiram kusumba , Maitreya Patel , Kyle Min , Changhoon Kim , Chitta Baral , Yezhou Yang Erasing harmful or proprietary concepts from powerful text‑to‑image generators is an emerging safety requirement, yet current ``concept erasure'' techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion‑based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with a GFlowNets equipped with the trajectory‑balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model’s prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade-off between performance and prior preservation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "PluDA8DEar@OpenReview",
      "index": 479,
      "title": "Boundary-Value PDEs Meet Higher-Order Differential Topology-aware GNNs",
      "authors": [
        "Yunfeng Liao",
        "Yangxin Wu",
        "Xiucheng Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "differential",
        "bvps",
        "pdes",
        "gnn",
        "higher",
        "order",
        "electromagnetism",
        "topology",
        "aware",
        "gnns"
      ],
      "summary": "Recent advances in graph neural network (GNN)-based neural operators have demonstrated significant progress in solving partial differential equations (PDEs) by effectively representing computational meshes. However, most existing approaches overlook the intrinsic physical and topological meaning of higher-order elements in the mesh, which are closely tied to differential forms. In this paper, we propose a higher-order GNN framework that incorporates higher-order interactions based on discrete and finite element exterior calculus. The time-independent boundary value problems (BVPs) in electromagnetism are instantiated to illustrate the proposed framework. It can be easily generalized to other PDEs that admit differential form formulations. Moreover, the novel physics-informed loss terms, integrated form estimators, and theoretical support are derived correspondingly. Experiments show that our proposed method outperforms the existing neural operators by large margins on BVPs in electromagnetism. Our code is available at https://github.com/Supradax/Higher-Order-Differential-Topology-aware-GNN.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PluDA8DEar"
        ],
        "venue": [
          "/venue/PluDA8DEar@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PluDA8DEar"
        ],
        "detail": [
          "https://openreview.net/forum?id=PluDA8DEar"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Boundary-Value PDEs Meet Higher-Order Differential Topology-aware GNNs [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Yunfeng Liao , Yangxin Wu , Xiucheng Li Recent advances in graph neural network (GNN)-based neural operators have demonstrated significant progress in solving partial differential equations (PDEs) by effectively representing computational meshes. However, most existing approaches overlook the intrinsic physical and topological meaning of higher-order elements in the mesh, which are closely tied to differential forms. In this paper, we propose a higher-order GNN framework that incorporates higher-order interactions based on discrete and finite element exterior calculus. The time-independent boundary value problems (BVPs) in electromagnetism are instantiated to illustrate the proposed framework. It can be easily generalized to other PDEs that admit differential form formulations. Moreover, the novel physics-informed loss terms, integrated form estimators, and theoretical support are derived correspondingly. Experiments show that our proposed method outperforms the existing neural operators by large margins on BVPs in electromagnetism. Our code is available at https://github.com/Supradax/Higher-Order-Differential-Topology-aware-GNN. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qMRFNxioPC@OpenReview",
      "index": 480,
      "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos",
      "authors": [
        "Zhen Xu",
        "Zhengqin Li",
        "Zhao Dong",
        "Xiaowei Zhou",
        "Richard Newcombe",
        "Zhaoyang Lv"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "4dgt",
        "videos",
        "monocular",
        "gaussian",
        "posed",
        "world",
        "transformer",
        "lifespans",
        "real",
        "scene"
      ],
      "summary": "We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene reconstruction, trained entirely on real-world monocular posed videos. Using 4D Gaussian as an inductive bias, 4DGT unifies static and dynamic components, enabling the modeling of complex, time-varying environments with varying object lifespans. We proposed a novel density control strategy in training, which enables our 4DGT to handle longer space-time input. Our model processes 64 consecutive posed frames in a rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike optimization-based methods, 4DGT performs purely feed-forward inference, reducing reconstruction time from hours to seconds and scaling effectively to long video sequences. Trained only on large-scale monocular posed video datasets, 4DGT can outperform prior Gaussian-based networks significantly in real-world videos and achieve on-par accuracy with optimization-based methods on cross-domain videos.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qMRFNxioPC"
        ],
        "venue": [
          "/venue/qMRFNxioPC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qMRFNxioPC"
        ],
        "detail": [
          "https://openreview.net/forum?id=qMRFNxioPC"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Zhen Xu , Zhengqin Li , Zhao Dong , Xiaowei Zhou , Richard Newcombe , Zhaoyang Lv We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene reconstruction, trained entirely on real-world monocular posed videos. Using 4D Gaussian as an inductive bias, 4DGT unifies static and dynamic components, enabling the modeling of complex, time-varying environments with varying object lifespans. We proposed a novel density control strategy in training, which enables our 4DGT to handle longer space-time input. Our model processes 64 consecutive posed frames in a rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike optimization-based methods, 4DGT performs purely feed-forward inference, reducing reconstruction time from hours to seconds and scaling effectively to long video sequences. Trained only on large-scale monocular posed video datasets, 4DGT can outperform prior Gaussian-based networks significantly in real-world videos and achieve on-par accuracy with optimization-based methods on cross-domain videos. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "wPdBe9zxNr@OpenReview",
      "index": 481,
      "title": "CURE: Co-Evolving Coders and Unit Testers via Reinforcement Learning",
      "authors": [
        "Yinjie Wang",
        "Ling Yang",
        "Ye Tian",
        "Ke Shen",
        "Mengdi Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "unit",
        "cure",
        "test",
        "generation",
        "reinforcement",
        "coding",
        "coders",
        "base",
        "testers",
        "truth"
      ],
      "summary": "Mathematical reasoning in large language models has been successfully incentivized through reinforcement learning with verifiable rewards, leading to improved one-shot precision. In this work, we turn our focus to the coding domain. Beyond one-shot precision, we highlight unit test generation as another key factor for enhancing coding ability, since accurate unit tests are essential for enabling self-checking and self-correction during inference. Traditional approaches for fine-tuning LLMs on unit test generation rely heavily on ground-truth code solutions in the training data. We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes—without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder’s mistakes. Through extensive evaluations, we demonstrate that our CURE models, derived from base models of varying sizes, excel in both code generation and unit test generation. They naturally extend to downstream tasks such as test-time scaling—achieving a 6.2\\% improvement over the base model—and agentic unit test generation, with a 25.1\\% improvement. Our 4B model consistently outperforms Qwen3-4B while achieving 64.8\\% inference efficiency in unit test generation. Notably, we also find that the CURE model can serve as an effective reward model for reinforcement learning on base models, even in the absence of any labeled supervision.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wPdBe9zxNr"
        ],
        "venue": [
          "/venue/wPdBe9zxNr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wPdBe9zxNr"
        ],
        "detail": [
          "https://openreview.net/forum?id=wPdBe9zxNr"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "CURE: Co-Evolving Coders and Unit Testers via Reinforcement Learning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yinjie Wang , Ling Yang , Ye Tian , Ke Shen , Mengdi Wang Mathematical reasoning in large language models has been successfully incentivized through reinforcement learning with verifiable rewards, leading to improved one-shot precision. In this work, we turn our focus to the coding domain. Beyond one-shot precision, we highlight unit test generation as another key factor for enhancing coding ability, since accurate unit tests are essential for enabling self-checking and self-correction during inference. Traditional approaches for fine-tuning LLMs on unit test generation rely heavily on ground-truth code solutions in the training data. We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes—without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder’s mistakes. Through extensive evaluations, we demonstrate that our CURE models, derived from base models of varying sizes, excel in both code generation and unit test generation. They naturally extend to downstream tasks such as test-time scaling—achieving a 6.2\\% improvement over the base model—and agentic unit test generation, with a 25.1\\% improvement. Our 4B model consistently outperforms Qwen3-4B while achieving 64.8\\% inference efficiency in unit test generation. Notably, we also find that the CURE model can serve as an effective reward model for reinforcement learning on base models, even in the absence of any labeled supervision. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IIiRwgkZcm@OpenReview",
      "index": 482,
      "title": "How many measurements are enough? Bayesian recovery in inverse problems with general distributions",
      "authors": [
        "Ben Adcock",
        "Nick Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "recovery",
        "bayesian",
        "complexity",
        "sample",
        "mathcal",
        "distributions",
        "inverse",
        "prior",
        "priors",
        "dnn"
      ],
      "summary": "We study the sample complexity of Bayesian recovery for solving inverse problems with general prior, forward operator and noise distributions. We consider posterior sampling according to an approximate prior P P , and establish sufficient conditions for stable and accurate recovery with high probability. Our main result is a non-asymptotic bound that shows that the sample complexity depends on (i) the intrinsic complexity of P P , quantified by its *approximate covering number*, and (ii) concentration bounds for the forward operator and noise distributions. As a key application, we specialize to generative priors, where P P is the pushforward of a latent distribution via a Deep Neural Network (DNN). We show that the sample complexity scales log-linearly with the latent dimension k k , thus establishing the efficacy of DNN-based priors. Generalizing existing results on deterministic (i.e., non-Bayesian) recovery for the important problem of random sampling with an orthogonal matrix U U , we show how the sample complexity is determined by the *coherence* of U U with respect to the support of P P . Hence, we establish that coherence plays a fundamental role in Bayesian recovery as well. Overall, our framework unifies and extends prior work, providing rigorous guarantees for the sample complexity of solving Bayesian inverse problems with arbitrary distributions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IIiRwgkZcm"
        ],
        "venue": [
          "/venue/IIiRwgkZcm@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IIiRwgkZcm"
        ],
        "detail": [
          "https://openreview.net/forum?id=IIiRwgkZcm"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "How many measurements are enough? Bayesian recovery in inverse problems with general distributions [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ben Adcock , Nick Huang We study the sample complexity of Bayesian recovery for solving inverse problems with general prior, forward operator and noise distributions. We consider posterior sampling according to an approximate prior P P , and establish sufficient conditions for stable and accurate recovery with high probability. Our main result is a non-asymptotic bound that shows that the sample complexity depends on (i) the intrinsic complexity of P P , quantified by its *approximate covering number*, and (ii) concentration bounds for the forward operator and noise distributions. As a key application, we specialize to generative priors, where P P is the pushforward of a latent distribution via a Deep Neural Network (DNN). We show that the sample complexity scales log-linearly with the latent dimension k k , thus establishing the efficacy of DNN-based priors. Generalizing existing results on deterministic (i.e., non-Bayesian) recovery for the important problem of random sampling with an orthogonal matrix U U , we show how the sample complexity is determined by the *coherence* of U U with respect to the support of P P . Hence, we establish that coherence plays a fundamental role in Bayesian recovery as well. Overall, our framework unifies and extends prior work, providing rigorous guarantees for the sample complexity of solving Bayesian inverse problems with arbitrary distributions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Jom8tNYuQI@OpenReview",
      "index": 483,
      "title": "Diffusion Generative Modeling on Lie Group Representations",
      "authors": [
        "Marco Bertolini",
        "Tuan Le",
        "Djork-Arné Clevert"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lie",
        "group",
        "modeling",
        "score",
        "diffusion",
        "generative",
        "representations",
        "matching",
        "class",
        "conformer"
      ],
      "summary": "We introduce a novel class of score-based diffusion processes that operate directly in the representation space of Lie groups. Leveraging the framework of Generalized Score Matching, we derive a class of Langevin dynamics that decomposes as a direct sum of Lie algebra representations, enabling the modeling of any target distribution on any (non-Abelian) Lie group. Standard score-matching emerges as a special case of our framework when the Lie group is the translation group. We prove that our generalized generative processes arise as solutions to a new class of paired stochastic differential equations (SDEs), introduced here for the first time. We validate our approach through experiments on diverse data types, demonstrating its effectiveness in real-world applications such as SO ( 3 ) SO ( 3 ) -guided molecular conformer generation and modeling ligand-specific global SE ( 3 ) SE ( 3 ) transformations for molecular docking, showing improvement in comparison to Riemannian diffusion on the group itself. We show that an appropriate choice of Lie group enhances learning efficiency by reducing the effective dimensionality of the trajectory space and enables the modeling of transitions between complex data distributions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Jom8tNYuQI"
        ],
        "venue": [
          "/venue/Jom8tNYuQI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Jom8tNYuQI"
        ],
        "detail": [
          "https://openreview.net/forum?id=Jom8tNYuQI"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Diffusion Generative Modeling on Lie Group Representations [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Marco Bertolini , Tuan Le , Djork-Arné Clevert We introduce a novel class of score-based diffusion processes that operate directly in the representation space of Lie groups. Leveraging the framework of Generalized Score Matching, we derive a class of Langevin dynamics that decomposes as a direct sum of Lie algebra representations, enabling the modeling of any target distribution on any (non-Abelian) Lie group. Standard score-matching emerges as a special case of our framework when the Lie group is the translation group. We prove that our generalized generative processes arise as solutions to a new class of paired stochastic differential equations (SDEs), introduced here for the first time. We validate our approach through experiments on diverse data types, demonstrating its effectiveness in real-world applications such as SO ( 3 ) SO ( 3 ) -guided molecular conformer generation and modeling ligand-specific global SE ( 3 ) SE ( 3 ) transformations for molecular docking, showing improvement in comparison to Riemannian diffusion on the group itself. We show that an appropriate choice of Lie group enhances learning efficiency by reducing the effective dimensionality of the trajectory space and enables the modeling of transitions between complex data distributions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "25C8oC1pb2@OpenReview",
      "index": 484,
      "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis",
      "authors": [
        "Yancheng Zhang",
        "Guangyu Sun",
        "Chen Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "splatting",
        "eggs",
        "gaussian",
        "exchangeable",
        "nvs",
        "view",
        "rendering",
        "appearance",
        "gaussians",
        "2dgs"
      ],
      "summary": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=25C8oC1pb2"
        ],
        "venue": [
          "/venue/25C8oC1pb2@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=25C8oC1pb2"
        ],
        "detail": [
          "https://openreview.net/forum?id=25C8oC1pb2"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Yancheng Zhang , Guangyu Sun , Chen Chen Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "zprMrpiLgT@OpenReview",
      "index": 485,
      "title": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models",
      "authors": [
        "Shristi Das Biswas",
        "Arani Roy",
        "Kaushik Roy"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cure",
        "unlearning",
        "concept",
        "eraser",
        "undesired",
        "concepts",
        "damage",
        "unrelated",
        "orthogonal",
        "filtering"
      ],
      "summary": "As Text-to-Image models continue to evolve, so does the risk of generating unsafe, copyrighted, or privacy-violating content. Existing safety interventions - ranging from training data curation and model fine-tuning to inference-time filtering and guidance - often suffer from incomplete concept removal, susceptibility to jail-breaking, computational inefficiency, or collateral damage to unrelated capabilities. In this paper, we introduce CURE, a training-free concept unlearning framework that operates directly in the weight space of pre-trained diffusion models, enabling fast, interpretable, and highly specific suppression of undesired concepts. At the core of our method is the Spectral Eraser, a closed-form, orthogonal projection module that identifies discriminative subspaces using Singular Value Decomposition over token embeddings associated with the concepts to forget and retain. Intuitively, the Spectral Eraser identifies and isolates features unique to the undesired concept while preserving safe attributes. This operator is then applied in a single step update to yield an edited model in which the target concept is effectively unlearned - without retraining, supervision, or iterative optimization. To balance the trade-off between filtering toxicity and preserving unrelated concepts, we further introduce an Expansion Mechanism for spectral regularization which selectively modulates singular vectors based on their relative significance to control the strength of forgetting. All the processes above are in closed-form, guaranteeing extremely efficient erasure in only 2 2 seconds. Benchmarking against prior approaches, CURE achieves a more efficient and thorough removal for targeted artistic styles, objects, identities, or explicit content, with minor damage to original generation ability and demonstrates enhanced robustness against red-teaming. Project Page at \\url{https://sites.google.com/view/cure-unlearning/home}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zprMrpiLgT"
        ],
        "venue": [
          "/venue/zprMrpiLgT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zprMrpiLgT"
        ],
        "detail": [
          "https://openreview.net/forum?id=zprMrpiLgT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Shristi Das Biswas , Arani Roy , Kaushik Roy As Text-to-Image models continue to evolve, so does the risk of generating unsafe, copyrighted, or privacy-violating content. Existing safety interventions - ranging from training data curation and model fine-tuning to inference-time filtering and guidance - often suffer from incomplete concept removal, susceptibility to jail-breaking, computational inefficiency, or collateral damage to unrelated capabilities. In this paper, we introduce CURE, a training-free concept unlearning framework that operates directly in the weight space of pre-trained diffusion models, enabling fast, interpretable, and highly specific suppression of undesired concepts. At the core of our method is the Spectral Eraser, a closed-form, orthogonal projection module that identifies discriminative subspaces using Singular Value Decomposition over token embeddings associated with the concepts to forget and retain. Intuitively, the Spectral Eraser identifies and isolates features unique to the undesired concept while preserving safe attributes. This operator is then applied in a single step update to yield an edited model in which the target concept is effectively unlearned - without retraining, supervision, or iterative optimization. To balance the trade-off between filtering toxicity and preserving unrelated concepts, we further introduce an Expansion Mechanism for spectral regularization which selectively modulates singular vectors based on their relative significance to control the strength of forgetting. All the processes above are in closed-form, guaranteeing extremely efficient erasure in only 2 2 seconds. Benchmarking against prior approaches, CURE achieves a more efficient and thorough removal for targeted artistic styles, objects, identities, or explicit content, with minor damage to original generation ability and demonstrates enhanced robustness against red-teaming. Project Page at \\url{https://sites.google.com/view/cure-unlearning/home}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "alw3e1Qa7I@OpenReview",
      "index": 486,
      "title": "Projection-based Lyapunov method for fully heterogeneous weakly-coupled MDPs",
      "authors": [
        "XiangCheng Zhang",
        "Yige Hong",
        "Weina Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "wcmdps",
        "heterogeneous",
        "fully",
        "lyapunov",
        "wcmdp",
        "projection",
        "weakly",
        "mdps",
        "heterogeneity",
        "optimality"
      ],
      "summary": "Heterogeneity poses a fundamental challenge for many real-world large-scale decision-making problems but remains largely understudied. In this paper, we study the _fully heterogeneous_ setting of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs). Each WCMDP consists of N N arms (or subproblems), which have distinct model parameters in the fully heterogeneous setting, leading to the curse of dimensionality when N N is large. We show that, under mild assumptions, an efficiently computable policy achieves an O ( 1 / N − − √ ) O ( 1 / N ) optimality gap in the long-run average reward per arm for fully heterogeneous WCMDPs as N N becomes large. This is the _first asymptotic optimality result_ for fully heterogeneous average-reward WCMDPs. Our main technical innovation is the construction of projection-based Lyapunov functions that certify the convergence of rewards and costs to an optimal region, even under full heterogeneity.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=alw3e1Qa7I"
        ],
        "venue": [
          "/venue/alw3e1Qa7I@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=alw3e1Qa7I"
        ],
        "detail": [
          "https://openreview.net/forum?id=alw3e1Qa7I"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Projection-based Lyapunov method for fully heterogeneous weakly-coupled MDPs [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : XiangCheng Zhang , Yige Hong , Weina Wang Heterogeneity poses a fundamental challenge for many real-world large-scale decision-making problems but remains largely understudied. In this paper, we study the _fully heterogeneous_ setting of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs). Each WCMDP consists of N N arms (or subproblems), which have distinct model parameters in the fully heterogeneous setting, leading to the curse of dimensionality when N N is large. We show that, under mild assumptions, an efficiently computable policy achieves an O ( 1 / N − − √ ) O ( 1 / N ) optimality gap in the long-run average reward per arm for fully heterogeneous WCMDPs as N N becomes large. This is the _first asymptotic optimality result_ for fully heterogeneous average-reward WCMDPs. Our main technical innovation is the construction of projection-based Lyapunov functions that certify the convergence of rewards and costs to an optimal region, even under full heterogeneity. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fBNaGVMDD9@OpenReview",
      "index": 487,
      "title": "Low-degree evidence for computational transition of recovery rate in stochastic block model",
      "authors": [
        "Jingqiu Ding",
        "Yiding Hua",
        "Lucas Slot",
        "David Steurer"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "block",
        "hopkins",
        "degree",
        "bandeira",
        "stochastic",
        "recovery",
        "conjecture",
        "polynomial",
        "probability",
        "low"
      ],
      "summary": "We investigate implications of the (extended) low-degree conjecture (recently formalized in [moitra et al2023]) in the context of the symmetric stochastic block model. Assuming the conjecture holds, we establish that no polynomial-time algorithm can weakly recover community labels below the Kesten-Stigum (KS) threshold. In particular, we rule out polynomial-time estimators that, with constant probability, achieve n − 0.49 n − 0.49 correlation with the true communities. Whereas, above the KS threshold, polynomial-time algorithms are known to achieve constant correlation with the true communities with high probability [massoulie et al 2014,abbe et al 2015]. To our knowledge, we provide the first rigorous evidence for such sharp transition in recovery rate for polynomial-time algorithms at the KS threshold. Notably, under a stronger version of the low-degree conjecture, our lower bound remains valid even when the number of blocks diverges. Furthermore, our results provide evidence of a computational-to-statistical gap in learning the parameters of stochastic block models. In contrast, prior work either (i) rules out polynomial-time algorithms with 1 − o ( 1 ) 1 − o ( 1 ) success probability [Hopkins 18, bandeira et al 2021] under the low-degree conjecture, or (ii) degree- poly ( k ) poly ( k ) polynomials for learning the stochastic block model [Luo et al 2023]. For this, we design a hypothesis test which succeeeds with constant probability under symmetric stochastic block model, and 1 − o ( 1 ) 1 − o ( 1 ) probability under the distribution of \\Erdos \\Renyi random graphs. Our proof combines low-degree lower bounds from [Hopkins 18, bandeira et al 2021] with graph splitting and cross-validation techniques. In order to rule out general recovery algorithms, we employ the correlation preserving projection method developed in [Hopkins et al 17].",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fBNaGVMDD9"
        ],
        "venue": [
          "/venue/fBNaGVMDD9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fBNaGVMDD9"
        ],
        "detail": [
          "https://openreview.net/forum?id=fBNaGVMDD9"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Low-degree evidence for computational transition of recovery rate in stochastic block model [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jingqiu Ding , Yiding Hua , Lucas Slot , David Steurer We investigate implications of the (extended) low-degree conjecture (recently formalized in [moitra et al2023]) in the context of the symmetric stochastic block model. Assuming the conjecture holds, we establish that no polynomial-time algorithm can weakly recover community labels below the Kesten-Stigum (KS) threshold. In particular, we rule out polynomial-time estimators that, with constant probability, achieve n − 0.49 n − 0.49 correlation with the true communities. Whereas, above the KS threshold, polynomial-time algorithms are known to achieve constant correlation with the true communities with high probability [massoulie et al 2014,abbe et al 2015]. To our knowledge, we provide the first rigorous evidence for such sharp transition in recovery rate for polynomial-time algorithms at the KS threshold. Notably, under a stronger version of the low-degree conjecture, our lower bound remains valid even when the number of blocks diverges. Furthermore, our results provide evidence of a computational-to-statistical gap in learning the parameters of stochastic block models. In contrast, prior work either (i) rules out polynomial-time algorithms with 1 − o ( 1 ) 1 − o ( 1 ) success probability [Hopkins 18, bandeira et al 2021] under the low-degree conjecture, or (ii) degree- poly ( k ) poly ( k ) polynomials for learning the stochastic block model [Luo et al 2023]. For this, we design a hypothesis test which succeeeds with constant probability under symmetric stochastic block model, and 1 − o ( 1 ) 1 − o ( 1 ) probability under the distribution of \\Erdos \\Renyi random graphs. Our proof combines low-degree lower bounds from [Hopkins 18, bandeira et al 2021] with graph splitting and cross-validation techniques. In order to rule out general recovery algorithms, we employ the correlation preserving projection method developed in [Hopkins et al 17]. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hLZrqDFugY@OpenReview",
      "index": 488,
      "title": "Nonlinear Laplacians: Tunable principal component analysis under directional prior information",
      "authors": [
        "Yuxin Ma",
        "Dmitriy Kunisky"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sigma",
        "mathbf",
        "algorithms",
        "signal",
        "nonlinear",
        "matrix",
        "laplacian",
        "spectral",
        "strength",
        "principal"
      ],
      "summary": "We introduce a new family of algorithms for detecting and estimating a rank-one signal from a noisy observation under prior information about that signal's direction, focusing on examples where the signal is known to have entries biased to be positive. Given a matrix observation Y Y , our algorithms construct a *nonlinear Laplacian*, another matrix of the form Y + d i a g ( σ ( Y 1 ) ) Y + d i a g ( σ ( Y 1 ) ) for a nonlinear σ : R → R σ : R → R , and examine the top eigenvalue and eigenvector of this matrix. When Y Y is the (suitably normalized) adjacency matrix of a graph, our approach gives a class of algorithms that search for unusually dense subgraphs by computing a spectrum of the graph \"deformed\" by the degree profile Y 1 Y 1 . We study the performance of such algorithms compared to direct spectral algorithms (the case σ = 0 σ = 0 ) on models of sparse principal component analysis with biased signals, including the Gaussian planted submatrix problem. For such models, we rigorously characterize the strength of rank-one signal, as a function of the nonlinearity σ σ , required for an outlier eigenvalue to appear in the spectrum of a nonlinear Laplacian matrix. While identifying the σ σ that minimizes the required signal strength in closed form seems intractable, we explore three approaches to design σ σ numerically: exhaustively searching over simple classes of σ σ , learning σ σ from datasets of problem instances, and tuning σ σ using black-box optimization of the critical signal strength. We find both theoretically and empirically that, if σ σ is chosen appropriately, then nonlinear Laplacian spectral algorithms substantially outperform direct spectral algorithms, while retaining the conceptual simplicity of spectral methods compared to broader classes of computations like approximate message passing or general first order methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hLZrqDFugY"
        ],
        "venue": [
          "/venue/hLZrqDFugY@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hLZrqDFugY"
        ],
        "detail": [
          "https://openreview.net/forum?id=hLZrqDFugY"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Nonlinear Laplacians: Tunable principal component analysis under directional prior information [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Yuxin Ma , Dmitriy Kunisky We introduce a new family of algorithms for detecting and estimating a rank-one signal from a noisy observation under prior information about that signal's direction, focusing on examples where the signal is known to have entries biased to be positive. Given a matrix observation Y Y , our algorithms construct a *nonlinear Laplacian*, another matrix of the form Y + d i a g ( σ ( Y 1 ) ) Y + d i a g ( σ ( Y 1 ) ) for a nonlinear σ : R → R σ : R → R , and examine the top eigenvalue and eigenvector of this matrix. When Y Y is the (suitably normalized) adjacency matrix of a graph, our approach gives a class of algorithms that search for unusually dense subgraphs by computing a spectrum of the graph \"deformed\" by the degree profile Y 1 Y 1 . We study the performance of such algorithms compared to direct spectral algorithms (the case σ = 0 σ = 0 ) on models of sparse principal component analysis with biased signals, including the Gaussian planted submatrix problem. For such models, we rigorously characterize the strength of rank-one signal, as a function of the nonlinearity σ σ , required for an outlier eigenvalue to appear in the spectrum of a nonlinear Laplacian matrix. While identifying the σ σ that minimizes the required signal strength in closed form seems intractable, we explore three approaches to design σ σ numerically: exhaustively searching over simple classes of σ σ , learning σ σ from datasets of problem instances, and tuning σ σ using black-box optimization of the critical signal strength. We find both theoretically and empirically that, if σ σ is chosen appropriately, then nonlinear Laplacian spectral algorithms substantially outperform direct spectral algorithms, while retaining the conceptual simplicity of spectral methods compared to broader classes of computations like approximate message passing or general first order methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "I8S4ASqO5H@OpenReview",
      "index": 489,
      "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment",
      "authors": [
        "Bryan Sangwoo Kim",
        "Jeongsol Kim",
        "Jong Chul Ye"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "coz",
        "zoom",
        "sisr",
        "vlm",
        "chain",
        "scale",
        "preference",
        "grpo",
        "autoregression",
        "extreme"
      ],
      "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4 × 4 × diffusion SR model wrapped in CoZ attains beyond 256 × 256 × enlargement with high perceptual quality and fidelity.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=I8S4ASqO5H"
        ],
        "venue": [
          "/venue/I8S4ASqO5H@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=I8S4ASqO5H"
        ],
        "detail": [
          "https://openreview.net/forum?id=I8S4ASqO5H"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Bryan Sangwoo Kim , Jeongsol Kim , Jong Chul Ye Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4 × 4 × diffusion SR model wrapped in CoZ attains beyond 256 × 256 × enlargement with high perceptual quality and fidelity. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RDt0crdC7N@OpenReview",
      "index": 490,
      "title": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning",
      "authors": [
        "Yibo Zhao",
        "Yang Zhao",
        "Hongru Du",
        "Hao Frank Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "athena",
        "symbolic",
        "utility",
        "personalized",
        "textualized",
        "decision",
        "reasoning",
        "vaccine",
        "modeling",
        "textual"
      ],
      "summary": "Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5\\% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RDt0crdC7N"
        ],
        "venue": [
          "/venue/RDt0crdC7N@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RDt0crdC7N"
        ],
        "detail": [
          "https://openreview.net/forum?id=RDt0crdC7N"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yibo Zhao , Yang Zhao , Hongru Du , Hao Frank Yang Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5\\% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "OiC78C68sJ@OpenReview",
      "index": 491,
      "title": "What are you sinking? A geometric approach on attention sink",
      "authors": [
        "Valeria Ruscio",
        "Umberto Nanni",
        "Fabrizio Silvestri"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sink",
        "attention",
        "tokens",
        "sinking",
        "reference",
        "geometric",
        "transformer",
        "frame",
        "disproportionately",
        "anchors"
      ],
      "summary": "Attention sink (AS) is a consistent pattern in transformer attention maps where certain tokens (often special tokens or positional anchors) disproportionately attract attention from other tokens. We show that in transformers, AS is not an architectural artifact, but it is the manifestation of a fundamental geometric principle: the establishment of reference frames that anchor representational spaces. We analyze several architectures and identify three distinct reference frame types, centralized, distributed, and bidirectional, that correlate with the attention sink phenomenon. We show that they emerge during the earliest stages of training as optimal solutions to the problem of establishing stable coordinate systems in high-dimensional spaces. We show the influence of architecture components, particularly position encoding implementations, on the specific type of reference frame. This perspective transforms our understanding of transformer attention mechanisms and provides insights for both architecture design and the relationship with AS.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OiC78C68sJ"
        ],
        "venue": [
          "/venue/OiC78C68sJ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OiC78C68sJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=OiC78C68sJ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "What are you sinking? A geometric approach on attention sink [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Valeria Ruscio , Umberto Nanni , Fabrizio Silvestri Attention sink (AS) is a consistent pattern in transformer attention maps where certain tokens (often special tokens or positional anchors) disproportionately attract attention from other tokens. We show that in transformers, AS is not an architectural artifact, but it is the manifestation of a fundamental geometric principle: the establishment of reference frames that anchor representational spaces. We analyze several architectures and identify three distinct reference frame types, centralized, distributed, and bidirectional, that correlate with the attention sink phenomenon. We show that they emerge during the earliest stages of training as optimal solutions to the problem of establishing stable coordinate systems in high-dimensional spaces. We show the influence of architecture components, particularly position encoding implementations, on the specific type of reference frame. This perspective transforms our understanding of transformer attention mechanisms and provides insights for both architecture design and the relationship with AS. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "k0wyi4cOGy@OpenReview",
      "index": 492,
      "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment",
      "authors": [
        "Yuxing Lu",
        "Wei Wu",
        "Xukai Zhao",
        "Rui Peng",
        "Jinzhuo Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "karma",
        "enrichment",
        "knowledge",
        "graph",
        "schema",
        "conflict",
        "agent",
        "llms",
        "multi",
        "adhering"
      ],
      "summary": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=k0wyi4cOGy"
        ],
        "venue": [
          "/venue/k0wyi4cOGy@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=k0wyi4cOGy"
        ],
        "detail": [
          "https://openreview.net/forum?id=k0wyi4cOGy"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 6
      },
      "raw_excerpt": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment [PDF 2 ] [Copy] [Kimi 6 ] [REL] Authors : Yuxing Lu , Wei Wu , Xukai Zhao , Rui Peng , Jinzhuo Wang Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rGMaZkn1ve@OpenReview",
      "index": 493,
      "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems",
      "authors": [
        "Xuanming Zhang",
        "Yuxuan Chen",
        "Samuel Yeh",
        "Sharon Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "metamind",
        "social",
        "agent",
        "tom",
        "human",
        "metacognitive",
        "psychological",
        "mind",
        "intent",
        "thoughts"
      ],
      "summary": "Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs—a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce **MetaMind**, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a *Theory-of-Mind Agent* generates hypotheses about user mental states (e.g., intent, emotion), (2) a *Moral Agent* refines these hypotheses using cultural norms and ethical constraints, and (3) a *Response Agent* generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework’s ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rGMaZkn1ve"
        ],
        "venue": [
          "/venue/rGMaZkn1ve@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rGMaZkn1ve"
        ],
        "detail": [
          "https://openreview.net/forum?id=rGMaZkn1ve"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Xuanming Zhang , Yuxuan Chen , Samuel Yeh , Sharon Li Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs—a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce **MetaMind**, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a *Theory-of-Mind Agent* generates hypotheses about user mental states (e.g., intent, emotion), (2) a *Moral Agent* refines these hypotheses using cultural norms and ethical constraints, and (3) a *Response Agent* generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework’s ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "t77EZLjvd5@OpenReview",
      "index": 494,
      "title": "Enhancing CLIP Robustness via Cross-Modality Alignment",
      "authors": [
        "Xingyu Zhu",
        "Beier Zhu",
        "Shuo Wang",
        "Kesen Zhao",
        "Hanwang Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cola",
        "adversarial",
        "clip",
        "alignment",
        "misalignment",
        "classification",
        "text",
        "moda",
        "lignment",
        "subspace"
      ],
      "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization in zero-shot classification but remain highly vulnerable to adversarial perturbations. Existing methods primarily focus on adversarial fine-tuning or prompt optimization, they often overlook the gaps in CLIP’s encoded features, which is shown as the text and image features lie far apart from each other. This misalignment is significantly amplified under adversarial perturbations, leading to severe degradation in classification performance. To address this problem, we propose **C**r**O**ss-moda**L**ity **A**lignment, dubbed **COLA**, an optimal transport-based framework that explicitly addresses adversarial misalignment by restoring both global image-text alignment and local structural consistency in the feature space. (1) COLA first projects adversarial image embeddings onto a subspace spanned by class text features, effectively filtering out non-semantic distortions while preserving discriminative information. (2) It then models images and texts as discrete distributions over multiple augmented views and refines their alignment via OT, with the subspace projection seamlessly integrated into the cost computation. This design ensures stable cross-modal alignment even under adversarial conditions. COLA is training-free and compatible with existing fine-tuned models. Extensive evaluations across 14 zero-shot classification benchmarks demonstrate the effectiveness of COLA, especially with an average improvement of 6.7% on ImageNet and its variants under PGD adversarial attacks, while maintaining high accuracy on clean samples.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=t77EZLjvd5"
        ],
        "venue": [
          "/venue/t77EZLjvd5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=t77EZLjvd5"
        ],
        "detail": [
          "https://openreview.net/forum?id=t77EZLjvd5"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Enhancing CLIP Robustness via Cross-Modality Alignment [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Xingyu Zhu , Beier Zhu , Shuo Wang , Kesen Zhao , Hanwang Zhang Vision-language models (VLMs) such as CLIP demonstrate strong generalization in zero-shot classification but remain highly vulnerable to adversarial perturbations. Existing methods primarily focus on adversarial fine-tuning or prompt optimization, they often overlook the gaps in CLIP’s encoded features, which is shown as the text and image features lie far apart from each other. This misalignment is significantly amplified under adversarial perturbations, leading to severe degradation in classification performance. To address this problem, we propose **C**r**O**ss-moda**L**ity **A**lignment, dubbed **COLA**, an optimal transport-based framework that explicitly addresses adversarial misalignment by restoring both global image-text alignment and local structural consistency in the feature space. (1) COLA first projects adversarial image embeddings onto a subspace spanned by class text features, effectively filtering out non-semantic distortions while preserving discriminative information. (2) It then models images and texts as discrete distributions over multiple augmented views and refines their alignment via OT, with the subspace projection seamlessly integrated into the cost computation. This design ensures stable cross-modal alignment even under adversarial conditions. COLA is training-free and compatible with existing fine-tuned models. Extensive evaluations across 14 zero-shot classification benchmarks demonstrate the effectiveness of COLA, especially with an average improvement of 6.7% on ImageNet and its variants under PGD adversarial attacks, while maintaining high accuracy on clean samples. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "BKYFAutCDZ@OpenReview",
      "index": 495,
      "title": "Rethinking Entropy in Test-Time Adaptation: The Missing Piece from Energy Duality",
      "authors": [
        "Mincheol Park",
        "Heeji Won",
        "Won Woo Ro",
        "Suhyun Kim"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "entropy",
        "energy",
        "minimization",
        "tta",
        "likelihood",
        "test",
        "retta",
        "adaptation",
        "rethinking",
        "shifts"
      ],
      "summary": "Test-time adaptation (TTA) aims to preserve model performance under distribution shifts. Yet, most existing methods rely on entropy minimization for confident predictions. This paper re-examines the sufficiency of entropy minimization by analyzing its dual relationship with energy. We view energy as a proxy for likelihood, where lower energy indicates higher observability under the learned distribution. We uncover that entropy and energy are tightly associated, controlled by the model’s confidence or ambiguity, and show that simultaneous reduction of both is essential. Importantly, we reveal that entropy minimization alone neither ensures energy reduction nor supports reliable likelihood estimation, and it requires explicit discriminative guidance to reach zero entropy. To combat these problems, we propose a twofold solution. First, we introduce a likelihood-based objective grounded in energy-based models, which reshape the energy landscape to favor test samples. For stable and scalable training, we adopt sliced score matching—a sampling-free, Hessian-insensitive approximation of Fisher divergence. Second, we enhance entropy minimization with a cross-entropy that treats the predicted class as a target to promote discriminability. By counterbalancing entropy and energy through the solution of multi-objective optimization, our unified TTA, ReTTA, outperforms existing entropy- or energy-based approaches across diverse distribution shifts.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BKYFAutCDZ"
        ],
        "venue": [
          "/venue/BKYFAutCDZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BKYFAutCDZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=BKYFAutCDZ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Rethinking Entropy in Test-Time Adaptation: The Missing Piece from Energy Duality [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Mincheol Park , Heeji Won , Won Woo Ro , Suhyun Kim Test-time adaptation (TTA) aims to preserve model performance under distribution shifts. Yet, most existing methods rely on entropy minimization for confident predictions. This paper re-examines the sufficiency of entropy minimization by analyzing its dual relationship with energy. We view energy as a proxy for likelihood, where lower energy indicates higher observability under the learned distribution. We uncover that entropy and energy are tightly associated, controlled by the model’s confidence or ambiguity, and show that simultaneous reduction of both is essential. Importantly, we reveal that entropy minimization alone neither ensures energy reduction nor supports reliable likelihood estimation, and it requires explicit discriminative guidance to reach zero entropy. To combat these problems, we propose a twofold solution. First, we introduce a likelihood-based objective grounded in energy-based models, which reshape the energy landscape to favor test samples. For stable and scalable training, we adopt sliced score matching—a sampling-free, Hessian-insensitive approximation of Fisher divergence. Second, we enhance entropy minimization with a cross-entropy that treats the predicted class as a target to promote discriminability. By counterbalancing entropy and energy through the solution of multi-objective optimization, our unified TTA, ReTTA, outperforms existing entropy- or energy-based approaches across diverse distribution shifts. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rVT1GK60Nt@OpenReview",
      "index": 496,
      "title": "On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization",
      "authors": [
        "Shaocong Ma",
        "Heng Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "estimators",
        "zeroth",
        "zoo",
        "unbiased",
        "gradient",
        "optimization",
        "biasedness",
        "optimal",
        "bias",
        "perturbation"
      ],
      "summary": "Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of *unbiased* gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rVT1GK60Nt"
        ],
        "venue": [
          "/venue/rVT1GK60Nt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rVT1GK60Nt"
        ],
        "detail": [
          "https://openreview.net/forum?id=rVT1GK60Nt"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Shaocong Ma , Heng Huang Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of *unbiased* gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "pVaqdFlUAO@OpenReview",
      "index": 497,
      "title": "Gradient Variance Reveals Failure Modes in Flow-Based Generative Models",
      "authors": [
        "Teodora Reu",
        "Sixtine Dromigny",
        "Michael M. Bronstein",
        "Francisco Vargas"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pairings",
        "intersect",
        "straight",
        "deterministic",
        "interpolants",
        "training",
        "variance",
        "rectified",
        "memorization",
        "failure"
      ],
      "summary": "Rectified Flows learn ODE vector fields whose trajectories are straight between source and target distributions, enabling near one-step inference. We show that this straight-path objective reveals fundamental failure modes: under deterministic training, low gradient variance drives memorization of arbitrary training pairings, even when interpolant lines between training pairs intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport and use the loss gradient variance across stochastic and deterministic regimes to characterize which vector fields optimization favors in each setting. We then show that, in a setting where all interpolating lines intersect, applying Rectified Flow yields the same specific pairings at inference as during training. More generally, we prove that a memorizing vector field exists even when training interpolants intersect, and that optimizing the straight-path objective converges to this ill-defined field. At inference, deterministic integration reproduces the exact training pairings. We validate our findings empirically on the CelebA dataset, confirming that deterministic interpolants induce memorization, while the injection of small noise restores generalization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pVaqdFlUAO"
        ],
        "venue": [
          "/venue/pVaqdFlUAO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pVaqdFlUAO"
        ],
        "detail": [
          "https://openreview.net/forum?id=pVaqdFlUAO"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Gradient Variance Reveals Failure Modes in Flow-Based Generative Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Teodora Reu , Sixtine Dromigny , Michael M. Bronstein , Francisco Vargas Rectified Flows learn ODE vector fields whose trajectories are straight between source and target distributions, enabling near one-step inference. We show that this straight-path objective reveals fundamental failure modes: under deterministic training, low gradient variance drives memorization of arbitrary training pairings, even when interpolant lines between training pairs intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport and use the loss gradient variance across stochastic and deterministic regimes to characterize which vector fields optimization favors in each setting. We then show that, in a setting where all interpolating lines intersect, applying Rectified Flow yields the same specific pairings at inference as during training. More generally, we prove that a memorizing vector field exists even when training interpolants intersect, and that optimizing the straight-path objective converges to this ill-defined field. At inference, deterministic integration reproduces the exact training pairings. We validate our findings empirically on the CelebA dataset, confirming that deterministic interpolants induce memorization, while the injection of small noise restores generalization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "AHccBzULR7@OpenReview",
      "index": 498,
      "title": "TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception",
      "authors": [
        "Runjian Chen",
        "Hyoungseob Park",
        "Bo Zhang",
        "Wenqi Shao",
        "Ping Luo",
        "Alex Wong"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lidar",
        "trend",
        "unsupervised",
        "forecasting",
        "temporal",
        "nuscenes",
        "perception",
        "representation",
        "labeling",
        "rendering"
      ],
      "summary": "Labeling LiDAR point clouds is notoriously time-and-energy-consuming, which spurs recent unsupervised 3D representation learning methods to alleviate the labeling burden in LiDAR perception via pretrained weights. Existing work focus on either masked auto encoding or contrastive learning on LiDAR point clouds, which neglects the temporal LiDAR sequence that naturally accounts for object motion (and their semantics). Instead, we propose TREND, short for Temporal REndering with Neural fielD, to learn 3D representation via forecasting the future observation in an unsupervised manner. TREND integrates forecasting for 3D pre-training through a Recurrent Embedding scheme to generate 3D embeddings across time and a Temporal LiDAR Neural Field specifically designed for LiDAR modality to represent the 3D scene, with which we compute the loss using differentiable rendering. We evaluate TREND on 3D object detection and LiDAR semantic segmentation tasks on popular datasets, including Once, Waymo, NuScenes, and SemanticKITTI. TREND generally improves from-scratch models across datasets and tasks and brings gains of 1.77\\% mAP on Once and 2.11\\% mAP on NuScenes, which are up to 400\\% more improvement compared to previous SOTA unsupervised 3D pre-training methods. Codes and models will be available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AHccBzULR7"
        ],
        "venue": [
          "/venue/AHccBzULR7@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AHccBzULR7"
        ],
        "detail": [
          "https://openreview.net/forum?id=AHccBzULR7"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Runjian Chen , Hyoungseob Park , Bo Zhang , Wenqi Shao , Ping Luo , Alex Wong Labeling LiDAR point clouds is notoriously time-and-energy-consuming, which spurs recent unsupervised 3D representation learning methods to alleviate the labeling burden in LiDAR perception via pretrained weights. Existing work focus on either masked auto encoding or contrastive learning on LiDAR point clouds, which neglects the temporal LiDAR sequence that naturally accounts for object motion (and their semantics). Instead, we propose TREND, short for Temporal REndering with Neural fielD, to learn 3D representation via forecasting the future observation in an unsupervised manner. TREND integrates forecasting for 3D pre-training through a Recurrent Embedding scheme to generate 3D embeddings across time and a Temporal LiDAR Neural Field specifically designed for LiDAR modality to represent the 3D scene, with which we compute the loss using differentiable rendering. We evaluate TREND on 3D object detection and LiDAR semantic segmentation tasks on popular datasets, including Once, Waymo, NuScenes, and SemanticKITTI. TREND generally improves from-scratch models across datasets and tasks and brings gains of 1.77\\% mAP on Once and 2.11\\% mAP on NuScenes, which are up to 400\\% more improvement compared to previous SOTA unsupervised 3D pre-training methods. Codes and models will be available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "i4qAfV04rZ@OpenReview",
      "index": 499,
      "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
      "authors": [
        "Theodoros Kouzelis",
        "Efstathios Karypidis",
        "Ioannis Kakogeorgiou",
        "Spyros Gidaris",
        "Nikos Komodakis"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "generative",
        "image",
        "modeling",
        "diffusion",
        "quality",
        "representation",
        "ldms",
        "feature",
        "latent",
        "boosting"
      ],
      "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=i4qAfV04rZ"
        ],
        "venue": [
          "/venue/i4qAfV04rZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=i4qAfV04rZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=i4qAfV04rZ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Theodoros Kouzelis , Efstathios Karypidis , Ioannis Kakogeorgiou , Spyros Gidaris , Nikos Komodakis Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "LVDRJE4xQ2@OpenReview",
      "index": 500,
      "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone",
      "authors": [
        "Jitai Hao",
        "Qiang Huang",
        "Hao Liu",
        "Xinyan Xiao",
        "Zhaochun Ren",
        "Jun Yu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lrc",
        "clone",
        "teacher",
        "tokens",
        "slms",
        "pruning",
        "instruct",
        "rank",
        "000",
        "distillation"
      ],
      "summary": "Training high-performing Small Language Models (SLMs) remains computationally expensive, even with knowledge distillation and pruning from larger teacher models. Existing approaches often face three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce \\textbf{Low-Rank Clone (LRC)}, an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers such as Llama-3.2-3B-Instruct and Qwen2.5-3B/7B-Instruct show that LRC matches or surpasses the performance of state-of-the-art models trained on trillions of tokens--using only 20B tokens, achieving over \\textbf{1,000 × × } greater training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/JitaiHao/LRC-4B-Base.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LVDRJE4xQ2"
        ],
        "venue": [
          "/venue/LVDRJE4xQ2@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LVDRJE4xQ2"
        ],
        "detail": [
          "https://openreview.net/forum?id=LVDRJE4xQ2"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Jitai Hao , Qiang Huang , Hao Liu , Xinyan Xiao , Zhaochun Ren , Jun Yu Training high-performing Small Language Models (SLMs) remains computationally expensive, even with knowledge distillation and pruning from larger teacher models. Existing approaches often face three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce \\textbf{Low-Rank Clone (LRC)}, an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers such as Llama-3.2-3B-Instruct and Qwen2.5-3B/7B-Instruct show that LRC matches or surpasses the performance of state-of-the-art models trained on trillions of tokens--using only 20B tokens, achieving over \\textbf{1,000 × × } greater training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/JitaiHao/LRC-4B-Base. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "cvJvk6oYfC@OpenReview",
      "index": 501,
      "title": "SparseMVC: Probing Cross-view Sparsity Variations for Multi-view Clustering",
      "authors": [
        "Ruimeng Liu",
        "Xin Zou",
        "Chang Tang",
        "Xiao Zheng",
        "Xingchen Hu",
        "Kun Sun",
        "Xinwang Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "view",
        "sparsemvc",
        "sparsity",
        "clustering",
        "level",
        "discrepancies",
        "encoding",
        "cross",
        "weighting",
        "pome"
      ],
      "summary": "Existing multi-view clustering methods employ various strategies to address data-level sparsity and view-level dynamic fusion. However, we identify a critical yet overlooked issue: varying sparsity across views. Cross-view sparsity variations lead to encoding discrepancies, heightening sample-level semantic heterogeneity and making view-level dynamic weighting inappropriate. To tackle these challenges, we propose Adaptive Sparse Autoencoders for Multi-View Clustering (SparseMVC), a framework with three key modules. Initially, the sparse autoencoder probes the sparsity of each view and adaptively adjusts encoding formats via an entropy-matching loss term, mitigating cross-view inconsistencies. Subsequently, the correlation-informed sample reweighting module employs attention mechanisms to assign weights by capturing correlations between early-fused global and view-specific features, reducing encoding discrepancies and balancing contributions. Furthermore, the cross-view distribution alignment module aligns feature distributions during the late fusion stage, accommodating datasets with an arbitrary number of views. Extensive experiments demonstrate that SparseMVC achieves state-of-the-art clustering performance. Our framework advances the field by extending sparsity handling from the data-level to view-level and mitigating the adverse effects of encoding discrepancies through sample-level dynamic weighting. The source code is publicly available at https://github.com/cleste-pome/SparseMVC.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cvJvk6oYfC"
        ],
        "venue": [
          "/venue/cvJvk6oYfC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cvJvk6oYfC"
        ],
        "detail": [
          "https://openreview.net/forum?id=cvJvk6oYfC"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "SparseMVC: Probing Cross-view Sparsity Variations for Multi-view Clustering [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Ruimeng Liu , Xin Zou , Chang Tang , Xiao Zheng , Xingchen Hu , Kun Sun , Xinwang Liu Existing multi-view clustering methods employ various strategies to address data-level sparsity and view-level dynamic fusion. However, we identify a critical yet overlooked issue: varying sparsity across views. Cross-view sparsity variations lead to encoding discrepancies, heightening sample-level semantic heterogeneity and making view-level dynamic weighting inappropriate. To tackle these challenges, we propose Adaptive Sparse Autoencoders for Multi-View Clustering (SparseMVC), a framework with three key modules. Initially, the sparse autoencoder probes the sparsity of each view and adaptively adjusts encoding formats via an entropy-matching loss term, mitigating cross-view inconsistencies. Subsequently, the correlation-informed sample reweighting module employs attention mechanisms to assign weights by capturing correlations between early-fused global and view-specific features, reducing encoding discrepancies and balancing contributions. Furthermore, the cross-view distribution alignment module aligns feature distributions during the late fusion stage, accommodating datasets with an arbitrary number of views. Extensive experiments demonstrate that SparseMVC achieves state-of-the-art clustering performance. Our framework advances the field by extending sparsity handling from the data-level to view-level and mitigating the adverse effects of encoding discrepancies through sample-level dynamic weighting. The source code is publicly available at https://github.com/cleste-pome/SparseMVC. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WWa5x1WnEw@OpenReview",
      "index": 502,
      "title": "Unbiased Prototype Consistency Learning for Multi-Modal and Multi-Task Object Re-Identification",
      "authors": [
        "Zhongao Zhou",
        "Bin Yang",
        "Wenke Huang",
        "Jun Chen",
        "Mang Ye"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reid",
        "modal",
        "upcl",
        "upme",
        "cpcr",
        "unbiased",
        "multi",
        "task",
        "modality",
        "prototype"
      ],
      "summary": "In object re-identification (ReID) task, both cross-modal and multi-modal retrieval methods have achieved notable progress. However, existing approaches are designed for specific modality and category (person or vehicle) retrieval task, lacking generalizability to others. Acquiring multiple task-specific models would result in wasteful allocation of both training and deployment resources. To address the practical requirements for unified retrieval, we introduce Multi-Modal and Multi-Task object ReID ( M 3 T M 3 T -ReID). The M 3 T M 3 T -ReID task aims to utilize a unified model to simultaneously achieve retrieval tasks across different modalities and different categories. Specifically, to tackle the challenges of modality distibution divergence and category semantics discrepancy posed in M 3 T M 3 T -ReID, we design a novel Unbiased Prototype Consistency Learning (UPCL) framework, which consists of two main modules: Unbiased Prototypes-guided Modality Enhancement (UPME) and Cluster Prototype Consistency Regularization (CPCR). UPME leverages modality-unbiased prototypes to simultaneously enhance cross-modal shared features and multi-modal fused features. Additionally, CPCR regulates discriminative semantics learning with category-consistent information through prototypes clustering. Under the collaborative operation of these two modules, our model can simultaneously learn robust cross-modal shared feature and multi-modal fused feature spaces, while also exhibiting strong category-discriminative capabilities. Extensive experiments on multi-modal datasets RGBNT201 and RGBNT100 demonstrates our UPCL framework showcasing exceptional performance for M 3 T M 3 T -ReID. The code is available at https://github.com/ZhouZhongao/UPCL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WWa5x1WnEw"
        ],
        "venue": [
          "/venue/WWa5x1WnEw@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WWa5x1WnEw"
        ],
        "detail": [
          "https://openreview.net/forum?id=WWa5x1WnEw"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Unbiased Prototype Consistency Learning for Multi-Modal and Multi-Task Object Re-Identification [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Zhongao Zhou , Bin Yang , Wenke Huang , Jun Chen , Mang Ye In object re-identification (ReID) task, both cross-modal and multi-modal retrieval methods have achieved notable progress. However, existing approaches are designed for specific modality and category (person or vehicle) retrieval task, lacking generalizability to others. Acquiring multiple task-specific models would result in wasteful allocation of both training and deployment resources. To address the practical requirements for unified retrieval, we introduce Multi-Modal and Multi-Task object ReID ( M 3 T M 3 T -ReID). The M 3 T M 3 T -ReID task aims to utilize a unified model to simultaneously achieve retrieval tasks across different modalities and different categories. Specifically, to tackle the challenges of modality distibution divergence and category semantics discrepancy posed in M 3 T M 3 T -ReID, we design a novel Unbiased Prototype Consistency Learning (UPCL) framework, which consists of two main modules: Unbiased Prototypes-guided Modality Enhancement (UPME) and Cluster Prototype Consistency Regularization (CPCR). UPME leverages modality-unbiased prototypes to simultaneously enhance cross-modal shared features and multi-modal fused features. Additionally, CPCR regulates discriminative semantics learning with category-consistent information through prototypes clustering. Under the collaborative operation of these two modules, our model can simultaneously learn robust cross-modal shared feature and multi-modal fused feature spaces, while also exhibiting strong category-discriminative capabilities. Extensive experiments on multi-modal datasets RGBNT201 and RGBNT100 demonstrates our UPCL framework showcasing exceptional performance for M 3 T M 3 T -ReID. The code is available at https://github.com/ZhouZhongao/UPCL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "56C0n6zSpC@OpenReview",
      "index": 503,
      "title": "MJ-Video: Benchmarking and Rewarding Video Generation with Fine-Grained Video Preference",
      "authors": [
        "Haibo Tong",
        "Zhaoyang Wang",
        "Zhaorun Chen",
        "Haonian Ji",
        "Shi Qiu",
        "Siwei Han",
        "Kexin Geng",
        "Zhongkai Xue",
        "Yiyang Zhou",
        "Peng Xia",
        "Mingyu Ding",
        "Rafael Rafailov",
        "Chelsea Finn",
        "Huaxiu Yao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "video",
        "preference",
        "grained",
        "generation",
        "fine",
        "reward",
        "bench",
        "judgments",
        "benchmarking",
        "rewarding"
      ],
      "summary": "Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and generation bias. To address these limitations, we introduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: Alignment, Safety, Fineness, Coherence & Consistency, and Bias & Fairness. This benchmark further incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose MJ-VIDEO, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. MJ-VIDEO can dynamically select relevant experts to accurately judge the preference based on the input text-video pair. This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of existing video reward models and demonstrate the superior performance of MJ-VIDEO in video preference assessment, achieving 17.58% and 15.87% improvements in overall and fine-grained preference judgments, respectively. Additionally, MJ-VIDEO is able to improve the alignment performance in video generation via preference fine-tuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=56C0n6zSpC"
        ],
        "venue": [
          "/venue/56C0n6zSpC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=56C0n6zSpC"
        ],
        "detail": [
          "https://openreview.net/forum?id=56C0n6zSpC"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 4
      },
      "raw_excerpt": "MJ-Video: Benchmarking and Rewarding Video Generation with Fine-Grained Video Preference [PDF 2 ] [Copy] [Kimi 4 ] [REL] Authors : Haibo Tong , Zhaoyang Wang , Zhaorun Chen , Haonian Ji , Shi Qiu , Siwei Han , Kexin Geng , Zhongkai Xue , Yiyang Zhou , Peng Xia , Mingyu Ding , Rafael Rafailov , Chelsea Finn , Huaxiu Yao Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and generation bias. To address these limitations, we introduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: Alignment, Safety, Fineness, Coherence & Consistency, and Bias & Fairness. This benchmark further incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose MJ-VIDEO, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. MJ-VIDEO can dynamically select relevant experts to accurately judge the preference based on the input text-video pair. This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of existing video reward models and demonstrate the superior performance of MJ-VIDEO in video preference assessment, achieving 17.58% and 15.87% improvements in overall and fine-grained preference judgments, respectively. Additionally, MJ-VIDEO is able to improve the alignment performance in video generation via preference fine-tuning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bP5cU0OYSn@OpenReview",
      "index": 504,
      "title": "Fast Projection-Free Approach (without Optimization Oracle) for Optimization over Compact Convex Set",
      "authors": [
        "Chenghao Liu",
        "Enming Liang",
        "Minghua Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "convex",
        "pgd",
        "optimization",
        "hom",
        "projection",
        "oracles",
        "epsilon",
        "loo",
        "mathcal",
        "iteration"
      ],
      "summary": "Projection-free first-order methods, e.g., the celebrated Frank-Wolfe (FW) algorithms, have emerged as powerful tools for optimization over simple convex sets such as polyhedra, because of their scalability, fast convergence, and iteration-wise feasibility without costly projections. However, extending these methods effectively to general compact convex sets remains challenging and largely open, as FW methods rely on expensive linear optimization oracles (LOO), while penalty-based methods often struggle with poor feasibility. We tackle this open challenge by presenting **Hom-PGD**, a novel projection-free method without expensive (optimization) oracles. Our method constructs a homeomorphism between the convex constraint set and a unit ball, transforming the original problem into an equivalent ball-constrained formulation, thus enabling efficient gradient-based optimization while preserving the original problem structure. We prove that Hom-PGD attains *optimal* convergence rates matching gradient descent with constant step-size to find an ϵ ϵ -approximate (stationary) solution: O ( log ( 1 / ϵ ) ) O ( log ⁡ ( 1 / ϵ ) ) for strongly convex objectives, O ( ϵ − 1 ) O ( ϵ − 1 ) for convex objectives, and O ( ϵ − 2 ) O ( ϵ − 2 ) for non-convex objectives. Meanwhile, Hom-PGD enjoys a low per-iteration complexity of O ( n 2 ) O ( n 2 ) , without expensive oracles like LOO or projection, where n n is the input size. Our framework further extends to certain non-convex sets, broadening its applicability in practical optimization scenarios with complex constraints. Extensive numerical experiments demonstrate that Hom-PGD achieves comparable convergence rates to state-of-the-art projection-free methods, while significantly reducing per-iteration runtime (up to 5 orders of magnitude faster) and thus the total problem-solving time.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bP5cU0OYSn"
        ],
        "venue": [
          "/venue/bP5cU0OYSn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bP5cU0OYSn"
        ],
        "detail": [
          "https://openreview.net/forum?id=bP5cU0OYSn"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Fast Projection-Free Approach (without Optimization Oracle) for Optimization over Compact Convex Set [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Chenghao Liu , Enming Liang , Minghua Chen Projection-free first-order methods, e.g., the celebrated Frank-Wolfe (FW) algorithms, have emerged as powerful tools for optimization over simple convex sets such as polyhedra, because of their scalability, fast convergence, and iteration-wise feasibility without costly projections. However, extending these methods effectively to general compact convex sets remains challenging and largely open, as FW methods rely on expensive linear optimization oracles (LOO), while penalty-based methods often struggle with poor feasibility. We tackle this open challenge by presenting **Hom-PGD**, a novel projection-free method without expensive (optimization) oracles. Our method constructs a homeomorphism between the convex constraint set and a unit ball, transforming the original problem into an equivalent ball-constrained formulation, thus enabling efficient gradient-based optimization while preserving the original problem structure. We prove that Hom-PGD attains *optimal* convergence rates matching gradient descent with constant step-size to find an ϵ ϵ -approximate (stationary) solution: O ( log ( 1 / ϵ ) ) O ( log ⁡ ( 1 / ϵ ) ) for strongly convex objectives, O ( ϵ − 1 ) O ( ϵ − 1 ) for convex objectives, and O ( ϵ − 2 ) O ( ϵ − 2 ) for non-convex objectives. Meanwhile, Hom-PGD enjoys a low per-iteration complexity of O ( n 2 ) O ( n 2 ) , without expensive oracles like LOO or projection, where n n is the input size. Our framework further extends to certain non-convex sets, broadening its applicability in practical optimization scenarios with complex constraints. Extensive numerical experiments demonstrate that Hom-PGD achieves comparable convergence rates to state-of-the-art projection-free methods, while significantly reducing per-iteration runtime (up to 5 orders of magnitude faster) and thus the total problem-solving time. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "jAsr5GHt3P@OpenReview",
      "index": 505,
      "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search",
      "authors": [
        "Yuichi Inoue",
        "Kou Misaki",
        "Yuki Imajuku",
        "So Kuroki",
        "Taishi Nakamura",
        "Takuya Akiba"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mcts",
        "inference",
        "repeated",
        "tree",
        "wider",
        "deeper",
        "branching",
        "search",
        "sampling",
        "refinement"
      ],
      "summary": "Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to ''go wider'' by expanding new candidate responses or ''go deeper'' by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jAsr5GHt3P"
        ],
        "venue": [
          "/venue/jAsr5GHt3P@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jAsr5GHt3P"
        ],
        "detail": [
          "https://openreview.net/forum?id=jAsr5GHt3P"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yuichi Inoue , Kou Misaki , Yuki Imajuku , So Kuroki , Taishi Nakamura , Takuya Akiba Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to ''go wider'' by expanding new candidate responses or ''go deeper'' by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "aSfBbhUJAa@OpenReview",
      "index": 506,
      "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
      "authors": [
        "Huacan Wang",
        "Ziyi Ni",
        "Shuo Zhang",
        "Shuo Lu",
        "Sen Hu",
        "Ziyang He",
        "Chen Hu",
        "Jiaye Lin",
        "Yifu Guo",
        "Yuntao Du",
        "Pin Lyu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "repomaster",
        "repositories",
        "openhands",
        "code",
        "github",
        "autonomous",
        "exploration",
        "tasks",
        "llms",
        "reuse"
      ],
      "summary": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110\\% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aSfBbhUJAa"
        ],
        "venue": [
          "/venue/aSfBbhUJAa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aSfBbhUJAa"
        ],
        "detail": [
          "https://openreview.net/forum?id=aSfBbhUJAa"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Huacan Wang , Ziyi Ni , Shuo Zhang , Shuo Lu , Sen Hu , Ziyang He , Chen Hu , Jiaye Lin , Yifu Guo , Yuntao Du , Pin Lyu The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110\\% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "721bDIvjen@OpenReview",
      "index": 507,
      "title": "Hierarchical Shortest-Path Graph Kernel Network",
      "authors": [
        "Jiaxin Wang",
        "Wenxuan Tu",
        "Jieren Cheng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "graph",
        "kernel",
        "hierarchical",
        "shortest",
        "similarity",
        "path",
        "end",
        "network",
        "gkn",
        "framework"
      ],
      "summary": "Graph kernels have emerged as a fundamental and widely adopted technique in graph machine learning. However, most existing graph kernel methods rely on fixed graph similarity estimation that cannot be directly optimized for task-specific objectives, leading to sub-optimal performance. To address this limitation, we propose a kernel-based learning framework called Hierarchical Shortest-Path Graph Kernel Network HSP-GKN, which seamlessly integrates graph similarity estimation with downstream tasks within a unified optimization framework. Specifically, we design a hierarchical shortest-path graph kernel that efficiently preserves both the semantic and structural information of a given graph by transforming it into hierarchical features used for subsequent neural network learning. Building upon this kernel, we develop a novel end-to-end learning framework that matches hierarchical graph features with learnable h i d d e n h i d d e n graph features to produce a similarity vector. This similarity vector subsequently serves as the graph embedding for end-to-end training, enabling the neural network to learn task-specific representations. Extensive experimental results demonstrate the effectiveness and superiority of the designed kernel and its corresponding learning framework compared to current competitors.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=721bDIvjen"
        ],
        "venue": [
          "/venue/721bDIvjen@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=721bDIvjen"
        ],
        "detail": [
          "https://openreview.net/forum?id=721bDIvjen"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Hierarchical Shortest-Path Graph Kernel Network [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jiaxin Wang , Wenxuan Tu , Jieren Cheng Graph kernels have emerged as a fundamental and widely adopted technique in graph machine learning. However, most existing graph kernel methods rely on fixed graph similarity estimation that cannot be directly optimized for task-specific objectives, leading to sub-optimal performance. To address this limitation, we propose a kernel-based learning framework called Hierarchical Shortest-Path Graph Kernel Network HSP-GKN, which seamlessly integrates graph similarity estimation with downstream tasks within a unified optimization framework. Specifically, we design a hierarchical shortest-path graph kernel that efficiently preserves both the semantic and structural information of a given graph by transforming it into hierarchical features used for subsequent neural network learning. Building upon this kernel, we develop a novel end-to-end learning framework that matches hierarchical graph features with learnable h i d d e n h i d d e n graph features to produce a similarity vector. This similarity vector subsequently serves as the graph embedding for end-to-end training, enabling the neural network to learn task-specific representations. Extensive experimental results demonstrate the effectiveness and superiority of the designed kernel and its corresponding learning framework compared to current competitors. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "piM21sPyVL@OpenReview",
      "index": 508,
      "title": "Differential Privacy on Fully Dynamic Streams",
      "authors": [
        "Yuan Qiu",
        "Ke Yi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dynamic",
        "privacy",
        "release",
        "answers",
        "privatized",
        "differential",
        "fully",
        "static",
        "streams",
        "deleted"
      ],
      "summary": "A fundamental problem in differential privacy is to release privatized answers to a class of linear queries with small error. This problem has been well studied in the static case. In this paper, we consider the fully dynamic setting where items may be inserted into or deleted from the dataset over time, and we need to continually release query answers at every time instance. We present efficient black-box constructions of such dynamic differentially private mechanisms from static ones with only a polylogarithmic degradation in the utility.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=piM21sPyVL"
        ],
        "venue": [
          "/venue/piM21sPyVL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=piM21sPyVL"
        ],
        "detail": [
          "https://openreview.net/forum?id=piM21sPyVL"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Differential Privacy on Fully Dynamic Streams [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yuan Qiu , Ke Yi A fundamental problem in differential privacy is to release privatized answers to a class of linear queries with small error. This problem has been well studied in the static case. In this paper, we consider the fully dynamic setting where items may be inserted into or deleted from the dataset over time, and we need to continually release query answers at every time instance. We present efficient black-box constructions of such dynamic differentially private mechanisms from static ones with only a polylogarithmic degradation in the utility. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TYroQXu6X0@OpenReview",
      "index": 509,
      "title": "Shortcut Features as Top Eigenfunctions of NTK: A Linear Neural Network Case and More",
      "authors": [
        "Jinwoo Lim",
        "Suhyun Kim",
        "Soo-Mook Moon"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shortcut",
        "ntk",
        "neural",
        "features",
        "network",
        "feature",
        "networks",
        "eigenvalues",
        "margin",
        "eigenfunctions"
      ],
      "summary": "One of the chronic problems of deep-learning models is shortcut learning. In a case where the majority of training data are dominated by a certain feature, neural networks prefer to learn such a feature even if the feature is not generalizable outside the training set. Based on the framework of Neural Tangent Kernel (NTK), we analyzed the case of linear neural networks to derive some important properties of shortcut learning. We defined a “feature” of a neural network as an eigenfunction of NTK. Then, we found that shortcut features correspond to features with larger eigenvalues when the shortcuts stem from the imbalanced number of samples in the clustered distribution. We also showed that the features with larger eigenvalues still have a large influence on the neural network output even after training, due to data variances in the clusters. Such a preference for certain features remains even when a margin of a neural network output is controlled, which shows that the max-margin bias is not the only major reason for shortcut learning. These properties of linear neural networks are empirically extended for more complex neural networks as a two-layer ReLU FC network and a ResNet-18.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TYroQXu6X0"
        ],
        "venue": [
          "/venue/TYroQXu6X0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TYroQXu6X0"
        ],
        "detail": [
          "https://openreview.net/forum?id=TYroQXu6X0"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Shortcut Features as Top Eigenfunctions of NTK: A Linear Neural Network Case and More [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jinwoo Lim , Suhyun Kim , Soo-Mook Moon One of the chronic problems of deep-learning models is shortcut learning. In a case where the majority of training data are dominated by a certain feature, neural networks prefer to learn such a feature even if the feature is not generalizable outside the training set. Based on the framework of Neural Tangent Kernel (NTK), we analyzed the case of linear neural networks to derive some important properties of shortcut learning. We defined a “feature” of a neural network as an eigenfunction of NTK. Then, we found that shortcut features correspond to features with larger eigenvalues when the shortcuts stem from the imbalanced number of samples in the clustered distribution. We also showed that the features with larger eigenvalues still have a large influence on the neural network output even after training, due to data variances in the clusters. Such a preference for certain features remains even when a margin of a neural network output is controlled, which shows that the max-margin bias is not the only major reason for shortcut learning. These properties of linear neural networks are empirically extended for more complex neural networks as a two-layer ReLU FC network and a ResNet-18. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "uwL0vbeEVn@OpenReview",
      "index": 510,
      "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications",
      "authors": [
        "Gabriele Oliaro",
        "Zhihao Jia",
        "Daniel F Campos",
        "Aurick Qiao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "suffixdecoding",
        "speculative",
        "decoding",
        "agentic",
        "requests",
        "emerging",
        "llm",
        "outputs",
        "token",
        "opportunities"
      ],
      "summary": "Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 3.9 × × , outperforming state-of-the-art methods -- 2.2 × × faster than model-based approaches like EAGLE-2/3 and 1.6 × × faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uwL0vbeEVn"
        ],
        "venue": [
          "/venue/uwL0vbeEVn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uwL0vbeEVn"
        ],
        "detail": [
          "https://openreview.net/forum?id=uwL0vbeEVn"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Gabriele Oliaro , Zhihao Jia , Daniel F Campos , Aurick Qiao Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 3.9 × × , outperforming state-of-the-art methods -- 2.2 × × faster than model-based approaches like EAGLE-2/3 and 1.6 × × faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yOs12gdsaL@OpenReview",
      "index": 511,
      "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference",
      "authors": [
        "Weizhi Fei",
        "Xueyan Niu",
        "XIE GUOQING",
        "Yingqing Liu",
        "Bo Bai",
        "Wei Han"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "evaluator",
        "prompt",
        "heads",
        "ehpc",
        "compression",
        "llms",
        "context",
        "inference",
        "long",
        "transformer"
      ],
      "summary": "Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly \"skim through'' input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively improves performance with the reduced costs associated with commercial API calls compared to prompt compressing methods. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yOs12gdsaL"
        ],
        "venue": [
          "/venue/yOs12gdsaL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yOs12gdsaL"
        ],
        "detail": [
          "https://openreview.net/forum?id=yOs12gdsaL"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Weizhi Fei , Xueyan Niu , XIE GUOQING , Yingqing Liu , Bo Bai , Wei Han Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly \"skim through'' input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively improves performance with the reduced costs associated with commercial API calls compared to prompt compressing methods. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "1bO9wIdyKa@OpenReview",
      "index": 512,
      "title": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting",
      "authors": [
        "Kai He",
        "Ruofan Liang",
        "Jacob Munkberg",
        "Jon Hasselgren",
        "Nandita Vijaykumar",
        "Alexander Keller",
        "Sanja Fidler",
        "Igor Gilitschenski",
        "Zan Gojcic",
        "Zian Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "relighting",
        "unirelight",
        "video",
        "lighting",
        "joint",
        "synthesis",
        "illumination",
        "outputs",
        "relit",
        "scene"
      ],
      "summary": "We address the challenge of relighting a single image or video, a task that demands precise scene intrinsic understanding and high-quality light transport synthesis. Existing end-to-end relighting models are often limited by the scarcity of paired multi-illumination data, restricting their ability to generalize across diverse scenes. Conversely, two-stage pipelines that combine inverse and forward rendering can mitigate data requirements but are susceptible to error accumulation and often fail to produce realistic outputs under complex lighting conditions or with sophisticated materials. In this work, we introduce a general-purpose approach that jointly estimates albedo and synthesizes relit outputs in a single pass, harnessing the generative capabilities of video diffusion models. This joint formulation enhances implicit scene comprehension and facilitates the creation of realistic lighting effects and intricate material interactions, such as shadows, reflections, and transparency. Trained on synthetic multi-illumination data and extensive automatically labeled real-world videos, our model demonstrates strong generalization across diverse domains and surpasses previous methods in both visual fidelity and temporal consistency. Our project page is https://research.nvidia.com/labs/toronto-ai/UniRelight/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1bO9wIdyKa"
        ],
        "venue": [
          "/venue/1bO9wIdyKa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1bO9wIdyKa"
        ],
        "detail": [
          "https://openreview.net/forum?id=1bO9wIdyKa"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Kai He , Ruofan Liang , Jacob Munkberg , Jon Hasselgren , Nandita Vijaykumar , Alexander Keller , Sanja Fidler , Igor Gilitschenski , Zan Gojcic , Zian Wang We address the challenge of relighting a single image or video, a task that demands precise scene intrinsic understanding and high-quality light transport synthesis. Existing end-to-end relighting models are often limited by the scarcity of paired multi-illumination data, restricting their ability to generalize across diverse scenes. Conversely, two-stage pipelines that combine inverse and forward rendering can mitigate data requirements but are susceptible to error accumulation and often fail to produce realistic outputs under complex lighting conditions or with sophisticated materials. In this work, we introduce a general-purpose approach that jointly estimates albedo and synthesizes relit outputs in a single pass, harnessing the generative capabilities of video diffusion models. This joint formulation enhances implicit scene comprehension and facilitates the creation of realistic lighting effects and intricate material interactions, such as shadows, reflections, and transparency. Trained on synthetic multi-illumination data and extensive automatically labeled real-world videos, our model demonstrates strong generalization across diverse domains and surpasses previous methods in both visual fidelity and temporal consistency. Our project page is https://research.nvidia.com/labs/toronto-ai/UniRelight/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "w5uUvxp81b@OpenReview",
      "index": 513,
      "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs",
      "authors": [
        "Xiaomin Li",
        "Zhou Yu",
        "Zhiwei Zhang",
        "Xupeng Chen",
        "Ziji Zhang",
        "Yingying Zhuang",
        "Narayanan Sadagopan",
        "Anurag Beniwal"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "cot",
        "instruction",
        "selective",
        "complexbench",
        "strategies",
        "following",
        "ifeval",
        "pitfalls",
        "constraints"
      ],
      "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 20+ models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=w5uUvxp81b"
        ],
        "venue": [
          "/venue/w5uUvxp81b@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=w5uUvxp81b"
        ],
        "detail": [
          "https://openreview.net/forum?id=w5uUvxp81b"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Xiaomin Li , Zhou Yu , Zhiwei Zhang , Xupeng Chen , Ziji Zhang , Yingying Zhuang , Narayanan Sadagopan , Anurag Beniwal Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 20+ models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6LOgOsIcXe@OpenReview",
      "index": 514,
      "title": "On Agnostic PAC Learning in the Small Error Regime",
      "authors": [
        "Julian Asilis",
        "Mikael Møller Høgsgaard",
        "Grigoris Velegkas"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tau",
        "agnostic",
        "error",
        "mathcal",
        "zhivotovskiy",
        "realizable",
        "hanneke",
        "err",
        "learner",
        "pac"
      ],
      "summary": "Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions m a t h c a l D m a t h c a l D are more difficult to learn than realizable distributions -- even when one discounts a learner's error by \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) , i.e., the error of the best hypothesis in m a t h c a l H m a t h c a l H . Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions m a t h c a l D m a t h c a l D for which \\\\tau = \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) \\\\tau = \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) is small. Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS '24) addresses this shortcoming by including t a u t a u itself as a parameter in the agnostic error term. In this more fine-grained model, they demonstrate tightness of the error lower bound t a u + O m e g a l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) t a u + O m e g a l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) in a regime where t a u > d / m t a u > d / m , and leave open the question of whether there may be a higher lower bound when t a u a p p r o x d / m t a u a p p r o x d / m , with d d denoting m a t h r m V C ( m a t h c a l H ) m a t h r m V C ( m a t h c a l H ) . In this work, we resolve this question by exhibiting a learner which achieves error c c d o t t a u + O l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) c c d o t t a u + O l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) for a constant c l e q 2.1 c l e q 2.1 , matching the lower bound and demonstrating optimality when t a u = O ( d / m ) t a u = O ( d / m ) . Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS '24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6LOgOsIcXe"
        ],
        "venue": [
          "/venue/6LOgOsIcXe@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6LOgOsIcXe"
        ],
        "detail": [
          "https://openreview.net/forum?id=6LOgOsIcXe"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "On Agnostic PAC Learning in the Small Error Regime [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Julian Asilis , Mikael Møller Høgsgaard , Grigoris Velegkas Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions m a t h c a l D m a t h c a l D are more difficult to learn than realizable distributions -- even when one discounts a learner's error by \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) , i.e., the error of the best hypothesis in m a t h c a l H m a t h c a l H . Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions m a t h c a l D m a t h c a l D for which \\\\tau = \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) \\\\tau = \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D}) is small. Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS '24) addresses this shortcoming by including t a u t a u itself as a parameter in the agnostic error term. In this more fine-grained model, they demonstrate tightness of the error lower bound t a u + O m e g a l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) t a u + O m e g a l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) in a regime where t a u > d / m t a u > d / m , and leave open the question of whether there may be a higher lower bound when t a u a p p r o x d / m t a u a p p r o x d / m , with d d denoting m a t h r m V C ( m a t h c a l H ) m a t h r m V C ( m a t h c a l H ) . In this work, we resolve this question by exhibiting a learner which achieves error c c d o t t a u + O l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) c c d o t t a u + O l e f t ( s q r t f r a c t a u ( d + l o g ( 1 / d e l t a ) ) m + f r a c d + l o g ( 1 / d e l t a ) m r i g h t ) for a constant c l e q 2.1 c l e q 2.1 , matching the lower bound and demonstrating optimality when t a u = O ( d / m ) t a u = O ( d / m ) . Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS '24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "uUIgxjWkCI@OpenReview",
      "index": 515,
      "title": "Wavelet Canonical Coherence for Nonstationary Signals",
      "authors": [
        "Haibo Wu",
        "Marina I. Knight",
        "Keiland W. Cooper",
        "Norbert J. Fortin",
        "Hernando Ombao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "wavecancoh",
        "nonstationary",
        "coherence",
        "wavelet",
        "multivariate",
        "canonical",
        "clusters",
        "signals",
        "specific",
        "lfp"
      ],
      "summary": "Understanding the evolving dependence between two sets of multivariate signals is fundamental in neuroscience and other domains where sub-networks in a system interact dynamically over time. Despite the growing interest in multivariate time series analysis, existing methods for between-clusters dependence typically rely on the assumption of stationarity and lack the temporal resolution to capture transient, frequency-specific interactions. To overcome this limitation, we propose scale-specific wavelet canonical coherence (WaveCanCoh), a novel framework that extends canonical coherence analysis to the nonstationary setting by leveraging the multivariate locally stationary wavelet model. The proposed WaveCanCoh enables the estimation of time-varying canonical coherence between clusters, providing interpretable insight into scale-specific time-varying interactions between clusters. Through extensive simulation studies, we demonstrate that WaveCanCoh accurately recovers true coherence structures under both locally stationary and general nonstationary conditions. Application to local field potential (LFP) activity data recorded from the hippocampus reveals distinct dynamic coherence patterns between correct and incorrect memory-guided decisions, illustrating capacity of the method to detect behaviorally relevant neural coordination. These results highlight WaveCanCoh as a flexible and principled tool for modeling complex cross-group dependencies in nonstationary multivariate systems. Code for implementing WaveCanCoh is available at https://github.com/mhaibo/WaveCanCoh.git.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uUIgxjWkCI"
        ],
        "venue": [
          "/venue/uUIgxjWkCI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uUIgxjWkCI"
        ],
        "detail": [
          "https://openreview.net/forum?id=uUIgxjWkCI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Wavelet Canonical Coherence for Nonstationary Signals [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Haibo Wu , Marina I. Knight , Keiland W. Cooper , Norbert J. Fortin , Hernando Ombao Understanding the evolving dependence between two sets of multivariate signals is fundamental in neuroscience and other domains where sub-networks in a system interact dynamically over time. Despite the growing interest in multivariate time series analysis, existing methods for between-clusters dependence typically rely on the assumption of stationarity and lack the temporal resolution to capture transient, frequency-specific interactions. To overcome this limitation, we propose scale-specific wavelet canonical coherence (WaveCanCoh), a novel framework that extends canonical coherence analysis to the nonstationary setting by leveraging the multivariate locally stationary wavelet model. The proposed WaveCanCoh enables the estimation of time-varying canonical coherence between clusters, providing interpretable insight into scale-specific time-varying interactions between clusters. Through extensive simulation studies, we demonstrate that WaveCanCoh accurately recovers true coherence structures under both locally stationary and general nonstationary conditions. Application to local field potential (LFP) activity data recorded from the hippocampus reveals distinct dynamic coherence patterns between correct and incorrect memory-guided decisions, illustrating capacity of the method to detect behaviorally relevant neural coordination. These results highlight WaveCanCoh as a flexible and principled tool for modeling complex cross-group dependencies in nonstationary multivariate systems. Code for implementing WaveCanCoh is available at https://github.com/mhaibo/WaveCanCoh.git. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VrCdsZBbIg@OpenReview",
      "index": 516,
      "title": "Language Modeling by Language Models",
      "authors": [
        "Junyan Cheng",
        "Peter Clark",
        "Kyle Richardson"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "genesys",
        "designs",
        "language",
        "mamba2",
        "generation",
        "verified",
        "350m",
        "14m",
        "architectures",
        "ideation"
      ],
      "summary": "*Can we leverage LLMs to model the process of discovering novel language model (LM) architectures?* Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system *Genesys* employs a *Ladder of Scales* approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M ∼ ∼ 350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., ∼ ∼ 86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified) and find the best designs to be competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks). We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VrCdsZBbIg"
        ],
        "venue": [
          "/venue/VrCdsZBbIg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VrCdsZBbIg"
        ],
        "detail": [
          "https://openreview.net/forum?id=VrCdsZBbIg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Language Modeling by Language Models [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Junyan Cheng , Peter Clark , Kyle Richardson *Can we leverage LLMs to model the process of discovering novel language model (LM) architectures?* Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system *Genesys* employs a *Ladder of Scales* approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M ∼ ∼ 350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., ∼ ∼ 86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified) and find the best designs to be competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks). We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "eOZFqyE9Ok@OpenReview",
      "index": 517,
      "title": "Plasticity as the Mirror of Empowerment",
      "authors": [
        "David Abel",
        "Michael Bowling",
        "Andre Barreto",
        "Will Dabney",
        "Shi Dong",
        "Steven Stenberg Hansen",
        "Anna Harutyunyan",
        "Khimya Khetarpal",
        "Clare Lyle",
        "Razvan Pascanu",
        "Georgios Piliouras",
        "Doina Precup",
        "Jonathan Richens",
        "Mark Rowland",
        "Tom Schaul",
        "Satinder Singh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "empowerment",
        "plasticity",
        "agent",
        "mirror",
        "influenced",
        "quantity",
        "directed",
        "mindful",
        "capacity",
        "definition"
      ],
      "summary": "Agents are minimally entities that are influenced by their past observations and act to influence future observations. This latter capacity is captured by empowerment, which has served as a vital framing concept across artificial intelligence and cognitive science. This former capacity, however, is equally foundational: In what ways, and to what extent, can an agent be influenced by what it observes? In this paper, we ground this concept in a universal agent-centric measure that we refer to as plasticity, and reveal a fundamental connection to empowerment. Following a set of desiderata on a suitable definition, we define plasticity using a new information-theoretic quantity we call the generalized directed information. We show that this new quantity strictly generalizes the directed information introduced by Massey (1990) while preserving all of its desirable properties. Under this definition, we find that plasticity is well thought of as the mirror of empowerment: The two concepts are defined using the same measure, with only the direction of influence reversed. Our main result establishes a tension between the plasticity and empowerment of an agent, suggesting that agent design needs to be mindful of both characteristics. We explore the implications of these findings, and suggest that plasticity, empowerment, and their relationship are essential to understanding agency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eOZFqyE9Ok"
        ],
        "venue": [
          "/venue/eOZFqyE9Ok@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eOZFqyE9Ok"
        ],
        "detail": [
          "https://openreview.net/forum?id=eOZFqyE9Ok"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Plasticity as the Mirror of Empowerment [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : David Abel , Michael Bowling , Andre Barreto , Will Dabney , Shi Dong , Steven Stenberg Hansen , Anna Harutyunyan , Khimya Khetarpal , Clare Lyle , Razvan Pascanu , Georgios Piliouras , Doina Precup , Jonathan Richens , Mark Rowland , Tom Schaul , Satinder Singh Agents are minimally entities that are influenced by their past observations and act to influence future observations. This latter capacity is captured by empowerment, which has served as a vital framing concept across artificial intelligence and cognitive science. This former capacity, however, is equally foundational: In what ways, and to what extent, can an agent be influenced by what it observes? In this paper, we ground this concept in a universal agent-centric measure that we refer to as plasticity, and reveal a fundamental connection to empowerment. Following a set of desiderata on a suitable definition, we define plasticity using a new information-theoretic quantity we call the generalized directed information. We show that this new quantity strictly generalizes the directed information introduced by Massey (1990) while preserving all of its desirable properties. Under this definition, we find that plasticity is well thought of as the mirror of empowerment: The two concepts are defined using the same measure, with only the direction of influence reversed. Our main result establishes a tension between the plasticity and empowerment of an agent, suggesting that agent design needs to be mindful of both characteristics. We explore the implications of these findings, and suggest that plasticity, empowerment, and their relationship are essential to understanding agency. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xNJenVNmzL@OpenReview",
      "index": 518,
      "title": "PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts",
      "authors": [
        "Zeman Li",
        "Yuan Deng",
        "Peilin Zhong",
        "Meisam Razaviyayn",
        "Vahab Mirrokni"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pike",
        "textbf",
        "gradient",
        "conflicts",
        "mtl",
        "mixing",
        "stimator",
        "adaptive",
        "task",
        "pretraining"
      ],
      "summary": "Modern foundation models are trained on diverse datasets to enhance generalization across tasks and domains. A central challenge in this process is determining how to effectively mix and sample data from multiple sources. This naturally leads to a multi-task learning (MTL) perspective. While prior work in MTL has emphasized mitigating gradient conflicts, we observe that large-scale pretraining scenarios—such as multilingual or multi-domain training—often exhibit little to no gradient conflict. Motivated by this observation, we propose PiKE PiKE ( P P ositive gradient i i nteraction-based K K -task weights E E stimator), an adaptive data mixing algorithm that dynamically adjusts sampling weights during training. PiKE exploits non-conflicting gradient interactions to minimize a near-tight upper bound on the average loss decrease at each step, while incurring negligible computational overhead. We provide theoretical convergence guarantees and show that PiKE outperforms static and non-adaptive mixing baselines. Furthermore, we extend PiKE to promote balanced learning across tasks. Extensive experiments on large-scale language model pretraining confirm that PiKE achieves faster convergence and improved downstream performance compared to existing approaches.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xNJenVNmzL"
        ],
        "venue": [
          "/venue/xNJenVNmzL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xNJenVNmzL"
        ],
        "detail": [
          "https://openreview.net/forum?id=xNJenVNmzL"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Zeman Li , Yuan Deng , Peilin Zhong , Meisam Razaviyayn , Vahab Mirrokni Modern foundation models are trained on diverse datasets to enhance generalization across tasks and domains. A central challenge in this process is determining how to effectively mix and sample data from multiple sources. This naturally leads to a multi-task learning (MTL) perspective. While prior work in MTL has emphasized mitigating gradient conflicts, we observe that large-scale pretraining scenarios—such as multilingual or multi-domain training—often exhibit little to no gradient conflict. Motivated by this observation, we propose PiKE PiKE ( P P ositive gradient i i nteraction-based K K -task weights E E stimator), an adaptive data mixing algorithm that dynamically adjusts sampling weights during training. PiKE exploits non-conflicting gradient interactions to minimize a near-tight upper bound on the average loss decrease at each step, while incurring negligible computational overhead. We provide theoretical convergence guarantees and show that PiKE outperforms static and non-adaptive mixing baselines. Furthermore, we extend PiKE to promote balanced learning across tasks. Extensive experiments on large-scale language model pretraining confirm that PiKE achieves faster convergence and improved downstream performance compared to existing approaches. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "W2874Arl4g@OpenReview",
      "index": 519,
      "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
      "authors": [
        "Ethan Mendes",
        "Alan Ritter"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "stl",
        "value",
        "lookahead",
        "reasoning",
        "language",
        "self",
        "search",
        "web",
        "models",
        "state"
      ],
      "summary": "Collecting ground-truth rewards or human demonstrations for multi-step reasoning tasks is often prohibitively expensive, especially in interactive domains such as web tasks. We introduce Self-Taught Lookahead (STL), a reward-free framework that improves language model–based value functions by reasoning explicitly about state transitions. STL can be viewed as a chain-of-thought analogue of the value iteration algorithm: instead of regressing directly on numeric values, a value LLM is trained to simulate a step of lookahead in natural language—predicting the next action, resulting state, and rationale for its value. This process refines value estimates without any labeled data. The self-supervised procedure yields more accurate state-value predictions, which in turn enable lightweight search algorithms to expand fewer states while maintaining strong performance. Empirically, STL-trained value models built on moderately sized (8B-parameter) open-weight LLMs boost web agent success rates by over 39%, achieving performance comparable to proprietary models. STL also generalizes to multi-hop question answering and math puzzles. Overall, STL enables small open-source models to guide efficient search, reducing inference costs by integrating explicit reasoning with value learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=W2874Arl4g"
        ],
        "venue": [
          "/venue/W2874Arl4g@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=W2874Arl4g"
        ],
        "detail": [
          "https://openreview.net/forum?id=W2874Arl4g"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Language Models can Self-Improve at State-Value Estimation for Better Search [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Ethan Mendes , Alan Ritter Collecting ground-truth rewards or human demonstrations for multi-step reasoning tasks is often prohibitively expensive, especially in interactive domains such as web tasks. We introduce Self-Taught Lookahead (STL), a reward-free framework that improves language model–based value functions by reasoning explicitly about state transitions. STL can be viewed as a chain-of-thought analogue of the value iteration algorithm: instead of regressing directly on numeric values, a value LLM is trained to simulate a step of lookahead in natural language—predicting the next action, resulting state, and rationale for its value. This process refines value estimates without any labeled data. The self-supervised procedure yields more accurate state-value predictions, which in turn enable lightweight search algorithms to expand fewer states while maintaining strong performance. Empirically, STL-trained value models built on moderately sized (8B-parameter) open-weight LLMs boost web agent success rates by over 39%, achieving performance comparable to proprietary models. STL also generalizes to multi-hop question answering and math puzzles. Overall, STL enables small open-source models to guide efficient search, reducing inference costs by integrating explicit reasoning with value learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oysfr9yqUI@OpenReview",
      "index": 520,
      "title": "Scalable Cross-View Sample Alignment for Multi-View Clustering with View Structure Similarity",
      "authors": [
        "Jun Wang",
        "Zhenglai Li",
        "Chang Tang",
        "Suyuan Liu",
        "Hao Yu",
        "Chuan Tang",
        "Miaomiao Li",
        "Xinwang Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "view",
        "clustering",
        "views",
        "alignment",
        "sample",
        "aligned",
        "mvc",
        "ssa",
        "multi",
        "samples"
      ],
      "summary": "Most existing multi-view clustering methods aim to generate a consensus partition across all views, based on the assumption that all views share the same sample arrangement. However, in real-world scenarios, the collected data across different views is often unsynchronized, making it difficult to ensure consistent sample correspondence between views. To address this issue, we propose a scalable sample-alignment-based multi-view clustering method, referred to as SSA-MVC. Specifically, we first employ a cluster-label matching (CLM) algorithm to select the view whose clustering labels best match those of the others as the benchmark view. Then, for each of the remaining views, we construct representations of non-aligned samples by computing their similarities with aligned samples. Based on these representations, we build a similarity graph between the non-aligned samples of each view and those in the benchmark view, which serves as the alignment criterion. This alignment criterion is then integrated into a late-fusion framework to enable clustering without requiring aligned samples. Notably, the learned sample alignment matrix can be used to enhance existing multi-view clustering methods in scenarios where sample correspondence is unavailable. The effectiveness of the proposed SSA-MVC algorithm is validated through extensive experiments conducted on eight real-world multi-view datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oysfr9yqUI"
        ],
        "venue": [
          "/venue/oysfr9yqUI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oysfr9yqUI"
        ],
        "detail": [
          "https://openreview.net/forum?id=oysfr9yqUI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Scalable Cross-View Sample Alignment for Multi-View Clustering with View Structure Similarity [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jun Wang , Zhenglai Li , Chang Tang , Suyuan Liu , Hao Yu , Chuan Tang , Miaomiao Li , Xinwang Liu Most existing multi-view clustering methods aim to generate a consensus partition across all views, based on the assumption that all views share the same sample arrangement. However, in real-world scenarios, the collected data across different views is often unsynchronized, making it difficult to ensure consistent sample correspondence between views. To address this issue, we propose a scalable sample-alignment-based multi-view clustering method, referred to as SSA-MVC. Specifically, we first employ a cluster-label matching (CLM) algorithm to select the view whose clustering labels best match those of the others as the benchmark view. Then, for each of the remaining views, we construct representations of non-aligned samples by computing their similarities with aligned samples. Based on these representations, we build a similarity graph between the non-aligned samples of each view and those in the benchmark view, which serves as the alignment criterion. This alignment criterion is then integrated into a late-fusion framework to enable clustering without requiring aligned samples. Notably, the learned sample alignment matrix can be used to enhance existing multi-view clustering methods in scenarios where sample correspondence is unavailable. The effectiveness of the proposed SSA-MVC algorithm is validated through extensive experiments conducted on eight real-world multi-view datasets. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SX6nL00JvM@OpenReview",
      "index": 521,
      "title": "Multidimensional Bayesian Utility Maximization: Tight Approximations to Welfare",
      "authors": [
        "Kira Goldner",
        "Taylor Lundy"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "welfare",
        "items",
        "utility",
        "buyers",
        "setting",
        "multidimensional",
        "maximization",
        "bayesian",
        "tight",
        "hartline"
      ],
      "summary": "We initiate the study of multidimensional Bayesian utility maximization, focusing on the unit-demand setting where values are i.i.d. across both items and buyers. The seminal result of Hartline and Roughgarden '08 studies simple, information-robust mechanisms that maximize utility for n n i.i.d. agents and m m identical items via an approximation to social welfare as an upper bound, and they prove this gap between optimal utility and social welfare is Θ ( 1 + log n / m ) Θ ( 1 + log ⁡ n / m ) in this setting. We extend these results to the multidimensional setting. To do so, we develop simple, prior-independent, approximately-optimal mechanisms, targeting the simplest benchmark of optimal welfare. We give a ( 1 − 1 / e ) ( 1 − 1 / e ) -approximation when there are more items than buyers, and a Θ ( log n / m ) Θ ( log ⁡ n / m ) -approximation when there are more buyers than items, and we prove that this bound is tight in both n n and m m by reducing the i.i.d. unit-demand setting to the identical items setting. Finally, we include an extensive discussion section on why Bayesian utility maximization is a promising research direction. In particular, we characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better benchmark than welfare is a hard problem itself.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SX6nL00JvM"
        ],
        "venue": [
          "/venue/SX6nL00JvM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SX6nL00JvM"
        ],
        "detail": [
          "https://openreview.net/forum?id=SX6nL00JvM"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Multidimensional Bayesian Utility Maximization: Tight Approximations to Welfare [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Kira Goldner , Taylor Lundy We initiate the study of multidimensional Bayesian utility maximization, focusing on the unit-demand setting where values are i.i.d. across both items and buyers. The seminal result of Hartline and Roughgarden '08 studies simple, information-robust mechanisms that maximize utility for n n i.i.d. agents and m m identical items via an approximation to social welfare as an upper bound, and they prove this gap between optimal utility and social welfare is Θ ( 1 + log n / m ) Θ ( 1 + log ⁡ n / m ) in this setting. We extend these results to the multidimensional setting. To do so, we develop simple, prior-independent, approximately-optimal mechanisms, targeting the simplest benchmark of optimal welfare. We give a ( 1 − 1 / e ) ( 1 − 1 / e ) -approximation when there are more items than buyers, and a Θ ( log n / m ) Θ ( log ⁡ n / m ) -approximation when there are more buyers than items, and we prove that this bound is tight in both n n and m m by reducing the i.i.d. unit-demand setting to the identical items setting. Finally, we include an extensive discussion section on why Bayesian utility maximization is a promising research direction. In particular, we characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better benchmark than welfare is a hard problem itself. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Dt5vRmUjAv@OpenReview",
      "index": 522,
      "title": "Decomposing stimulus-specific sensory neural information via diffusion models",
      "authors": [
        "Steeve Laquitaine",
        "Simone Azeglio",
        "Carlo Paris",
        "Ulisse Ferrari",
        "Matthew Chalk"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "stimulus",
        "information",
        "stimuli",
        "decomposition",
        "axioms",
        "neural",
        "neurons",
        "sensitivity",
        "sensory",
        "fisher"
      ],
      "summary": "A central question in sensory neuroscience is how much, but also what information neurons transmit about the world. While Shannon’s information theory provides a principled framework to quantify the amount of information neurons encode about all stimuli, it does not reveal which stimuli contribute most, or what stimulus features are encoded. As a concrete example, it is known that neurons in the early visual cortex are 'sensitive' to stimuli in a small region of space (their receptive field). However, it is not clear how such simple intuitions carry to more complex scenarios, e.g. with large, noisy & non-linear population of neurons and high-dimensional stimuli. Several previous measures of neural sensitivity have been proposed. For example, the Fisher information quantifies the sensitivity of neural responses to infinitesimal stimulus perturbations. However, as the Fisher is not a valid decomposition of the mutual information it cannot say how different stimuli contribute to the total encoded information. On the other hand, previous works have proposed stimulus dependent decompositions of mutual information, which define a function I ( x ) I ( x ) such that I ( R ; X ) = E [ I ( x ) ] I ( R ; X ) = E [ I ( x ) ] . However, this decomposition is inherently ill-posed: infinitely many functions I ( x ) I ( x ) satisfy the constraint, with no principled way to select among them. Further, different decompositions behave in qualitatively different ways, making it hard to interpret what are they are telling us. Finally, most proposed decompositions are computationally intractable for the high-dimensional stimuli and non-linear encoding models relevant for neuroscience. To resolve these limitations, we propose a set of axioms that any stimulus specific and feature-specific information decomposition should satisfy in order to serve as a meaningful and interpretable measure of neural sensitivity. These axioms formalize intuitive desiderata: that the information assigned to each stimulus, and stimulus feature, should be non-negative, and additive with respect to repeated measurements. We also require the decomposition to respect a form of locality: changes in how a neuron responds to a stimulus x x should not affect the information attributed to a distant stimulus x ′ x ′ . Finally, the attribution must be insensitive to irrelevant features, which do not contribute to the total information. Together, these constraints ensure that the decomposition is both interpretable and theoretically grounded. We show that existing decompositions violate one or more of these axioms, limiting their interpretability and use as information theoretic measures of neural sensitivity. We then introduce a novel decomposition that satisfies all of our axioms. It generalizes Fisher information by capturing neural sensitivity to both infinitesimal and finite stimulus perturbations. Moreover, it supports further decomposition across individual stimulus features (e.g., image pixels), enabling fine-grained analysis of neural representations. Beyond satisfying our theoretical axioms, our decomposition is computationally tractable for large neural populations and high-dimensional naturalistic stimuli, through the use of diffusion models. We demonstrate the power of our method by quantifying the information encoded by a model of visual neurons about individual images and pixels. Our approach uncovers aspects of the neural code that are not picked up by standard methods, such as the Fisher information, and opens the door to similar analyses in higher-order sensory areas, and artificial neural networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Dt5vRmUjAv"
        ],
        "venue": [
          "/venue/Dt5vRmUjAv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Dt5vRmUjAv"
        ],
        "detail": [
          "https://openreview.net/forum?id=Dt5vRmUjAv"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Decomposing stimulus-specific sensory neural information via diffusion models [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Steeve Laquitaine , Simone Azeglio , Carlo Paris , Ulisse Ferrari , Matthew Chalk A central question in sensory neuroscience is how much, but also what information neurons transmit about the world. While Shannon’s information theory provides a principled framework to quantify the amount of information neurons encode about all stimuli, it does not reveal which stimuli contribute most, or what stimulus features are encoded. As a concrete example, it is known that neurons in the early visual cortex are 'sensitive' to stimuli in a small region of space (their receptive field). However, it is not clear how such simple intuitions carry to more complex scenarios, e.g. with large, noisy & non-linear population of neurons and high-dimensional stimuli. Several previous measures of neural sensitivity have been proposed. For example, the Fisher information quantifies the sensitivity of neural responses to infinitesimal stimulus perturbations. However, as the Fisher is not a valid decomposition of the mutual information it cannot say how different stimuli contribute to the total encoded information. On the other hand, previous works have proposed stimulus dependent decompositions of mutual information, which define a function I ( x ) I ( x ) such that I ( R ; X ) = E [ I ( x ) ] I ( R ; X ) = E [ I ( x ) ] . However, this decomposition is inherently ill-posed: infinitely many functions I ( x ) I ( x ) satisfy the constraint, with no principled way to select among them. Further, different decompositions behave in qualitatively different ways, making it hard to interpret what are they are telling us. Finally, most proposed decompositions are computationally intractable for the high-dimensional stimuli and non-linear encoding models relevant for neuroscience. To resolve these limitations, we propose a set of axioms that any stimulus specific and feature-specific information decomposition should satisfy in order to serve as a meaningful and interpretable measure of neural sensitivity. These axioms formalize intuitive desiderata: that the information assigned to each stimulus, and stimulus feature, should be non-negative, and additive with respect to repeated measurements. We also require the decomposition to respect a form of locality: changes in how a neuron responds to a stimulus x x should not affect the information attributed to a distant stimulus x ′ x ′ . Finally, the attribution must be insensitive to irrelevant features, which do not contribute to the total information. Together, these constraints ensure that the decomposition is both interpretable and theoretically grounded. We show that existing decompositions violate one or more of these axioms, limiting their interpretability and use as information theoretic measures of neural sensitivity. We then introduce a novel decomposition that satisfies all of our axioms. It generalizes Fisher information by capturing neural sensitivity to both infinitesimal and finite stimulus perturbations. Moreover, it supports further decomposition across individual stimulus features (e.g., image pixels), enabling fine-grained analysis of neural representations. Beyond satisfying our theoretical axioms, our decomposition is computationally tractable for large neural populations and high-dimensional naturalistic stimuli, through the use of diffusion models. We demonstrate the power of our method by quantifying the information encoded by a model of visual neurons about individual images and pixels. Our approach uncovers aspects of the neural code that are not picked up by standard methods, such as the Fisher information, and opens the door to similar analyses in higher-order sensory areas, and artificial neural networks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "D6aCr4RRdt@OpenReview",
      "index": 523,
      "title": "Any-stepsize Gradient Descent for Separable Data under Fenchel–Young Losses",
      "authors": [
        "Han Bao",
        "Shinsaku Sakaue",
        "Yuki Takezawa"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "stepsize",
        "loss",
        "fenchel",
        "descent",
        "losses",
        "bounding",
        "epsilon",
        "young",
        "convergence",
        "property"
      ],
      "summary": "The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \\emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving ϵ ϵ -optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate T = Ω ( ϵ − 1 / 2 ) T = Ω ( ϵ − 1 / 2 ) , and the Rényi entropy achieves the far better rate T = Ω ( ϵ − 1 / 3 ) T = Ω ( ϵ − 1 / 3 ) . We argue that these better rate is possible because of \\emph{separation margin} of loss functions, instead of the self-bounding property.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=D6aCr4RRdt"
        ],
        "venue": [
          "/venue/D6aCr4RRdt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=D6aCr4RRdt"
        ],
        "detail": [
          "https://openreview.net/forum?id=D6aCr4RRdt"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Any-stepsize Gradient Descent for Separable Data under Fenchel–Young Losses [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Han Bao , Shinsaku Sakaue , Yuki Takezawa The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \\emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving ϵ ϵ -optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate T = Ω ( ϵ − 1 / 2 ) T = Ω ( ϵ − 1 / 2 ) , and the Rényi entropy achieves the far better rate T = Ω ( ϵ − 1 / 3 ) T = Ω ( ϵ − 1 / 3 ) . We argue that these better rate is possible because of \\emph{separation margin} of loss functions, instead of the self-bounding property. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rgoSyTCTkn@OpenReview",
      "index": 524,
      "title": "Provably Efficient RL under Episode-Wise Safety in Constrained MDPs with Linear Function Approximation",
      "authors": [
        "Toshinori Kitamura",
        "Arnob Ghosh",
        "Tadashi Kozuno",
        "Wataru Kumagai",
        "Kazumi Kasaura",
        "Kenta Hoshino",
        "Yohei Hosoe",
        "Yutaka Matsuo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "episode",
        "cmdp",
        "wise",
        "constrained",
        "mdps",
        "cmdps",
        "provably",
        "linear",
        "approximation",
        "constraint"
      ],
      "summary": "We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves O ˜ ( K − − √ ) O ~ ( K ) regret with an episode-wise zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rgoSyTCTkn"
        ],
        "venue": [
          "/venue/rgoSyTCTkn@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rgoSyTCTkn"
        ],
        "detail": [
          "https://openreview.net/forum?id=rgoSyTCTkn"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Provably Efficient RL under Episode-Wise Safety in Constrained MDPs with Linear Function Approximation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Toshinori Kitamura , Arnob Ghosh , Tadashi Kozuno , Wataru Kumagai , Kazumi Kasaura , Kenta Hoshino , Yohei Hosoe , Yutaka Matsuo We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves O ˜ ( K − − √ ) O ~ ( K ) regret with an episode-wise zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "iEgaS6wbLa@OpenReview",
      "index": 525,
      "title": "To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable RL",
      "authors": [
        "Yuda Song",
        "Dhruv Rohatgi",
        "Aarti Singh",
        "Drew Bagnell"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "privileged",
        "latent",
        "distill",
        "policy",
        "expert",
        "distillation",
        "trade",
        "algorithmic",
        "mdp",
        "partially"
      ],
      "summary": "Partial observability is a notorious challenge in reinforcement learning (RL), due to the need to learn complex, history-dependent policies. Recent empirical successes have used *privileged expert distillation* -- which leverages availability of latent state information during training (e.g., from a simulator) to learn and imitate the optimal latent, Markovian policy -- to disentangle the task of ''learning to see'' from ''learning to act''. While expert distillation is more computationally efficient than RL without latent state information, it also has well-documented failure modes. In this paper -- through a simple but instructive theoretical model called the *perturbed Block MDP*, and controlled experiments on challenging simulated locomotion tasks -- we investigate the algorithmic trade-off between privileged expert distillation and standard RL without privileged information. Our main findings are: **(1)** The trade-off empirically hinges on the *stochasticity* of the latent dynamics, as theoretically predicted by contrasting *approximate decodability* with *belief contraction* in the perturbed Block MDP; and **(2)** The optimal latent policy is not always the best latent policy to distill. Our results suggest new guidelines for effectively exploiting privileged information, potentially advancing the efficiency of policy learning across many practical partially observable domains.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=iEgaS6wbLa"
        ],
        "venue": [
          "/venue/iEgaS6wbLa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=iEgaS6wbLa"
        ],
        "detail": [
          "https://openreview.net/forum?id=iEgaS6wbLa"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable RL [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Yuda Song , Dhruv Rohatgi , Aarti Singh , Drew Bagnell Partial observability is a notorious challenge in reinforcement learning (RL), due to the need to learn complex, history-dependent policies. Recent empirical successes have used *privileged expert distillation* -- which leverages availability of latent state information during training (e.g., from a simulator) to learn and imitate the optimal latent, Markovian policy -- to disentangle the task of ''learning to see'' from ''learning to act''. While expert distillation is more computationally efficient than RL without latent state information, it also has well-documented failure modes. In this paper -- through a simple but instructive theoretical model called the *perturbed Block MDP*, and controlled experiments on challenging simulated locomotion tasks -- we investigate the algorithmic trade-off between privileged expert distillation and standard RL without privileged information. Our main findings are: **(1)** The trade-off empirically hinges on the *stochasticity* of the latent dynamics, as theoretically predicted by contrasting *approximate decodability* with *belief contraction* in the perturbed Block MDP; and **(2)** The optimal latent policy is not always the best latent policy to distill. Our results suggest new guidelines for effectively exploiting privileged information, potentially advancing the efficiency of policy learning across many practical partially observable domains. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "FrdX7K4Gli@OpenReview",
      "index": 526,
      "title": "A learnability analysis on neuro-symbolic learning",
      "authors": [
        "Hao-Yuan He",
        "Ming Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "learnability",
        "nesy",
        "dcsp",
        "neuro",
        "tasks",
        "symbolic",
        "dcsps",
        "learnable",
        "theoretical",
        "shortcuts"
      ],
      "summary": "This paper presents a comprehensive theoretical analysis of the learnability of neuro-symbolic (NeSy) tasks within hybrid systems. We characterize the learnability of NeSy tasks by their derived constraint satisfaction problems (DCSPs), demonstrating that a task is learnable if and only if its corresponding DCSP admits a unique solution. Under mild assumptions, we establish the sample complexity for learnable tasks and show that, for general tasks, the asymptotic expected concept error is controlled by the degree of disagreement among DCSP solutions. Our findings unify the characterization of learnability and the phenomenon of reasoning shortcuts, providing theoretical guarantees and actionable guidance for the principled design of NeSy systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FrdX7K4Gli"
        ],
        "venue": [
          "/venue/FrdX7K4Gli@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FrdX7K4Gli"
        ],
        "detail": [
          "https://openreview.net/forum?id=FrdX7K4Gli"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "A learnability analysis on neuro-symbolic learning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Hao-Yuan He , Ming Li This paper presents a comprehensive theoretical analysis of the learnability of neuro-symbolic (NeSy) tasks within hybrid systems. We characterize the learnability of NeSy tasks by their derived constraint satisfaction problems (DCSPs), demonstrating that a task is learnable if and only if its corresponding DCSP admits a unique solution. Under mild assumptions, we establish the sample complexity for learnable tasks and show that, for general tasks, the asymptotic expected concept error is controlled by the degree of disagreement among DCSP solutions. Our findings unify the characterization of learnability and the phenomenon of reasoning shortcuts, providing theoretical guarantees and actionable guidance for the principled design of NeSy systems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hJJnwcvE2M@OpenReview",
      "index": 527,
      "title": "Self-Perturbed Anomaly-Aware Graph Dynamics for Multivariate Time-Series Anomaly Detection",
      "authors": [
        "Jinyu Cai",
        "Yuan Xie",
        "Glynnis Lim",
        "Yifang Yin",
        "Roger Zimmermann",
        "See-Kiong Ng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "anomaly",
        "perturbed",
        "spagd",
        "self",
        "series",
        "graph",
        "aware",
        "normal",
        "multivariate",
        "reconstruction"
      ],
      "summary": "Detecting anomalies in multivariate time-series data is an essential task across various domains, yet there are unresolved challenges such as (1) severe class imbalance between normal and anomalous data due to rare anomaly availability in the real world; (2) limited adaptability of the static graph-based methods to dynamically changing inter-variable correlations; and (3) neglect of subtle anomalies due to overfitting to normal patterns in reconstruction-based methods. To tackle these issues, we propose Self-Perturbed Anomaly-Aware Graph Dynamics (SPAGD), a framework for time-series anomaly detection. SPAGD employs a self-perturbation module that generates self-perturbed time series from the reconstruction process of normal ones, which provide auxiliary signals to alleviate class imbalance during training. Concurrently, an anomaly-aware graph construction module is proposed to dynamically adjust the graph structure by leveraging the reconstruction residuals of self-perturbed time series, thereby emphasizing the inter-variable disruptions induced by anomalous candidates. A unified spatio-temporal anomaly detection module then integrates both spatial and temporal convolutions to train a classifier that distinguishes normal time series from the auxiliary self-perturbed samples. Extensive experiments across multiple benchmark datasets demonstrate the effectiveness of SPAGD compared to state-of-the-art baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hJJnwcvE2M"
        ],
        "venue": [
          "/venue/hJJnwcvE2M@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hJJnwcvE2M"
        ],
        "detail": [
          "https://openreview.net/forum?id=hJJnwcvE2M"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Self-Perturbed Anomaly-Aware Graph Dynamics for Multivariate Time-Series Anomaly Detection [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Jinyu Cai , Yuan Xie , Glynnis Lim , Yifang Yin , Roger Zimmermann , See-Kiong Ng Detecting anomalies in multivariate time-series data is an essential task across various domains, yet there are unresolved challenges such as (1) severe class imbalance between normal and anomalous data due to rare anomaly availability in the real world; (2) limited adaptability of the static graph-based methods to dynamically changing inter-variable correlations; and (3) neglect of subtle anomalies due to overfitting to normal patterns in reconstruction-based methods. To tackle these issues, we propose Self-Perturbed Anomaly-Aware Graph Dynamics (SPAGD), a framework for time-series anomaly detection. SPAGD employs a self-perturbation module that generates self-perturbed time series from the reconstruction process of normal ones, which provide auxiliary signals to alleviate class imbalance during training. Concurrently, an anomaly-aware graph construction module is proposed to dynamically adjust the graph structure by leveraging the reconstruction residuals of self-perturbed time series, thereby emphasizing the inter-variable disruptions induced by anomalous candidates. A unified spatio-temporal anomaly detection module then integrates both spatial and temporal convolutions to train a classifier that distinguishes normal time series from the auxiliary self-perturbed samples. Extensive experiments across multiple benchmark datasets demonstrate the effectiveness of SPAGD compared to state-of-the-art baselines. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "cECo8tetzF@OpenReview",
      "index": 528,
      "title": "Restoring Pruned Large Language Models via Lost Component Compensation",
      "authors": [
        "Zijian Feng",
        "Hanzhang Zhou",
        "Zixiao Zhu",
        "Tianjiao Li",
        "Chua Jia Jim Deryl",
        "Mak Lee Onn",
        "Gee Wah Ng",
        "Kezhi Mao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "pruned",
        "restorelcc",
        "lost",
        "compensation",
        "restoring",
        "pruning",
        "peft",
        "recovery",
        "performance",
        "restoration"
      ],
      "summary": "Pruning is a widely used technique to reduce the size and inference cost of large language models (LLMs), but it often causes performance degradation. To mitigate this, existing restoration methods typically employ parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned model's performance. However, most PEFT methods are designed for dense models and overlook the distinct properties of pruned models, often resulting in suboptimal recovery. In this work, we propose a targeted restoration strategy for pruned models that restores performance while preserving their low cost and high efficiency. We observe that pruning-induced information loss is reflected in attention activations, and selectively reintroducing components of this information can significantly recover model performance. Based on this insight, we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component Compensation), a plug-and-play method that contrastively probes critical attention heads via activation editing, extracts lost components from activation differences, and finally injects them back into the corresponding pruned heads for compensation and recovery. RestoreLCC is compatible with structured, semi-structured, and unstructured pruning schemes. Extensive experiments demonstrate that RestoreLCC consistently outperforms state-of-the-art baselines in both general and task-specific performance recovery, without compromising the sparsity or inference efficiency of pruned models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cECo8tetzF"
        ],
        "venue": [
          "/venue/cECo8tetzF@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cECo8tetzF"
        ],
        "detail": [
          "https://openreview.net/forum?id=cECo8tetzF"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Restoring Pruned Large Language Models via Lost Component Compensation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Zijian Feng , Hanzhang Zhou , Zixiao Zhu , Tianjiao Li , Chua Jia Jim Deryl , Mak Lee Onn , Gee Wah Ng , Kezhi Mao Pruning is a widely used technique to reduce the size and inference cost of large language models (LLMs), but it often causes performance degradation. To mitigate this, existing restoration methods typically employ parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned model's performance. However, most PEFT methods are designed for dense models and overlook the distinct properties of pruned models, often resulting in suboptimal recovery. In this work, we propose a targeted restoration strategy for pruned models that restores performance while preserving their low cost and high efficiency. We observe that pruning-induced information loss is reflected in attention activations, and selectively reintroducing components of this information can significantly recover model performance. Based on this insight, we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component Compensation), a plug-and-play method that contrastively probes critical attention heads via activation editing, extracts lost components from activation differences, and finally injects them back into the corresponding pruned heads for compensation and recovery. RestoreLCC is compatible with structured, semi-structured, and unstructured pruning schemes. Extensive experiments demonstrate that RestoreLCC consistently outperforms state-of-the-art baselines in both general and task-specific performance recovery, without compromising the sparsity or inference efficiency of pruned models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TyW1V1KukG@OpenReview",
      "index": 529,
      "title": "Towards Building Model/Prompt-Transferable Attackers against Large Vision-Language Models",
      "authors": [
        "Xiaowen Cai",
        "Daizong Liu",
        "Xiaoye Qu",
        "Xiang Fang",
        "Jianfeng Dong",
        "Keke Tang",
        "Pan Zhou",
        "Lichao Sun",
        "Wei Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lvlm",
        "lvlms",
        "adversarial",
        "transferable",
        "prompt",
        "attackers",
        "attack",
        "vision",
        "prompts",
        "outputs"
      ],
      "summary": "Although Large Vision-Language Models (LVLMs) exhibit impressive multimodal capabilities, their vulnerability to adversarial examples has raised serious security concerns. Existing LVLM attackers simply optimize adversarial images that easily overfit a certain model/prompt, making them ineffective once they are transferred to attack a different model/prompt. Motivated by this research gap, this paper aims to develop a more powerful attack that is transferable to black-box LVLM models of different structures and task-aware prompts of different semantics. Specifically, we introduce a new perspective of information theory to investigate LVLMs' transferable characteristics by exploring the relative dependence between outputs of the LVLM model and input adversarial samples. Our empirical observations suggest that enlarging/decreasing the mutual information between outputs and the disentangled adversarial/benign patterns of input images helps to generate more agnostic perturbations for misleading LVLMs' perception with better transferability. In particular, we formulate the complicated calculation of information gain as an estimation problem and incorporate such informative constraints into the adversarial learning process. Extensive experiments on various LVLM models/prompts demonstrate our significant transfer-attack performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TyW1V1KukG"
        ],
        "venue": [
          "/venue/TyW1V1KukG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TyW1V1KukG"
        ],
        "detail": [
          "https://openreview.net/forum?id=TyW1V1KukG"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Towards Building Model/Prompt-Transferable Attackers against Large Vision-Language Models [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Xiaowen Cai , Daizong Liu , Xiaoye Qu , Xiang Fang , Jianfeng Dong , Keke Tang , Pan Zhou , Lichao Sun , Wei Hu Although Large Vision-Language Models (LVLMs) exhibit impressive multimodal capabilities, their vulnerability to adversarial examples has raised serious security concerns. Existing LVLM attackers simply optimize adversarial images that easily overfit a certain model/prompt, making them ineffective once they are transferred to attack a different model/prompt. Motivated by this research gap, this paper aims to develop a more powerful attack that is transferable to black-box LVLM models of different structures and task-aware prompts of different semantics. Specifically, we introduce a new perspective of information theory to investigate LVLMs' transferable characteristics by exploring the relative dependence between outputs of the LVLM model and input adversarial samples. Our empirical observations suggest that enlarging/decreasing the mutual information between outputs and the disentangled adversarial/benign patterns of input images helps to generate more agnostic perturbations for misleading LVLMs' perception with better transferability. In particular, we formulate the complicated calculation of information gain as an estimation problem and incorporate such informative constraints into the adversarial learning process. Extensive experiments on various LVLM models/prompts demonstrate our significant transfer-attack performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "LBo4e6Y7Zg@OpenReview",
      "index": 530,
      "title": "GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End Autonomous Driving",
      "authors": [
        "Shuai Liu",
        "Quanmin Liang",
        "Zefeng Li",
        "Boyang Li",
        "Kai Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gaussianfusion",
        "gaussians",
        "end",
        "fusion",
        "driving",
        "gaussian",
        "sensor",
        "autonomous",
        "bench2drive",
        "navsim"
      ],
      "summary": "Multi-sensor fusion is crucial for improving the performance and robustness of end-to-end autonomous driving systems. Existing methods predominantly adopt either attention-based flatten fusion or bird’s eye view fusion through geometric transformations. However, these approaches often suffer from limited interpretability or dense computational overhead. In this paper, we introduce GaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end autonomous driving. Our method employs intuitive and compact Gaussian representations as intermediate carriers to aggregate information from diverse sensors. Specifically, we initialize a set of 2D Gaussians uniformly across the driving scene, where each Gaussian is parameterized by physical attributes and equipped with explicit and implicit features. These Gaussians are progressively refined by integrating multi-modal features. The explicit features capture rich semantic and spatial information about the traffic scene, while the implicit features provide complementary cues beneficial for trajectory planning. To fully exploit rich spatial and semantic information in Gaussians, we design a cascade planning head that iteratively refines trajectory predictions through interactions with Gaussians. Extensive experiments on the NAVSIM and Bench2Drive benchmarks demonstrate the effectiveness and robustness of the proposed GaussianFusion framework. The source code is included in the supplementary material and will be released publicly.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LBo4e6Y7Zg"
        ],
        "venue": [
          "/venue/LBo4e6Y7Zg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LBo4e6Y7Zg"
        ],
        "detail": [
          "https://openreview.net/forum?id=LBo4e6Y7Zg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End Autonomous Driving [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Shuai Liu , Quanmin Liang , Zefeng Li , Boyang Li , Kai Huang Multi-sensor fusion is crucial for improving the performance and robustness of end-to-end autonomous driving systems. Existing methods predominantly adopt either attention-based flatten fusion or bird’s eye view fusion through geometric transformations. However, these approaches often suffer from limited interpretability or dense computational overhead. In this paper, we introduce GaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end autonomous driving. Our method employs intuitive and compact Gaussian representations as intermediate carriers to aggregate information from diverse sensors. Specifically, we initialize a set of 2D Gaussians uniformly across the driving scene, where each Gaussian is parameterized by physical attributes and equipped with explicit and implicit features. These Gaussians are progressively refined by integrating multi-modal features. The explicit features capture rich semantic and spatial information about the traffic scene, while the implicit features provide complementary cues beneficial for trajectory planning. To fully exploit rich spatial and semantic information in Gaussians, we design a cascade planning head that iteratively refines trajectory predictions through interactions with Gaussians. Extensive experiments on the NAVSIM and Bench2Drive benchmarks demonstrate the effectiveness and robustness of the proposed GaussianFusion framework. The source code is included in the supplementary material and will be released publicly. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "GtImvTta8x@OpenReview",
      "index": 531,
      "title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment",
      "authors": [
        "Qi Xu",
        "Dongxu Wei",
        "Lingzhe Zhao",
        "Wenpu Li",
        "Zhangchi Huang",
        "Shunping Ji",
        "Peidong Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "understanding",
        "siu3r",
        "reconstruction",
        "alignment",
        "simultaneous",
        "tasks",
        "mutual",
        "unposed",
        "feature",
        "end"
      ],
      "summary": "Simultaneous understanding and 3D reconstruction plays an important role in developing end-to-end embodied intelligent systems. To achieve this, recent approaches resort to 2D-to-3D feature alignment paradigm, which leads to limited 3D understanding capability and potential semantic information loss. In light of this, we propose SIU3R, the first alignment-free framework for generalizable simultaneous understanding and 3D reconstruction from unposed images. Specifically, SIU3R bridges reconstruction and understanding tasks via pixel-aligned 3D representation, and unifies multiple understanding tasks into a set of unified learnable queries, enabling native 3D understanding without the need of alignment with 2D models. To encourage collaboration between the two tasks with shared representation, we further conduct in-depth analyses of their mutual benefits, and propose two lightweight modules to facilitate their interaction. Extensive experiments demonstrate that our method achieves state-of-the-art performance not only on the individual tasks of 3D reconstruction and understanding, but also on the task of simultaneous understanding and 3D reconstruction, highlighting the advantages of our alignment-free framework and the effectiveness of the mutual benefit designs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GtImvTta8x"
        ],
        "venue": [
          "/venue/GtImvTta8x@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GtImvTta8x"
        ],
        "detail": [
          "https://openreview.net/forum?id=GtImvTta8x"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Qi Xu , Dongxu Wei , Lingzhe Zhao , Wenpu Li , Zhangchi Huang , Shunping Ji , Peidong Liu Simultaneous understanding and 3D reconstruction plays an important role in developing end-to-end embodied intelligent systems. To achieve this, recent approaches resort to 2D-to-3D feature alignment paradigm, which leads to limited 3D understanding capability and potential semantic information loss. In light of this, we propose SIU3R, the first alignment-free framework for generalizable simultaneous understanding and 3D reconstruction from unposed images. Specifically, SIU3R bridges reconstruction and understanding tasks via pixel-aligned 3D representation, and unifies multiple understanding tasks into a set of unified learnable queries, enabling native 3D understanding without the need of alignment with 2D models. To encourage collaboration between the two tasks with shared representation, we further conduct in-depth analyses of their mutual benefits, and propose two lightweight modules to facilitate their interaction. Extensive experiments demonstrate that our method achieves state-of-the-art performance not only on the individual tasks of 3D reconstruction and understanding, but also on the task of simultaneous understanding and 3D reconstruction, highlighting the advantages of our alignment-free framework and the effectiveness of the mutual benefit designs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TZB6YT8Owr@OpenReview",
      "index": 532,
      "title": "HYPERION: Fine-Grained Hypersphere Alignment for Robust Federated Graph Learning",
      "authors": [
        "Guancheng Wan",
        "Xiaoran Shang",
        "Yuxin Wu",
        "Guibin Zhang",
        "Jinhe Bi",
        "Liangtao Zheng",
        "Xin Lin",
        "Yue Liu",
        "Yanbiao Ma",
        "Wenke Huang",
        "Bo Du"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hyperion",
        "fgl",
        "hyperspherical",
        "hypersphere",
        "robust",
        "graph",
        "grained",
        "anonymous",
        "federated",
        "fine"
      ],
      "summary": "Robust Federated Graph Learning (FGL) provides an effective decentralized framework for training Graph Neural Networks (GNNs) in noisy-label environments. However, the subtlety of noise during training presents formidable obstacles for developing robust FGL systems. Previous robust FL approaches neither adequately constrain edge-mediated error propagation nor account for intra-class topological differences. At the client level, we innovatively demonstrate that hyperspherical embedding can effectively capture graph structures in a fine-grained manner. Correspondingly, our method effectively addresses the aforementioned issues through fine-grained hypersphere alignment. Moreover, we uncover undetected noise arising from localized perspective constraints and propose the geometric-aware hyperspherical purification module at the server level. Combining both level strategies, we present our robust FGL framework,**HYPERION**, which operates all components within a unified hyperspherical space. **HYPERION** demonstrates remarkable robustness across multiple datasets, for instance, achieving a 29.7\\% ↑ ↑ F1-macro score with 50\\%-pair noise on Cora. The code is available for anonymous access at \\url{https://anonymous.4open.science/r/Hyperion-NeurIPS/}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TZB6YT8Owr"
        ],
        "venue": [
          "/venue/TZB6YT8Owr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TZB6YT8Owr"
        ],
        "detail": [
          "https://openreview.net/forum?id=TZB6YT8Owr"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "HYPERION: Fine-Grained Hypersphere Alignment for Robust Federated Graph Learning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Guancheng Wan , Xiaoran Shang , Yuxin Wu , Guibin Zhang , Jinhe Bi , Liangtao Zheng , Xin Lin , Yue Liu , Yanbiao Ma , Wenke Huang , Bo Du Robust Federated Graph Learning (FGL) provides an effective decentralized framework for training Graph Neural Networks (GNNs) in noisy-label environments. However, the subtlety of noise during training presents formidable obstacles for developing robust FGL systems. Previous robust FL approaches neither adequately constrain edge-mediated error propagation nor account for intra-class topological differences. At the client level, we innovatively demonstrate that hyperspherical embedding can effectively capture graph structures in a fine-grained manner. Correspondingly, our method effectively addresses the aforementioned issues through fine-grained hypersphere alignment. Moreover, we uncover undetected noise arising from localized perspective constraints and propose the geometric-aware hyperspherical purification module at the server level. Combining both level strategies, we present our robust FGL framework,**HYPERION**, which operates all components within a unified hyperspherical space. **HYPERION** demonstrates remarkable robustness across multiple datasets, for instance, achieving a 29.7\\% ↑ ↑ F1-macro score with 50\\%-pair noise on Cora. The code is available for anonymous access at \\url{https://anonymous.4open.science/r/Hyperion-NeurIPS/}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "REIo9ZLSYo@OpenReview",
      "index": 533,
      "title": "Brain-Inspired fMRI-to-Text Decoding via Incremental and Wrap-Up Language Modeling",
      "authors": [
        "Wentao Lu",
        "Dong Nie",
        "Pengcheng Xue",
        "Zheng Cui",
        "Piji Li",
        "Daoqiang Zhang",
        "Xuyun Wen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fmri",
        "decoding",
        "brain",
        "wrap",
        "text",
        "language",
        "semantic",
        "mae",
        "inspired",
        "segments"
      ],
      "summary": "Decoding natural language text from non-invasive brain signals, such as functional magnetic resonance imaging (fMRI), remains a central challenge in brain-computer interface research. While recent advances in large language models (LLMs) have enabled open-vocabulary fMRI-to-text decoding, existing frameworks typically process the entire fMRI sequence in a single step, leading to performance degradation when handling long input sequences due to memory overload and semantic drift. To address this limitation, we propose a brain-inspired sequential fMRI-to-text decoding framework that mimics the human cognitive strategy of segmented and inductive language processing. Specifically, we divide long fMRI time series into consecutive segments aligned with optimal language comprehension length. Each segment is decoded incrementally, followed by a wrap-up mechanism that summarizes the semantic content and incorporates it as prior knowledge into subsequent decoding steps. This sequence-wise approach alleviates memory burden and ensures semantic continuity across segments. In addition, we introduce a text-guided masking strategy integrated with a masked autoencoder (MAE) framework for fMRI representation learning. This method leverages attention distributions over key semantic tokens to selectively mask the corresponding fMRI time points, and employs MAE to guide the model toward focusing on neural activity at semantically salient moments, thereby enhancing the capability of fMRI embeddings to represent textual information. Experimental results on the two datasets demonstrate that our method significantly outperforms state-of-the-art approaches, with performance gains increasing as decoding length grows.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=REIo9ZLSYo"
        ],
        "venue": [
          "/venue/REIo9ZLSYo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=REIo9ZLSYo"
        ],
        "detail": [
          "https://openreview.net/forum?id=REIo9ZLSYo"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Brain-Inspired fMRI-to-Text Decoding via Incremental and Wrap-Up Language Modeling [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Wentao Lu , Dong Nie , Pengcheng Xue , Zheng Cui , Piji Li , Daoqiang Zhang , Xuyun Wen Decoding natural language text from non-invasive brain signals, such as functional magnetic resonance imaging (fMRI), remains a central challenge in brain-computer interface research. While recent advances in large language models (LLMs) have enabled open-vocabulary fMRI-to-text decoding, existing frameworks typically process the entire fMRI sequence in a single step, leading to performance degradation when handling long input sequences due to memory overload and semantic drift. To address this limitation, we propose a brain-inspired sequential fMRI-to-text decoding framework that mimics the human cognitive strategy of segmented and inductive language processing. Specifically, we divide long fMRI time series into consecutive segments aligned with optimal language comprehension length. Each segment is decoded incrementally, followed by a wrap-up mechanism that summarizes the semantic content and incorporates it as prior knowledge into subsequent decoding steps. This sequence-wise approach alleviates memory burden and ensures semantic continuity across segments. In addition, we introduce a text-guided masking strategy integrated with a masked autoencoder (MAE) framework for fMRI representation learning. This method leverages attention distributions over key semantic tokens to selectively mask the corresponding fMRI time points, and employs MAE to guide the model toward focusing on neural activity at semantically salient moments, thereby enhancing the capability of fMRI embeddings to represent textual information. Experimental results on the two datasets demonstrate that our method significantly outperforms state-of-the-art approaches, with performance gains increasing as decoding length grows. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "BcKYVmh3yH@OpenReview",
      "index": 534,
      "title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding",
      "authors": [
        "Yiming Wang",
        "Pei Zhang",
        "Siyuan Huang",
        "Baosong Yang",
        "Zhuosheng Zhang",
        "Fei Huang",
        "Rui Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bon",
        "sampling",
        "decoding",
        "cost",
        "reward",
        "scaling",
        "best",
        "early",
        "performance",
        "gpu"
      ],
      "summary": "Test-time scaling enhances large language model performance by allocating additional compute resources during decoding. Best-of- N N (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost–performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling: (1) Generating N N full samples consumes substantial GPU memory, reducing inference capacity under limited resources. (2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs. Although some studies have explored efficiency improvements, none have addressed both challenges at once. To address this gap, we propose **Self-Truncation Best-of- N N (ST-BoN)**, a decoding method that avoids fully generating all N N samples and eliminates the need for reward models. It leverages early sampling consistency in the model’s internal states to identify the most promising path and truncate suboptimal ones. In terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%. In terms of cost–performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%–80%, and under the same cost, it can improve accuracy by 3–4 points.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BcKYVmh3yH"
        ],
        "venue": [
          "/venue/BcKYVmh3yH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BcKYVmh3yH"
        ],
        "detail": [
          "https://openreview.net/forum?id=BcKYVmh3yH"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yiming Wang , Pei Zhang , Siyuan Huang , Baosong Yang , Zhuosheng Zhang , Fei Huang , Rui Wang Test-time scaling enhances large language model performance by allocating additional compute resources during decoding. Best-of- N N (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost–performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling: (1) Generating N N full samples consumes substantial GPU memory, reducing inference capacity under limited resources. (2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs. Although some studies have explored efficiency improvements, none have addressed both challenges at once. To address this gap, we propose **Self-Truncation Best-of- N N (ST-BoN)**, a decoding method that avoids fully generating all N N samples and eliminates the need for reward models. It leverages early sampling consistency in the model’s internal states to identify the most promising path and truncate suboptimal ones. In terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%. In terms of cost–performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%–80%, and under the same cost, it can improve accuracy by 3–4 points. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "obwRcksFZw@OpenReview",
      "index": 535,
      "title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts",
      "authors": [
        "Wasu Top Piriyakulkij",
        "Yichao Liang",
        "Hao Tang",
        "Adrian Weller",
        "Marta Kryven",
        "Kevin Ellis"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "world",
        "poe",
        "programmatic",
        "experts",
        "program",
        "models",
        "montezuma",
        "revenge",
        "modeling",
        "synthesis"
      ],
      "summary": "Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep-learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=obwRcksFZw"
        ],
        "venue": [
          "/venue/obwRcksFZw@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=obwRcksFZw"
        ],
        "detail": [
          "https://openreview.net/forum?id=obwRcksFZw"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "PoE-World: Compositional World Modeling with Products of Programmatic Experts [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Wasu Top Piriyakulkij , Yichao Liang , Hao Tang , Adrian Weller , Marta Kryven , Kevin Ellis Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep-learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hzBqQZK2iV@OpenReview",
      "index": 536,
      "title": "Uni-LoRA: One Vector is All You Need",
      "authors": [
        "Kaiyang Li",
        "Shaobo Han",
        "Qing Su",
        "Wei Li",
        "Zhipeng Cai",
        "Shihao Ji"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lora",
        "uni",
        "parameter",
        "vector",
        "unified",
        "projection",
        "trainable",
        "efficiency",
        "space",
        "variants"
      ],
      "summary": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space R^D, can be reconstructed through a projection from a subspace R^d, with d << D. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, P ∈ R^{D×d}. Most existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM -- making Uni-LoRA both a unified framework and a “one-vector-only” solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hzBqQZK2iV"
        ],
        "venue": [
          "/venue/hzBqQZK2iV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hzBqQZK2iV"
        ],
        "detail": [
          "https://openreview.net/forum?id=hzBqQZK2iV"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Uni-LoRA: One Vector is All You Need [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Kaiyang Li , Shaobo Han , Qing Su , Wei Li , Zhipeng Cai , Shihao Ji Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space R^D, can be reconstructed through a projection from a subspace R^d, with d << D. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, P ∈ R^{D×d}. Most existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM -- making Uni-LoRA both a unified framework and a “one-vector-only” solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0jvnfH0WYV@OpenReview",
      "index": 537,
      "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding",
      "authors": [
        "Hanyin Wang",
        "Zhenbang Wu",
        "Gururaj J. Kolar",
        "Hariprasad Reddy Korsapati",
        "Brian Bartlett",
        "Bryan Hull",
        "Jimeng Sun"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "drg",
        "coding",
        "ood",
        "sft",
        "sapphire",
        "tasks",
        "diagnosis",
        "grpo",
        "group",
        "reinforcement"
      ],
      "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0jvnfH0WYV"
        ],
        "venue": [
          "/venue/0jvnfH0WYV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0jvnfH0WYV"
        ],
        "detail": [
          "https://openreview.net/forum?id=0jvnfH0WYV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hanyin Wang , Zhenbang Wu , Gururaj J. Kolar , Hariprasad Reddy Korsapati , Brian Bartlett , Bryan Hull , Jimeng Sun Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Quvnn2o17a@OpenReview",
      "index": 538,
      "title": "Stochastic Process Learning via Operator Flow Matching",
      "authors": [
        "Yaozhong Shi",
        "Zachary E Ross",
        "Domniki Asimaki",
        "Kamyar Azizzadenesheli"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ofm",
        "stochastic",
        "process",
        "matching",
        "learning",
        "operator",
        "flow",
        "regression",
        "points",
        "functional"
      ],
      "summary": "Expanding on neural operators, we propose a novel framework for stochastic process learning across arbitrary domains. In particular, we develop operator flow matching (OFM) for learning stochastic process priors on function spaces. OFM provides the probability density of the values of any collection of points and enables mathematically tractable functional regression at new points with mean and density estimation. Our method outperforms state-of-the-art models in stochastic process learning, functional regression, and prior learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Quvnn2o17a"
        ],
        "venue": [
          "/venue/Quvnn2o17a@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Quvnn2o17a"
        ],
        "detail": [
          "https://openreview.net/forum?id=Quvnn2o17a"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Stochastic Process Learning via Operator Flow Matching [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Yaozhong Shi , Zachary E Ross , Domniki Asimaki , Kamyar Azizzadenesheli Expanding on neural operators, we propose a novel framework for stochastic process learning across arbitrary domains. In particular, we develop operator flow matching (OFM) for learning stochastic process priors on function spaces. OFM provides the probability density of the values of any collection of points and enables mathematically tractable functional regression at new points with mean and density estimation. Our method outperforms state-of-the-art models in stochastic process learning, functional regression, and prior learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "W8xcKoJcrl@OpenReview",
      "index": 539,
      "title": "Strategic Costs of Perceived Bias in Fair Selection",
      "authors": [
        "L. Elisa Celis",
        "Lingxiao Huang",
        "Milind Sohoni",
        "Nisheeth K. Vishnoi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "disparities",
        "effort",
        "perceived",
        "meritocratic",
        "selection",
        "institutional",
        "selectivity",
        "perceptions",
        "incentives",
        "fair"
      ],
      "summary": "Meritocratic systems, from admissions to hiring, aim to impartially reward skill and effort. Yet persistent disparities across race, gender, and class challenge this ideal. Some attribute these gaps to structural inequality; others to individual choice. We develop a game-theoretic model in which candidates from different socioeconomic groups differ in their perceived post-selection value—shaped by social context and, increasingly, by AI-powered tools offering personalized career or salary guidance. Each candidate strategically chooses effort, balancing its cost against expected reward; effort translates into observable merit, and selection is based solely on merit. We characterize the unique Nash equilibrium in the large-agent limit and derive explicit formulas showing how valuation disparities and institutional selectivity jointly determine effort, representation, social welfare, and utility. We further propose a cost-sensitive optimization framework that quantifies how modifying selectivity or perceived value can reduce disparities without compromising institutional goals. Our analysis reveals a perception-driven bias: when perceptions of post-selection value differ across groups, these differences translate into rational differences in effort, propagating disparities backward through otherwise \"fair\" selection processes. While the model is static, it captures one stage of a broader feedback cycle linking perceptions, incentives, and outcomes—bridging rational-choice and structural explanations of inequality by showing how techno-social environments shape individual incentives in meritocratic systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=W8xcKoJcrl"
        ],
        "venue": [
          "/venue/W8xcKoJcrl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=W8xcKoJcrl"
        ],
        "detail": [
          "https://openreview.net/forum?id=W8xcKoJcrl"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Strategic Costs of Perceived Bias in Fair Selection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : L. Elisa Celis , Lingxiao Huang , Milind Sohoni , Nisheeth K. Vishnoi Meritocratic systems, from admissions to hiring, aim to impartially reward skill and effort. Yet persistent disparities across race, gender, and class challenge this ideal. Some attribute these gaps to structural inequality; others to individual choice. We develop a game-theoretic model in which candidates from different socioeconomic groups differ in their perceived post-selection value—shaped by social context and, increasingly, by AI-powered tools offering personalized career or salary guidance. Each candidate strategically chooses effort, balancing its cost against expected reward; effort translates into observable merit, and selection is based solely on merit. We characterize the unique Nash equilibrium in the large-agent limit and derive explicit formulas showing how valuation disparities and institutional selectivity jointly determine effort, representation, social welfare, and utility. We further propose a cost-sensitive optimization framework that quantifies how modifying selectivity or perceived value can reduce disparities without compromising institutional goals. Our analysis reveals a perception-driven bias: when perceptions of post-selection value differ across groups, these differences translate into rational differences in effort, propagating disparities backward through otherwise \"fair\" selection processes. While the model is static, it captures one stage of a broader feedback cycle linking perceptions, incentives, and outcomes—bridging rational-choice and structural explanations of inequality by showing how techno-social environments shape individual incentives in meritocratic systems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6xCcjYa97j@OpenReview",
      "index": 540,
      "title": "A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective",
      "authors": [
        "Lianghe Shi",
        "Meng Wu",
        "Huijie Zhang",
        "Zekai Zhang",
        "Molei Tao",
        "Qing Qu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "collapse",
        "memorization",
        "training",
        "synthetic",
        "generalization",
        "model",
        "recursive",
        "degradation",
        "transition",
        "data"
      ],
      "summary": "The widespread use of diffusion models has led to an abundance of AI-generated data, raising concerns about model collapse---a phenomenon in which recursive iterations of training on synthetic data lead to performance degradation. Prior work primarily characterizes this collapse via variance shrinkage or distribution shift, but these perspectives miss practical manifestations of model collapse. This paper identifies a transition from generalization to memorization during model collapse in diffusion models, where models increasingly replicate training data instead of generating novel content during iterative training on synthetic samples. This transition is directly driven by the declining entropy of the synthetic training data produced in each training cycle, which serves as a clear indicator of model degradation. Motivated by this insight, we propose an entropy-based data selection strategy to mitigate the transition from generalization to memorization and alleviate model collapse. Empirical results show that our approach significantly enhances visual quality and diversity in recursive generation, effectively preventing collapse.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6xCcjYa97j"
        ],
        "venue": [
          "/venue/6xCcjYa97j@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6xCcjYa97j"
        ],
        "detail": [
          "https://openreview.net/forum?id=6xCcjYa97j"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Lianghe Shi , Meng Wu , Huijie Zhang , Zekai Zhang , Molei Tao , Qing Qu The widespread use of diffusion models has led to an abundance of AI-generated data, raising concerns about model collapse---a phenomenon in which recursive iterations of training on synthetic data lead to performance degradation. Prior work primarily characterizes this collapse via variance shrinkage or distribution shift, but these perspectives miss practical manifestations of model collapse. This paper identifies a transition from generalization to memorization during model collapse in diffusion models, where models increasingly replicate training data instead of generating novel content during iterative training on synthetic samples. This transition is directly driven by the declining entropy of the synthetic training data produced in each training cycle, which serves as a clear indicator of model degradation. Motivated by this insight, we propose an entropy-based data selection strategy to mitigate the transition from generalization to memorization and alleviate model collapse. Empirical results show that our approach significantly enhances visual quality and diversity in recursive generation, effectively preventing collapse. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "zNLlglSOwD@OpenReview",
      "index": 541,
      "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
      "authors": [
        "Yuezhou Hu",
        "Jiaxin Guo",
        "Xinyu Feng",
        "Tuo Zhao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "adaspec",
        "draft",
        "distillation",
        "acceptance",
        "speculative",
        "tokens",
        "token",
        "model",
        "target",
        "selective"
      ],
      "summary": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at \\url{https://github.com/yuezhouhu/adaspec}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zNLlglSOwD"
        ],
        "venue": [
          "/venue/zNLlglSOwD@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zNLlglSOwD"
        ],
        "detail": [
          "https://openreview.net/forum?id=zNLlglSOwD"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Yuezhou Hu , Jiaxin Guo , Xinyu Feng , Tuo Zhao Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at \\url{https://github.com/yuezhouhu/adaspec}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "A2pmNL7L1E@OpenReview",
      "index": 542,
      "title": "Depth-Width Tradeoffs for Transformers on Graph Tasks",
      "authors": [
        "Gilad Yehudai",
        "Clayton Sanford",
        "Maya Bechler-Speicher",
        "Orr Fischer",
        "Ran Gilad-Bachrach",
        "Amir Globerson"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "width",
        "depth",
        "graph",
        "tasks",
        "suffices",
        "transformers",
        "tradeoffs",
        "algorithmic",
        "transformer",
        "train"
      ],
      "summary": "Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=A2pmNL7L1E"
        ],
        "venue": [
          "/venue/A2pmNL7L1E@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=A2pmNL7L1E"
        ],
        "detail": [
          "https://openreview.net/forum?id=A2pmNL7L1E"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Depth-Width Tradeoffs for Transformers on Graph Tasks [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Gilad Yehudai , Clayton Sanford , Maya Bechler-Speicher , Orr Fischer , Ran Gilad-Bachrach , Amir Globerson Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0SYkQ50imt@OpenReview",
      "index": 543,
      "title": "Refinement Methods for Distributed Distribution Estimation under ℓ p ℓ p -Losses",
      "authors": [
        "Deheng Yuan",
        "Tao Guo",
        "Zhongyi Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "refinement",
        "estimation",
        "rates",
        "ell",
        "losses",
        "regimes",
        "minimax",
        "customized",
        "rough",
        "optimal"
      ],
      "summary": "Consider the communication-constrained estimation of discrete distributions under ℓ p ℓ p losses, where each distributed terminal holds multiple independent samples and uses limited number of bits to describe the samples. We obtain the minimax optimal rates of the problem for most parameter regimes. As a result, an elbow effect of the optimal rates at p = 2 p = 2 is clearly identified. In order to achieve the optimal rates for different parameter regimes, we introduce refinement methods and develop additional customized techniques in the estimation protocols. The general idea of the refinement methods is to first generate rough estimate by partial information and then establish refined estimate in subsequent steps guided by the rough estimate. Then customized techniques such as successive refinement, sample compression, thresholding and random hashing are leveraged to achieve the optimal rates in different parameter regimes. The optimality of the estimation protocols is shown by deriving compatible minimax lower bounds.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0SYkQ50imt"
        ],
        "venue": [
          "/venue/0SYkQ50imt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0SYkQ50imt"
        ],
        "detail": [
          "https://openreview.net/forum?id=0SYkQ50imt"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Refinement Methods for Distributed Distribution Estimation under ℓ p ℓ p -Losses [PDF ] [Copy] [Kimi ] [REL] Authors : Deheng Yuan , Tao Guo , Zhongyi Huang Consider the communication-constrained estimation of discrete distributions under ℓ p ℓ p losses, where each distributed terminal holds multiple independent samples and uses limited number of bits to describe the samples. We obtain the minimax optimal rates of the problem for most parameter regimes. As a result, an elbow effect of the optimal rates at p = 2 p = 2 is clearly identified. In order to achieve the optimal rates for different parameter regimes, we introduce refinement methods and develop additional customized techniques in the estimation protocols. The general idea of the refinement methods is to first generate rough estimate by partial information and then establish refined estimate in subsequent steps guided by the rough estimate. Then customized techniques such as successive refinement, sample compression, thresholding and random hashing are leveraged to achieve the optimal rates in different parameter regimes. The optimality of the estimation protocols is shown by deriving compatible minimax lower bounds. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "REHjkmWdQL@OpenReview",
      "index": 544,
      "title": "Measuring and Guiding Monosemanticity",
      "authors": [
        "Ruben Härle",
        "Felix Friedrich",
        "Manuel Brack",
        "Björn Deiseroth",
        "Stephan Waeldchen",
        "Patrick Schramowski",
        "Kristian Kersting"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "monosemanticity",
        "feature",
        "sae",
        "interpretability",
        "mechanistic",
        "latent",
        "autoencoders",
        "measuring",
        "saes",
        "guiding"
      ],
      "summary": "There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=REHjkmWdQL"
        ],
        "venue": [
          "/venue/REHjkmWdQL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=REHjkmWdQL"
        ],
        "detail": [
          "https://openreview.net/forum?id=REHjkmWdQL"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Measuring and Guiding Monosemanticity [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Ruben Härle , Felix Friedrich , Manuel Brack , Björn Deiseroth , Stephan Waeldchen , Patrick Schramowski , Kristian Kersting There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "KqhMpsWiz2@OpenReview",
      "index": 545,
      "title": "Amortized Variational Transdimensional Inference",
      "authors": [
        "Laurence Davies",
        "Dan MacKinlay",
        "Rafael Oliveira",
        "Scott A Sisson"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "transdimensional",
        "inference",
        "vti",
        "variational",
        "svi",
        "amortized",
        "bayesian",
        "normalizing",
        "flow",
        "cosmic"
      ],
      "summary": "The expressiveness of flow-based models combined with stochastic variational inference (SVI) has expanded the application of optimization-based Bayesian inference to highly complex problems. However, despite the importance of multi-model Bayesian inference, defined over a transdimensional joint model and parameter space, flow-based SVI has been limited to problems defined over a fixed-dimensional parameter space. We introduce CoSMIC normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables use of a single amortized variational density for inference over a transdimensional (multi-model) conditional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC flows using ideas from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments show the performance of VTI on challenging problems that scale to high-cardinality model spaces.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KqhMpsWiz2"
        ],
        "venue": [
          "/venue/KqhMpsWiz2@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KqhMpsWiz2"
        ],
        "detail": [
          "https://openreview.net/forum?id=KqhMpsWiz2"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Amortized Variational Transdimensional Inference [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Laurence Davies , Dan MacKinlay , Rafael Oliveira , Scott A Sisson The expressiveness of flow-based models combined with stochastic variational inference (SVI) has expanded the application of optimization-based Bayesian inference to highly complex problems. However, despite the importance of multi-model Bayesian inference, defined over a transdimensional joint model and parameter space, flow-based SVI has been limited to problems defined over a fixed-dimensional parameter space. We introduce CoSMIC normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables use of a single amortized variational density for inference over a transdimensional (multi-model) conditional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC flows using ideas from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments show the performance of VTI on challenging problems that scale to high-cardinality model spaces. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "u1j6RqH8nM@OpenReview",
      "index": 546,
      "title": "Scaling Laws For Scalable Oversight",
      "authors": [
        "Joshua Engels",
        "David D. Baek",
        "Subhash Kantamneni",
        "Max Tegmark"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "oversight",
        "nso",
        "wargames",
        "scalable",
        "mafia",
        "elo",
        "backdoor",
        "stronger",
        "game",
        "debate"
      ],
      "summary": "Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each game, we find scaling laws that approximate how domain performance depends on general AI system capability. We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. We also apply our theory to our four oversight games, where we find that NSO success rates at a general Elo gap of 400 are 13.5\\% for Mafia, 51.7\\% for Debate, 10.0\\% for Backdoor Code, and 9.4\\% for Wargames; these rates decline further when overseeing stronger systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=u1j6RqH8nM"
        ],
        "venue": [
          "/venue/u1j6RqH8nM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=u1j6RqH8nM"
        ],
        "detail": [
          "https://openreview.net/forum?id=u1j6RqH8nM"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Scaling Laws For Scalable Oversight [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Joshua Engels , David D. Baek , Subhash Kantamneni , Max Tegmark Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each game, we find scaling laws that approximate how domain performance depends on general AI system capability. We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. We also apply our theory to our four oversight games, where we find that NSO success rates at a general Elo gap of 400 are 13.5\\% for Mafia, 51.7\\% for Debate, 10.0\\% for Backdoor Code, and 9.4\\% for Wargames; these rates decline further when overseeing stronger systems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "raZEmZ48h4@OpenReview",
      "index": 547,
      "title": "A Near-Optimal Algorithm for Decentralized Convex-Concave Finite-Sum Minimax Optimization",
      "authors": [
        "Hongxu Chen",
        "Ke Wei",
        "Haishan Ye",
        "Luo Luo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "concave",
        "convex",
        "minimax",
        "decentralized",
        "rounds",
        "bounds",
        "sum",
        "optimization",
        "diverse",
        "optimistic"
      ],
      "summary": "In this paper, we study the distributed convex-concave finite-sum minimax optimization over the network, and a decentralized variance-reduced optimistic gradient method with stochastic mini-batch sizes (DIVERSE) is proposed. For the strongly-convex-strongly-concave objective, it is shown that DIVERSE can achieve a linear convergence rate that depends on the global smoothness parameters, yielding sharper computation and communication complexity bounds than existing results. Furthermore, we also establish the lower complexity bounds, which show that our upper bounds are optimal up to a logarithmic factor in terms of the local incremental first-order oracle calls, the computation rounds, and the communication rounds. Numerical experiments demonstrate that our algorithm outperforms existing methods in practice.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=raZEmZ48h4"
        ],
        "venue": [
          "/venue/raZEmZ48h4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=raZEmZ48h4"
        ],
        "detail": [
          "https://openreview.net/forum?id=raZEmZ48h4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "A Near-Optimal Algorithm for Decentralized Convex-Concave Finite-Sum Minimax Optimization [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hongxu Chen , Ke Wei , Haishan Ye , Luo Luo In this paper, we study the distributed convex-concave finite-sum minimax optimization over the network, and a decentralized variance-reduced optimistic gradient method with stochastic mini-batch sizes (DIVERSE) is proposed. For the strongly-convex-strongly-concave objective, it is shown that DIVERSE can achieve a linear convergence rate that depends on the global smoothness parameters, yielding sharper computation and communication complexity bounds than existing results. Furthermore, we also establish the lower complexity bounds, which show that our upper bounds are optimal up to a logarithmic factor in terms of the local incremental first-order oracle calls, the computation rounds, and the communication rounds. Numerical experiments demonstrate that our algorithm outperforms existing methods in practice. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Zapn9l2LMY@OpenReview",
      "index": 548,
      "title": "Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting",
      "authors": [
        "Wei Chen",
        "Yuxuan Liang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "forecasting",
        "ttc",
        "calibration",
        "spatio",
        "omputing",
        "patio",
        "emporal",
        "ime",
        "test",
        "temporal"
      ],
      "summary": "Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a _novel **t**est-**t**ime **c**omputing paradigm, namely learning with calibration, **ST-TTC**_, for **s**patio-**t**emporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. _**ST-TTC**_ effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Zapn9l2LMY"
        ],
        "venue": [
          "/venue/Zapn9l2LMY@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Zapn9l2LMY"
        ],
        "detail": [
          "https://openreview.net/forum?id=Zapn9l2LMY"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Wei Chen , Yuxuan Liang Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a _novel **t**est-**t**ime **c**omputing paradigm, namely learning with calibration, **ST-TTC**_, for **s**patio-**t**emporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. _**ST-TTC**_ effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "T62TYoF8R3@OpenReview",
      "index": 549,
      "title": "FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion",
      "authors": [
        "Akide Liu",
        "Zeyu Zhang",
        "Zhexin Li",
        "Xuehai Bai",
        "Yuanjie Xing",
        "Yizeng Han",
        "Jiasheng Tang",
        "Jichao Wu",
        "Mingyang Yang",
        "Weihua Chen",
        "Jiahao He",
        "Yuanyu He",
        "Fan Wang",
        "Gholamreza Haffari",
        "Bohan Zhuang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fpsattention",
        "sparsity",
        "fp8",
        "video",
        "quantization",
        "aware",
        "generation",
        "wan2",
        "speedup",
        "vbench"
      ],
      "summary": "Diffusion generative models have become the standard for producing high-quality, coherent video content, yet their slow inference speeds and high computational demands hinder practical deployment. Although both quantization and sparsity can independently accelerate inference while maintaining generation quality, naively combining these techniques in existing training-free approaches leads to significant performance degradation, as they fail to achieve proper joint optimization. We introduce FPSAttention, a novel training-aware co-design of FP8 quantization and Sparsity for video generation, with a focus on the 3D bi-directional attention mechanism. Our approach features three key innovations: 1) A unified 3D tile-wise granularity that simultaneously supports both quantization and sparsity. 2) A denoising step-aware strategy that adapts to the noise schedule, addressing the strong correlation between quantization/sparsity errors and denoising steps. 3) A native, hardware-friendly kernel that leverages FlashAttention and is implemented with optimized Hopper architecture features, enabling highly efficient execution. Trained on Wan2.1's 1.3B and 14B models and evaluated on the vBench benchmark, FPSAttention achieves a 7.09 × × kernel speedup for attention operations and a 4.96 × × end-to-end speedup for video generation compared to the BF16 baseline at 720p resolution—without sacrificing generation quality.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=T62TYoF8R3"
        ],
        "venue": [
          "/venue/T62TYoF8R3@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=T62TYoF8R3"
        ],
        "detail": [
          "https://openreview.net/forum?id=T62TYoF8R3"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Akide Liu , Zeyu Zhang , Zhexin Li , Xuehai Bai , Yuanjie Xing , Yizeng Han , Jiasheng Tang , Jichao Wu , Mingyang Yang , Weihua Chen , Jiahao He , Yuanyu He , Fan Wang , Gholamreza Haffari , Bohan Zhuang Diffusion generative models have become the standard for producing high-quality, coherent video content, yet their slow inference speeds and high computational demands hinder practical deployment. Although both quantization and sparsity can independently accelerate inference while maintaining generation quality, naively combining these techniques in existing training-free approaches leads to significant performance degradation, as they fail to achieve proper joint optimization. We introduce FPSAttention, a novel training-aware co-design of FP8 quantization and Sparsity for video generation, with a focus on the 3D bi-directional attention mechanism. Our approach features three key innovations: 1) A unified 3D tile-wise granularity that simultaneously supports both quantization and sparsity. 2) A denoising step-aware strategy that adapts to the noise schedule, addressing the strong correlation between quantization/sparsity errors and denoising steps. 3) A native, hardware-friendly kernel that leverages FlashAttention and is implemented with optimized Hopper architecture features, enabling highly efficient execution. Trained on Wan2.1's 1.3B and 14B models and evaluated on the vBench benchmark, FPSAttention achieves a 7.09 × × kernel speedup for attention operations and a 4.96 × × end-to-end speedup for video generation compared to the BF16 baseline at 720p resolution—without sacrificing generation quality. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "paRLw86ONU@OpenReview",
      "index": 550,
      "title": "Disentangled Concepts Speak Louder Than Words: Explainable Video Action Recognition",
      "authors": [
        "Jongseo Lee",
        "Wooil Lee",
        "Gyeong-Moon Park",
        "Seong Tae Kim",
        "Jinwoo Choi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dance",
        "disentangled",
        "concepts",
        "action",
        "video",
        "louder",
        "explainable",
        "recognition",
        "haa500",
        "speak"
      ],
      "summary": "Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods—based on saliency—produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature—intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets—KTH, Penn Action, HAA500, and UCF101—demonstrate that DANCE significantly improves explanation clarity with competitive performance. Through a user study, we validate the superior interpretability of DANCE. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=paRLw86ONU"
        ],
        "venue": [
          "/venue/paRLw86ONU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=paRLw86ONU"
        ],
        "detail": [
          "https://openreview.net/forum?id=paRLw86ONU"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Disentangled Concepts Speak Louder Than Words: Explainable Video Action Recognition [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jongseo Lee , Wooil Lee , Gyeong-Moon Park , Seong Tae Kim , Jinwoo Choi Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods—based on saliency—produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature—intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets—KTH, Penn Action, HAA500, and UCF101—demonstrate that DANCE significantly improves explanation clarity with competitive performance. Through a user study, we validate the superior interpretability of DANCE. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "HiBoJLCyEo@OpenReview",
      "index": 551,
      "title": "Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection",
      "authors": [
        "Shuhai Zhang",
        "ZiHao Lian",
        "Jiahao Yang",
        "Daiyuan Li",
        "Guoxuan Pang",
        "Feng Liu",
        "Bo Han",
        "Shutao Li",
        "Mingkui Tan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "nsg",
        "videos",
        "spatiotemporal",
        "video",
        "generated",
        "detection",
        "zshsh98",
        "modeling",
        "gradients",
        "sora"
      ],
      "summary": "AI-generated videos have achieved near-perfect visual realism (e.g., Sora), urgently necessitating reliable detection mechanisms. However, detecting such videos faces significant challenges in modeling high-dimensional spatiotemporal dynamics and identifying subtle anomalies that violate physical laws. In this paper, we propose a physics-driven AI-generated video detection paradigm based on probability flow conservation principles. Specifically, we propose a statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the ratio of spatial probability gradients to temporal density changes, explicitly capturing deviations from natural video dynamics. Leveraging pre-trained diffusion models, we develop an NSG estimator through spatial gradients approximation and motion-aware temporal modeling without complex motion decomposition while preserving physical constraints. Building on this, we propose an NSG-based video detection method (NSG-VD) that computes the Maximum Mean Discrepancy (MMD) between NSG features of the test and real videos as a detection metric. Last, we derive an upper bound of NSG feature distances between real and generated videos, proving that generated videos exhibit amplified discrepancies due to distributional shifts. Extensive experiments confirm that NSG-VD outperforms state-of-the-art baselines by 16.00\\% in Recall and 10.75\\% in F1-Score, validating the superior performance of NSG-VD. The source code is available at \\url{https://github.com/ZSHsh98/NSG-VD}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HiBoJLCyEo"
        ],
        "venue": [
          "/venue/HiBoJLCyEo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HiBoJLCyEo"
        ],
        "detail": [
          "https://openreview.net/forum?id=HiBoJLCyEo"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Shuhai Zhang , ZiHao Lian , Jiahao Yang , Daiyuan Li , Guoxuan Pang , Feng Liu , Bo Han , Shutao Li , Mingkui Tan AI-generated videos have achieved near-perfect visual realism (e.g., Sora), urgently necessitating reliable detection mechanisms. However, detecting such videos faces significant challenges in modeling high-dimensional spatiotemporal dynamics and identifying subtle anomalies that violate physical laws. In this paper, we propose a physics-driven AI-generated video detection paradigm based on probability flow conservation principles. Specifically, we propose a statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the ratio of spatial probability gradients to temporal density changes, explicitly capturing deviations from natural video dynamics. Leveraging pre-trained diffusion models, we develop an NSG estimator through spatial gradients approximation and motion-aware temporal modeling without complex motion decomposition while preserving physical constraints. Building on this, we propose an NSG-based video detection method (NSG-VD) that computes the Maximum Mean Discrepancy (MMD) between NSG features of the test and real videos as a detection metric. Last, we derive an upper bound of NSG feature distances between real and generated videos, proving that generated videos exhibit amplified discrepancies due to distributional shifts. Extensive experiments confirm that NSG-VD outperforms state-of-the-art baselines by 16.00\\% in Recall and 10.75\\% in F1-Score, validating the superior performance of NSG-VD. The source code is available at \\url{https://github.com/ZSHsh98/NSG-VD}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "vATe64ktAo@OpenReview",
      "index": 552,
      "title": "Robust Graph Condensation via Classification Complexity Mitigation",
      "authors": [
        "Jiayi Luo",
        "Qingyun Sun",
        "Beining Yang",
        "Haonan Yuan",
        "Xingcheng Fu",
        "Yanbiao Ma",
        "Jianxin Li",
        "Philip S. Yu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "graph",
        "mrgc",
        "condensation",
        "robustness",
        "manifold",
        "ondensation",
        "robust",
        "classification",
        "complexity",
        "anifold"
      ],
      "summary": "Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel **M**anifold-constrained **R**obust **G**raph **C**ondensation framework named **MRGC**. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of MRGC across diverse attack scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vATe64ktAo"
        ],
        "venue": [
          "/venue/vATe64ktAo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vATe64ktAo"
        ],
        "detail": [
          "https://openreview.net/forum?id=vATe64ktAo"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Robust Graph Condensation via Classification Complexity Mitigation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jiayi Luo , Qingyun Sun , Beining Yang , Haonan Yuan , Xingcheng Fu , Yanbiao Ma , Jianxin Li , Philip S. Yu Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel **M**anifold-constrained **R**obust **G**raph **C**ondensation framework named **MRGC**. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of MRGC across diverse attack scenarios. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RblaNJGx8C@OpenReview",
      "index": 553,
      "title": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning",
      "authors": [
        "Vahid Balazadeh",
        "Hamidreza Kamkari",
        "Valentin Thomas",
        "Junwei Ma",
        "Bingru Li",
        "Jesse C. Cresswell",
        "Rahul Krishnan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "causalpfn",
        "causal",
        "estimation",
        "ihdp",
        "acic",
        "amortized",
        "pfns",
        "lalonde",
        "amortizes",
        "effect"
      ],
      "summary": "Causal effect estimation from observational data is fundamental across various applications. However, selecting an appropriate estimator from dozens of specialized methods demands substantial manual effort and domain expertise. We present CausalPFN, a single transformer that *amortizes* this workflow: trained once on a large library of simulated data-generating processes that satisfy ignorability, it infers causal effects for new observational datasets out of the box. CausalPFN combines ideas from Bayesian causal inference with the large-scale training protocol of prior-fitted networks (PFNs), learning to map raw observations directly to causal effects without any task-specific adjustment. Our approach achieves superior average performance on heterogeneous and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC). Moreover, it shows competitive performance for real-world policy making on uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to support reliable decision-making based on Bayesian principles. This ready-to-use model requires no further training or tuning and takes a step toward automated causal inference (https://github.com/vdblm/CausalPFN/).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RblaNJGx8C"
        ],
        "venue": [
          "/venue/RblaNJGx8C@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RblaNJGx8C"
        ],
        "detail": [
          "https://openreview.net/forum?id=RblaNJGx8C"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Vahid Balazadeh , Hamidreza Kamkari , Valentin Thomas , Junwei Ma , Bingru Li , Jesse C. Cresswell , Rahul Krishnan Causal effect estimation from observational data is fundamental across various applications. However, selecting an appropriate estimator from dozens of specialized methods demands substantial manual effort and domain expertise. We present CausalPFN, a single transformer that *amortizes* this workflow: trained once on a large library of simulated data-generating processes that satisfy ignorability, it infers causal effects for new observational datasets out of the box. CausalPFN combines ideas from Bayesian causal inference with the large-scale training protocol of prior-fitted networks (PFNs), learning to map raw observations directly to causal effects without any task-specific adjustment. Our approach achieves superior average performance on heterogeneous and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC). Moreover, it shows competitive performance for real-world policy making on uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to support reliable decision-making based on Bayesian principles. This ready-to-use model requires no further training or tuning and takes a step toward automated causal inference (https://github.com/vdblm/CausalPFN/). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "iFIjNXb0Y5@OpenReview",
      "index": 554,
      "title": "High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction",
      "authors": [
        "Seongsu Kim",
        "Nayoung Kim",
        "Dongwoo Kim",
        "Sungsoo Ahn"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "qhflow",
        "hamiltonian",
        "equivariant",
        "scf",
        "matching",
        "flow",
        "qh9",
        "dft",
        "hamiltonians",
        "md17"
      ],
      "summary": "Density functional theory (DFT) is a fundamental method for simulating quantum chemical properties, but it remains expensive due to the iterative self-consistent field (SCF) process required to solve the Kohn–Sham equations. Recently, deep learning methods are gaining attention as a way to bypass this step by directly predicting the Hamiltonian. However, they rely on deterministic regression and do not consider the highly structured nature of Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow matching framework that generates Hamiltonian matrices conditioned on molecular geometry. Flow matching models continuous-time trajectories between simple priors and complex targets, learning the structured distributions over Hamiltonians instead of direct regression. To further incorporate symmetry, we use a neural architecture that predicts SE(3)-equivariant vector fields, improving accuracy and generalization across diverse geometries. To further enhance physical fidelity, we additionally introduce a fine-tuning scheme to align predicted orbital energies with the target. QHFlow achieves state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53% on QH9. Moreover, we further show that QHFlow accelerates the DFT process without trading off the solution quality when initializing SCF iterations with the predicted Hamiltonian, significantly reducing the number of iterations and runtime.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=iFIjNXb0Y5"
        ],
        "venue": [
          "/venue/iFIjNXb0Y5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=iFIjNXb0Y5"
        ],
        "detail": [
          "https://openreview.net/forum?id=iFIjNXb0Y5"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Seongsu Kim , Nayoung Kim , Dongwoo Kim , Sungsoo Ahn Density functional theory (DFT) is a fundamental method for simulating quantum chemical properties, but it remains expensive due to the iterative self-consistent field (SCF) process required to solve the Kohn–Sham equations. Recently, deep learning methods are gaining attention as a way to bypass this step by directly predicting the Hamiltonian. However, they rely on deterministic regression and do not consider the highly structured nature of Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow matching framework that generates Hamiltonian matrices conditioned on molecular geometry. Flow matching models continuous-time trajectories between simple priors and complex targets, learning the structured distributions over Hamiltonians instead of direct regression. To further incorporate symmetry, we use a neural architecture that predicts SE(3)-equivariant vector fields, improving accuracy and generalization across diverse geometries. To further enhance physical fidelity, we additionally introduce a fine-tuning scheme to align predicted orbital energies with the target. QHFlow achieves state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53% on QH9. Moreover, we further show that QHFlow accelerates the DFT process without trading off the solution quality when initializing SCF iterations with the predicted Hamiltonian, significantly reducing the number of iterations and runtime. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "X2r9D46kvI@OpenReview",
      "index": 555,
      "title": "🎧MOSPA: Human Motion Generation Driven by Spatial Audio",
      "authors": [
        "Shuyang Xu",
        "Zhiyang Dou",
        "Mingyi Shi",
        "Liang Pan",
        "Leo Ho",
        "Jingbo Wang",
        "Yuan Liu",
        "Cheng Lin",
        "Yuexin Ma",
        "Wenping Wang",
        "Taku Komura"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mospa",
        "audio",
        "motion",
        "spatial",
        "human",
        "driven",
        "diverse",
        "generation",
        "benchmarking",
        "xsy27"
      ],
      "summary": "Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive \"Spatial Audio-Driven Human Motion\" (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human \"MOtion generation driven by SPatial Audio,\" termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA can generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our code and model are publicly available at https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation.git",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=X2r9D46kvI"
        ],
        "venue": [
          "/venue/X2r9D46kvI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=X2r9D46kvI"
        ],
        "detail": [
          "https://openreview.net/forum?id=X2r9D46kvI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "🎧MOSPA: Human Motion Generation Driven by Spatial Audio [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Shuyang Xu , Zhiyang Dou , Mingyi Shi , Liang Pan , Leo Ho , Jingbo Wang , Yuan Liu , Cheng Lin , Yuexin Ma , Wenping Wang , Taku Komura Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive \"Spatial Audio-Driven Human Motion\" (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human \"MOtion generation driven by SPatial Audio,\" termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA can generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our code and model are publicly available at https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation.git Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "neZSGqhxDa@OpenReview",
      "index": 556,
      "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
      "authors": [
        "Andrew Zhao",
        "Yiran Wu",
        "Yang Yue",
        "Tong Wu",
        "Quentin Xu",
        "Matthieu Lin",
        "Shenzhi Wang",
        "Qingyun Wu",
        "Zilong Zheng",
        "Gao Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "azr",
        "reasoning",
        "rlvr",
        "zero",
        "human",
        "absolute",
        "verifiable",
        "tasks",
        "self",
        "rewards"
      ],
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from rule-based outcome rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external human or distillation data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability. AZR uses a code executor to both validate self-proposed code reasoning tasks and verify answers, serving as an unified source of verifiable feedback to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=neZSGqhxDa"
        ],
        "venue": [
          "/venue/neZSGqhxDa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=neZSGqhxDa"
        ],
        "detail": [
          "https://openreview.net/forum?id=neZSGqhxDa"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Andrew Zhao , Yiran Wu , Yang Yue , Tong Wu , Quentin Xu , Yang Yue , Matthieu Lin , Shenzhi Wang , Qingyun Wu , Zilong Zheng , Gao Huang Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from rule-based outcome rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external human or distillation data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability. AZR uses a code executor to both validate self-proposed code reasoning tasks and verify answers, serving as an unified source of verifiable feedback to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Vws7eXQXsa@OpenReview",
      "index": 557,
      "title": "Graph–Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry",
      "authors": [
        "Masanari Kimura"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "shift",
        "graph",
        "priors",
        "smoothed",
        "bbse",
        "confusion",
        "bayesian",
        "box",
        "target",
        "mid"
      ],
      "summary": "Label shift adaptation aims to recover target class priors when the labelled source distribution P P and the unlabelled target distribution Q Q share P ( X ∣ Y ) = Q ( X ∣ Y ) P ( X ∣ Y ) = Q ( X ∣ Y ) but P ( Y ) ≠ Q ( Y ) P ( Y ) ≠ Q ( Y ) . Classical black‑box shift estimators invert an empirical confusion matrix of a frozen classifier, producing a brittle point estimate that ignores sampling noise and similarity among classes. We present Graph‑Smoothed Bayesian BBSE (GS‑B 3 3 SE), a fully probabilistic alternative that places Laplacian–Gaussian priors on both target log‑priors and confusion‑matrix columns, tying them together on a label‑similarity graph. The resulting posterior is tractable with HMC or a fast block Newton–CG scheme. We prove identifiability, N − 1 / 2 N − 1 / 2 contraction, variance bounds that shrink with the graph’s algebraic connectivity, and robustness to Laplacian misspecification. We also reinterpret GS‑B 3 3 SE through information geometry, showing that it generalizes existing shift estimators.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Vws7eXQXsa"
        ],
        "venue": [
          "/venue/Vws7eXQXsa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Vws7eXQXsa"
        ],
        "detail": [
          "https://openreview.net/forum?id=Vws7eXQXsa"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Graph–Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry [PDF 1 ] [Copy] [Kimi ] [REL] Author : Masanari Kimura Label shift adaptation aims to recover target class priors when the labelled source distribution P P and the unlabelled target distribution Q Q share P ( X ∣ Y ) = Q ( X ∣ Y ) P ( X ∣ Y ) = Q ( X ∣ Y ) but P ( Y ) ≠ Q ( Y ) P ( Y ) ≠ Q ( Y ) . Classical black‑box shift estimators invert an empirical confusion matrix of a frozen classifier, producing a brittle point estimate that ignores sampling noise and similarity among classes. We present Graph‑Smoothed Bayesian BBSE (GS‑B 3 3 SE), a fully probabilistic alternative that places Laplacian–Gaussian priors on both target log‑priors and confusion‑matrix columns, tying them together on a label‑similarity graph. The resulting posterior is tractable with HMC or a fast block Newton–CG scheme. We prove identifiability, N − 1 / 2 N − 1 / 2 contraction, variance bounds that shrink with the graph’s algebraic connectivity, and robustness to Laplacian misspecification. We also reinterpret GS‑B 3 3 SE through information geometry, showing that it generalizes existing shift estimators. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bA02DmQN5d@OpenReview",
      "index": 558,
      "title": "Vision Transformers Don't Need Trained Registers",
      "authors": [
        "Nicholas Jiang",
        "Amil Dravid",
        "Alexei A Efros",
        "Yossi Gandelsman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "register",
        "tokens",
        "registers",
        "norm",
        "trained",
        "neurons",
        "cleaner",
        "vision",
        "attention",
        "transformers"
      ],
      "summary": "We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps (Darcet et al., 2024). We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens register tokens , we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models, yielding cleaner attention-based, text-to-image attribution. Finally, we outline a simple mathematical model that reflects the observed behavior of register neurons and high norm tokens. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bA02DmQN5d"
        ],
        "venue": [
          "/venue/bA02DmQN5d@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bA02DmQN5d"
        ],
        "detail": [
          "https://openreview.net/forum?id=bA02DmQN5d"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Vision Transformers Don't Need Trained Registers [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Nicholas Jiang , Amil Dravid , Alexei A Efros , Yossi Gandelsman We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps (Darcet et al., 2024). We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens register tokens , we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models, yielding cleaner attention-based, text-to-image attribution. Finally, we outline a simple mathematical model that reflects the observed behavior of register neurons and high norm tokens. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "kp9B9iQDIt@OpenReview",
      "index": 559,
      "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos",
      "authors": [
        "Zhao-Heng Yin",
        "Sherry Yang",
        "Pieter Abbeel"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "videos",
        "robot",
        "motion",
        "centric",
        "object",
        "human",
        "field",
        "action",
        "learning",
        "extract"
      ],
      "summary": "Learning robot control policies from human videos is a promising direction for scaling up robot learning. However, how to extract action knowledge (or action representations) from videos for policy learning remains a key challenge. Existing action representations such as video frames, pixelflow, and pointcloud flow have inherent limitations such as modeling complexity or loss of information. In this paper, we propose to use object-centric 3D motion field to represent actions for robot learning from human videos, and present a novel framework for extracting this representation from videos for zero-shot control. We introduce two novel components. First, a novel training pipeline for training a ``denoising'' 3D motion field estimator to extract fine object 3D motions from human videos with noisy depth robustly. Second, a dense object-centric 3D motion field prediction architecture that favors both cross-embodiment transfer and policy generalization to background. We evaluate the system in real world setups. Experiments show that our method reduces 3D motion estimation error by over 50% compared to the latest method, achieve 55% average success rate in diverse tasks where prior approaches fail ( ≲ 10 ≲ 10 \\%), and can even acquire fine-grained manipulation skills like insertion.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kp9B9iQDIt"
        ],
        "venue": [
          "/venue/kp9B9iQDIt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kp9B9iQDIt"
        ],
        "detail": [
          "https://openreview.net/forum?id=kp9B9iQDIt"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Object-centric 3D Motion Field for Robot Learning from Human Videos [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zhao-Heng Yin , Sherry Yang , Pieter Abbeel Learning robot control policies from human videos is a promising direction for scaling up robot learning. However, how to extract action knowledge (or action representations) from videos for policy learning remains a key challenge. Existing action representations such as video frames, pixelflow, and pointcloud flow have inherent limitations such as modeling complexity or loss of information. In this paper, we propose to use object-centric 3D motion field to represent actions for robot learning from human videos, and present a novel framework for extracting this representation from videos for zero-shot control. We introduce two novel components. First, a novel training pipeline for training a ``denoising'' 3D motion field estimator to extract fine object 3D motions from human videos with noisy depth robustly. Second, a dense object-centric 3D motion field prediction architecture that favors both cross-embodiment transfer and policy generalization to background. We evaluate the system in real world setups. Experiments show that our method reduces 3D motion estimation error by over 50% compared to the latest method, achieve 55% average success rate in diverse tasks where prior approaches fail ( ≲ 10 ≲ 10 \\%), and can even acquire fine-grained manipulation skills like insertion. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "WPU17d1l7R@OpenReview",
      "index": 560,
      "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation",
      "authors": [
        "Shuo Yang",
        "Haocheng Xi",
        "Yilong Zhao",
        "Muyang Li",
        "Jintao Zhang",
        "Han Cai",
        "Yujun Lin",
        "Xiuyu Li",
        "Chenfeng Xu",
        "Kelly Peng",
        "Jianfei Chen",
        "Song Han",
        "Kurt Keutzer",
        "Ion Stoica"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tokens",
        "svg2",
        "sparse",
        "critical",
        "computation",
        "generation",
        "waste",
        "videogen2",
        "permutation",
        "identification"
      ],
      "summary": "Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates Top-p dynamic budget control and customized kernel implementations, achieving up to 2.30 × 2.30 × and 1.89 × 1.89 × speedup while maintaining a PSNR of up to 30 30 and 26 26 on HunyuanVideo and Wan 2.1, respectively. Our code is open-sourced at https://github.com/svg-project/Sparse-VideoGen.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WPU17d1l7R"
        ],
        "venue": [
          "/venue/WPU17d1l7R@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WPU17d1l7R"
        ],
        "detail": [
          "https://openreview.net/forum?id=WPU17d1l7R"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Shuo Yang , Haocheng Xi , Yilong Zhao , Muyang Li , Jintao Zhang , Han Cai , Yujun Lin , Xiuyu Li , Chenfeng Xu , Kelly Peng , Jianfei Chen , Song Han , Kurt Keutzer , Ion Stoica Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates Top-p dynamic budget control and customized kernel implementations, achieving up to 2.30 × 2.30 × and 1.89 × 1.89 × speedup while maintaining a PSNR of up to 30 30 and 26 26 on HunyuanVideo and Wan 2.1, respectively. Our code is open-sourced at https://github.com/svg-project/Sparse-VideoGen. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "w97lDmoD0U@OpenReview",
      "index": 561,
      "title": "Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations",
      "authors": [
        "Olawale Elijah Salaudeen",
        "Haoran Zhang",
        "Kumail Alhamoud",
        "Sara Beery",
        "Marzyeh Ghassemi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ood",
        "accuracy",
        "oodselect",
        "subsets",
        "spurious",
        "hides",
        "distribution",
        "failures",
        "generalization",
        "correlations"
      ],
      "summary": "Benchmarks for out-of-distribution (OOD) generalization often reveal a strong positive correlation between in-distribution (ID) and OOD accuracy across models, a phenomenon known as “accuracy-on-the-line.” This pattern is commonly interpreted as evidence that spurious correlations—relationships that improve ID but harm OOD performance—are rare in practice. We show that this positive correlation can be an artifact of aggregating heterogeneous OOD examples. Using a simple gradient-based method, OODSelect, we identify semantically coherent OOD subsets where accuracy-on-the-line breaks down. Across widely used distribution-shift benchmarks, OODSelect uncovers subsets—sometimes comprising more than half of the standard OOD set—where higher ID accuracy predicts lower OOD accuracy. These results suggest that aggregate metrics can mask critical failure modes in OOD robustness. We release code and the identified subsets to support further research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=w97lDmoD0U"
        ],
        "venue": [
          "/venue/w97lDmoD0U@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=w97lDmoD0U"
        ],
        "detail": [
          "https://openreview.net/forum?id=w97lDmoD0U"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations [PDF ] [Copy] [Kimi ] [REL] Authors : Olawale Elijah Salaudeen , Haoran Zhang , Kumail Alhamoud , Sara Beery , Marzyeh Ghassemi Benchmarks for out-of-distribution (OOD) generalization often reveal a strong positive correlation between in-distribution (ID) and OOD accuracy across models, a phenomenon known as “accuracy-on-the-line.” This pattern is commonly interpreted as evidence that spurious correlations—relationships that improve ID but harm OOD performance—are rare in practice. We show that this positive correlation can be an artifact of aggregating heterogeneous OOD examples. Using a simple gradient-based method, OODSelect, we identify semantically coherent OOD subsets where accuracy-on-the-line breaks down. Across widely used distribution-shift benchmarks, OODSelect uncovers subsets—sometimes comprising more than half of the standard OOD set—where higher ID accuracy predicts lower OOD accuracy. These results suggest that aggregate metrics can mask critical failure modes in OOD robustness. We release code and the identified subsets to support further research. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "CTsdZ3j6dR@OpenReview",
      "index": 562,
      "title": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning",
      "authors": [
        "Emile Timothy Anand",
        "Ishani Karmarkar",
        "Guannan Qu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "textbf",
        "agents",
        "policy",
        "agent",
        "subsample",
        "mfq",
        "ield",
        "reinforcement",
        "ean",
        "cooperative"
      ],
      "summary": "Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm SUBSAMPLE-MFQ SUBSAMPLE-MFQ ( Subsample Subsample - M M ean- F F ield- Q Q -learning) and a decentralized randomized policy for a system with n n agents. For any k ≤ n k ≤ n , our algorithm learns a policy for the system in time polynomial in k k . We prove that this learned policy converges to the optimal policy on the order of O ~ ( 1 / k − − √ ) O ~ ( 1 / k ) as the number of subsampled agents k k increases. In particular, this bound is independent of the number of agents n n .",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CTsdZ3j6dR"
        ],
        "venue": [
          "/venue/CTsdZ3j6dR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CTsdZ3j6dR"
        ],
        "detail": [
          "https://openreview.net/forum?id=CTsdZ3j6dR"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Emile Timothy Anand , Ishani Karmarkar , Guannan Qu Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm SUBSAMPLE-MFQ SUBSAMPLE-MFQ ( Subsample Subsample - M M ean- F F ield- Q Q -learning) and a decentralized randomized policy for a system with n n agents. For any k ≤ n k ≤ n , our algorithm learns a policy for the system in time polynomial in k k . We prove that this learned policy converges to the optimal policy on the order of O ~ ( 1 / k − − √ ) O ~ ( 1 / k ) as the number of subsampled agents k k increases. In particular, this bound is independent of the number of agents n n . Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Lf0W2gmNBg@OpenReview",
      "index": 563,
      "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
      "authors": [
        "Xiaoshan Wu",
        "Yifei Yu",
        "Xiaoyang Lyu",
        "Yi-Hua Huang",
        "Bo Wang",
        "Baoheng Zhang",
        "Zhongrui Wang",
        "XIAOJUAN QI"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "eag3r",
        "event",
        "estimation",
        "reconstruction",
        "geometry",
        "dynamic",
        "rgb",
        "scenes",
        "pointmap",
        "monst3r"
      ],
      "summary": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose \\textbf{EAG3R}, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Lf0W2gmNBg"
        ],
        "venue": [
          "/venue/Lf0W2gmNBg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Lf0W2gmNBg"
        ],
        "detail": [
          "https://openreview.net/forum?id=Lf0W2gmNBg"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Xiaoshan Wu , Yifei Yu , Xiaoyang Lyu , Yi-Hua Huang , Bo Wang , Baoheng Zhang , Zhongrui Wang , XIAOJUAN QI Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose \\textbf{EAG3R}, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6WnBITpnzD@OpenReview",
      "index": 564,
      "title": "LaViDa: A Large Diffusion Model for Vision-Language Understanding",
      "authors": [
        "Shufan Li",
        "Konstantinos Kallidromitis",
        "Hritik Bansal",
        "Akash Gokul",
        "Yusuke Kato",
        "Kazuki Kozuka",
        "Jason Kuen",
        "Zhe Lin",
        "Kai-Wei Chang",
        "Aditya Grover"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lavida",
        "vlms",
        "dms",
        "bidirectional",
        "llava",
        "vision",
        "jacklishufan",
        "language",
        "tasks",
        "inference"
      ],
      "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models is available at https://github.com/jacklishufan/LaViDa",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6WnBITpnzD"
        ],
        "venue": [
          "/venue/6WnBITpnzD@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6WnBITpnzD"
        ],
        "detail": [
          "https://openreview.net/forum?id=6WnBITpnzD"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "LaViDa: A Large Diffusion Model for Vision-Language Understanding [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Shufan Li , Konstantinos Kallidromitis , Hritik Bansal , Akash Gokul , Yusuke Kato , Kazuki Kozuka , Jason Kuen , Zhe Lin , Kai-Wei Chang , Aditya Grover Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models is available at https://github.com/jacklishufan/LaViDa Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "0Az25lvdT2@OpenReview",
      "index": 565,
      "title": "Theory-Driven Label-Specific Representation for Incomplete Multi-View Multi-Label Learning",
      "authors": [
        "Quanjiang Li",
        "Tianxiang Xu",
        "Tingjin Luo",
        "Yan Zhong",
        "Yang Li",
        "Yiyun Zhou",
        "Chenping Hou"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "label",
        "tdlsr",
        "specific",
        "representation",
        "view",
        "multi",
        "geneous",
        "driven",
        "incomplete",
        "learning"
      ],
      "summary": "Multi-view multi-label learning typically suffers from dual data incompleteness due to limitations in feature storage and annotation costs. The interplay of hetero geneous features, numerous labels, and missing information significantly degrades model performance. To tackle the complex yet highly practical challenges, we propose a Theory-Driven Label-Specific Representation (TDLSR) framework. Through constructing the view-specific sample topology and prototype association graph, we develop the proximity-aware imputation mechanism, while deriving class representatives that capture the label correlation semantics. To obtain semantically distinct view representations, we introduce principles of information shift, inter action and orthogonality, which promotes the disentanglement of representation information, and mitigates message distortion and redundancy. Besides, label semantic-guided feature learning is employed to identify the discriminative shared and specific representations and refine the label preference across views. Moreover, we theoretically investigate the characteristics of representation learning and the generalization performance. Finally, extensive experiments on public datasets and real-world applications validate the effectiveness of TDLSR.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0Az25lvdT2"
        ],
        "venue": [
          "/venue/0Az25lvdT2@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0Az25lvdT2"
        ],
        "detail": [
          "https://openreview.net/forum?id=0Az25lvdT2"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Theory-Driven Label-Specific Representation for Incomplete Multi-View Multi-Label Learning [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Quanjiang Li , Tianxiang Xu , Tingjin Luo , Yan Zhong , Yang Li , Yiyun Zhou , Chenping Hou Multi-view multi-label learning typically suffers from dual data incompleteness due to limitations in feature storage and annotation costs. The interplay of hetero geneous features, numerous labels, and missing information significantly degrades model performance. To tackle the complex yet highly practical challenges, we propose a Theory-Driven Label-Specific Representation (TDLSR) framework. Through constructing the view-specific sample topology and prototype association graph, we develop the proximity-aware imputation mechanism, while deriving class representatives that capture the label correlation semantics. To obtain semantically distinct view representations, we introduce principles of information shift, inter action and orthogonality, which promotes the disentanglement of representation information, and mitigates message distortion and redundancy. Besides, label semantic-guided feature learning is employed to identify the discriminative shared and specific representations and refine the label preference across views. Moreover, we theoretically investigate the characteristics of representation learning and the generalization performance. Finally, extensive experiments on public datasets and real-world applications validate the effectiveness of TDLSR. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "CRyOyiVvvJ@OpenReview",
      "index": 566,
      "title": "Scalable Fingerprinting of Large Language Models",
      "authors": [
        "Anshul Nasery",
        "Jonathan Hayase",
        "Creston Brooks",
        "Peiyao Sheng",
        "Himanshu Tyagi",
        "Pramod Viswanath",
        "Sewoong Oh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fingerprinting",
        "fingerprints",
        "scalable",
        "model",
        "fingerprint",
        "scalability",
        "risks",
        "schemes",
        "perinucleus",
        "harmless"
      ],
      "summary": "Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. In order to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that scaling up the number of fingerprints one can embed into a model, i.e. *Scalability* of fingerprints, is critical. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. We experiment with fingerprint design at a scale significantly larger than previously considered, and introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model---two orders of magnitude more than existing schemes---without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CRyOyiVvvJ"
        ],
        "venue": [
          "/venue/CRyOyiVvvJ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CRyOyiVvvJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=CRyOyiVvvJ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Scalable Fingerprinting of Large Language Models [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Anshul Nasery , Jonathan Hayase , Creston Brooks , Peiyao Sheng , Himanshu Tyagi , Pramod Viswanath , Sewoong Oh Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. In order to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that scaling up the number of fingerprints one can embed into a model, i.e. *Scalability* of fingerprints, is critical. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. We experiment with fingerprint design at a scale significantly larger than previously considered, and introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model---two orders of magnitude more than existing schemes---without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "po0eyoYFUa@OpenReview",
      "index": 567,
      "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?",
      "authors": [
        "Junchi Yu",
        "Yujie Liu",
        "Jindong Gu",
        "Philip Torr",
        "Dongzhan Zhou"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "graphflow",
        "kgs",
        "retrieval",
        "rag",
        "retrieve",
        "reward",
        "knowledge",
        "queries",
        "diverse",
        "rich"
      ],
      "summary": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances large language models (LLMs) by providing structured and interpretable external knowledge. However, existing KG-based RAG methods struggle to retrieve accurate and diverse information from text-rich KGs for complex real-world queries. Process Reward Models (PRMs) offer a way to align the retrieval process of KG-based RAG with query-specific knowledge requirements, but they heavily rely on process-level supervision signals that are expensive and hard to obtain on KGs. To address this challenge, we propose GraphFlow, a framework that efficiently retrieves accurate and diverse knowledge required for real-world queries from text-rich KGs. GraphFlow employs a transition-based flow matching objective to jointly optimize a retrieval policy and a flow estimator. The flow estimator factorizes the reward of the retrieval outcome into the intermediate retrieval states. Such reward factorization guides the retrieval policy to retrieve candidates from KGs in proportion to their reward. This allows GraphFlow to explore high-quality regions of KGs that yield diverse and relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. GraphFlow outperforms strong KG-RAG baselines, including GPT-4o, by 10\\% on average in hit rate and recall. It also shows strong generalization to unseen KGs, demonstrating its effectiveness and robustness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=po0eyoYFUa"
        ],
        "venue": [
          "/venue/po0eyoYFUa@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=po0eyoYFUa"
        ],
        "detail": [
          "https://openreview.net/forum?id=po0eyoYFUa"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need? [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Junchi Yu , Yujie Liu , Jindong Gu , Philip Torr , Dongzhan Zhou Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances large language models (LLMs) by providing structured and interpretable external knowledge. However, existing KG-based RAG methods struggle to retrieve accurate and diverse information from text-rich KGs for complex real-world queries. Process Reward Models (PRMs) offer a way to align the retrieval process of KG-based RAG with query-specific knowledge requirements, but they heavily rely on process-level supervision signals that are expensive and hard to obtain on KGs. To address this challenge, we propose GraphFlow, a framework that efficiently retrieves accurate and diverse knowledge required for real-world queries from text-rich KGs. GraphFlow employs a transition-based flow matching objective to jointly optimize a retrieval policy and a flow estimator. The flow estimator factorizes the reward of the retrieval outcome into the intermediate retrieval states. Such reward factorization guides the retrieval policy to retrieve candidates from KGs in proportion to their reward. This allows GraphFlow to explore high-quality regions of KGs that yield diverse and relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. GraphFlow outperforms strong KG-RAG baselines, including GPT-4o, by 10\\% on average in hit rate and recall. It also shows strong generalization to unseen KGs, demonstrating its effectiveness and robustness. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yPnEvPq3kV@OpenReview",
      "index": 568,
      "title": "Decomposing Interventional Causality into Synergistic, Redundant, and Unique Components",
      "authors": [
        "Abel Jansma"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "interventional",
        "causal",
        "decomposing",
        "synergistic",
        "decomposition",
        "redundant",
        "möbius",
        "causality",
        "unique",
        "components"
      ],
      "summary": "We introduce a novel framework for decomposing interventional causal effects into synergistic, redundant, and unique components, building on the intuition of Partial Information Decomposition (PID) and the principle of Möbius inversion. While recent work has explored a similar decomposition of an observational measure, we argue that a proper causal decomposition must be interventional in nature. We develop a mathematical approach that systematically quantifies how causal power is distributed among variables in a system, using a recently derived closed-form expression for the Möbius function of the redundancy lattice. The formalism is then illustrated by decomposing the causal power in logic gates, cellular automata, and chemical reaction networks. Our results reveal how the distribution of causal power can be context- and parameter-dependent. The decomposition provides new insights into complex systems by revealing how causal influences are shared and combined among multiple variables, with potential applications ranging from attribution of responsibility in legal or AI systems, to the analysis of biological networks or climate models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yPnEvPq3kV"
        ],
        "venue": [
          "/venue/yPnEvPq3kV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yPnEvPq3kV"
        ],
        "detail": [
          "https://openreview.net/forum?id=yPnEvPq3kV"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Decomposing Interventional Causality into Synergistic, Redundant, and Unique Components [PDF 1 ] [Copy] [Kimi 1 ] [REL] Author : Abel Jansma We introduce a novel framework for decomposing interventional causal effects into synergistic, redundant, and unique components, building on the intuition of Partial Information Decomposition (PID) and the principle of Möbius inversion. While recent work has explored a similar decomposition of an observational measure, we argue that a proper causal decomposition must be interventional in nature. We develop a mathematical approach that systematically quantifies how causal power is distributed among variables in a system, using a recently derived closed-form expression for the Möbius function of the redundancy lattice. The formalism is then illustrated by decomposing the causal power in logic gates, cellular automata, and chemical reaction networks. Our results reveal how the distribution of causal power can be context- and parameter-dependent. The decomposition provides new insights into complex systems by revealing how causal influences are shared and combined among multiple variables, with potential applications ranging from attribution of responsibility in legal or AI systems, to the analysis of biological networks or climate models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "UBsYf2lyNE@OpenReview",
      "index": 569,
      "title": "Long-Tailed Recognition via Information-Preservable Two-Stage Learning",
      "authors": [
        "Fudong Lin",
        "Xu Yuan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "stage",
        "preservable",
        "tailed",
        "tail",
        "majority",
        "biased",
        "long",
        "information",
        "fudong03",
        "ipdpp"
      ],
      "summary": "The imbalance (or long-tail) is the nature of many real-world data distributions, which often induces the undesirable bias of deep classification models toward frequent classes, resulting in poor performance for tail classes. In this paper, we propose a novel two-stage learning approach to mitigate such a majority-biased tendency while preserving valuable information within datasets. Specifically, the first stage proposes a new representation learning technique from the information theory perspective. This approach is theoretically equivalent to minimizing intra-class distance, yielding an effective and well-separated feature space. The second stage develops a novel sampling strategy that selects mathematically informative instances, able to rectify majority-biased decision boundaries without compromising a model’s overall performance. As a result, our approach achieves the state-of-the-art performance across various long-tailed benchmark datasets, validated via extensive experiments. Our code is available at https://github.com/fudong03/BNS_IPDPP.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UBsYf2lyNE"
        ],
        "venue": [
          "/venue/UBsYf2lyNE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UBsYf2lyNE"
        ],
        "detail": [
          "https://openreview.net/forum?id=UBsYf2lyNE"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Long-Tailed Recognition via Information-Preservable Two-Stage Learning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Fudong Lin , Xu Yuan The imbalance (or long-tail) is the nature of many real-world data distributions, which often induces the undesirable bias of deep classification models toward frequent classes, resulting in poor performance for tail classes. In this paper, we propose a novel two-stage learning approach to mitigate such a majority-biased tendency while preserving valuable information within datasets. Specifically, the first stage proposes a new representation learning technique from the information theory perspective. This approach is theoretically equivalent to minimizing intra-class distance, yielding an effective and well-separated feature space. The second stage develops a novel sampling strategy that selects mathematically informative instances, able to rectify majority-biased decision boundaries without compromising a model’s overall performance. As a result, our approach achieves the state-of-the-art performance across various long-tailed benchmark datasets, validated via extensive experiments. Our code is available at https://github.com/fudong03/BNS_IPDPP. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "zWHKKspghT@OpenReview",
      "index": 570,
      "title": "Mozart: Modularized and Efficient MoE Training on 3.5D Wafer-Scale Chiplet Architectures",
      "authors": [
        "Shuqing Luo",
        "Ye Han",
        "Pingzhi Li",
        "Jiayin Qin",
        "Jie Peng",
        "Yang Katie Zhao",
        "Yu Cao",
        "Tianlong Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mozart",
        "moe",
        "modularized",
        "chiplet",
        "texttt",
        "chiplets",
        "wafer",
        "llms",
        "architectures",
        "experts"
      ],
      "summary": "Mixture-of-Experts (MoE) architecture offers enhanced efficiency for Large Language Models (LLMs) with modularized computation, yet its inherent sparsity poses significant hardware deployment challenges, including memory locality issues, communication overhead, and inefficient computing resource utilization. Inspired by the modular organization of the human brain, we propose Mozart Mozart , a novel algorithm-hardware co-design framework tailored for efficient training of MoE-based LLMs on 3.5D wafer-scale chiplet architectures. On the algorithm side, Mozart Mozart exploits the inherent modularity of chiplets and introduces: ( 1 1 ) an expert allocation strategy that enables efficient on-package all-to-all communication, and ( 2 2 ) a fine-grained scheduling mechanism that improves communication-computation overlap through streaming tokens and experts. On the architecture side, Mozart Mozart adaptively co-locates heterogeneous modules on specialized chiplets with a 2.5D NoP-Tree topology and hierarchical memory structure. Evaluation across three popular MoE models demonstrates significant efficiency gains, enabling more effective parallelization and resource utilization for large-scale modularized MoE-LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zWHKKspghT"
        ],
        "venue": [
          "/venue/zWHKKspghT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zWHKKspghT"
        ],
        "detail": [
          "https://openreview.net/forum?id=zWHKKspghT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Mozart: Modularized and Efficient MoE Training on 3.5D Wafer-Scale Chiplet Architectures [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Shuqing Luo , Ye Han , Pingzhi Li , Jiayin Qin , Jie Peng , Yang Katie Zhao , Yu Cao , Tianlong Chen Mixture-of-Experts (MoE) architecture offers enhanced efficiency for Large Language Models (LLMs) with modularized computation, yet its inherent sparsity poses significant hardware deployment challenges, including memory locality issues, communication overhead, and inefficient computing resource utilization. Inspired by the modular organization of the human brain, we propose Mozart Mozart , a novel algorithm-hardware co-design framework tailored for efficient training of MoE-based LLMs on 3.5D wafer-scale chiplet architectures. On the algorithm side, Mozart Mozart exploits the inherent modularity of chiplets and introduces: ( 1 1 ) an expert allocation strategy that enables efficient on-package all-to-all communication, and ( 2 2 ) a fine-grained scheduling mechanism that improves communication-computation overlap through streaming tokens and experts. On the architecture side, Mozart Mozart adaptively co-locates heterogeneous modules on specialized chiplets with a 2.5D NoP-Tree topology and hierarchical memory structure. Evaluation across three popular MoE models demonstrates significant efficiency gains, enabling more effective parallelization and resource utilization for large-scale modularized MoE-LLMs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "tQZK5frjVU@OpenReview",
      "index": 571,
      "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
      "authors": [
        "Xinran Gu",
        "Kaifeng Lyu",
        "Jiazheng Li",
        "Jingzhao Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mixing",
        "transitions",
        "knowledge",
        "biographies",
        "memorizes",
        "ratio",
        "size",
        "acquisition",
        "dense",
        "model"
      ],
      "summary": "Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets—unlike training exclusively on knowledge-dense data—does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tQZK5frjVU"
        ],
        "venue": [
          "/venue/tQZK5frjVU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tQZK5frjVU"
        ],
        "detail": [
          "https://openreview.net/forum?id=tQZK5frjVU"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xinran Gu , Kaifeng Lyu , Jiazheng Li , Jingzhao Zhang Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets—unlike training exclusively on knowledge-dense data—does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Vj56Z9yNCr@OpenReview",
      "index": 572,
      "title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dim Subspaces in Diffusion Models",
      "authors": [
        "Wenda Li",
        "Huijie Zhang",
        "Qing Qu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "watermarking",
        "shallow",
        "diffuse",
        "diffusion",
        "watermark",
        "invisible",
        "subspace",
        "generation",
        "dim",
        "robust"
      ],
      "summary": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce *Shallow Diffuse*, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, *Shallow Diffuse* decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that *Shallow Diffuse* outperforms existing watermarking methods in terms of consistency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Vj56Z9yNCr"
        ],
        "venue": [
          "/venue/Vj56Z9yNCr@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Vj56Z9yNCr"
        ],
        "detail": [
          "https://openreview.net/forum?id=Vj56Z9yNCr"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dim Subspaces in Diffusion Models [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Wenda Li , Huijie Zhang , Qing Qu The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce *Shallow Diffuse*, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, *Shallow Diffuse* decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that *Shallow Diffuse* outperforms existing watermarking methods in terms of consistency. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4P6Mployhf@OpenReview",
      "index": 573,
      "title": "Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies",
      "authors": [
        "Runze Yan",
        "Xun Shen",
        "Akifumi Wachi",
        "Sebastien Gros",
        "Anni Zhao",
        "Xiao Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ogsrl",
        "clinician",
        "ood",
        "mathsf",
        "offline",
        "safe",
        "guarded",
        "treatment",
        "policy",
        "safety"
      ],
      "summary": "When applying offline reinforcement learning (RL) in healthcare scenarios, the out-of-distribution (OOD) issues pose significant risks, as inappropriate generalization beyond clinical expertise can result in potentially harmful recommendations. While existing methods like conservative Q-learning (CQL) attempt to address the OOD issue, their effectiveness is limited by only constraining action selection by suppressing uncertain actions. This action-only regularization imitates clinician actions that prioritize short-term rewards, but it fails to regulate downstream state trajectories, thereby limiting the discovery of improved long-term treatment strategies. To safely improve policy beyond clinician recommendations while ensuring that state-action trajectories remain in-distribution, we propose \\textit{Offline Guarded Safe Reinforcement Learning} ( O G S R L O G S R L ), a theoretically grounded model-based offline RL framework. O G S R L O G S R L introduces a novel dual constraint mechanism for improving policy with reliability and safety. First, the OOD guardian is established to specify clinically validated regions for safe policy exploration. By constraining optimization within these regions, it enables the reliable exploration of treatment strategies that outperform clinician behavior by leveraging the full patient state history, without drifting into unsupported state-action trajectories. Second, we introduce a safety cost constraint that encodes medical knowledge about physiological safety boundaries, providing domain-specific safeguards even in areas where training data might contain potentially unsafe interventions. Notably, we provide theoretical guarantees on safety and near-optimality: policies that satisfy these constraints remain in safe and reliable regions and achieve performance close to the best possible policy supported by the data. When evaluated on the MIMIC-III sepsis treatment dataset, O G S R L O G S R L demonstrated significantly better OOD handling than baselines. O G S R L O G S R L achieved a 78\\% reduction in mortality estimates and a 51\\% increase in reward compared to clinician decisions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4P6Mployhf"
        ],
        "venue": [
          "/venue/4P6Mployhf@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4P6Mployhf"
        ],
        "detail": [
          "https://openreview.net/forum?id=4P6Mployhf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Runze Yan , Xun Shen , Akifumi Wachi , Sebastien Gros , Anni Zhao , Xiao Hu When applying offline reinforcement learning (RL) in healthcare scenarios, the out-of-distribution (OOD) issues pose significant risks, as inappropriate generalization beyond clinical expertise can result in potentially harmful recommendations. While existing methods like conservative Q-learning (CQL) attempt to address the OOD issue, their effectiveness is limited by only constraining action selection by suppressing uncertain actions. This action-only regularization imitates clinician actions that prioritize short-term rewards, but it fails to regulate downstream state trajectories, thereby limiting the discovery of improved long-term treatment strategies. To safely improve policy beyond clinician recommendations while ensuring that state-action trajectories remain in-distribution, we propose \\textit{Offline Guarded Safe Reinforcement Learning} ( O G S R L O G S R L ), a theoretically grounded model-based offline RL framework. O G S R L O G S R L introduces a novel dual constraint mechanism for improving policy with reliability and safety. First, the OOD guardian is established to specify clinically validated regions for safe policy exploration. By constraining optimization within these regions, it enables the reliable exploration of treatment strategies that outperform clinician behavior by leveraging the full patient state history, without drifting into unsupported state-action trajectories. Second, we introduce a safety cost constraint that encodes medical knowledge about physiological safety boundaries, providing domain-specific safeguards even in areas where training data might contain potentially unsafe interventions. Notably, we provide theoretical guarantees on safety and near-optimality: policies that satisfy these constraints remain in safe and reliable regions and achieve performance close to the best possible policy supported by the data. When evaluated on the MIMIC-III sepsis treatment dataset, O G S R L O G S R L demonstrated significantly better OOD handling than baselines. O G S R L O G S R L achieved a 78\\% reduction in mortality estimates and a 51\\% increase in reward compared to clinician decisions. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "6vcgsrK6pN@OpenReview",
      "index": 574,
      "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
      "authors": [
        "Awa Khouna",
        "Julien Ferry",
        "Thibaut Vidal"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "trees",
        "extraction",
        "attacks",
        "explainability",
        "counterfactuals",
        "model",
        "competitive",
        "analysis",
        "mlaas",
        "security"
      ],
      "summary": "The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the \"oracle'' queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6vcgsrK6pN"
        ],
        "venue": [
          "/venue/6vcgsrK6pN@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6vcgsrK6pN"
        ],
        "detail": [
          "https://openreview.net/forum?id=6vcgsrK6pN"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Awa Khouna , Julien Ferry , Thibaut Vidal The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the \"oracle'' queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fZsd3KLMje@OpenReview",
      "index": 575,
      "title": "Repo2Run: Automated Building Executable Environment for Code Repository at Scale",
      "authors": [
        "Ruida Hu",
        "Chao Peng",
        "XinchenWang",
        "Junjielong Xu",
        "Cuiyun Gao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "repo2run",
        "executable",
        "repositories",
        "dockerfile",
        "building",
        "code",
        "tests",
        "docker",
        "repository",
        "environments"
      ],
      "summary": "Scaling up executable code data is significant for improving language models’ software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fZsd3KLMje"
        ],
        "venue": [
          "/venue/fZsd3KLMje@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fZsd3KLMje"
        ],
        "detail": [
          "https://openreview.net/forum?id=fZsd3KLMje"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Repo2Run: Automated Building Executable Environment for Code Repository at Scale [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ruida Hu , Chao Peng , XinchenWang , Junjielong Xu , Cuiyun Gao Scaling up executable code data is significant for improving language models’ software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "pOJBw1YQgL@OpenReview",
      "index": 576,
      "title": "Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery",
      "authors": [
        "Ming Hu",
        "Zhengdi Yu",
        "Feilong Tang",
        "Kaiwen Chen",
        "Yulong Li",
        "Imran Razzak",
        "Junjun He",
        "Tolga Birdal",
        "Kaijing Zhou",
        "Zongyuan Ge"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "instrument",
        "hand",
        "ophthalmic",
        "reconstruction",
        "ophnet",
        "surgery",
        "interaction",
        "microsurgery",
        "annotation",
        "mano"
      ],
      "summary": "Accurate 3D reconstruction of hands and instruments is critical for vision-based analysis of ophthalmic microsurgery, yet progress has been hampered by the lack of realistic, large-scale datasets and reliable annotation tools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic 3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from 40 surgeons and totaling 7.1 million frames, with fine-grained annotations of 12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full 6-DoF instrument poses. To scalably produce high-fidelity labels, we design a multi-stage automatic annotation pipeline that integrates multi-view data observation, data-driven motion prior with cross-view geometric consistency and biomechanical constraints, along with a combination of collision-aware interaction constraints for instrument interactions. Building upon OphNet-3D, we establish two challenging benchmarks—bimanual hand pose estimation and hand–instrument interaction reconstruction—and propose two dedicated architectures: H-Net for dual-hand mesh recovery and OH-Net for joint reconstruction of two-hand–two-instrument interactions. These models leverage a novel spatial reasoning module with weak-perspective camera modeling and collision-aware center-based representation. Both architectures outperform existing methods by substantial margins, achieving improvements of over 2mm in Mean Per Joint Position Error (MPJPE) and up to 23\\% in ADD-S metrics for hand and instrument reconstruction, respectively.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pOJBw1YQgL"
        ],
        "venue": [
          "/venue/pOJBw1YQgL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pOJBw1YQgL"
        ],
        "detail": [
          "https://openreview.net/forum?id=pOJBw1YQgL"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Ming Hu , Zhengdi Yu , Feilong Tang , Kaiwen Chen , Yulong Li , Imran Razzak , Junjun He , Tolga Birdal , Kaijing Zhou , Zongyuan Ge Accurate 3D reconstruction of hands and instruments is critical for vision-based analysis of ophthalmic microsurgery, yet progress has been hampered by the lack of realistic, large-scale datasets and reliable annotation tools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic 3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from 40 surgeons and totaling 7.1 million frames, with fine-grained annotations of 12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full 6-DoF instrument poses. To scalably produce high-fidelity labels, we design a multi-stage automatic annotation pipeline that integrates multi-view data observation, data-driven motion prior with cross-view geometric consistency and biomechanical constraints, along with a combination of collision-aware interaction constraints for instrument interactions. Building upon OphNet-3D, we establish two challenging benchmarks—bimanual hand pose estimation and hand–instrument interaction reconstruction—and propose two dedicated architectures: H-Net for dual-hand mesh recovery and OH-Net for joint reconstruction of two-hand–two-instrument interactions. These models leverage a novel spatial reasoning module with weak-perspective camera modeling and collision-aware center-based representation. Both architectures outperform existing methods by substantial margins, achieving improvements of over 2mm in Mean Per Joint Position Error (MPJPE) and up to 23\\% in ADD-S metrics for hand and instrument reconstruction, respectively. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "c6O18DyBBx@OpenReview",
      "index": 577,
      "title": "Unlocking Dataset Distillation with Diffusion Models",
      "authors": [
        "Brian Bernhard Moser",
        "Federico Raue",
        "Sebastian Palacio",
        "Stanislav Frolov",
        "Andreas Dengel"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ld3m",
        "distillation",
        "diffusion",
        "ipc",
        "dataset",
        "times256",
        "unlocking",
        "times128",
        "brian",
        "synthetic"
      ],
      "summary": "Dataset distillation seeks to condense datasets into smaller but highly representative synthetic samples. While diffusion models now lead all generative benchmarks, current distillation methods avoid them and rely instead on GANs or autoencoders, or, at best, sampling from a fixed diffusion prior. This trend arises because naive backpropagation through the long denoising chain leads to vanishing gradients, which prevents effective synthetic sample optimization. To address this limitation, we introduce Latent Dataset Distillation with Diffusion Models (LD3M), the first method to learn gradient-based distilled latents and class embeddings end-to-end through a pre-trained latent diffusion model. A linearly decaying skip connection, injected from the initial noisy state into every reverse step, preserves the gradient signal across dozens of timesteps without requiring diffusion weight fine-tuning. Across multiple ImageNet subsets at 128 × 128 128 × 128 and 256 × 256 256 × 256 , LD3M improves downstream accuracy by up to 4.8 percentage points (1 IPC) and 4.2 points (10 IPC) over the prior state-of-the-art. The code for LD3M is provided at https://github.com/Brian-Moser/prune_and_distill.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=c6O18DyBBx"
        ],
        "venue": [
          "/venue/c6O18DyBBx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=c6O18DyBBx"
        ],
        "detail": [
          "https://openreview.net/forum?id=c6O18DyBBx"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Unlocking Dataset Distillation with Diffusion Models [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Brian Bernhard Moser , Federico Raue , Sebastian Palacio , Stanislav Frolov , Andreas Dengel Dataset distillation seeks to condense datasets into smaller but highly representative synthetic samples. While diffusion models now lead all generative benchmarks, current distillation methods avoid them and rely instead on GANs or autoencoders, or, at best, sampling from a fixed diffusion prior. This trend arises because naive backpropagation through the long denoising chain leads to vanishing gradients, which prevents effective synthetic sample optimization. To address this limitation, we introduce Latent Dataset Distillation with Diffusion Models (LD3M), the first method to learn gradient-based distilled latents and class embeddings end-to-end through a pre-trained latent diffusion model. A linearly decaying skip connection, injected from the initial noisy state into every reverse step, preserves the gradient signal across dozens of timesteps without requiring diffusion weight fine-tuning. Across multiple ImageNet subsets at 128 × 128 128 × 128 and 256 × 256 256 × 256 , LD3M improves downstream accuracy by up to 4.8 percentage points (1 IPC) and 4.2 points (10 IPC) over the prior state-of-the-art. The code for LD3M is provided at https://github.com/Brian-Moser/prune_and_distill. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YdggdEL41C@OpenReview",
      "index": 578,
      "title": "Vision-centric Token Compression in Large Language Model",
      "authors": [
        "Ling Xing",
        "Alex Jinpeng Wang",
        "Rui Yan",
        "Xiangbo Shu",
        "Jinhui Tang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "textbf",
        "compression",
        "token",
        "tokens",
        "textit",
        "vist",
        "vision",
        "centric",
        "nlui",
        "cepe"
      ],
      "summary": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression token compression indispensable. We introduce Vision Centric Token Compression ( Vist Vist ), a slow–fast slow–fast compression framework that mirrors human reading: the fast fast path renders distant tokens into images, letting a frozen, lightweight vision encoder frozen, lightweight vision encoder skim the low-salience context; the slow slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions—just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist Vist achieves the same accuracy with 2.3 × × fewer tokens, cutting FLOPs by 16\\% and memory by 50\\%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6 7.6 \\% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The source code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YdggdEL41C"
        ],
        "venue": [
          "/venue/YdggdEL41C@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YdggdEL41C"
        ],
        "detail": [
          "https://openreview.net/forum?id=YdggdEL41C"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Vision-centric Token Compression in Large Language Model [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Ling Xing , Alex Jinpeng Wang , Rui Yan , Xiangbo Shu , Jinhui Tang Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression token compression indispensable. We introduce Vision Centric Token Compression ( Vist Vist ), a slow–fast slow–fast compression framework that mirrors human reading: the fast fast path renders distant tokens into images, letting a frozen, lightweight vision encoder frozen, lightweight vision encoder skim the low-salience context; the slow slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions—just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist Vist achieves the same accuracy with 2.3 × × fewer tokens, cutting FLOPs by 16\\% and memory by 50\\%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6 7.6 \\% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The source code will be released. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "8PUzLga3lU@OpenReview",
      "index": 579,
      "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
      "authors": [
        "Chaoyou Fu",
        "Haojia Lin",
        "Xiong Wang",
        "YiFan Zhang",
        "Yunhang Shen",
        "Xiaoyu Liu",
        "Haoyu Cao",
        "Zuwei Long",
        "Heting Gao",
        "Ke Li",
        "Long MA",
        "Xiawu Zheng",
        "Rongrong Ji",
        "Xing Sun",
        "Caifeng Shan",
        "Ran He"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "speech",
        "vision",
        "interaction",
        "multimodal",
        "omni",
        "vita",
        "visual",
        "dialogue",
        "capabilities",
        "gpt"
      ],
      "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing against state-of-the-art counterparts across benchmarks for image, video, and speech, we demonstrate that our omni model is equipped with both strong visual and speech capabilities, making omni understanding and interaction.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8PUzLga3lU"
        ],
        "venue": [
          "/venue/8PUzLga3lU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8PUzLga3lU"
        ],
        "detail": [
          "https://openreview.net/forum?id=8PUzLga3lU"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Chaoyou Fu , Haojia Lin , Xiong Wang , YiFan Zhang , Yunhang Shen , Xiaoyu Liu , Haoyu Cao , Zuwei Long , Heting Gao , Ke Li , Long MA , Xiawu Zheng , Rongrong Ji , Xing Sun , Caifeng Shan , Ran He Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing against state-of-the-art counterparts across benchmarks for image, video, and speech, we demonstrate that our omni model is equipped with both strong visual and speech capabilities, making omni understanding and interaction. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ph1V6n7BSv@OpenReview",
      "index": 580,
      "title": "EDELINE: Enhancing Memory in Diffusion-based World Models via Linear-Time Sequence Modeling",
      "authors": [
        "Jia-Hua Lee",
        "Bor-Jiun Lin",
        "Wei-Fang Sun",
        "Chun-Yi Lee"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "edeline",
        "world",
        "memory",
        "diffusion",
        "models",
        "crafter",
        "vizdoom",
        "reinforcement",
        "ljh",
        "100k"
      ],
      "summary": "World models represent a promising approach for training reinforcement learning agents with significantly improved sample efficiency. While most world model methods primarily rely on sequences of discrete latent variables to model environment dynamics, this compression often neglects critical visual details essential for reinforcement learning. Recent diffusion-based world models condition generation on a fixed context length of frames to predict the next observation, using separate recurrent neural networks to model rewards and termination signals. Although this architecture effectively enhances visual fidelity, the fixed context length approach inherently limits memory capacity. In this paper, we introduce EDELINE, a unified world model architecture that integrates state space models with diffusion models. Our approach outperforms existing baselines across visually challenging Atari 100k tasks, memory-demanding Crafter benchmark, and 3D first-person ViZDoom environments, demonstrating superior performance in all these diverse challenges. Code is available at https://github.com/LJH-coding/EDELINE.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ph1V6n7BSv"
        ],
        "venue": [
          "/venue/ph1V6n7BSv@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ph1V6n7BSv"
        ],
        "detail": [
          "https://openreview.net/forum?id=ph1V6n7BSv"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "EDELINE: Enhancing Memory in Diffusion-based World Models via Linear-Time Sequence Modeling [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Jia-Hua Lee , Bor-Jiun Lin , Wei-Fang Sun , Chun-Yi Lee World models represent a promising approach for training reinforcement learning agents with significantly improved sample efficiency. While most world model methods primarily rely on sequences of discrete latent variables to model environment dynamics, this compression often neglects critical visual details essential for reinforcement learning. Recent diffusion-based world models condition generation on a fixed context length of frames to predict the next observation, using separate recurrent neural networks to model rewards and termination signals. Although this architecture effectively enhances visual fidelity, the fixed context length approach inherently limits memory capacity. In this paper, we introduce EDELINE, a unified world model architecture that integrates state space models with diffusion models. Our approach outperforms existing baselines across visually challenging Atari 100k tasks, memory-demanding Crafter benchmark, and 3D first-person ViZDoom environments, demonstrating superior performance in all these diverse challenges. Code is available at https://github.com/LJH-coding/EDELINE. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "f6aOPkGE8L@OpenReview",
      "index": 581,
      "title": "UniTok: a Unified Tokenizer for Visual Generation and Understanding",
      "authors": [
        "Chuofan Ma",
        "Yi Jiang",
        "Junfeng Wu",
        "Jihan Yang",
        "Xin Yu",
        "Zehuan Yuan",
        "BINGYUE PENG",
        "XIAOJUAN QI"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "unitok",
        "tokenizer",
        "generation",
        "understanding",
        "unified",
        "256",
        "gfid",
        "visual",
        "bottleneck",
        "imagenet"
      ],
      "summary": "Visual generative and understanding models typically rely on distinct tokenizers to process images, presenting a key challenge for unifying them within a single framework. Recent studies attempt to address this by connecting the training of VQVAE (for autoregressive generation) and CLIP (for understanding) to build a unified tokenizer. However, directly combining these training objectives has been observed to cause severe loss conflicts. In this paper, we show that reconstruction and semantic supervision do not inherently conflict. Instead, the underlying bottleneck stems from limited representational capacity of discrete token space. Building on these insights, we introduce UniTok, a unified tokenizer featuring a novel multi-codebook quantization mechanism that effectively scales up the vocabulary size and bottleneck dimension. In terms of final performance, UniTok sets a new record of 0.38 rFID and 78.6\\% zero-shot accuracy on ImageNet. Besides, UniTok can be seamlessly integrated into MLLMs to unlock native visual generation capability, without compromising the understanding performance. Additionally, we show that UniTok favors cfg-free generation, reducing gFID from 14.6 to 2.5 on ImageNet 256 × × 256 benchmark. All codes and models have been made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=f6aOPkGE8L"
        ],
        "venue": [
          "/venue/f6aOPkGE8L@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=f6aOPkGE8L"
        ],
        "detail": [
          "https://openreview.net/forum?id=f6aOPkGE8L"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "UniTok: a Unified Tokenizer for Visual Generation and Understanding [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Chuofan Ma , Yi Jiang , Junfeng Wu , Jihan Yang , Xin Yu , Zehuan Yuan , BINGYUE PENG , XIAOJUAN QI Visual generative and understanding models typically rely on distinct tokenizers to process images, presenting a key challenge for unifying them within a single framework. Recent studies attempt to address this by connecting the training of VQVAE (for autoregressive generation) and CLIP (for understanding) to build a unified tokenizer. However, directly combining these training objectives has been observed to cause severe loss conflicts. In this paper, we show that reconstruction and semantic supervision do not inherently conflict. Instead, the underlying bottleneck stems from limited representational capacity of discrete token space. Building on these insights, we introduce UniTok, a unified tokenizer featuring a novel multi-codebook quantization mechanism that effectively scales up the vocabulary size and bottleneck dimension. In terms of final performance, UniTok sets a new record of 0.38 rFID and 78.6\\% zero-shot accuracy on ImageNet. Besides, UniTok can be seamlessly integrated into MLLMs to unlock native visual generation capability, without compromising the understanding performance. Additionally, we show that UniTok favors cfg-free generation, reducing gFID from 14.6 to 2.5 on ImageNet 256 × × 256 benchmark. All codes and models have been made publicly available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "LP4Q7tPMbs@OpenReview",
      "index": 582,
      "title": "NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data",
      "authors": [
        "Azadeh Motamedi",
        "Jae-Mo Kang",
        "Il-Min Kim"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "normfit",
        "shot",
        "layernorm",
        "iid",
        "vlms",
        "lightweight",
        "federated",
        "performance",
        "communication",
        "demands"
      ],
      "summary": "Vision–Language Models (VLMs) have recently attracted considerable attention in Federated Learning (FL) due to their strong and robust performance. In particular, few-shot adaptation with pre-trained VLMs like CLIP enhances the performance of downstream tasks. However, existing methods still suffer from substantial communication overhead, high local computational demands, and suboptimal performance under non-IID user data. To simultaneously address all those limitations, we propose NormFit, a lightweight solution that selectively fine-tunes only a very small portion of the model parameters, specifically only the Pre-LayerNorm parameters of the vision encoder within a VLM. Overcoming the existing tradeoff between performance and communication/computation efficiency in few-shot FL, NormFit sets a new benchmark by simultaneously achieving superior accuracy and substantially reduced communication and computational demands. Theoretically, we show that NormFit yields a considerably smaller generalization gap compared to tuning all LayerNorm parameters. Importantly, NormFit can function effectively as a standalone solution or integrate seamlessly with existing few-shot fine-tuning methods to further enhance their performance. Notably, NormFit offers implementation simplicity, achieving these improvements without any algorithmic modifications, changes to the underlying model architecture, or the addition of external parameters.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LP4Q7tPMbs"
        ],
        "venue": [
          "/venue/LP4Q7tPMbs@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LP4Q7tPMbs"
        ],
        "detail": [
          "https://openreview.net/forum?id=LP4Q7tPMbs"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Azadeh Motamedi , Jae-Mo Kang , Il-Min Kim Vision–Language Models (VLMs) have recently attracted considerable attention in Federated Learning (FL) due to their strong and robust performance. In particular, few-shot adaptation with pre-trained VLMs like CLIP enhances the performance of downstream tasks. However, existing methods still suffer from substantial communication overhead, high local computational demands, and suboptimal performance under non-IID user data. To simultaneously address all those limitations, we propose NormFit, a lightweight solution that selectively fine-tunes only a very small portion of the model parameters, specifically only the Pre-LayerNorm parameters of the vision encoder within a VLM. Overcoming the existing tradeoff between performance and communication/computation efficiency in few-shot FL, NormFit sets a new benchmark by simultaneously achieving superior accuracy and substantially reduced communication and computational demands. Theoretically, we show that NormFit yields a considerably smaller generalization gap compared to tuning all LayerNorm parameters. Importantly, NormFit can function effectively as a standalone solution or integrate seamlessly with existing few-shot fine-tuning methods to further enhance their performance. Notably, NormFit offers implementation simplicity, achieving these improvements without any algorithmic modifications, changes to the underlying model architecture, or the addition of external parameters. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "m5wrqqcWbN@OpenReview",
      "index": 583,
      "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs",
      "authors": [
        "Insu Lee",
        "Wooje Park",
        "Jaeyun Jang",
        "Minyoung Noh",
        "Kyuhong Shim",
        "Byonghyo Shim"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lvlms",
        "scene",
        "m3cot",
        "egocentric",
        "exocentric",
        "person",
        "views",
        "view",
        "leeinsu1",
        "comprehensive"
      ],
      "summary": "Large vision-language models (LVLMs) are increasingly deployed in interactive applications such as virtual and augmented reality, where a first-person (egocentric) view captured by head-mounted cameras serves as key input. While this view offers fine-grained cues about user attention and hand-object interactions, its narrow field of view and lack of global context often lead to failures on spatially or contextually demanding queries. To address this, we introduce a framework that augments egocentric inputs with third-person (exocentric) views, providing complementary information such as global scene layout and object visibility to LVLMs. We present E3VQA, the first benchmark for multi-view question answering with 4K high-quality question-answer pairs grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a training-free prompting technique that constructs a unified scene representation by integrating scene graphs from three complementary perspectives. M3CoT enables LVLMs to reason more effectively across views, yielding consistent performance gains (4.84\\% for GPT-4o and 5.94\\% for Gemini 2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key strengths and limitations of LVLMs in multi-view reasoning and highlights the value of leveraging both egocentric and exocentric inputs. The dataset and source code are available at [https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding](https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=m5wrqqcWbN"
        ],
        "venue": [
          "/venue/m5wrqqcWbN@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=m5wrqqcWbN"
        ],
        "detail": [
          "https://openreview.net/forum?id=m5wrqqcWbN"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Insu Lee , Wooje Park , Jaeyun Jang , Minyoung Noh , Kyuhong Shim , Byonghyo Shim Large vision-language models (LVLMs) are increasingly deployed in interactive applications such as virtual and augmented reality, where a first-person (egocentric) view captured by head-mounted cameras serves as key input. While this view offers fine-grained cues about user attention and hand-object interactions, its narrow field of view and lack of global context often lead to failures on spatially or contextually demanding queries. To address this, we introduce a framework that augments egocentric inputs with third-person (exocentric) views, providing complementary information such as global scene layout and object visibility to LVLMs. We present E3VQA, the first benchmark for multi-view question answering with 4K high-quality question-answer pairs grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a training-free prompting technique that constructs a unified scene representation by integrating scene graphs from three complementary perspectives. M3CoT enables LVLMs to reason more effectively across views, yielding consistent performance gains (4.84\\% for GPT-4o and 5.94\\% for Gemini 2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key strengths and limitations of LVLMs in multi-view reasoning and highlights the value of leveraging both egocentric and exocentric inputs. The dataset and source code are available at [https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding](https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "4frj038M6W@OpenReview",
      "index": 584,
      "title": "Accelerating Visual-Policy Learning through Parallel Differentiable Simulation",
      "authors": [
        "Haoxiang You",
        "Yilang Liu",
        "Ian Abraham"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "policy",
        "differentiable",
        "simulation",
        "humanoid",
        "visual",
        "rendering",
        "gpu",
        "dva",
        "reduces",
        "attenuates"
      ],
      "summary": "In this work, we propose a computationally efficient algorithm for visual policy learning that leverages differentiable simulation and first-order analytical policy gradients. Our approach decouple the rendering process from the computation graph, enabling seamless integration with existing differentiable simulation ecosystems without the need for specialized differentiable rendering software. This decoupling not only reduces computational and memory overhead but also effectively attenuates the policy gradient norm, leading to more stable and smoother optimization. We evaluate our method on standard visual control benchmarks using modern GPU-accelerated simulation. Experiments show that our approach significantly reduces wall-clock training time and consistently outperforms all baseline methods in terms of final returns. Notably, on complex tasks such as humanoid locomotion, our method achieves a 4 × 4 × improvement in final return, and successfully learns a humanoid running policy within 4 hours on a single GPU. Videos and code are available on https://haoxiangyou.github.io/Dva_website",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4frj038M6W"
        ],
        "venue": [
          "/venue/4frj038M6W@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4frj038M6W"
        ],
        "detail": [
          "https://openreview.net/forum?id=4frj038M6W"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Accelerating Visual-Policy Learning through Parallel Differentiable Simulation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Haoxiang You , Yilang Liu , Ian Abraham In this work, we propose a computationally efficient algorithm for visual policy learning that leverages differentiable simulation and first-order analytical policy gradients. Our approach decouple the rendering process from the computation graph, enabling seamless integration with existing differentiable simulation ecosystems without the need for specialized differentiable rendering software. This decoupling not only reduces computational and memory overhead but also effectively attenuates the policy gradient norm, leading to more stable and smoother optimization. We evaluate our method on standard visual control benchmarks using modern GPU-accelerated simulation. Experiments show that our approach significantly reduces wall-clock training time and consistently outperforms all baseline methods in terms of final returns. Notably, on complex tasks such as humanoid locomotion, our method achieves a 4 × 4 × improvement in final return, and successfully learns a humanoid running policy within 4 hours on a single GPU. Videos and code are available on https://haoxiangyou.github.io/Dva_website Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "HFT821Q83J@OpenReview",
      "index": 585,
      "title": "Online Prediction with Limited Selectivity",
      "authors": [
        "Licheng Liu",
        "Mingda Qiao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "forecaster",
        "prediction",
        "selectivity",
        "instance",
        "pls",
        "error",
        "limited",
        "dru13",
        "qv19",
        "decides"
      ],
      "summary": "Selective prediction [Dru13, QV19] models the scenario where a forecaster freely decides on the prediction window that their forecast spans. Many data statistics can be predicted to a non-trivial error rate *without any* distributional assumptions or expert advice, yet these results rely on that the forecaster may predict at any time. We introduce a model of Prediction with Limited Selectivity (PLS) where the forecaster can start the prediction only on a subset of the time horizon. We study the optimal prediction error both on an instance-by-instance basis and via an average-case analysis. We introduce a complexity measure that gives instance-dependent bounds on the optimal error. For a randomly-generated PLS instance, these bounds match with high probability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HFT821Q83J"
        ],
        "venue": [
          "/venue/HFT821Q83J@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HFT821Q83J"
        ],
        "detail": [
          "https://openreview.net/forum?id=HFT821Q83J"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Online Prediction with Limited Selectivity [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Licheng Liu , Mingda Qiao Selective prediction [Dru13, QV19] models the scenario where a forecaster freely decides on the prediction window that their forecast spans. Many data statistics can be predicted to a non-trivial error rate *without any* distributional assumptions or expert advice, yet these results rely on that the forecaster may predict at any time. We introduce a model of Prediction with Limited Selectivity (PLS) where the forecaster can start the prediction only on a subset of the time horizon. We study the optimal prediction error both on an instance-by-instance basis and via an average-case analysis. We introduce a complexity measure that gives instance-dependent bounds on the optimal error. For a randomly-generated PLS instance, these bounds match with high probability. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VCTt5DXiBe@OpenReview",
      "index": 586,
      "title": "Generative Trajectory Stitching through Diffusion Composition",
      "authors": [
        "Yunhao Luo",
        "Utkarsh Aashu Mishra",
        "Yilun Du",
        "Danfei Xu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "trajectory",
        "stitching",
        "compdiffuser",
        "chunks",
        "tasks",
        "diffusion",
        "generative",
        "planning",
        "seen",
        "stitch"
      ],
      "summary": "Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VCTt5DXiBe"
        ],
        "venue": [
          "/venue/VCTt5DXiBe@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VCTt5DXiBe"
        ],
        "detail": [
          "https://openreview.net/forum?id=VCTt5DXiBe"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Generative Trajectory Stitching through Diffusion Composition [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yunhao Luo , Utkarsh Aashu Mishra , Yilun Du , Danfei Xu Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bRWkBD2BfK@OpenReview",
      "index": 587,
      "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs",
      "authors": [
        "Shmuel Berman",
        "Jia Deng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "vlms",
        "visual",
        "vision",
        "emph",
        "reasoning",
        "nonlocal",
        "tests",
        "humans",
        "saccadic",
        "sonnet"
      ],
      "summary": "Vision Language Models (VLMs) excel at complex visual tasks such as VQA and chart understanding, yet recent work suggests they struggle with simple perceptual tests. We present an evaluation that tests vision-language models’ capacity for \\emph{nonlocal visual reasoning}- reasoning that requires chaining evidence collected from multiple, possibly distant, regions of an image. We isolate three distinct forms of non‑local vision: \\emph{comparative perception}, which demands holding two images in working memory and comparing them; \\emph{saccadic search}, which requires making discrete, evidence‑driven jumps to locate successive targets; and \\emph{smooth visual search}, which involves searching smoothly along a continuous contour. Flagship models (e.g. GPT-5, Gemini 2.5 Pro, Claude Sonnet 4), even those that perform well on prior primitive‑vision benchmarks, fail these tests and barely exceed random accuracy on two variants of our tasks that are trivial for humans. Our structured evaluation suite allows us to test if VLMs can perform similar visual algorithms to humans. Our findings show that despite gains in raw visual acuity, current models lack core visual reasoning capabilities.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bRWkBD2BfK"
        ],
        "venue": [
          "/venue/bRWkBD2BfK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bRWkBD2BfK"
        ],
        "detail": [
          "https://openreview.net/forum?id=bRWkBD2BfK"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Shmuel Berman , Jia Deng Vision Language Models (VLMs) excel at complex visual tasks such as VQA and chart understanding, yet recent work suggests they struggle with simple perceptual tests. We present an evaluation that tests vision-language models’ capacity for \\emph{nonlocal visual reasoning}- reasoning that requires chaining evidence collected from multiple, possibly distant, regions of an image. We isolate three distinct forms of non‑local vision: \\emph{comparative perception}, which demands holding two images in working memory and comparing them; \\emph{saccadic search}, which requires making discrete, evidence‑driven jumps to locate successive targets; and \\emph{smooth visual search}, which involves searching smoothly along a continuous contour. Flagship models (e.g. GPT-5, Gemini 2.5 Pro, Claude Sonnet 4), even those that perform well on prior primitive‑vision benchmarks, fail these tests and barely exceed random accuracy on two variants of our tasks that are trivial for humans. Our structured evaluation suite allows us to test if VLMs can perform similar visual algorithms to humans. Our findings show that despite gains in raw visual acuity, current models lack core visual reasoning capabilities. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "tBhEHymG1m@OpenReview",
      "index": 588,
      "title": "Neighborhood Self-Dissimilarity Attention for Medical Image Segmentation",
      "authors": [
        "Chen Junren",
        "Rui Chen",
        "Wang-wei",
        "Junlong Cheng",
        "Gang Liang",
        "zhanglei-scu",
        "Liangyin Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dissimilarity",
        "attention",
        "neighborhood",
        "nsda",
        "medical",
        "segmentation",
        "self",
        "accuracy",
        "chenjunren",
        "equity"
      ],
      "summary": "Medical image segmentation based on neural networks is pivotal in promoting digital health equity. The attention mechanism increasingly serves as a key component in modern neural networks, as it enables the network to focus on regions of interest, thus improving the segmentation accuracy in medical images. However, current attention mechanisms confront an accuracy-complexity trade-off paradox: accuracy gains demand higher computational costs, while reducing complexity sacrifices model accuracy. Such a contradiction inherently restricts the real-world deployment of attention mechanisms in resource-limited settings, thus exacerbating healthcare disparities. To overcome this dilemma, we propose parameter-free Neighborhood Self-Dissimilarity Attention (NSDA), inspired by radiologists' diagnostic patterns of prioritizing regions exhibiting substantial differences during clinical image interpretation. Unlike pairwise-similarity-based self-attention mechanisms, NSDA constructs a size-adaptive local dissimilarity measure that quantifies element-neighborhood differences. By assigning higher attention weights to regions with larger feature differences, NSDA directs the neural network to focus on high-discrepancy regions, thus improving segmentation accuracy without adding trainable parameters directly related to computational complexity. The experimental results demonstrate the effectiveness and generalization of our method. This study presents a parameter-free attention paradigm, designed with clinical prior knowledge, to improve neural network performance for medical image analysis and contribute to digital health equity in low-resource settings. The code is available at [https://github.com/ChenJunren-Lab/Neighborhood-Self-Dissimilarity-Attention](https://github.com/ChenJunren-Lab/Neighborhood-Self-Dissimilarity-Attention).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tBhEHymG1m"
        ],
        "venue": [
          "/venue/tBhEHymG1m@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tBhEHymG1m"
        ],
        "detail": [
          "https://openreview.net/forum?id=tBhEHymG1m"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "Neighborhood Self-Dissimilarity Attention for Medical Image Segmentation [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Chen Junren , Rui Chen , Wang-wei , Junlong Cheng , Gang Liang , zhanglei-scu , Liangyin Chen Medical image segmentation based on neural networks is pivotal in promoting digital health equity. The attention mechanism increasingly serves as a key component in modern neural networks, as it enables the network to focus on regions of interest, thus improving the segmentation accuracy in medical images. However, current attention mechanisms confront an accuracy-complexity trade-off paradox: accuracy gains demand higher computational costs, while reducing complexity sacrifices model accuracy. Such a contradiction inherently restricts the real-world deployment of attention mechanisms in resource-limited settings, thus exacerbating healthcare disparities. To overcome this dilemma, we propose parameter-free Neighborhood Self-Dissimilarity Attention (NSDA), inspired by radiologists' diagnostic patterns of prioritizing regions exhibiting substantial differences during clinical image interpretation. Unlike pairwise-similarity-based self-attention mechanisms, NSDA constructs a size-adaptive local dissimilarity measure that quantifies element-neighborhood differences. By assigning higher attention weights to regions with larger feature differences, NSDA directs the neural network to focus on high-discrepancy regions, thus improving segmentation accuracy without adding trainable parameters directly related to computational complexity. The experimental results demonstrate the effectiveness and generalization of our method. This study presents a parameter-free attention paradigm, designed with clinical prior knowledge, to improve neural network performance for medical image analysis and contribute to digital health equity in low-resource settings. The code is available at [https://github.com/ChenJunren-Lab/Neighborhood-Self-Dissimilarity-Attention](https://github.com/ChenJunren-Lab/Neighborhood-Self-Dissimilarity-Attention). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "d4CZoiaXeC@OpenReview",
      "index": 589,
      "title": "Learning to Factorize Spatio-Temporal Foundation Models",
      "authors": [
        "Siru Zhong",
        "Junjie Qiu",
        "Yangyu Wu",
        "Xingchen Zou",
        "Zhongwen Rao",
        "Bin Yang",
        "Chenjuan Guo",
        "Hao Xu",
        "Yuxuan Liang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "spatio",
        "temporal",
        "stfms",
        "factost",
        "factorized",
        "foundation",
        "opencity",
        "pretraining",
        "stfm",
        "factorize"
      ],
      "summary": "Spatio-Temporal Foundation Models (STFMs) promise zero/few-shot generalization across various datasets, yet joint spatio-temporal pretraining is computationally prohibitive and struggles with domain-specific spatial correlations. To this end, we introduce FactoST, a factorized STFM that decouples universal temporal pretraining from spatio-temporal adaptation. The first stage pretrains a space-agnostic backbone with multi-frequency reconstruction and domain-aware prompting, capturing cross-domain temporal regularities at low computational cost. The second stage freezes or further fine-tunes the backbone and attaches an adapter that fuses spatial metadata, sparsifies interactions, and aligns domains with continual memory replay. Extensive forecasting experiments reveal that, in few-shot setting, FactoST reduces MAE by up to 46.4% versus UniST, uses 46.2% fewer parameters, and achieves 68% faster inference than OpenCity, while remaining competitive with expert models. We believe this factorized view offers a practical and scalable path toward truly universal STFMs. The code will be released upon notification.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=d4CZoiaXeC"
        ],
        "venue": [
          "/venue/d4CZoiaXeC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=d4CZoiaXeC"
        ],
        "detail": [
          "https://openreview.net/forum?id=d4CZoiaXeC"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Learning to Factorize Spatio-Temporal Foundation Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Siru Zhong , Junjie Qiu , Yangyu Wu , Xingchen Zou , Zhongwen Rao , Bin Yang , Chenjuan Guo , Hao Xu , Yuxuan Liang Spatio-Temporal Foundation Models (STFMs) promise zero/few-shot generalization across various datasets, yet joint spatio-temporal pretraining is computationally prohibitive and struggles with domain-specific spatial correlations. To this end, we introduce FactoST, a factorized STFM that decouples universal temporal pretraining from spatio-temporal adaptation. The first stage pretrains a space-agnostic backbone with multi-frequency reconstruction and domain-aware prompting, capturing cross-domain temporal regularities at low computational cost. The second stage freezes or further fine-tunes the backbone and attaches an adapter that fuses spatial metadata, sparsifies interactions, and aligns domains with continual memory replay. Extensive forecasting experiments reveal that, in few-shot setting, FactoST reduces MAE by up to 46.4% versus UniST, uses 46.2% fewer parameters, and achieves 68% faster inference than OpenCity, while remaining competitive with expert models. We believe this factorized view offers a practical and scalable path toward truly universal STFMs. The code will be released upon notification. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "KGt0F2yjBz@OpenReview",
      "index": 590,
      "title": "Angles Don’t Lie: Unlocking Training‑Efficient RL Through the Model’s Own Signals",
      "authors": [
        "Qinsi Wang",
        "Jinghan Ke",
        "Hancheng Ye",
        "Yueqian Lin",
        "Yuzhe Fu",
        "Jianyi Zhang",
        "Kurt Keutzer",
        "Chenfeng Xu",
        "Yiran Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "grpo",
        "training",
        "angle",
        "gain",
        "concentration",
        "data",
        "signals",
        "unlocking",
        "gradient",
        "model"
      ],
      "summary": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed *angle concentration* that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5 × × acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KGt0F2yjBz"
        ],
        "venue": [
          "/venue/KGt0F2yjBz@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KGt0F2yjBz"
        ],
        "detail": [
          "https://openreview.net/forum?id=KGt0F2yjBz"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Angles Don’t Lie: Unlocking Training‑Efficient RL Through the Model’s Own Signals [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Qinsi Wang , Jinghan Ke , Hancheng Ye , Yueqian Lin , Yuzhe Fu , Jianyi Zhang , Kurt Keutzer , Chenfeng Xu , Yiran Chen Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed *angle concentration* that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5 × × acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ROfYsQ2KNV@OpenReview",
      "index": 591,
      "title": "Generalizable Insights for Graph Transformers in Theory and Practice",
      "authors": [
        "Timo Stoll",
        "Luis Müller",
        "Christopher Morris"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "generalizable",
        "gdt",
        "gts",
        "pes",
        "insights",
        "expressivity",
        "transformers",
        "270m",
        "practice",
        "attention"
      ],
      "summary": "Graph Transformers (GTs) have shown strong empirical performance, yet current architectures vary widely in their use of attention mechanisms, positional embeddings (PEs), and expressivity. Existing expressivity results are often tied to specific design choices and lack comprehensive empirical validation on large-scale data. This leaves a gap between theory and practice, preventing generalizable insights that exceed particular application domains. Here, we propose the Generalized-Distance Transformer (GDT), a GT architecture using standard attention that incorporates many advancements for GTs from recent years, and develop a fine-grained understanding of the GDT's representation power in terms of attention and PEs. Through extensive experiments, we identify design choices that consistently perform well across various applications, tasks, and model scales, demonstrating strong performance in a few-shot transfer setting without fine-tuning. Our evaluation covers over eight million graphs with roughly 270M tokens across diverse domains, including image-based object detection, molecular property prediction, code summarization, and out-of-distribution algorithmic reasoning. We distill our theoretical and practical findings into several generalizable insights about effective GT design, training, and inference.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ROfYsQ2KNV"
        ],
        "venue": [
          "/venue/ROfYsQ2KNV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ROfYsQ2KNV"
        ],
        "detail": [
          "https://openreview.net/forum?id=ROfYsQ2KNV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Generalizable Insights for Graph Transformers in Theory and Practice [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Timo Stoll , Luis Müller , Christopher Morris Graph Transformers (GTs) have shown strong empirical performance, yet current architectures vary widely in their use of attention mechanisms, positional embeddings (PEs), and expressivity. Existing expressivity results are often tied to specific design choices and lack comprehensive empirical validation on large-scale data. This leaves a gap between theory and practice, preventing generalizable insights that exceed particular application domains. Here, we propose the Generalized-Distance Transformer (GDT), a GT architecture using standard attention that incorporates many advancements for GTs from recent years, and develop a fine-grained understanding of the GDT's representation power in terms of attention and PEs. Through extensive experiments, we identify design choices that consistently perform well across various applications, tasks, and model scales, demonstrating strong performance in a few-shot transfer setting without fine-tuning. Our evaluation covers over eight million graphs with roughly 270M tokens across diverse domains, including image-based object detection, molecular property prediction, code summarization, and out-of-distribution algorithmic reasoning. We distill our theoretical and practical findings into several generalizable insights about effective GT design, training, and inference. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "n5NZqzAITL@OpenReview",
      "index": 592,
      "title": "Principled Data Augmentation for Learning to Solve Quadratic Programming Problems",
      "authors": [
        "Chendi Qian",
        "Christopher Morris"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mpnns",
        "l2o",
        "qps",
        "augmentation",
        "optimization",
        "principled",
        "quadratic",
        "problems",
        "programs",
        "data"
      ],
      "summary": "Linear and quadratic optimization are crucial in numerous real-world applications, ranging from training machine learning models to solving integer linear programs. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, thereby reducing the need to solve many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised contrastive learning framework, thereby pretraining MPNNs for improved performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=n5NZqzAITL"
        ],
        "venue": [
          "/venue/n5NZqzAITL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=n5NZqzAITL"
        ],
        "detail": [
          "https://openreview.net/forum?id=n5NZqzAITL"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Principled Data Augmentation for Learning to Solve Quadratic Programming Problems [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Chendi Qian , Christopher Morris Linear and quadratic optimization are crucial in numerous real-world applications, ranging from training machine learning models to solving integer linear programs. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, thereby reducing the need to solve many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised contrastive learning framework, thereby pretraining MPNNs for improved performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "C39ShJwtD5@OpenReview",
      "index": 593,
      "title": "Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable",
      "authors": [
        "Ruoxin Chen",
        "Junwei Xi",
        "Zhiyuan Yan",
        "Ke-Yue Zhang",
        "Shuang Wu",
        "Jingyi Xie",
        "Xu Chen",
        "Lei Xu",
        "Isabel Guan",
        "Taiping Yao",
        "Shouhong Ding"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dda",
        "synthetic",
        "images",
        "alignment",
        "biased",
        "frequency",
        "misalignment",
        "real",
        "detector",
        "evalgen"
      ],
      "summary": "The rapid increase in AI-generated images (AIGIs) underscores the need for detection methods. Existing detectors are often trained on biased datasets, leading to overfitting on spurious correlations between non-causal image attributes and real/synthetic labels. While these biased features enhance performance on the training data, they result in substantial performance degradation when tested on unbiased datasets. A common solution is to perform data alignment through generative reconstruction, matching the content between real and synthetic images. However, we find that pixel-level alignment alone is inadequate, as the reconstructed images still suffer from frequency-level misalignment, perpetuating spurious correlations. To illustrate, we observe that reconstruction models restore the high-frequency details lost in real images, inadvertently creating a frequency-level misalignment, where synthetic images appear to have richer high-frequency content than real ones. This misalignment leads to models associating high-frequency features with synthetic labels, further reinforcing biased cues. To resolve this, we propose Dual Data Alignment (DDA), which aligns both the pixel and frequency domains. DDA generates synthetic images that closely resemble real ones by fusing real and synthetic image pairs in both domains, enhancing the detector's ability to identify forgeries without relying on biased features. Moreover, we introduce two new test sets: DDA-COCO, containing DDA-aligned synthetic images, and EvalGEN, featuring the latest generative models. Our extensive evaluations demonstrate that a detector trained exclusively on DDA-aligned MSCOCO improves across diverse benchmarks. Code is available at https://github.com/roy-ch/Dual-Data-Alignment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=C39ShJwtD5"
        ],
        "venue": [
          "/venue/C39ShJwtD5@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=C39ShJwtD5"
        ],
        "detail": [
          "https://openreview.net/forum?id=C39ShJwtD5"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ruoxin Chen , Junwei Xi , Zhiyuan Yan , Ke-Yue Zhang , Shuang Wu , Jingyi Xie , Xu Chen , Lei Xu , Isabel Guan , Taiping Yao , Shouhong Ding The rapid increase in AI-generated images (AIGIs) underscores the need for detection methods. Existing detectors are often trained on biased datasets, leading to overfitting on spurious correlations between non-causal image attributes and real/synthetic labels. While these biased features enhance performance on the training data, they result in substantial performance degradation when tested on unbiased datasets. A common solution is to perform data alignment through generative reconstruction, matching the content between real and synthetic images. However, we find that pixel-level alignment alone is inadequate, as the reconstructed images still suffer from frequency-level misalignment, perpetuating spurious correlations. To illustrate, we observe that reconstruction models restore the high-frequency details lost in real images, inadvertently creating a frequency-level misalignment, where synthetic images appear to have richer high-frequency content than real ones. This misalignment leads to models associating high-frequency features with synthetic labels, further reinforcing biased cues. To resolve this, we propose Dual Data Alignment (DDA), which aligns both the pixel and frequency domains. DDA generates synthetic images that closely resemble real ones by fusing real and synthetic image pairs in both domains, enhancing the detector's ability to identify forgeries without relying on biased features. Moreover, we introduce two new test sets: DDA-COCO, containing DDA-aligned synthetic images, and EvalGEN, featuring the latest generative models. Our extensive evaluations demonstrate that a detector trained exclusively on DDA-aligned MSCOCO improves across diverse benchmarks. Code is available at https://github.com/roy-ch/Dual-Data-Alignment. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "agcXjEHmyW@OpenReview",
      "index": 594,
      "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding",
      "authors": [
        "Yuchen Zhou",
        "Jiamin Wu",
        "Zichen Ren",
        "Zhouheng Yao",
        "Weiheng Lu",
        "Kunyu Peng",
        "Qihao Zheng",
        "Chunfeng Song",
        "Wanli Ouyang",
        "Chao Gou"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "csbrain",
        "eeg",
        "brain",
        "decoding",
        "spatiotemporal",
        "scale",
        "cross",
        "foundation",
        "cst",
        "ssa"
      ],
      "summary": "Understanding and decoding human brain activity from electroencephalography (EEG) signals is a fundamental problem in neuroscience and artificial intelligence, with applications ranging from cognition and emotion recognition to clinical diagnosis and brain–computer interfaces. While recent EEG foundation models have made progress in generalized brain decoding by leveraging unified architectures and large-scale pretraining, they inherit a scale-agnostic dense modeling paradigm from NLP and vision. This design overlooks an intrinsic property of neural activity—cross-scale spatiotemporal structure. Different EEG task patterns span a broad range of temporal and spatial scales, from brief neural activations to slow-varying rhythms, and from localized cortical activations to large-scale distributed interactions. Ignoring this diversity may lead to suboptimal representations and weakened generalization ability. To address these limitations, we propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces two key components: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features within localized temporal windows and anatomical brain regions into compact scale-aware token representations; and (ii) Structured Sparse Attention (SSA), which models cross-window and cross-region dependencies for diverse decoding tasks, further enriching scale diversities while eliminating the spurious dependencies. CST and SSA are alternately stacked to progressively integrate cross-scale spatiotemporal dependencies. Extensive experiments across 11 representative EEG tasks and 16 datasets demonstrate that CSBrain consistently outperforms both task-specific models and strong foundation baselines. These results establish cross-scale modeling as a key inductive bias for generalized EEG decoding and highlight CSBrain as a robust backbone for future brain–AI research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=agcXjEHmyW"
        ],
        "venue": [
          "/venue/agcXjEHmyW@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=agcXjEHmyW"
        ],
        "detail": [
          "https://openreview.net/forum?id=agcXjEHmyW"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Yuchen Zhou , Jiamin Wu , Zichen Ren , Zhouheng Yao , Weiheng Lu , Kunyu Peng , Qihao Zheng , Chunfeng Song , Wanli Ouyang , Chao Gou Understanding and decoding human brain activity from electroencephalography (EEG) signals is a fundamental problem in neuroscience and artificial intelligence, with applications ranging from cognition and emotion recognition to clinical diagnosis and brain–computer interfaces. While recent EEG foundation models have made progress in generalized brain decoding by leveraging unified architectures and large-scale pretraining, they inherit a scale-agnostic dense modeling paradigm from NLP and vision. This design overlooks an intrinsic property of neural activity—cross-scale spatiotemporal structure. Different EEG task patterns span a broad range of temporal and spatial scales, from brief neural activations to slow-varying rhythms, and from localized cortical activations to large-scale distributed interactions. Ignoring this diversity may lead to suboptimal representations and weakened generalization ability. To address these limitations, we propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces two key components: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features within localized temporal windows and anatomical brain regions into compact scale-aware token representations; and (ii) Structured Sparse Attention (SSA), which models cross-window and cross-region dependencies for diverse decoding tasks, further enriching scale diversities while eliminating the spurious dependencies. CST and SSA are alternately stacked to progressively integrate cross-scale spatiotemporal dependencies. Extensive experiments across 11 representative EEG tasks and 16 datasets demonstrate that CSBrain consistently outperforms both task-specific models and strong foundation baselines. These results establish cross-scale modeling as a key inductive bias for generalized EEG decoding and highlight CSBrain as a robust backbone for future brain–AI research. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "KaD2Dw8Ahz@OpenReview",
      "index": 595,
      "title": "Flattening Hierarchies with Policy Bootstrapping",
      "authors": [
        "John Luoyu Zhou",
        "Jonathan Kao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gcrl",
        "bootstrapping",
        "goal",
        "subgoal",
        "conditioned",
        "policies",
        "hierarchical",
        "policy",
        "flattening",
        "offline"
      ],
      "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail. Project page: https://johnlyzhou.github.io/saw/",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KaD2Dw8Ahz"
        ],
        "venue": [
          "/venue/KaD2Dw8Ahz@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KaD2Dw8Ahz"
        ],
        "detail": [
          "https://openreview.net/forum?id=KaD2Dw8Ahz"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Flattening Hierarchies with Policy Bootstrapping [PDF 1 ] [Copy] [Kimi ] [REL] Authors : John Luoyu Zhou , Jonathan Kao Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail. Project page: https://johnlyzhou.github.io/saw/ Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xOqCKB8XIl@OpenReview",
      "index": 596,
      "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios",
      "authors": [
        "Kunyu Peng",
        "Junchao Huang",
        "Xiangsheng Huang",
        "Di Wen",
        "Junwei Zheng",
        "Yufan Chen",
        "Kailun Yang",
        "Jiamin Wu",
        "Chongqing Hao",
        "Rainer Stiefelhagen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "hopadiff",
        "person",
        "segmentation",
        "action",
        "rhas133",
        "holistic",
        "textual",
        "fourier",
        "referring",
        "conditioned"
      ],
      "summary": "Action segmentation is a core challenge in high-level video understanding, aiming to partition untrimmed videos into segments and assign each a label from a predefined action set. Existing methods primarily address single-person activities with fixed action sequences, overlooking multi-person scenarios. In this work, we pioneer textual reference-guided human action segmentation in multi-person settings, where a textual description specifies the target person for segmentation. We introduce the first dataset for Referring Human Action Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137 fine-grained actions with 33h video data, together with textual descriptions for this new task. Benchmarking existing action segmentation methods on RHAS133 using VLM-based feature extractors reveals limited performance and poor aggregation of visual cues for the target person. To address this, we propose a holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF, leveraging a novel cross-input gate attentional xLSTM to enhance holistic-partial long-range reasoning and a novel Fourier condition to introduce more fine-grained control to improve the action segmentation generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse evaluation settings. The dataset and code are available at https://github.com/KPeng9510/HopaDIFF.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xOqCKB8XIl"
        ],
        "venue": [
          "/venue/xOqCKB8XIl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xOqCKB8XIl"
        ],
        "detail": [
          "https://openreview.net/forum?id=xOqCKB8XIl"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios [PDF ] [Copy] [Kimi ] [REL] Authors : Kunyu Peng , Junchao Huang , Xiangsheng Huang , Di Wen , Junwei Zheng , Yufan Chen , Kailun Yang , Jiamin Wu , Chongqing Hao , Rainer Stiefelhagen Action segmentation is a core challenge in high-level video understanding, aiming to partition untrimmed videos into segments and assign each a label from a predefined action set. Existing methods primarily address single-person activities with fixed action sequences, overlooking multi-person scenarios. In this work, we pioneer textual reference-guided human action segmentation in multi-person settings, where a textual description specifies the target person for segmentation. We introduce the first dataset for Referring Human Action Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137 fine-grained actions with 33h video data, together with textual descriptions for this new task. Benchmarking existing action segmentation methods on RHAS133 using VLM-based feature extractors reveals limited performance and poor aggregation of visual cues for the target person. To address this, we propose a holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF, leveraging a novel cross-input gate attentional xLSTM to enhance holistic-partial long-range reasoning and a novel Fourier condition to introduce more fine-grained control to improve the action segmentation generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse evaluation settings. The dataset and code are available at https://github.com/KPeng9510/HopaDIFF. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nwlX15Wnr9@OpenReview",
      "index": 597,
      "title": "Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts",
      "authors": [
        "Benjamin Schiffer",
        "Mark Sellke"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "exploration",
        "incentivized",
        "incentive",
        "contexts",
        "sample",
        "incentives",
        "initial",
        "regret",
        "exogenously",
        "meets"
      ],
      "summary": "In the incentivized exploration model, a principal aims to explore and learn over time by interacting with a sequence of self-interested agents. It has been recently understood that the main challenge in designing incentive-compatible algorithms for this problem is to gather a moderate amount of initial data, after which one can obtain near-optimal regret via posterior sampling. With high-dimensional contexts, however, this \\emph{initial exploration} phase requires exponential sample complexity in some cases, which prevents efficient learning unless initial data can be acquired exogenously. We show that these barriers to exploration disappear under mild geometric conditions on the set of available actions, in which case incentive-compatibility does not preclude regret-optimality. Namely, we consider the linear bandit model with actions in the Euclidean unit ball, and give an incentive-compatible exploration algorithm with sample complexity that scales polynomially with the dimension and other parameters.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nwlX15Wnr9"
        ],
        "venue": [
          "/venue/nwlX15Wnr9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nwlX15Wnr9"
        ],
        "detail": [
          "https://openreview.net/forum?id=nwlX15Wnr9"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Benjamin Schiffer , Mark Sellke In the incentivized exploration model, a principal aims to explore and learn over time by interacting with a sequence of self-interested agents. It has been recently understood that the main challenge in designing incentive-compatible algorithms for this problem is to gather a moderate amount of initial data, after which one can obtain near-optimal regret via posterior sampling. With high-dimensional contexts, however, this \\emph{initial exploration} phase requires exponential sample complexity in some cases, which prevents efficient learning unless initial data can be acquired exogenously. We show that these barriers to exploration disappear under mild geometric conditions on the set of available actions, in which case incentive-compatibility does not preclude regret-optimality. Namely, we consider the linear bandit model with actions in the Euclidean unit ball, and give an incentive-compatible exploration algorithm with sample complexity that scales polynomially with the dimension and other parameters. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RnfyqrkOxD@OpenReview",
      "index": 598,
      "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts",
      "authors": [
        "Zixin Zhu",
        "Haoxiang Li",
        "Xuelu Feng",
        "He Wu",
        "Chunming Qiao",
        "Junsong Yuan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "geometry",
        "causal",
        "artifacts",
        "georemover",
        "removal",
        "visual",
        "object",
        "stage",
        "objects",
        "mask"
      ],
      "summary": "Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these casual effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an object’s geometry presence and its visual effects. To address this limitation, we propose a geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render a photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as a result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce a preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The project page is available at https://buxiangzhiren.github.io/GeoRemover.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RnfyqrkOxD"
        ],
        "venue": [
          "/venue/RnfyqrkOxD@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RnfyqrkOxD"
        ],
        "detail": [
          "https://openreview.net/forum?id=RnfyqrkOxD"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "GeoRemover: Removing Objects and Their Causal Visual Artifacts [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Zixin Zhu , Haoxiang Li , Xuelu Feng , He Wu , Chunming Qiao , Junsong Yuan Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these casual effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an object’s geometry presence and its visual effects. To address this limitation, we propose a geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render a photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as a result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce a preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The project page is available at https://buxiangzhiren.github.io/GeoRemover. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "s4LnWgjacg@OpenReview",
      "index": 599,
      "title": "The Primacy of Magnitude in Low-Rank Adaptation",
      "authors": [
        "Zicheng Zhang",
        "Haoran Li",
        "Yifeng Zhang",
        "Guoqiang Gong",
        "Jiaxing Wang",
        "pengzhang liu",
        "Qixia Jiang",
        "Junxing Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "loram",
        "initialization",
        "magnitude",
        "lora",
        "spectral",
        "update",
        "rank",
        "primacy",
        "weight",
        "adaptation"
      ],
      "summary": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning large models. While recent spectral initialization methods improve convergence and performance over the naive “Noise \\& Zeros” scheme, their extra computational and storage overhead undermines efficiency. In this paper, we establish update magnitude as the fundamental driver of LoRA performance and propose LoRAM, a magnitude-driven “Basis \\& Basis” initialization scheme that matches spectral methods without their inefficiencies. Our key contributions are threefold: (i) Magnitude of weight updates determines convergence. We prove low-rank structures intrinsically bound update magnitudes, unifying hyperparameter tuning in learning rate, scaling factor, and initialization as mechanisms to optimize magnitude regulation. (ii) Spectral initialization succeeds via magnitude amplification. We demystify that the presumed knowledge-driven benefit of spectral component essentially arises from the boost in the weight update magnitude. (iii) A novel and compact initialization strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight magnitudes to simulate spectral gains. Extensive experiments show that LoRAM serves as a strong baseline, retaining the full efficiency of LoRA while matching or outperforming spectral initialization across benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=s4LnWgjacg"
        ],
        "venue": [
          "/venue/s4LnWgjacg@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=s4LnWgjacg"
        ],
        "detail": [
          "https://openreview.net/forum?id=s4LnWgjacg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "The Primacy of Magnitude in Low-Rank Adaptation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Zicheng Zhang , Haoran Li , Yifeng Zhang , Guoqiang Gong , Jiaxing Wang , pengzhang liu , Qixia Jiang , Junxing Hu Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning large models. While recent spectral initialization methods improve convergence and performance over the naive “Noise \\& Zeros” scheme, their extra computational and storage overhead undermines efficiency. In this paper, we establish update magnitude as the fundamental driver of LoRA performance and propose LoRAM, a magnitude-driven “Basis \\& Basis” initialization scheme that matches spectral methods without their inefficiencies. Our key contributions are threefold: (i) Magnitude of weight updates determines convergence. We prove low-rank structures intrinsically bound update magnitudes, unifying hyperparameter tuning in learning rate, scaling factor, and initialization as mechanisms to optimize magnitude regulation. (ii) Spectral initialization succeeds via magnitude amplification. We demystify that the presumed knowledge-driven benefit of spectral component essentially arises from the boost in the weight update magnitude. (iii) A novel and compact initialization strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight magnitudes to simulate spectral gains. Extensive experiments show that LoRAM serves as a strong baseline, retaining the full efficiency of LoRA while matching or outperforming spectral initialization across benchmarks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "z3PMVmzoya@OpenReview",
      "index": 600,
      "title": "GeRaF: Neural Geometry Reconstruction from Radio Frequency Signals",
      "authors": [
        "Jiachen Lu",
        "Hailan Shanbhag",
        "Haitham Al Hassanieh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "geraf",
        "signals",
        "lens",
        "rendering",
        "reflectiveness",
        "geometry",
        "volumetric",
        "reconstruction",
        "radio",
        "rgb"
      ],
      "summary": "GeRaF is the first method to use neural implicit learning for near-range 3D geometry reconstruction from radio frequency (RF) signals. Unlike RGB or LiDAR-based methods, RF sensing can see through occlusion but suffers from low resolution and noise due to its lens-less imaging nature. While lenses in RGB imaging constrain sampling to 1D rays, RF signals propagate through the entire space, introducing significant noise and leading to cubic complexity in volumetric rendering. Moreover, RF signals interact with surfaces via specular reflections requiring fundamentally different modeling. To address these challenges, GeRaF (1) introduces filter-based rendering to suppress irrelevant signals, (2) implements a physics-based RF volumetric rendering pipeline, and (3) proposes a novel lens-less sampling and lens-less alpha blending strategy that makes full-space sampling feasible during training. By learning signed distance functions, reflectiveness, and signal power through MLPs and trainable parameters, GeRaF takes the first step towards reconstructing millimeter-level geometry from RF signals in real-world settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=z3PMVmzoya"
        ],
        "venue": [
          "/venue/z3PMVmzoya@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=z3PMVmzoya"
        ],
        "detail": [
          "https://openreview.net/forum?id=z3PMVmzoya"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "GeRaF: Neural Geometry Reconstruction from Radio Frequency Signals [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Jiachen Lu , Hailan Shanbhag , Haitham Al Hassanieh GeRaF is the first method to use neural implicit learning for near-range 3D geometry reconstruction from radio frequency (RF) signals. Unlike RGB or LiDAR-based methods, RF sensing can see through occlusion but suffers from low resolution and noise due to its lens-less imaging nature. While lenses in RGB imaging constrain sampling to 1D rays, RF signals propagate through the entire space, introducing significant noise and leading to cubic complexity in volumetric rendering. Moreover, RF signals interact with surfaces via specular reflections requiring fundamentally different modeling. To address these challenges, GeRaF (1) introduces filter-based rendering to suppress irrelevant signals, (2) implements a physics-based RF volumetric rendering pipeline, and (3) proposes a novel lens-less sampling and lens-less alpha blending strategy that makes full-space sampling feasible during training. By learning signed distance functions, reflectiveness, and signal power through MLPs and trainable parameters, GeRaF takes the first step towards reconstructing millimeter-level geometry from RF signals in real-world settings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SBYCu5uJJf@OpenReview",
      "index": 601,
      "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data",
      "authors": [
        "Haoyu Zhang",
        "Meng Liu",
        "Zaijing Li",
        "Haokun Wen",
        "Weili Guan",
        "Yaowei Wang",
        "Liqiang Nie"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "spatial",
        "vlms",
        "reasoning",
        "understanding",
        "prompting",
        "structured",
        "visual",
        "scenes",
        "simulation",
        "spatialmind"
      ],
      "summary": "Visual-spatial understanding, the ability to infer object relationships and layouts from visual input, is fundamental to downstream tasks such as robotic navigation and embodied interaction. However, existing methods face spatial uncertainty and data scarcity, limiting the 3D spatial reasoning capability of pre-trained vision-language models (VLMs). To address these challenges, we present a unified framework for enhancing 3D spatial reasoning in pre-trained VLMs without modifying their architecture. This framework combines SpatialMind, a structured prompting strategy that decomposes complex scenes and questions into interpretable reasoning steps, with ScanForgeQA, a scalable question-answering dataset built from diverse 3D simulation scenes through an automated construction process designed for fine-tuning. Extensive experiments across multiple benchmarks demonstrate the individual and combined effectiveness of our prompting and fine-tuning strategies, and yield insights that may inspire future research on visual-spatial understanding.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SBYCu5uJJf"
        ],
        "venue": [
          "/venue/SBYCu5uJJf@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SBYCu5uJJf"
        ],
        "detail": [
          "https://openreview.net/forum?id=SBYCu5uJJf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Haoyu Zhang , Meng Liu , Zaijing Li , Haokun Wen , Weili Guan , Yaowei Wang , Liqiang Nie Visual-spatial understanding, the ability to infer object relationships and layouts from visual input, is fundamental to downstream tasks such as robotic navigation and embodied interaction. However, existing methods face spatial uncertainty and data scarcity, limiting the 3D spatial reasoning capability of pre-trained vision-language models (VLMs). To address these challenges, we present a unified framework for enhancing 3D spatial reasoning in pre-trained VLMs without modifying their architecture. This framework combines SpatialMind, a structured prompting strategy that decomposes complex scenes and questions into interpretable reasoning steps, with ScanForgeQA, a scalable question-answering dataset built from diverse 3D simulation scenes through an automated construction process designed for fine-tuning. Extensive experiments across multiple benchmarks demonstrate the individual and combined effectiveness of our prompting and fine-tuning strategies, and yield insights that may inspire future research on visual-spatial understanding. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Sct4sajCi6@OpenReview",
      "index": 602,
      "title": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning",
      "authors": [
        "Huanyu Liu",
        "Jia Li",
        "Hao Zhu",
        "Kechi Zhang",
        "Yihong Dong",
        "Ge Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "saturn",
        "sat",
        "difficulty",
        "llms",
        "reasoning",
        "tasks",
        "unleash",
        "hard",
        "constructing",
        "livecodebench"
      ],
      "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard. To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions. We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8\\%. We release the source code, data, and models to support future research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Sct4sajCi6"
        ],
        "venue": [
          "/venue/Sct4sajCi6@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Sct4sajCi6"
        ],
        "detail": [
          "https://openreview.net/forum?id=Sct4sajCi6"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Huanyu Liu , Jia Li , Hao Zhu , Kechi Zhang , Yihong Dong , Ge Li How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard. To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions. We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8\\%. We release the source code, data, and models to support future research. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MZoOpD9NHV@OpenReview",
      "index": 603,
      "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
      "authors": [
        "Kai Liu",
        "Jungang Li",
        "Yuchong Sun",
        "Shengqiong Wu",
        "jianzhang gao",
        "Daoan Zhang",
        "Wei Zhang",
        "Sheng Jin",
        "Sicheng Yu",
        "Geng Zhan",
        "Jiayi Ji",
        "Fan Zhou",
        "Liang Zheng",
        "Shuicheng YAN",
        "Hao Fei",
        "Tat-Seng Chua"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "javisgpt",
        "jav",
        "comprehension",
        "video",
        "audio",
        "generation",
        "multimodal",
        "llm",
        "syncfusion",
        "temporally"
      ],
      "summary": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder–LLM–decoder architecture, featuring a SyncFusion module for spatio-temporal audio- video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MZoOpD9NHV"
        ],
        "venue": [
          "/venue/MZoOpD9NHV@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MZoOpD9NHV"
        ],
        "detail": [
          "https://openreview.net/forum?id=MZoOpD9NHV"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Kai Liu , Jungang Li , Yuchong Sun , Shengqiong Wu , jianzhang gao , Daoan Zhang , Wei Zhang , Sheng Jin , Sicheng Yu , Geng Zhan , Jiayi Ji , Fan Zhou , Liang Zheng , Shuicheng YAN , Hao Fei , Tat-Seng Chua This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder–LLM–decoder architecture, featuring a SyncFusion module for spatio-temporal audio- video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "t4aN2G7Ucc@OpenReview",
      "index": 604,
      "title": "Dimension-adapted Momentum Outscales SGD",
      "authors": [
        "Damien Ferbach",
        "Katie Everett",
        "Gauthier Gidel",
        "Elliot Paquette",
        "Courtney Paquette"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dana",
        "sgd",
        "outscales",
        "momentum",
        "exponents",
        "scaling",
        "adapted",
        "complexities",
        "stochastic",
        "complexity"
      ],
      "summary": "We investigate scaling laws for stochastic momentum algorithms on the power law random features model, parameterized by data complexity, target complexity, and model size. When trained with a stochastic momentum algorithm, our analysis reveals four distinct loss curve shapes determined by varying data-target complexities. While traditional stochastic gradient descent with momentum (SGD-M) yields identical scaling law exponents to SGD, dimension-adapted Nesterov acceleration (DANA) improves these exponents by scaling momentum hyperparameters based on model size and data complexity. This outscaling phenomenon, which also improves compute-optimal scaling behavior, is achieved by DANA across a broad range of data and target complexities, while traditional methods fall short. Extensive experiments on high-dimensional synthetic quadratics validate our theoretical predictions and large-scale text experiments with LSTMs show DANA's improved loss exponents over SGD hold in a practical setting.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=t4aN2G7Ucc"
        ],
        "venue": [
          "/venue/t4aN2G7Ucc@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=t4aN2G7Ucc"
        ],
        "detail": [
          "https://openreview.net/forum?id=t4aN2G7Ucc"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Dimension-adapted Momentum Outscales SGD [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Damien Ferbach , Katie Everett , Gauthier Gidel , Elliot Paquette , Courtney Paquette We investigate scaling laws for stochastic momentum algorithms on the power law random features model, parameterized by data complexity, target complexity, and model size. When trained with a stochastic momentum algorithm, our analysis reveals four distinct loss curve shapes determined by varying data-target complexities. While traditional stochastic gradient descent with momentum (SGD-M) yields identical scaling law exponents to SGD, dimension-adapted Nesterov acceleration (DANA) improves these exponents by scaling momentum hyperparameters based on model size and data complexity. This outscaling phenomenon, which also improves compute-optimal scaling behavior, is achieved by DANA across a broad range of data and target complexities, while traditional methods fall short. Extensive experiments on high-dimensional synthetic quadratics validate our theoretical predictions and large-scale text experiments with LSTMs show DANA's improved loss exponents over SGD hold in a practical setting. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZBSkyMwdEB@OpenReview",
      "index": 605,
      "title": "From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots",
      "authors": [
        "Yuxuan Wang",
        "Ming Yang",
        "Ziluo Ding",
        "Yu Zhang",
        "Weishuai Zeng",
        "Xinrun Xu",
        "Haobin Jiang",
        "Zongqing Lu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "humanoid",
        "generalist",
        "motion",
        "control",
        "whole",
        "agile",
        "experts",
        "robots",
        "real",
        "body"
      ],
      "summary": "Achieving general agile whole-body control on humanoid robots remains a major challenge due to diverse motion demands and data conflicts. While existing frameworks excel in training single motion-specific policies, they struggle to generalize across highly varied behaviors due to conflicting control requirements and mismatched data distributions. In this work, we propose BumbleBee (BB), an expert-generalist learning framework that combines motion clustering and sim-to-real adaptation to overcome these challenges. BB first leverages an autoencoder-based clustering method to group behaviorally similar motions using motion features and motion descriptions. Expert policies are then trained within each cluster and refined with real-world data through iterative delta action modeling to bridge the sim-to-real gap. Finally, these experts are distilled into a unified generalist controller that preserves agility and robustness across all motion types. Experiments on two simulations and a real humanoid robot demonstrate that BB achieves state-of-the-art general whole-body control, setting a new benchmark for agile, robust, and generalizable humanoid performance in the real world.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZBSkyMwdEB"
        ],
        "venue": [
          "/venue/ZBSkyMwdEB@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZBSkyMwdEB"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZBSkyMwdEB"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yuxuan Wang , Ming Yang , Ziluo Ding , Yu Zhang , Weishuai Zeng , Xinrun Xu , Haobin Jiang , Zongqing Lu Achieving general agile whole-body control on humanoid robots remains a major challenge due to diverse motion demands and data conflicts. While existing frameworks excel in training single motion-specific policies, they struggle to generalize across highly varied behaviors due to conflicting control requirements and mismatched data distributions. In this work, we propose BumbleBee (BB), an expert-generalist learning framework that combines motion clustering and sim-to-real adaptation to overcome these challenges. BB first leverages an autoencoder-based clustering method to group behaviorally similar motions using motion features and motion descriptions. Expert policies are then trained within each cluster and refined with real-world data through iterative delta action modeling to bridge the sim-to-real gap. Finally, these experts are distilled into a unified generalist controller that preserves agility and robustness across all motion types. Experiments on two simulations and a real humanoid robot demonstrate that BB achieves state-of-the-art general whole-body control, setting a new benchmark for agile, robust, and generalizable humanoid performance in the real world. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "VlvtStQN34@OpenReview",
      "index": 606,
      "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers",
      "authors": [
        "Yusuf Dalva",
        "Hidir Yesiltepe",
        "Pinar Yanardag"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lorashop",
        "concept",
        "editing",
        "lora",
        "transformers",
        "rectified",
        "personalized",
        "photoshop",
        "loras",
        "generation"
      ],
      "summary": "We introduce LoRAShop, the first framework for multi-concept image generation and editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VlvtStQN34"
        ],
        "venue": [
          "/venue/VlvtStQN34@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VlvtStQN34"
        ],
        "detail": [
          "https://openreview.net/forum?id=VlvtStQN34"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Yusuf Dalva , Hidir Yesiltepe , Pinar Yanardag We introduce LoRAShop, the first framework for multi-concept image generation and editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "EtqwyqJrJO@OpenReview",
      "index": 607,
      "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction",
      "authors": [
        "Jiahe Li",
        "Jiawei Zhang",
        "Youmin Zhang",
        "Xiao Bai",
        "Jin Zheng",
        "Xiaohan Yu",
        "Lin Gu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "voxels",
        "geosvr",
        "voxel",
        "sparse",
        "accurate",
        "surface",
        "reconstruction",
        "scene",
        "taming",
        "completeness"
      ],
      "summary": "Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EtqwyqJrJO"
        ],
        "venue": [
          "/venue/EtqwyqJrJO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EtqwyqJrJO"
        ],
        "detail": [
          "https://openreview.net/forum?id=EtqwyqJrJO"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jiahe Li , Jiawei Zhang , Youmin Zhang , Xiao Bai , Jin Zheng , Xiaohan Yu , Lin Gu Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MRvxlTlkNQ@OpenReview",
      "index": 608,
      "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
      "authors": [
        "Jiaru Zou",
        "Yikun Ban",
        "Zihao Li",
        "Yunzhe Qi",
        "Ruizhong Qiu",
        "Ling Yang",
        "Jingrui He"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "copilot",
        "pilot",
        "mistake",
        "transformer",
        "tuning",
        "fine",
        "logits",
        "log",
        "model",
        "paradigm"
      ],
      "summary": "Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model’s own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model’s learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot’s inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot’s logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MRvxlTlkNQ"
        ],
        "venue": [
          "/venue/MRvxlTlkNQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MRvxlTlkNQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=MRvxlTlkNQ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jiaru Zou , Yikun Ban , Zihao Li , Yunzhe Qi , Ruizhong Qiu , Ling Yang , Jingrui He Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model’s own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model’s learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot’s inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot’s logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "tI04KmK27S@OpenReview",
      "index": 609,
      "title": "scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration",
      "authors": [
        "Jianle Sun",
        "Chaoqi Liang",
        "Ran Wei",
        "Peng Zheng",
        "LEI BAI",
        "Wanli Ouyang",
        "Hongliang Yan",
        "Peng Ye"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "omics",
        "unpaired",
        "cell",
        "scmrdr",
        "integration",
        "modality",
        "flexible",
        "multi",
        "biological",
        "single"
      ],
      "summary": "Advances in single-cell sequencing have enabled high-resolution profiling of diverse molecular modalities, while integrating unpaired multi-omics single-cell data remains challenging. Existing approaches either rely on pair information or prior correspondences, or require computing a global pairwise coupling matrix, limiting their scalability and flexibility. In this paper, we introduce a scalable and flexible generative framework called single-cell Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired multi-omics integration. Specifically, we disentangle each cell’s latent representations into modality-shared and modality-specific components using a well-designed β β -VAE architecture, which are augmented with isometric regularization to preserve intra-omics biological heterogeneity, adversarial objective to encourage cross-modal alignment, and masked reconstruction loss strategy to address the issue of missing features across modalities. Our method achieves excellent performance on benchmark datasets in terms of batch correction, modality alignment, and biological signal preservation. Crucially, it scales effectively to large-level datasets and supports integration of more than two omics, offering a powerful and flexible solution for large-scale multi-omics data integration and downstream biological discovery.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tI04KmK27S"
        ],
        "venue": [
          "/venue/tI04KmK27S@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tI04KmK27S"
        ],
        "detail": [
          "https://openreview.net/forum?id=tI04KmK27S"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jianle Sun , Chaoqi Liang , Ran Wei , Peng Zheng , LEI BAI , Wanli Ouyang , Hongliang Yan , Peng Ye Advances in single-cell sequencing have enabled high-resolution profiling of diverse molecular modalities, while integrating unpaired multi-omics single-cell data remains challenging. Existing approaches either rely on pair information or prior correspondences, or require computing a global pairwise coupling matrix, limiting their scalability and flexibility. In this paper, we introduce a scalable and flexible generative framework called single-cell Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired multi-omics integration. Specifically, we disentangle each cell’s latent representations into modality-shared and modality-specific components using a well-designed β β -VAE architecture, which are augmented with isometric regularization to preserve intra-omics biological heterogeneity, adversarial objective to encourage cross-modal alignment, and masked reconstruction loss strategy to address the issue of missing features across modalities. Our method achieves excellent performance on benchmark datasets in terms of batch correction, modality alignment, and biological signal preservation. Crucially, it scales effectively to large-level datasets and supports integration of more than two omics, offering a powerful and flexible solution for large-scale multi-omics data integration and downstream biological discovery. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YX5DHa9OfX@OpenReview",
      "index": 610,
      "title": "Imitation Beyond Expectation Using Pluralistic Stochastic Dominance",
      "authors": [
        "Ali Farajzadeh",
        "Danyal Saeed",
        "Syed M Abbas",
        "Rushit N. Shah",
        "Aadirupa Saha",
        "Brian D Ziebart"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "imitation",
        "pluralistic",
        "dominance",
        "demonstrations",
        "demonstrator",
        "reward",
        "expectation",
        "stochastic",
        "imitators",
        "imitator"
      ],
      "summary": "Imitation learning seeks policies reflecting the values of demonstrated behaviors. Prevalent approaches learn to match or exceed the demonstrator's performance in expectation without knowing the demonstrator’s reward function. Unfortunately, this does not induce pluralistic imitators that learn to support qualitatively distinct demonstrations. We reformulate imitation learning using stochastic dominance over the demonstrations' reward distribution across a range of reward functions as our foundational aim. Our approach matches imitator policy samples (or support) with demonstrations using optimal transport theory to define an imitation learning objective over trajectory pairs. We demonstrate the benefits of pluralistic stochastic dominance (PSD) for imitation in both theory and practice.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YX5DHa9OfX"
        ],
        "venue": [
          "/venue/YX5DHa9OfX@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YX5DHa9OfX"
        ],
        "detail": [
          "https://openreview.net/forum?id=YX5DHa9OfX"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Imitation Beyond Expectation Using Pluralistic Stochastic Dominance [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ali Farajzadeh , Danyal Saeed , Syed M Abbas , Rushit N. Shah , Aadirupa Saha , Brian D Ziebart Imitation learning seeks policies reflecting the values of demonstrated behaviors. Prevalent approaches learn to match or exceed the demonstrator's performance in expectation without knowing the demonstrator’s reward function. Unfortunately, this does not induce pluralistic imitators that learn to support qualitatively distinct demonstrations. We reformulate imitation learning using stochastic dominance over the demonstrations' reward distribution across a range of reward functions as our foundational aim. Our approach matches imitator policy samples (or support) with demonstrations using optimal transport theory to define an imitation learning objective over trajectory pairs. We demonstrate the benefits of pluralistic stochastic dominance (PSD) for imitation in both theory and practice. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "g2AAvmBwkS@OpenReview",
      "index": 611,
      "title": "Cloud4D: Estimating Cloud Properties at a High Spatial and Temporal Resolution",
      "authors": [
        "Jacob Lin",
        "Edward Gryspeerdt",
        "Ronald Clark"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cloud4d",
        "resolution",
        "skyward",
        "cloud",
        "cameras",
        "gusts",
        "spatial",
        "temporal",
        "wind",
        "liquid"
      ],
      "summary": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four–dimensional cloud state using only synchronized ground‐based cameras. Leveraging a homography-guided 2D‐to‐3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ( < 10 < 10 ) against collocated radar measurements.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=g2AAvmBwkS"
        ],
        "venue": [
          "/venue/g2AAvmBwkS@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=g2AAvmBwkS"
        ],
        "detail": [
          "https://openreview.net/forum?id=g2AAvmBwkS"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Cloud4D: Estimating Cloud Properties at a High Spatial and Temporal Resolution [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Jacob Lin , Edward Gryspeerdt , Ronald Clark There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four–dimensional cloud state using only synchronized ground‐based cameras. Leveraging a homography-guided 2D‐to‐3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ( < 10 < 10 ) against collocated radar measurements. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MGJVhzWa2s@OpenReview",
      "index": 612,
      "title": "CLiFT: Compressive Light-Field Tokens for Compute Efficient and Adaptive Neural Rendering",
      "authors": [
        "Zhengqing Wang",
        "Yuefan Wu",
        "Jiacheng Chen",
        "Fuyang Zhang",
        "Yasutaka Furukawa"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tokens",
        "rendering",
        "clift",
        "clifts",
        "view",
        "compute",
        "scene",
        "dl3dv",
        "compressed",
        "number"
      ],
      "summary": "This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. trained to handle a variable number of tokens. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MGJVhzWa2s"
        ],
        "venue": [
          "/venue/MGJVhzWa2s@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MGJVhzWa2s"
        ],
        "detail": [
          "https://openreview.net/forum?id=MGJVhzWa2s"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "CLiFT: Compressive Light-Field Tokens for Compute Efficient and Adaptive Neural Rendering [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zhengqing Wang , Yuefan Wu , Jiacheng Chen , Fuyang Zhang , Yasutaka Furukawa This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. trained to handle a variable number of tokens. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "qgi5TfBXBw@OpenReview",
      "index": 613,
      "title": "Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning",
      "authors": [
        "Yixiu Mao",
        "Yun Qu",
        "Cheems Wang",
        "Xiangyang Ji"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "neighborhood",
        "constraint",
        "anq",
        "offline",
        "actions",
        "adaptive",
        "extrapolation",
        "constrained",
        "reinforcement",
        "learning"
      ],
      "summary": "Offline reinforcement learning (RL) suffers from extrapolation errors induced by out-of-distribution (OOD) actions. To address this, offline RL algorithms typically impose constraints on action selection, which can be systematically categorized into density, support, and sample constraints. However, we show that each category has inherent limitations: density and sample constraints tend to be overly conservative in many scenarios, while the support constraint, though least restrictive, faces challenges in accurately modeling the behavior policy. To overcome these limitations, we propose a new neighborhood constraint that restricts action selection in the Bellman target to the union of neighborhoods of dataset actions. Theoretically, the constraint not only bounds extrapolation errors and distribution shift under certain conditions, but also approximates the support constraint without requiring behavior policy modeling. Moreover, it retains substantial flexibility and enables pointwise conservatism by adapting the neighborhood radius for each data point. In practice, we employ data quality as the adaptation criterion and design an adaptive neighborhood constraint. Building on an efficient bilevel optimization framework, we develop a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning (ANQ), to perform Q learning with target actions satisfying this constraint. Empirically, ANQ achieves state-of-the-art performance on standard offline RL benchmarks and exhibits strong robustness in scenarios with noisy or limited data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qgi5TfBXBw"
        ],
        "venue": [
          "/venue/qgi5TfBXBw@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qgi5TfBXBw"
        ],
        "detail": [
          "https://openreview.net/forum?id=qgi5TfBXBw"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Yixiu Mao , Yun Qu , Cheems Wang , Xiangyang Ji Offline reinforcement learning (RL) suffers from extrapolation errors induced by out-of-distribution (OOD) actions. To address this, offline RL algorithms typically impose constraints on action selection, which can be systematically categorized into density, support, and sample constraints. However, we show that each category has inherent limitations: density and sample constraints tend to be overly conservative in many scenarios, while the support constraint, though least restrictive, faces challenges in accurately modeling the behavior policy. To overcome these limitations, we propose a new neighborhood constraint that restricts action selection in the Bellman target to the union of neighborhoods of dataset actions. Theoretically, the constraint not only bounds extrapolation errors and distribution shift under certain conditions, but also approximates the support constraint without requiring behavior policy modeling. Moreover, it retains substantial flexibility and enables pointwise conservatism by adapting the neighborhood radius for each data point. In practice, we employ data quality as the adaptation criterion and design an adaptive neighborhood constraint. Building on an efficient bilevel optimization framework, we develop a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning (ANQ), to perform Q learning with target actions satisfying this constraint. Empirically, ANQ achieves state-of-the-art performance on standard offline RL benchmarks and exhibits strong robustness in scenarios with noisy or limited data. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "YexxvBGwQM@OpenReview",
      "index": 614,
      "title": "To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning",
      "authors": [
        "Ming Li",
        "Jike Zhong",
        "Shitian Zhao",
        "Yuxiang Lai",
        "Haoquan Zhang",
        "Wang Bill Zhu",
        "Kaipeng Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rft",
        "thinking",
        "think",
        "textbf",
        "mllms",
        "textit",
        "tags",
        "tuning",
        "explicit",
        "verifiable"
      ],
      "summary": "This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for multi-modal large language models (MLLMs). We first extend \\textit{Thinking-RFT} to image classification task, using verifiable rewards for fine-tuning~(FT). Experiments show {Thinking-RFT} significantly outperforms supervised FT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary and beneficial. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce \\textit{No-Thinking-RFT}, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RFT on six diverse tasks across different model sizes and types. Experiment results reveal four key findings: \\textbf{(1).} Visual perception tasks do not require thinking during RFT, as No-Thinking-RFT consistently outperforms or matches Thinking-RFT across model sizes and types. \\textbf{(2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-RFT less effective than No-Thinking-RFT. \\textbf{(3).} There are inconsistencies between the answers in the thinking tags and answer tags for some responses of Thinking-RFT, which show lower average accuracy than the overall accuracy. \\textbf{(4).} The performance gain of No-Thinking-RFT mainly stems from improved learning during no thinking FT and the avoidance of inference overthinking, as evidenced by the partial gains from appending empty thinking tags at inference time of Thinking-RFT. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance in certain scenarios. To test this, we propose \\textit{Think-After-Answer}, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an \\textit{Adaptive-Thinking} method. Experiments show that model converges to either thinking or not depending on model capability, achieving comparable or better performance than both Thinking and No-Thinking-RFT. Our findings suggest MLLMs can adaptively decide to think or not based on their capabilities and task complexity, offering insights into the thinking process in RFT.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YexxvBGwQM"
        ],
        "venue": [
          "/venue/YexxvBGwQM@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YexxvBGwQM"
        ],
        "detail": [
          "https://openreview.net/forum?id=YexxvBGwQM"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Ming Li , Jike Zhong , Shitian Zhao , Yuxiang Lai , Haoquan Zhang , Wang Bill Zhu , Kaipeng Zhang This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for multi-modal large language models (MLLMs). We first extend \\textit{Thinking-RFT} to image classification task, using verifiable rewards for fine-tuning~(FT). Experiments show {Thinking-RFT} significantly outperforms supervised FT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary and beneficial. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce \\textit{No-Thinking-RFT}, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RFT on six diverse tasks across different model sizes and types. Experiment results reveal four key findings: \\textbf{(1).} Visual perception tasks do not require thinking during RFT, as No-Thinking-RFT consistently outperforms or matches Thinking-RFT across model sizes and types. \\textbf{(2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-RFT less effective than No-Thinking-RFT. \\textbf{(3).} There are inconsistencies between the answers in the thinking tags and answer tags for some responses of Thinking-RFT, which show lower average accuracy than the overall accuracy. \\textbf{(4).} The performance gain of No-Thinking-RFT mainly stems from improved learning during no thinking FT and the avoidance of inference overthinking, as evidenced by the partial gains from appending empty thinking tags at inference time of Thinking-RFT. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance in certain scenarios. To test this, we propose \\textit{Think-After-Answer}, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an \\textit{Adaptive-Thinking} method. Experiments show that model converges to either thinking or not depending on model capability, achieving comparable or better performance than both Thinking and No-Thinking-RFT. Our findings suggest MLLMs can adaptively decide to think or not based on their capabilities and task complexity, offering insights into the thinking process in RFT. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ZZ09oX2Xpo@OpenReview",
      "index": 615,
      "title": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy",
      "authors": [
        "Yuran Wang",
        "Ruihai Wu",
        "Yue Chen",
        "Jiarui Wang",
        "Jiaqi Liang",
        "Ziyu Zhu",
        "Haoran Geng",
        "Jitendra Malik",
        "Pieter Abbeel",
        "Hao Dong"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "garment",
        "manipulation",
        "dexgarmentlab",
        "dexterous",
        "garments",
        "dexterity",
        "generalizable",
        "policy",
        "deformations",
        "expert"
      ],
      "summary": "Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: https://wayrise.github.io/DexGarmentLab/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZZ09oX2Xpo"
        ],
        "venue": [
          "/venue/ZZ09oX2Xpo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZZ09oX2Xpo"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZZ09oX2Xpo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yuran Wang , Ruihai Wu , Yue Chen , Jiarui Wang , Jiaqi Liang , Ziyu Zhu , Haoran Geng , Jitendra Malik , Pieter Abbeel , Hao Dong Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: https://wayrise.github.io/DexGarmentLab/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "AghtKxDf7f@OpenReview",
      "index": 616,
      "title": "STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation",
      "authors": [
        "Hossein Goli",
        "Michael Gimelfarb",
        "Nathan Samuel de Lara",
        "Haruki Nishimura",
        "Masha Itkina",
        "Florian Shkurti"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "ope",
        "stitch",
        "policy",
        "stitching",
        "horizon",
        "diffusion",
        "trajectories",
        "trajectory",
        "target",
        "behavior"
      ],
      "summary": "Off-policy evaluation (OPE) estimates the performance of a target policy using offline data collected from a behavior policy, and is crucial in domains such as robotics or healthcare where direct interaction with the environment is costly or unsafe. Existing OPE methods are ineffective for high-dimensional, long-horizon problems, due to exponential blow-ups in variance from importance weighting or compounding errors from learned dynamics models. To address these challenges, we propose STITCH-OPE, a model-based generative framework that leverages denoising diffusion for long-horizon OPE in high-dimensional state and action spaces. Starting with a diffusion model pre-trained on the behavior data, STITCH-OPE generates synthetic trajectories from the target policy by guiding the denoising process using the score function of the target policy. STITCH-OPE proposes two technical innovations that make it advantageous for OPE: (1) prevents over-regularization by subtracting the score of the behavior policy during guidance, and (2) generates long-horizon trajectories by stitching partial trajectories together end-to-end. We provide a theoretical guarantee that under mild assumptions, these modifications result in an exponential reduction in variance versus long-horizon trajectory diffusion. Experiments on the D4RL and OpenAI Gym benchmarks show substantial improvement in mean squared error, correlation, and regret metrics compared to state-of-the-art OPE methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AghtKxDf7f"
        ],
        "venue": [
          "/venue/AghtKxDf7f@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AghtKxDf7f"
        ],
        "detail": [
          "https://openreview.net/forum?id=AghtKxDf7f"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Hossein Goli , Michael Gimelfarb , Nathan Samuel de Lara , Haruki Nishimura , Masha Itkina , Florian Shkurti Off-policy evaluation (OPE) estimates the performance of a target policy using offline data collected from a behavior policy, and is crucial in domains such as robotics or healthcare where direct interaction with the environment is costly or unsafe. Existing OPE methods are ineffective for high-dimensional, long-horizon problems, due to exponential blow-ups in variance from importance weighting or compounding errors from learned dynamics models. To address these challenges, we propose STITCH-OPE, a model-based generative framework that leverages denoising diffusion for long-horizon OPE in high-dimensional state and action spaces. Starting with a diffusion model pre-trained on the behavior data, STITCH-OPE generates synthetic trajectories from the target policy by guiding the denoising process using the score function of the target policy. STITCH-OPE proposes two technical innovations that make it advantageous for OPE: (1) prevents over-regularization by subtracting the score of the behavior policy during guidance, and (2) generates long-horizon trajectories by stitching partial trajectories together end-to-end. We provide a theoretical guarantee that under mild assumptions, these modifications result in an exponential reduction in variance versus long-horizon trajectory diffusion. Experiments on the D4RL and OpenAI Gym benchmarks show substantial improvement in mean squared error, correlation, and regret metrics compared to state-of-the-art OPE methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "pkuVonMwhT@OpenReview",
      "index": 617,
      "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing",
      "authors": [
        "Jan Philipp Schneider",
        "Pratik Singh Bisht",
        "Ilya Chugunov",
        "Andreas Kolb",
        "Michael Moeller",
        "Felix Heide"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "editing",
        "scene",
        "nags",
        "scenes",
        "atlas",
        "driving",
        "editable",
        "psnr",
        "dynamic",
        "neural"
      ],
      "summary": "Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes. Project Page: https://princeton-computational-imaging.github.io/nag/",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pkuVonMwhT"
        ],
        "venue": [
          "/venue/pkuVonMwhT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pkuVonMwhT"
        ],
        "detail": [
          "https://openreview.net/forum?id=pkuVonMwhT"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jan Philipp Schneider , Pratik Singh Bisht , Ilya Chugunov , Andreas Kolb , Michael Moeller , Felix Heide Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes. Project Page: https://princeton-computational-imaging.github.io/nag/ Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "QIXdI207nq@OpenReview",
      "index": 618,
      "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
      "authors": [
        "Huanlin Gao",
        "Ping Chen",
        "Fuyuan Shi",
        "Chao Tan",
        "Zhaoxiang Liu",
        "Fang Zhao",
        "Kai Wang",
        "Shiguo Lian"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lemica",
        "video",
        "caching",
        "lexicographic",
        "generation",
        "minimax",
        "path",
        "diffusion",
        "degradation",
        "latte"
      ],
      "summary": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9× speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QIXdI207nq"
        ],
        "venue": [
          "/venue/QIXdI207nq@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QIXdI207nq"
        ],
        "detail": [
          "https://openreview.net/forum?id=QIXdI207nq"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Huanlin Gao , Ping Chen , Fuyuan Shi , Chao Tan , Zhaoxiang Liu , Fang Zhao , Kai Wang , Shiguo Lian We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9× speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rOR5IZcwJx@OpenReview",
      "index": 619,
      "title": "Robust SuperAlignment: Weak-to-Strong Robustness Generalization for Vision-Language Models",
      "authors": [
        "Junhao Dong",
        "Cong Zhang",
        "Xinghua Qu",
        "Zejun MA",
        "Piotr Koniusz",
        "Yew-Soon Ong"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "superalignment",
        "robustness",
        "weak",
        "adversarial",
        "vlms",
        "strong",
        "generalization",
        "vision",
        "language",
        "supervision"
      ],
      "summary": "Numerous well-established studies have demonstrated the superhuman capabilities of modern Vision-Language Models (VLMs) across a wide range of tasks. However, growing is the doubt about the continuing availability of reliable high-quality labeling (supervision) from human annotators, leading to stagnation of the model's performance. To address this challenge, ``superalignment'' employs the so-called weak-to-strong generalization paradigm, where the supervision from a weak model can provide generalizable knowledge for a strong model. While effective in aligning knowledge for clean samples between the strong and weak models, the standard weak-to-strong approach typically fails to capture adversarial robustness, exposing strong VLMs to adversarial attacks. This inability to transfer adversarial robustness is because adversarial samples are normally missing in the superalignment stage. To this end, we are the first to propose the weak-to-strong (adversarial) robustness generalization method to elicit zero-shot robustness in large-scale models by an unsupervised scheme, mitigating the unreliable information source for alignment from two perspectives: alignment re-weighting and source guidance refinement. We analyze settings under which robustness generalization is possible. Extensive experiments across various vision-language benchmarks validate the effectiveness of our method in numerous scenarios, demonstrating its plug-and-play applicability to large-scale VLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rOR5IZcwJx"
        ],
        "venue": [
          "/venue/rOR5IZcwJx@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rOR5IZcwJx"
        ],
        "detail": [
          "https://openreview.net/forum?id=rOR5IZcwJx"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Robust SuperAlignment: Weak-to-Strong Robustness Generalization for Vision-Language Models [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Junhao Dong , Cong Zhang , Xinghua Qu , Zejun MA , Piotr Koniusz , Yew-Soon Ong Numerous well-established studies have demonstrated the superhuman capabilities of modern Vision-Language Models (VLMs) across a wide range of tasks. However, growing is the doubt about the continuing availability of reliable high-quality labeling (supervision) from human annotators, leading to stagnation of the model's performance. To address this challenge, ``superalignment'' employs the so-called weak-to-strong generalization paradigm, where the supervision from a weak model can provide generalizable knowledge for a strong model. While effective in aligning knowledge for clean samples between the strong and weak models, the standard weak-to-strong approach typically fails to capture adversarial robustness, exposing strong VLMs to adversarial attacks. This inability to transfer adversarial robustness is because adversarial samples are normally missing in the superalignment stage. To this end, we are the first to propose the weak-to-strong (adversarial) robustness generalization method to elicit zero-shot robustness in large-scale models by an unsupervised scheme, mitigating the unreliable information source for alignment from two perspectives: alignment re-weighting and source guidance refinement. We analyze settings under which robustness generalization is possible. Extensive experiments across various vision-language benchmarks validate the effectiveness of our method in numerous scenarios, demonstrating its plug-and-play applicability to large-scale VLMs. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "tq9lyV9Cml@OpenReview",
      "index": 620,
      "title": "Thought Communication in Multiagent Collaboration",
      "authors": [
        "Yujia Zheng",
        "Zhuokai Zhao",
        "Zijian Li",
        "Yaqi Xie",
        "Mingze Gao",
        "Lizhu Zhang",
        "Kun Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "thoughts",
        "thought",
        "agents",
        "latent",
        "communication",
        "agent",
        "mind",
        "multiagent",
        "language",
        "telepathy"
      ],
      "summary": "Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, *thought communication*, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tq9lyV9Cml"
        ],
        "venue": [
          "/venue/tq9lyV9Cml@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tq9lyV9Cml"
        ],
        "detail": [
          "https://openreview.net/forum?id=tq9lyV9Cml"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Thought Communication in Multiagent Collaboration [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Yujia Zheng , Zhuokai Zhao , Zijian Li , Yaqi Xie , Mingze Gao , Lizhu Zhang , Kun Zhang Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, *thought communication*, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "EkoAKNikAj@OpenReview",
      "index": 621,
      "title": "Polyline Path Masked Attention for Vision Transformer",
      "authors": [
        "Zhongchen Zhao",
        "Chaodong Xiao",
        "Hui LIN",
        "Qi Xie",
        "Lei Zhang",
        "Deyu Meng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "polyline",
        "mask",
        "ppma",
        "mamba2",
        "path",
        "vits",
        "attention",
        "adjacency",
        "modeling",
        "structured"
      ],
      "summary": "Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks. Recently, Vision Transformers (ViTs) have achieved remarkable success in computer vision, leveraging the powerful global dependency modeling capability of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its significant potential in natural language processing tasks by explicitly modeling the spatial adjacency prior through the structured mask. In this paper, we propose Polyline Path Masked Attention (PPMA) that integrates the self-attention mechanism of ViTs with an enhanced structured mask of Mamba2, harnessing the complementary strengths of both architectures. Specifically, we first ameliorate the traditional structured mask of Mamba2 by introducing a 2D polyline path scanning strategy and derive its corresponding structured mask, polyline path mask, which better preserves the adjacency relationships among image tokens. Notably, we conduct a thorough theoretical analysis on the structural characteristics of the proposed polyline path mask and design an efficient algorithm for the computation of the polyline path mask. Next, we embed the polyline path mask into the self-attention mechanism of ViTs, enabling explicit modeling of spatial adjacency prior. Extensive experiments on standard benchmarks, including image classification, object detection, and segmentation, demonstrate that our model outperforms previous state-of-the-art approaches based on both state-space models and Transformers. For example, our proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%, respectively. Code is available at https://github.com/zhongchenzhao/PPMA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EkoAKNikAj"
        ],
        "venue": [
          "/venue/EkoAKNikAj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EkoAKNikAj"
        ],
        "detail": [
          "https://openreview.net/forum?id=EkoAKNikAj"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Polyline Path Masked Attention for Vision Transformer [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Zhongchen Zhao , Chaodong Xiao , Hui LIN , Qi Xie , Lei Zhang , Deyu Meng Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks. Recently, Vision Transformers (ViTs) have achieved remarkable success in computer vision, leveraging the powerful global dependency modeling capability of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its significant potential in natural language processing tasks by explicitly modeling the spatial adjacency prior through the structured mask. In this paper, we propose Polyline Path Masked Attention (PPMA) that integrates the self-attention mechanism of ViTs with an enhanced structured mask of Mamba2, harnessing the complementary strengths of both architectures. Specifically, we first ameliorate the traditional structured mask of Mamba2 by introducing a 2D polyline path scanning strategy and derive its corresponding structured mask, polyline path mask, which better preserves the adjacency relationships among image tokens. Notably, we conduct a thorough theoretical analysis on the structural characteristics of the proposed polyline path mask and design an efficient algorithm for the computation of the polyline path mask. Next, we embed the polyline path mask into the self-attention mechanism of ViTs, enabling explicit modeling of spatial adjacency prior. Extensive experiments on standard benchmarks, including image classification, object detection, and segmentation, demonstrate that our model outperforms previous state-of-the-art approaches based on both state-space models and Transformers. For example, our proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%, respectively. Code is available at https://github.com/zhongchenzhao/PPMA. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "SJPq1xBPHX@OpenReview",
      "index": 622,
      "title": "PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with Pre-trained Language Models",
      "authors": [
        "Tonglong Wei",
        "Yan Lin",
        "Youfang Lin",
        "Shengnan Guo",
        "Jilin Hu",
        "Haitao Yuan",
        "Gao Cong",
        "Huaiyu Wan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "trajectory",
        "plmtrajrec",
        "trajectories",
        "intervals",
        "road",
        "recovery",
        "sparse",
        "missing",
        "plm",
        "points"
      ],
      "summary": "Spatiotemporal trajectory data is crucial for various traffic-related applications. However, issues such as device malfunctions and network instability often result in sparse trajectories that lose detailed movement information compared to their dense counterparts. Recovering missing points in sparse trajectories is thus essential. Despite recent progress, three challenges remain. First, the lack of large-scale dense trajectory datasets hinders the training of a trajectory recovery model. Second, the varying spatiotemporal correlations in sparse trajectories make it hard to generalize across different sampling intervals. Third, extracting road conditions for missing points is non-trivial. To address these challenges, we propose PLMTrajRec PLMTrajRec , a novel trajectory recovery model. It leverages the scalability of a pre-trained language model (PLM) and can effectively recover trajectories by fine-tuning with small-scale dense trajectory datasets. To handle different sampling intervals in sparse trajectories, we first convert sampling intervals and movement features into prompts for the PLM to understand. We then introduce a trajectory encoder to unify trajectories of varying intervals into a single interval. To extract road conditions for missing points, we propose an area flow-guided implicit trajectory prompt that represents traffic conditions in each region, and a road condition passing mechanism that infers the road conditions of missing points using the observed ones. Experiments on four public trajectory datasets with three sampling intervals demonstrate the effectiveness, scalability, and generalization ability of PLMTrajRec. Code is available at https://github.com/wtl52656/PLMTrajRec.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SJPq1xBPHX"
        ],
        "venue": [
          "/venue/SJPq1xBPHX@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SJPq1xBPHX"
        ],
        "detail": [
          "https://openreview.net/forum?id=SJPq1xBPHX"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with Pre-trained Language Models [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Tonglong Wei , Yan Lin , Youfang Lin , Shengnan Guo , Jilin Hu , Haitao Yuan , Gao Cong , Huaiyu Wan Spatiotemporal trajectory data is crucial for various traffic-related applications. However, issues such as device malfunctions and network instability often result in sparse trajectories that lose detailed movement information compared to their dense counterparts. Recovering missing points in sparse trajectories is thus essential. Despite recent progress, three challenges remain. First, the lack of large-scale dense trajectory datasets hinders the training of a trajectory recovery model. Second, the varying spatiotemporal correlations in sparse trajectories make it hard to generalize across different sampling intervals. Third, extracting road conditions for missing points is non-trivial. To address these challenges, we propose PLMTrajRec PLMTrajRec , a novel trajectory recovery model. It leverages the scalability of a pre-trained language model (PLM) and can effectively recover trajectories by fine-tuning with small-scale dense trajectory datasets. To handle different sampling intervals in sparse trajectories, we first convert sampling intervals and movement features into prompts for the PLM to understand. We then introduce a trajectory encoder to unify trajectories of varying intervals into a single interval. To extract road conditions for missing points, we propose an area flow-guided implicit trajectory prompt that represents traffic conditions in each region, and a road condition passing mechanism that infers the road conditions of missing points using the observed ones. Experiments on four public trajectory datasets with three sampling intervals demonstrate the effectiveness, scalability, and generalization ability of PLMTrajRec. Code is available at https://github.com/wtl52656/PLMTrajRec. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "rZ2nSt1X58@OpenReview",
      "index": 623,
      "title": "Optimization Inspired Few-Shot Adaptation for Large Language Models",
      "authors": [
        "Boyan Gao",
        "Xin Wang",
        "Yibo Yang",
        "David A. Clifton"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "peft",
        "shot",
        "optimization",
        "adaptation",
        "preconditioners",
        "llms",
        "inspired",
        "ofa",
        "language",
        "tuning"
      ],
      "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in real-world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Existing approaches, such as In-context learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations: In-context learning introduces additional inference computational overhead with limited performance gains, while PEFT models are prone to overfitting on the few demonstration examples. In this work, we reinterpret the forward pass of LLMs as an optimization process, a sequence of preconditioned gradient descent steps refining internal representations. Based on this connection, we propose Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound, while simultaneously steering the optimization path toward the flat local minimum. Our method overcomes both issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rZ2nSt1X58"
        ],
        "venue": [
          "/venue/rZ2nSt1X58@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rZ2nSt1X58"
        ],
        "detail": [
          "https://openreview.net/forum?id=rZ2nSt1X58"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Optimization Inspired Few-Shot Adaptation for Large Language Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Boyan Gao , Xin Wang , Yibo Yang , David A. Clifton Large Language Models (LLMs) have demonstrated remarkable performance in real-world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Existing approaches, such as In-context learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations: In-context learning introduces additional inference computational overhead with limited performance gains, while PEFT models are prone to overfitting on the few demonstration examples. In this work, we reinterpret the forward pass of LLMs as an optimization process, a sequence of preconditioned gradient descent steps refining internal representations. Based on this connection, we propose Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound, while simultaneously steering the optimization path toward the flat local minimum. Our method overcomes both issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "26kUrQm4zw@OpenReview",
      "index": 624,
      "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging",
      "authors": [
        "Hongjin Qian",
        "Zheng Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "inforage",
        "retrieval",
        "reasoning",
        "foraging",
        "scent",
        "information",
        "llms",
        "search",
        "tasks",
        "adaptive"
      ],
      "summary": "Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents. We provide all codes and datasets in the supplementary materials.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=26kUrQm4zw"
        ],
        "venue": [
          "/venue/26kUrQm4zw@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=26kUrQm4zw"
        ],
        "detail": [
          "https://openreview.net/forum?id=26kUrQm4zw"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Hongjin Qian , Zheng Liu Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents. We provide all codes and datasets in the supplementary materials. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "PtGMadeONU@OpenReview",
      "index": 625,
      "title": "Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation",
      "authors": [
        "Yuyang Li",
        "Wenxin Du",
        "Chang Yu",
        "Puhao Li",
        "Zihang Zhao",
        "Tengyu Liu",
        "Chenfanfu Jiang",
        "Yixin Zhu",
        "Siyuan Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tactile",
        "taccel",
        "vbtss",
        "robotics",
        "simulation",
        "vision",
        "robotic",
        "manipulation",
        "robots",
        "abd"
      ],
      "summary": "Tactile sensing is crucial for achieving human-level robotic capabilities in manipulation tasks. As a promising solution, Vision-based Tactile Sensors (VBTSs) offer high spatial resolution and cost-effectiveness, but present unique challenges in robotics for their complex physical characteristics and visual signal processing requirements. The lack of efficient and accurate simulation tools for VBTSs has significantly limited the scale and scope of tactile robotics research. We present Taccel, a high-performance simulation platform that integrates Incremental Potential Contact (IPC) and Affine Body Dynamics (ABD) to model robots, tactile sensors, and objects with both accuracy and unprecedented speed, achieving a total of 915 FPS with 4096 parallel environments. Unlike previous simulators that operate at sub-real-time speeds with limited parallelization, Taccel provides precise physics simulation and realistic tactile signals while supporting flexible robot-sensor configurations through user-friendly APIs. Through extensive validation in object recognition, robotic grasping, and articulated object manipulation, we demonstrate precise simulation and successful sim-to-real transfer. These capabilities position Taccel as a powerful tool for scaling up tactile robotics research and development, potentially transforming how robots interact with and understand their physical environment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PtGMadeONU"
        ],
        "venue": [
          "/venue/PtGMadeONU@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PtGMadeONU"
        ],
        "detail": [
          "https://openreview.net/forum?id=PtGMadeONU"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yuyang Li , Wenxin Du , Chang Yu , Puhao Li , Zihang Zhao , Tengyu Liu , Chenfanfu Jiang , Yixin Zhu , Siyuan Huang Tactile sensing is crucial for achieving human-level robotic capabilities in manipulation tasks. As a promising solution, Vision-based Tactile Sensors (VBTSs) offer high spatial resolution and cost-effectiveness, but present unique challenges in robotics for their complex physical characteristics and visual signal processing requirements. The lack of efficient and accurate simulation tools for VBTSs has significantly limited the scale and scope of tactile robotics research. We present Taccel, a high-performance simulation platform that integrates Incremental Potential Contact (IPC) and Affine Body Dynamics (ABD) to model robots, tactile sensors, and objects with both accuracy and unprecedented speed, achieving a total of 915 FPS with 4096 parallel environments. Unlike previous simulators that operate at sub-real-time speeds with limited parallelization, Taccel provides precise physics simulation and realistic tactile signals while supporting flexible robot-sensor configurations through user-friendly APIs. Through extensive validation in object recognition, robotic grasping, and articulated object manipulation, we demonstrate precise simulation and successful sim-to-real transfer. These capabilities position Taccel as a powerful tool for scaling up tactile robotics research and development, potentially transforming how robots interact with and understand their physical environment. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "MDWJlTWZHH@OpenReview",
      "index": 626,
      "title": "Toward Relative Positional Encoding in Spiking Transformers",
      "authors": [
        "Changze Lv",
        "Yansen Wang",
        "Dongqi Han",
        "Yifei Shen",
        "Xiaoqing Zheng",
        "Xuanjing Huang",
        "Dongsheng Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "spiking",
        "rpe",
        "transformers",
        "positional",
        "snns",
        "encoding",
        "relative",
        "spikes",
        "tasks",
        "gray"
      ],
      "summary": "Spiking neural networks (SNNs) are bio-inspired networks that mimic how neurons in the brain communicate through discrete spikes, which have great potential in various tasks due to their energy efficiency and temporal processing capabilities. SNNs with self-attention mechanisms (spiking Transformers) have recently shown great advancements in various tasks, and inspired by traditional Transformers, several studies have demonstrated that spiking absolute positional encoding can help capture sequential relationships for input data, enhancing the capabilities of spiking Transformers for tasks such as sequential modeling and image classification. However, how to incorporate relative positional information into SNNs remains a challenge. In this paper, we introduce several strategies to approximate relative positional encoding (RPE) in spiking Transformers while preserving the binary nature of spikes. Firstly, we formally prove that encoding relative distances with Gray Code ensures that the binary representations of positional indices maintain a constant Hamming distance whenever their decimal values differ by a power of two, and we propose **Gray-PE** based on this property. In addition, we propose another RPE method called **Log-PE**, which combines the logarithmic form of the relative distance matrix directly into the spiking attention map. Furthermore, we extend our RPE methods to a two-dimensional form, making them suitable for processing image patches. We evaluate our RPE methods on various tasks, including time series forecasting, text classification, and patch-based image classification, and the experimental results demonstrate a satisfying performance gain by incorporating our RPE methods across many architectures. Our results provide fresh perspectives on designing spiking Transformers to advance their sequential modeling capability, thereby expanding their applicability across various domains. Our code is available at https://github.com/microsoft/SeqSNN.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MDWJlTWZHH"
        ],
        "venue": [
          "/venue/MDWJlTWZHH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MDWJlTWZHH"
        ],
        "detail": [
          "https://openreview.net/forum?id=MDWJlTWZHH"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Toward Relative Positional Encoding in Spiking Transformers [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Changze Lv , Yansen Wang , Dongqi Han , Yifei Shen , Xiaoqing Zheng , Xuanjing Huang , Dongsheng Li Spiking neural networks (SNNs) are bio-inspired networks that mimic how neurons in the brain communicate through discrete spikes, which have great potential in various tasks due to their energy efficiency and temporal processing capabilities. SNNs with self-attention mechanisms (spiking Transformers) have recently shown great advancements in various tasks, and inspired by traditional Transformers, several studies have demonstrated that spiking absolute positional encoding can help capture sequential relationships for input data, enhancing the capabilities of spiking Transformers for tasks such as sequential modeling and image classification. However, how to incorporate relative positional information into SNNs remains a challenge. In this paper, we introduce several strategies to approximate relative positional encoding (RPE) in spiking Transformers while preserving the binary nature of spikes. Firstly, we formally prove that encoding relative distances with Gray Code ensures that the binary representations of positional indices maintain a constant Hamming distance whenever their decimal values differ by a power of two, and we propose **Gray-PE** based on this property. In addition, we propose another RPE method called **Log-PE**, which combines the logarithmic form of the relative distance matrix directly into the spiking attention map. Furthermore, we extend our RPE methods to a two-dimensional form, making them suitable for processing image patches. We evaluate our RPE methods on various tasks, including time series forecasting, text classification, and patch-based image classification, and the experimental results demonstrate a satisfying performance gain by incorporating our RPE methods across many architectures. Our results provide fresh perspectives on designing spiking Transformers to advance their sequential modeling capability, thereby expanding their applicability across various domains. Our code is available at https://github.com/microsoft/SeqSNN. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "TcVCu2PKb9@OpenReview",
      "index": 627,
      "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup",
      "authors": [
        "Fanxu Meng",
        "Pingzhi Tang",
        "Zengwei Yao",
        "Xing Sun",
        "Muhan Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mla",
        "gqa",
        "deepseek",
        "transmla",
        "cache",
        "migrating",
        "speedup",
        "compressing",
        "llama",
        "sglang"
      ],
      "summary": "Modern large-language models often face communication bottlenecks on current hardware rather than computational limitations. *Multi-head latent attention (MLA)* addresses this by compressing the key-value cache using low-rank matrices, while the Absorb operation prevents the KV cache from reverting to its original size, significantly boosting both training and inference speed. Despite the success of DeepSeek V2/V3/R1, most model providers have heavily invested in optimizing GQA-based models and, therefore, lack strong incentives to retrain MLA-based models from scratch. This paper demonstrates that MLA provides superior expressive power compared to GQA with the same KV cache overhead, thereby offering a rationale for transitioning from GQA to MLA. In addition, we introduce TransMLA, a framework that seamlessly converts any GQA-based pre-trained model (e.g., LLaMA, Qwen, Gemma, Mistral/Mixtral) into an MLA-based model. For the first time, our method enables *direct conversion of these models into a format compatible with DeepSeek's codebase*, allowing them to fully leverage the existing, highly-optimized support for the DeepSeek architecture within inference engines like vLLM and SGlang. By compressing 93\\% of the KV cache in LLaMA-2-7B, we achieve a **10x speedup** with an 8K context length while maintaining meaningful output. Moreover, the model requires only **6B tokens** for fine-tuning to recover comparable performance across multiple benchmarks. TransMLA provides a practical path for migrating GQA-based models to the MLA structure, and when combined with DeepSeek’s advanced optimizations—such as FP8 quantization and Multi-Token Prediction—further inference acceleration can be achieved.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TcVCu2PKb9"
        ],
        "venue": [
          "/venue/TcVCu2PKb9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TcVCu2PKb9"
        ],
        "detail": [
          "https://openreview.net/forum?id=TcVCu2PKb9"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Fanxu Meng , Pingzhi Tang , Zengwei Yao , Xing Sun , Muhan Zhang Modern large-language models often face communication bottlenecks on current hardware rather than computational limitations. *Multi-head latent attention (MLA)* addresses this by compressing the key-value cache using low-rank matrices, while the Absorb operation prevents the KV cache from reverting to its original size, significantly boosting both training and inference speed. Despite the success of DeepSeek V2/V3/R1, most model providers have heavily invested in optimizing GQA-based models and, therefore, lack strong incentives to retrain MLA-based models from scratch. This paper demonstrates that MLA provides superior expressive power compared to GQA with the same KV cache overhead, thereby offering a rationale for transitioning from GQA to MLA. In addition, we introduce TransMLA, a framework that seamlessly converts any GQA-based pre-trained model (e.g., LLaMA, Qwen, Gemma, Mistral/Mixtral) into an MLA-based model. For the first time, our method enables *direct conversion of these models into a format compatible with DeepSeek's codebase*, allowing them to fully leverage the existing, highly-optimized support for the DeepSeek architecture within inference engines like vLLM and SGlang. By compressing 93\\% of the KV cache in LLaMA-2-7B, we achieve a **10x speedup** with an 8K context length while maintaining meaningful output. Moreover, the model requires only **6B tokens** for fine-tuning to recover comparable performance across multiple benchmarks. TransMLA provides a practical path for migrating GQA-based models to the MLA structure, and when combined with DeepSeek’s advanced optimizations—such as FP8 quantization and Multi-Token Prediction—further inference acceleration can be achieved. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bRAm7A02Qm@OpenReview",
      "index": 628,
      "title": "Towards Understanding the Mechanisms of Classifier-Free Guidance",
      "authors": [
        "Xiang Li",
        "Rongrong Wang",
        "Qing Qu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cfg",
        "cpc",
        "nonlinear",
        "guidance",
        "resembles",
        "mechanisms",
        "classifier",
        "linear",
        "term",
        "steers"
      ],
      "summary": "Classifier-free guidance (CFG) is a core technique powering state-of-the-art image generation systems, yet its underlying mechanisms remain poorly understood. In this work, we first analyze CFG in a simplified linear diffusion model, where we show its behavior closely resembles that observed in the nonlinear case. Our analysis reveals that linear CFG improves generation quality via three distinct components: (i) a mean-shift term that approximately steers samples in the direction of class means, (ii) a positive Contrastive Principal Components (CPC) term that amplifies class-specific features, and (iii) a negative CPC term that suppresses generic features prevalent in unconditional data. We then verify that these insights in real-world, nonlinear diffusion models: over a broad range of noise levels, linear CFG resembles the behavior of its nonlinear counterpart. Although the two eventually diverge at low noise levels, we discuss how the insights from the linear analysis still shed light on the CFG's mechanism within the nonlinear regime.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bRAm7A02Qm"
        ],
        "venue": [
          "/venue/bRAm7A02Qm@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bRAm7A02Qm"
        ],
        "detail": [
          "https://openreview.net/forum?id=bRAm7A02Qm"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Towards Understanding the Mechanisms of Classifier-Free Guidance [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Xiang Li , Rongrong Wang , Qing Qu Classifier-free guidance (CFG) is a core technique powering state-of-the-art image generation systems, yet its underlying mechanisms remain poorly understood. In this work, we first analyze CFG in a simplified linear diffusion model, where we show its behavior closely resembles that observed in the nonlinear case. Our analysis reveals that linear CFG improves generation quality via three distinct components: (i) a mean-shift term that approximately steers samples in the direction of class means, (ii) a positive Contrastive Principal Components (CPC) term that amplifies class-specific features, and (iii) a negative CPC term that suppresses generic features prevalent in unconditional data. We then verify that these insights in real-world, nonlinear diffusion models: over a broad range of noise levels, linear CFG resembles the behavior of its nonlinear counterpart. Although the two eventually diverge at low noise levels, we discuss how the insights from the linear analysis still shed light on the CFG's mechanism within the nonlinear regime. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hLJLP3CmHR@OpenReview",
      "index": 629,
      "title": "PhysX-3D: Physical-Grounded 3D Asset Generation",
      "authors": [
        "Ziang Cao",
        "Zhaoxi Chen",
        "Liang Pan",
        "Ziwei Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "textbf",
        "textcolor",
        "physical",
        "grounded",
        "physx",
        "assets",
        "asset",
        "physxgen",
        "generation",
        "physics"
      ],
      "summary": "3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset generation. \\textbf{1)} To bridge the critical gap in physics-annotated 3D datasets, we present \\textbf{\\ourname}\\ - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: \\textbf{\\textcolor{color2}{absolute scale}}, \\textbf{\\textcolor{color3}{material}}, \\textbf{\\textcolor{color1}{affordance}}, \\textbf{\\textcolor{color4}{kinematics}}, and \\textbf{\\textcolor{color5}{function description}}. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets. \\textbf{2)} Furthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for physics-grounded 3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hLJLP3CmHR"
        ],
        "venue": [
          "/venue/hLJLP3CmHR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hLJLP3CmHR"
        ],
        "detail": [
          "https://openreview.net/forum?id=hLJLP3CmHR"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "PhysX-3D: Physical-Grounded 3D Asset Generation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ziang Cao , Zhaoxi Chen , Liang Pan , Ziwei Liu 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset generation. \\textbf{1)} To bridge the critical gap in physics-annotated 3D datasets, we present \\textbf{\\ourname}\\ - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: \\textbf{\\textcolor{color2}{absolute scale}}, \\textbf{\\textcolor{color3}{material}}, \\textbf{\\textcolor{color1}{affordance}}, \\textbf{\\textcolor{color4}{kinematics}}, and \\textbf{\\textcolor{color5}{function description}}. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets. \\textbf{2)} Furthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for physics-grounded 3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "pEUBqS8nTk@OpenReview",
      "index": 630,
      "title": "Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study",
      "authors": [
        "Guanlin Wu",
        "Boyan Su",
        "Yang Zhao",
        "Pu Wang",
        "Yichen Lin",
        "Hao Frank Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "vsi",
        "sig",
        "spatial",
        "intelligence",
        "priors",
        "driving",
        "vqa",
        "informed",
        "grid",
        "human"
      ],
      "summary": "How to integrate and verify spatial intelligence in foundation models remains an open challenge. Current practice often proxies Visual-Spatial Intelligence (VSI) with purely textual prompts and VQA-style scoring, which obscures geometry, invites linguistic shortcuts, and weakens attribution to genuinely spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured, grid-based schema that explicitly encodes object layouts, inter-object relations, and physically grounded priors. As a complementary channel to text, SIG provides a faithful, compositional representation of scene structure for foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation metrics that quantify a model’s intrinsic VSI, which separates spatial capability from language priors. In few-shot in-context learning with state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG yields consistently larger, more stable, and more comprehensive gains across all VSI metrics compared to VQA-only representations, indicating its promise as a data-labeling and training schema for learning VSI. We also release SIGBench, a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and human gaze traces, supporting both grid-based machine VSI tasks and attention-driven, human-like VSI tasks in autonomous-driving scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pEUBqS8nTk"
        ],
        "venue": [
          "/venue/pEUBqS8nTk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pEUBqS8nTk"
        ],
        "detail": [
          "https://openreview.net/forum?id=pEUBqS8nTk"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Guanlin Wu , Boyan Su , Yang Zhao , Pu Wang , Yichen Lin , Hao Frank Yang How to integrate and verify spatial intelligence in foundation models remains an open challenge. Current practice often proxies Visual-Spatial Intelligence (VSI) with purely textual prompts and VQA-style scoring, which obscures geometry, invites linguistic shortcuts, and weakens attribution to genuinely spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured, grid-based schema that explicitly encodes object layouts, inter-object relations, and physically grounded priors. As a complementary channel to text, SIG provides a faithful, compositional representation of scene structure for foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation metrics that quantify a model’s intrinsic VSI, which separates spatial capability from language priors. In few-shot in-context learning with state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG yields consistently larger, more stable, and more comprehensive gains across all VSI metrics compared to VQA-only representations, indicating its promise as a data-labeling and training schema for learning VSI. We also release SIGBench, a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and human gaze traces, supporting both grid-based machine VSI tasks and attention-driven, human-like VSI tasks in autonomous-driving scenarios. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "9loSPaBwGO@OpenReview",
      "index": 631,
      "title": "StreamForest: Efficient Online Video Understanding with Persistent Event Memory",
      "authors": [
        "Xiangyu Zeng",
        "Kefan Qiu",
        "Qingyu Zhang",
        "Xinhao Li",
        "Jing Wang",
        "Jiaxin Li",
        "Ziang Yan",
        "Kun Tian",
        "Meng Tian",
        "Xinhai Zhao",
        "Yi Wang",
        "Limin Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "streamforest",
        "video",
        "streaming",
        "understanding",
        "onlineit",
        "memory",
        "perception",
        "event",
        "bench",
        "persistent"
      ],
      "summary": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in video understanding. However, their effectiveness in real-time streaming scenarios remains limited due to storage constraints of historical visual features and insufficient real-time spatiotemporal reasoning. To address these challenges, we propose StreamForest, a novel architecture specifically designed for streaming video understanding. Central to StreamForest is the Persistent Event Memory Forest, a memory mechanism that adaptively organizes video frames into multiple event-level tree structures. This process is guided by penalty functions based on temporal distance, content similarity, and merge frequency, enabling efficient long-term memory retention under limited computational resources. To enhance real-time perception, we introduce a Fine-grained Spatiotemporal Window, which captures detailed short-term visual cues to improve current scene perception. Additionally, we present OnlineIT, an instruction-tuning dataset tailored for streaming video tasks. OnlineIT significantly boosts MLLM performance in both real-time perception and future prediction. To evaluate generalization in practical applications, we introduce ODV-Bench, a new benchmark focused on real-time streaming video understanding in autonomous driving scenarios. Experimental results demonstrate that StreamForest achieves the state-of-the-art performance, with accuracies of 77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In particular, even under extreme visual token compression (limited to 1024 tokens), the model retains 96.8% of its average accuracy in eight benchmarks relative to the default setting. These results underscore the robustness, efficiency, and generalizability of StreamForest for streaming video understanding.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9loSPaBwGO"
        ],
        "venue": [
          "/venue/9loSPaBwGO@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9loSPaBwGO"
        ],
        "detail": [
          "https://openreview.net/forum?id=9loSPaBwGO"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "StreamForest: Efficient Online Video Understanding with Persistent Event Memory [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Xiangyu Zeng , Kefan Qiu , Qingyu Zhang , Xinhao Li , Jing Wang , Jiaxin Li , Ziang Yan , Kun Tian , Meng Tian , Xinhai Zhao , Yi Wang , Limin Wang Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in video understanding. However, their effectiveness in real-time streaming scenarios remains limited due to storage constraints of historical visual features and insufficient real-time spatiotemporal reasoning. To address these challenges, we propose StreamForest, a novel architecture specifically designed for streaming video understanding. Central to StreamForest is the Persistent Event Memory Forest, a memory mechanism that adaptively organizes video frames into multiple event-level tree structures. This process is guided by penalty functions based on temporal distance, content similarity, and merge frequency, enabling efficient long-term memory retention under limited computational resources. To enhance real-time perception, we introduce a Fine-grained Spatiotemporal Window, which captures detailed short-term visual cues to improve current scene perception. Additionally, we present OnlineIT, an instruction-tuning dataset tailored for streaming video tasks. OnlineIT significantly boosts MLLM performance in both real-time perception and future prediction. To evaluate generalization in practical applications, we introduce ODV-Bench, a new benchmark focused on real-time streaming video understanding in autonomous driving scenarios. Experimental results demonstrate that StreamForest achieves the state-of-the-art performance, with accuracies of 77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In particular, even under extreme visual token compression (limited to 1024 tokens), the model retains 96.8% of its average accuracy in eight benchmarks relative to the default setting. These results underscore the robustness, efficiency, and generalizability of StreamForest for streaming video understanding. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Ll29PmM3UH@OpenReview",
      "index": 632,
      "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2",
      "authors": [
        "Ziqi Zhou",
        "Yifan Hu",
        "Yufei Song",
        "Zijing Li",
        "Shengshan Hu",
        "Leo Yu Zhang",
        "Dezhong Yao",
        "Long Zheng",
        "Hai Jin"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sam2",
        "prompt",
        "uap",
        "sam",
        "attacks",
        "adversarial",
        "semantic",
        "segmentation",
        "cross",
        "universal"
      ],
      "summary": "Recent studies reveal the vulnerability of the image segmentation foundation model SAM to adversarial examples. Its successor, SAM2, has attracted significant attention due to its strong generalization capability in video segmentation. However, its robustness remains unexplored, and it is unclear whether existing attacks on SAM can be directly transferred to SAM2. In this paper, we first analyze the performance gap of existing attacks between SAM and SAM2 and highlight two key challenges arising from their architectural differences: directional guidance from the prompt and semantic entanglement across consecutive frames. To address these issues, we propose UAP-SAM2, the first cross-prompt universal adversarial attack against SAM2 driven by dual semantic deviation. For cross-prompt transferability, we begin by designing a target-scanning strategy that divides each frame into k regions, each randomly assigned a prompt, to reduce prompt dependency during optimization. For effectiveness, we design a dual semantic deviation framework that optimizes a UAP by distorting the semantics within the current frame and disrupting the semantic consistency across consecutive frames. Extensive experiments on six datasets across two segmentation tasks demonstrate the effectiveness of the proposed method for SAM2. The comparative results show that UAP-SAM2 significantly outperforms state-of-the-art (SOTA) attacks by a large margin.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Ll29PmM3UH"
        ],
        "venue": [
          "/venue/Ll29PmM3UH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Ll29PmM3UH"
        ],
        "detail": [
          "https://openreview.net/forum?id=Ll29PmM3UH"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2 [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ziqi Zhou , Yifan Hu , Yufei Song , Zijing Li , Shengshan Hu , Leo Yu Zhang , Dezhong Yao , Long Zheng , Hai Jin Recent studies reveal the vulnerability of the image segmentation foundation model SAM to adversarial examples. Its successor, SAM2, has attracted significant attention due to its strong generalization capability in video segmentation. However, its robustness remains unexplored, and it is unclear whether existing attacks on SAM can be directly transferred to SAM2. In this paper, we first analyze the performance gap of existing attacks between SAM and SAM2 and highlight two key challenges arising from their architectural differences: directional guidance from the prompt and semantic entanglement across consecutive frames. To address these issues, we propose UAP-SAM2, the first cross-prompt universal adversarial attack against SAM2 driven by dual semantic deviation. For cross-prompt transferability, we begin by designing a target-scanning strategy that divides each frame into k regions, each randomly assigned a prompt, to reduce prompt dependency during optimization. For effectiveness, we design a dual semantic deviation framework that optimizes a UAP by distorting the semantics within the current frame and disrupting the semantic consistency across consecutive frames. Extensive experiments on six datasets across two segmentation tasks demonstrate the effectiveness of the proposed method for SAM2. The comparative results show that UAP-SAM2 significantly outperforms state-of-the-art (SOTA) attacks by a large margin. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "xgTxQe3CNl@OpenReview",
      "index": 633,
      "title": "Evolutionary Multi-View Classification via Eliminating Individual Fitness Bias",
      "authors": [
        "Xinyan Liang",
        "ShuaiLi",
        "Qian Guo",
        "Yuhua Qian",
        "Bingbing Jiang",
        "Tingjin Luo",
        "Liang Du"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "emvc",
        "feb",
        "evolutionary",
        "fitness",
        "individual",
        "efb",
        "view",
        "mvm",
        "issue",
        "classification"
      ],
      "summary": "Evolutionary multi-view classification (EMVC) methods have gained wide recognition due to their adaptive mechanisms. Fitness evaluation (FE), which aims to calculate the classification performance of each individual in the population and provide reliable performance ranking for subsequent operations, is a core step in such methods. Its accuracy directly determines the correctness of the evolutionary direction. However, when FE fails to correctly reflect the superiority-inferiority relationship among individuals, it will lead to confusion in individual performance ranking, which in turn misleads the evolutionary direction and results in trapping into local optima. This paper is the first to identify the aforementioned issue in the field of EMVC and call it as fitness evaluation bias (FEB). FEB may be caused by a variety of factors, and this paper approaches the issue from the perspective of view information content: existing methods generally adopt joint training strategies, which restrict the exploration of key information in views with low information content. This makes it difficult for multi-view model (MVM) to achieve optimal performance during convergence, which in turn leads to FE failing to accurately reflect individual performance rankings and ultimately triggering FEB. To address this issue, we propose an evolutionary multi-view classification via eliminating individual fitness bias (EFB-EMVC) method, which alleviates the FEB issue by introducing evolutionary navigators for each MVM, thereby providing more accurate individual ranking. Experimental results fully verify the effectiveness of the proposed method in alleviating the FEB problem, and the EMVC method equipped with this strategy exhibits more superior performance compared with the original EMVC method. (The code is available at https://github.com/LiShuailzn/Neurips-2025-EFB-EMVC)",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xgTxQe3CNl"
        ],
        "venue": [
          "/venue/xgTxQe3CNl@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xgTxQe3CNl"
        ],
        "detail": [
          "https://openreview.net/forum?id=xgTxQe3CNl"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Evolutionary Multi-View Classification via Eliminating Individual Fitness Bias [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Xinyan Liang , ShuaiLi , Qian Guo , Yuhua Qian , Bingbing Jiang , Tingjin Luo , Liang Du Evolutionary multi-view classification (EMVC) methods have gained wide recognition due to their adaptive mechanisms. Fitness evaluation (FE), which aims to calculate the classification performance of each individual in the population and provide reliable performance ranking for subsequent operations, is a core step in such methods. Its accuracy directly determines the correctness of the evolutionary direction. However, when FE fails to correctly reflect the superiority-inferiority relationship among individuals, it will lead to confusion in individual performance ranking, which in turn misleads the evolutionary direction and results in trapping into local optima. This paper is the first to identify the aforementioned issue in the field of EMVC and call it as fitness evaluation bias (FEB). FEB may be caused by a variety of factors, and this paper approaches the issue from the perspective of view information content: existing methods generally adopt joint training strategies, which restrict the exploration of key information in views with low information content. This makes it difficult for multi-view model (MVM) to achieve optimal performance during convergence, which in turn leads to FE failing to accurately reflect individual performance rankings and ultimately triggering FEB. To address this issue, we propose an evolutionary multi-view classification via eliminating individual fitness bias (EFB-EMVC) method, which alleviates the FEB issue by introducing evolutionary navigators for each MVM, thereby providing more accurate individual ranking. Experimental results fully verify the effectiveness of the proposed method in alleviating the FEB problem, and the EMVC method equipped with this strategy exhibits more superior performance compared with the original EMVC method. (The code is available at https://github.com/LiShuailzn/Neurips-2025-EFB-EMVC) Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "k8Mim6RI5O@OpenReview",
      "index": 634,
      "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization",
      "authors": [
        "Qingyang Zhang",
        "Haitao Wu",
        "Changqing Zhang",
        "Peilin Zhao",
        "Yatao Bian"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "empo",
        "reasoning",
        "incentivization",
        "qwen2",
        "llm",
        "qingyangzhang",
        "unsupervised",
        "supervised",
        "math",
        "fully"
      ],
      "summary": "Existing methods to enhance the reasoning capability of large language models predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data. These approaches critically depend on external supervisions--such as labeled reasoning traces, verified golden answers, or pre-trained reward models. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. By minimizing the semantic entropy of LLMs on unlabeled questions, EMPO achieves competitive performance compared to supervised counterparts. Specifically, without any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B Base from 33.7\\% to 51.6\\% on math benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro. Primary analysis are also provided to interpret the effectiveness of EMPO. Code is available at https://github.com/QingyangZhang/EMPO.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=k8Mim6RI5O"
        ],
        "venue": [
          "/venue/k8Mim6RI5O@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=k8Mim6RI5O"
        ],
        "detail": [
          "https://openreview.net/forum?id=k8Mim6RI5O"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Qingyang Zhang , Haitao Wu , Changqing Zhang , Peilin Zhao , Yatao Bian Existing methods to enhance the reasoning capability of large language models predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data. These approaches critically depend on external supervisions--such as labeled reasoning traces, verified golden answers, or pre-trained reward models. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. By minimizing the semantic entropy of LLMs on unlabeled questions, EMPO achieves competitive performance compared to supervised counterparts. Specifically, without any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B Base from 33.7\\% to 51.6\\% on math benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro. Primary analysis are also provided to interpret the effectiveness of EMPO. Code is available at https://github.com/QingyangZhang/EMPO. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7jg26Fd1ra@OpenReview",
      "index": 635,
      "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
      "authors": [
        "Yingying Feng",
        "Jie Li",
        "Jie Hu",
        "Yukang Zhang",
        "Lei Tan",
        "Jiayi Ji"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "modality",
        "mdreid",
        "reid",
        "mismatched",
        "mml",
        "mdm",
        "scenarios",
        "shared",
        "rgbnt100",
        "msvr310"
      ],
      "summary": "The challenge of inconsistent modalities in real-world applications presents significant obstacles to effective object re-identification (ReID). However, most existing approaches assume modality-matched conditions, significantly limiting their effectiveness in modality-mismatched scenarios. To overcome this limitation and achieve a more flexible ReID, we introduce MDReID to allow any-to-any image-level ReID systems. MDReID is inspired by the widely recognized perspective that modality information comprises both modality-shared features, predictable across modalities, and unpredictable modality-specific features, which are inherently modality-dependent and consist of two key components: the Modality Decoupling Module (MDM) and Modality-aware Metric Learning (MML). Specifically, MDM explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enhances feature discrimination and decoupling by exploiting distributional relationships between shared and specific modality features. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDL. MDReID achieves significant mAP improvements of 9.8\\%, 3.0\\%, and 11.5\\% in modality-matched scenarios, and average gains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios, respectively.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7jg26Fd1ra"
        ],
        "venue": [
          "/venue/7jg26Fd1ra@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7jg26Fd1ra"
        ],
        "detail": [
          "https://openreview.net/forum?id=7jg26Fd1ra"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Yingying Feng , Jie Li , Jie Hu , Yukang Zhang , Lei Tan , Jiayi Ji The challenge of inconsistent modalities in real-world applications presents significant obstacles to effective object re-identification (ReID). However, most existing approaches assume modality-matched conditions, significantly limiting their effectiveness in modality-mismatched scenarios. To overcome this limitation and achieve a more flexible ReID, we introduce MDReID to allow any-to-any image-level ReID systems. MDReID is inspired by the widely recognized perspective that modality information comprises both modality-shared features, predictable across modalities, and unpredictable modality-specific features, which are inherently modality-dependent and consist of two key components: the Modality Decoupling Module (MDM) and Modality-aware Metric Learning (MML). Specifically, MDM explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enhances feature discrimination and decoupling by exploiting distributional relationships between shared and specific modality features. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDL. MDReID achieves significant mAP improvements of 9.8\\%, 3.0\\%, and 11.5\\% in modality-matched scenarios, and average gains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios, respectively. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "U7RZ9cC73S@OpenReview",
      "index": 636,
      "title": "RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness",
      "authors": [
        "Fanhu Zeng",
        "Haiyang Guo",
        "Fei Zhu",
        "Li Shen",
        "Hao Tang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "merging",
        "robustmerge",
        "robustness",
        "direction",
        "task",
        "efficient",
        "parameter",
        "tuning",
        "mllms",
        "refraining"
      ],
      "summary": "Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter-efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, few methods are dedicated to efficient merging, and existing methods designed for full fine-tuning merging fail under efficient merging. To address the issue, we analyze from low-rank decomposition and reveal that direction robustness during merging is crucial for merging efficient modules. We furthermore uncover that compensating for the gap between stark singular values contributes to direction robustness. Therefore, we propose RobustMerge, a training-free parameter-efficient merging method with complementary parameter adaptation to maintain direction robustness. Specifically, we (1) prune parameters and scale coefficients from inter-parameter relations for singular values to maintain direction stability away from task interference, and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certify the outstanding performance and generalizability of our method. Additional studies and extensive analyses further showcase the effectiveness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=U7RZ9cC73S"
        ],
        "venue": [
          "/venue/U7RZ9cC73S@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=U7RZ9cC73S"
        ],
        "detail": [
          "https://openreview.net/forum?id=U7RZ9cC73S"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Fanhu Zeng , Haiyang Guo , Fei Zhu , Li Shen , Hao Tang Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter-efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, few methods are dedicated to efficient merging, and existing methods designed for full fine-tuning merging fail under efficient merging. To address the issue, we analyze from low-rank decomposition and reveal that direction robustness during merging is crucial for merging efficient modules. We furthermore uncover that compensating for the gap between stark singular values contributes to direction robustness. Therefore, we propose RobustMerge, a training-free parameter-efficient merging method with complementary parameter adaptation to maintain direction robustness. Specifically, we (1) prune parameters and scale coefficients from inter-parameter relations for singular values to maintain direction stability away from task interference, and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certify the outstanding performance and generalizability of our method. Additional studies and extensive analyses further showcase the effectiveness. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "I9F53Qlwur@OpenReview",
      "index": 637,
      "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation",
      "authors": [
        "Hao Zhang",
        "Chun-Han Yao",
        "Simon Donné",
        "Narendra Ahuja",
        "Varun Jampani"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "part",
        "sp4d",
        "rgb",
        "kinematic",
        "branch",
        "segmentation",
        "diffusion",
        "paired",
        "parts",
        "video"
      ],
      "summary": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts --- structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latents VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL, each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=I9F53Qlwur"
        ],
        "venue": [
          "/venue/I9F53Qlwur@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=I9F53Qlwur"
        ],
        "detail": [
          "https://openreview.net/forum?id=I9F53Qlwur"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Hao Zhang , Chun-Han Yao , Simon Donné , Narendra Ahuja , Varun Jampani We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts --- structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latents VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL, each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Y9AdTCCEgI@OpenReview",
      "index": 638,
      "title": "OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects",
      "authors": [
        "Mark He Huang",
        "Lin Geng Foo",
        "Christian Theobalt",
        "Ying Sun",
        "De Wen Soh"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "onlinesplatter",
        "object",
        "pose",
        "reconstruction",
        "free",
        "moving",
        "keys",
        "online",
        "memory",
        "objects"
      ],
      "summary": "Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Y9AdTCCEgI"
        ],
        "venue": [
          "/venue/Y9AdTCCEgI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Y9AdTCCEgI"
        ],
        "detail": [
          "https://openreview.net/forum?id=Y9AdTCCEgI"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Mark He Huang , Lin Geng Foo , Christian Theobalt , Ying Sun , De Wen Soh Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "bNTezDPlFH@OpenReview",
      "index": 639,
      "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
      "authors": [
        "TAO SUN",
        "Liyuan Zhu",
        "Shengyu Huang",
        "Shuran Song",
        "Iro Armeni"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rectified",
        "point",
        "assembly",
        "part",
        "flow",
        "registration",
        "pointflow",
        "cloud",
        "pairwise",
        "poses"
      ],
      "summary": "We present Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with an overlap-aware encoder focused on inter-part contacts, Rectified Point Flow achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Our code and models are available at https://rectified-pointflow.github.io/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bNTezDPlFH"
        ],
        "venue": [
          "/venue/bNTezDPlFH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bNTezDPlFH"
        ],
        "detail": [
          "https://openreview.net/forum?id=bNTezDPlFH"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Rectified Point Flow: Generic Point Cloud Pose Estimation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : TAO SUN , Liyuan Zhu , Shengyu Huang , Shuran Song , Iro Armeni We present Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with an overlap-aware encoder focused on inter-part contacts, Rectified Point Flow achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Our code and models are available at https://rectified-pointflow.github.io/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "8a9bAZFeIu@OpenReview",
      "index": 640,
      "title": "Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation",
      "authors": [
        "Xiang Li",
        "Zirui Wang",
        "Zixuan Huang",
        "James Matthew Rehg"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cues",
        "cue3d",
        "image",
        "shading",
        "generation",
        "texture",
        "silhouette",
        "single",
        "quantifying",
        "continuity"
      ],
      "summary": "Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8a9bAZFeIu"
        ],
        "venue": [
          "/venue/8a9bAZFeIu@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8a9bAZFeIu"
        ],
        "detail": [
          "https://openreview.net/forum?id=8a9bAZFeIu"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xiang Li , Zirui Wang , Zixuan Huang , James Matthew Rehg Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yFasd68NyI@OpenReview",
      "index": 641,
      "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
      "authors": [
        "Zhenyuan Qin",
        "Xincheng Shuai",
        "Henghui Ding"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "scenedesigner",
        "pose",
        "object",
        "dof",
        "controllable",
        "manipulation",
        "generation",
        "controllability",
        "multi",
        "control"
      ],
      "summary": "Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose ***SceneDesigner***, a method for accurate and flexible multi-object 9-DoF pose manipulation. ***SceneDesigner*** incorporates a branched network to the pre-trained base model and leverages a new representation, ***CNOCS map***, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ***ObjectPose9D***, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose ***Disentangled Object Sampling***, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, ***SceneDesigner*** enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that ***SceneDesigner*** significantly outperforms existing approaches in both controllability and quality.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yFasd68NyI"
        ],
        "venue": [
          "/venue/yFasd68NyI@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yFasd68NyI"
        ],
        "detail": [
          "https://openreview.net/forum?id=yFasd68NyI"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zhenyuan Qin , Xincheng Shuai , Henghui Ding Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose ***SceneDesigner***, a method for accurate and flexible multi-object 9-DoF pose manipulation. ***SceneDesigner*** incorporates a branched network to the pre-trained base model and leverages a new representation, ***CNOCS map***, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ***ObjectPose9D***, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose ***Disentangled Object Sampling***, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, ***SceneDesigner*** enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that ***SceneDesigner*** significantly outperforms existing approaches in both controllability and quality. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "55Lv1unlUL@OpenReview",
      "index": 642,
      "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold",
      "authors": [
        "Zhizhong Li",
        "Sina Sajadmanesh",
        "Jingtao Li",
        "Lingjuan Lyu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "stiefel",
        "stella",
        "lora",
        "rank",
        "manifold",
        "subspace",
        "tuning",
        "sonyresearch",
        "adaptation",
        "fine"
      ],
      "summary": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient technique for fine-tuning large-scale pre-trained models. However, it still lags behind full fine-tuning in performance, partly due to its insufficient exploitation of the geometric structure underlying low-rank manifolds. In this paper, we propose a geometry-aware extension of LoRA that uses a three-factor decomposition U S V ⊤ U S V ⊤ . Analogous to the structure of singular value decomposition (SVD), it separates the adapter's input and output subspaces, V V and U U , from the scaling factor S S . Our method constrains U U and V V to lie on the Stiefel manifold, ensuring their orthonormality throughout the training. To optimize on the Stiefel manifold, we employ a flexible and modular geometric optimization design that converts any Euclidean optimizer to a Riemannian one. It enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines. Empirical results across a wide range of downstream tasks, including commonsense reasoning, math and code generation, image classification, and image generation, demonstrate the superior performance of our approach against the recent state-of-the-art variants of LoRA. Code is available at https://github.com/SonyResearch/stella.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=55Lv1unlUL"
        ],
        "venue": [
          "/venue/55Lv1unlUL@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=55Lv1unlUL"
        ],
        "detail": [
          "https://openreview.net/forum?id=55Lv1unlUL"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Zhizhong Li , Sina Sajadmanesh , Jingtao Li , Lingjuan Lyu Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient technique for fine-tuning large-scale pre-trained models. However, it still lags behind full fine-tuning in performance, partly due to its insufficient exploitation of the geometric structure underlying low-rank manifolds. In this paper, we propose a geometry-aware extension of LoRA that uses a three-factor decomposition U S V ⊤ U S V ⊤ . Analogous to the structure of singular value decomposition (SVD), it separates the adapter's input and output subspaces, V V and U U , from the scaling factor S S . Our method constrains U U and V V to lie on the Stiefel manifold, ensuring their orthonormality throughout the training. To optimize on the Stiefel manifold, we employ a flexible and modular geometric optimization design that converts any Euclidean optimizer to a Riemannian one. It enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines. Empirical results across a wide range of downstream tasks, including commonsense reasoning, math and code generation, image classification, and image generation, demonstrate the superior performance of our approach against the recent state-of-the-art variants of LoRA. Code is available at https://github.com/SonyResearch/stella. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "gYbreatcV1@OpenReview",
      "index": 643,
      "title": "Non-Clairvoyant Scheduling with Progress Bars",
      "authors": [
        "Ziyad Benomar",
        "Romain Cosson",
        "Alexander Lindermayr",
        "Jens Schlöter"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "scheduling",
        "bars",
        "job",
        "clairvoyant",
        "progress",
        "algorithms",
        "augmented",
        "adversarial",
        "setting",
        "scheduler"
      ],
      "summary": "In non-clairvoyant scheduling, the goal is to minimize the total job completion time without prior knowledge of individual job processing times. This classical online optimization problem has recently gained attention through the framework of learning-augmented algorithms. We introduce a natural setting in which the scheduler receives continuous feedback in the form of progress bars—estimates of the fraction of each job completed over time. We design new algorithms for both adversarial and stochastic progress bars and prove strong competitive bounds. Our results in the adversarial case surprisingly induce improved guarantees for learning-augmented scheduling with job size predictions. We also introduce a general method for combining scheduling algorithms, yielding further insights in scheduling with predictions. Finally, we propose a stochastic model of progress bars as a more optimistic alternative to conventional worst-case models, and present an asymptotically optimal scheduling algorithm in this setting.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gYbreatcV1"
        ],
        "venue": [
          "/venue/gYbreatcV1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gYbreatcV1"
        ],
        "detail": [
          "https://openreview.net/forum?id=gYbreatcV1"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Non-Clairvoyant Scheduling with Progress Bars [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ziyad Benomar , Romain Cosson , Alexander Lindermayr , Jens Schlöter In non-clairvoyant scheduling, the goal is to minimize the total job completion time without prior knowledge of individual job processing times. This classical online optimization problem has recently gained attention through the framework of learning-augmented algorithms. We introduce a natural setting in which the scheduler receives continuous feedback in the form of progress bars—estimates of the fraction of each job completed over time. We design new algorithms for both adversarial and stochastic progress bars and prove strong competitive bounds. Our results in the adversarial case surprisingly induce improved guarantees for learning-augmented scheduling with job size predictions. We also introduce a general method for combining scheduling algorithms, yielding further insights in scheduling with predictions. Finally, we propose a stochastic model of progress bars as a more optimistic alternative to conventional worst-case models, and present an asymptotically optimal scheduling algorithm in this setting. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "P37GIj4wB7@OpenReview",
      "index": 644,
      "title": "Jacobian-Based Interpretation of Nonlinear Neural Encoding Model",
      "authors": [
        "Xiaohui Gao",
        "Haoran Yang",
        "Yue Cheng",
        "Mengfei Zuo",
        "Yiheng Liu",
        "Peiyang Li",
        "Xintao Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "jne",
        "bold",
        "nonlinear",
        "interpretability",
        "brain",
        "responses",
        "nonlinearity",
        "jacobian",
        "encoding",
        "neural"
      ],
      "summary": "In recent years, the alignment between artificial neural network (ANN) embeddings and blood oxygenation level dependent (BOLD) responses in functional magnetic resonance imaging (fMRI) via neural encoding models has significantly advanced research on neural representation mechanisms and interpretability in the brain. However, these approaches remain limited in characterizing the brain’s inherently nonlinear response properties. To address this, we propose the Jacobian-based Nonlinearity Evaluation (JNE), an interpretability metric for nonlinear neural encoding models. JNE quantifies nonlinearity by statistically measuring the dispersion of local linear mappings (Jacobians) from model representations to predicted BOLD responses, thereby approximating the nonlinearity of BOLD signals. Centered on proposing JNE as a novel interpretability metric, we validated its effectiveness through controlled simulation experiments on various activation functions and network architectures, and further verified it on real fMRI data, demonstrating a hierarchical progression of nonlinear characteristics from primary to higher-order visual cortices, consistent with established cortical organization. We further extended JNE with Sample-Specificity (JNE-SS), revealing stimulus-selective nonlinear response patterns in functionally specialized brain regions. As the first interpretability metric for quantifying nonlinear responses, JNE provides new insights into brain information processing. Code available at https://github.com/Gaitxh/JNE.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=P37GIj4wB7"
        ],
        "venue": [
          "/venue/P37GIj4wB7@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=P37GIj4wB7"
        ],
        "detail": [
          "https://openreview.net/forum?id=P37GIj4wB7"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Jacobian-Based Interpretation of Nonlinear Neural Encoding Model [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Xiaohui Gao , Haoran Yang , Yue Cheng , Mengfei Zuo , Yiheng Liu , Peiyang Li , Xintao Hu In recent years, the alignment between artificial neural network (ANN) embeddings and blood oxygenation level dependent (BOLD) responses in functional magnetic resonance imaging (fMRI) via neural encoding models has significantly advanced research on neural representation mechanisms and interpretability in the brain. However, these approaches remain limited in characterizing the brain’s inherently nonlinear response properties. To address this, we propose the Jacobian-based Nonlinearity Evaluation (JNE), an interpretability metric for nonlinear neural encoding models. JNE quantifies nonlinearity by statistically measuring the dispersion of local linear mappings (Jacobians) from model representations to predicted BOLD responses, thereby approximating the nonlinearity of BOLD signals. Centered on proposing JNE as a novel interpretability metric, we validated its effectiveness through controlled simulation experiments on various activation functions and network architectures, and further verified it on real fMRI data, demonstrating a hierarchical progression of nonlinear characteristics from primary to higher-order visual cortices, consistent with established cortical organization. We further extended JNE with Sample-Specificity (JNE-SS), revealing stimulus-selective nonlinear response patterns in functionally specialized brain regions. As the first interpretability metric for quantifying nonlinear responses, JNE provides new insights into brain information processing. Code available at https://github.com/Gaitxh/JNE. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "te2RsWcyQp@OpenReview",
      "index": 645,
      "title": "Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning",
      "authors": [
        "Jian Liu",
        "Jing Xu",
        "Song Guo",
        "Jing Li",
        "Guojingfeng",
        "Jiaao Yu",
        "Haohan Weng",
        "Biwen Lei",
        "Xianghui Yang",
        "Zhuo Chen",
        "Fangqi Zhu",
        "Tao Han",
        "Chunchao Guo"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mesh",
        "rft",
        "dpo",
        "fine",
        "grained",
        "reinforcement",
        "quality",
        "topology",
        "generation",
        "integrity"
      ],
      "summary": "Existing pretrained models for 3D mesh generation often suffer from data biases and produce low-quality results, while global reinforcement learning (RL) methods rely on object-level rewards that struggle to capture local structure details. To address these challenges, we present Mesh-RFT Mesh-RFT , a novel fine-grained reinforcement fine-tuning framework that employs Masked Direct Preference Optimization (M-DPO) to enable localized refinement via quality-aware face masking. To facilitate efficient quality evaluation, we introduce an objective topology-aware scoring system to evaluate geometric integrity and topological regularity at both object and face levels through two metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first method to optimize mesh quality at the granularity of individual faces, resolving localized errors while preserving global coherence. Experiment results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\\% and improves Topology Score (TS) by 3.8\\% over pre-trained models, while outperforming global DPO methods with a 17.4\\% HD reduction and 4.9\\% TS gain. These results demonstrate Mesh-RFT’s ability to improve geometric integrity and topological regularity, achieving new state-of-the-art performance in production-ready mesh generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=te2RsWcyQp"
        ],
        "venue": [
          "/venue/te2RsWcyQp@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=te2RsWcyQp"
        ],
        "detail": [
          "https://openreview.net/forum?id=te2RsWcyQp"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jian Liu , Jing Xu , Song Guo , Jing Li , Guojingfeng , Jiaao Yu , Haohan Weng , Biwen Lei , Xianghui Yang , Zhuo Chen , Fangqi Zhu , Tao Han , Chunchao Guo Existing pretrained models for 3D mesh generation often suffer from data biases and produce low-quality results, while global reinforcement learning (RL) methods rely on object-level rewards that struggle to capture local structure details. To address these challenges, we present Mesh-RFT Mesh-RFT , a novel fine-grained reinforcement fine-tuning framework that employs Masked Direct Preference Optimization (M-DPO) to enable localized refinement via quality-aware face masking. To facilitate efficient quality evaluation, we introduce an objective topology-aware scoring system to evaluate geometric integrity and topological regularity at both object and face levels through two metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first method to optimize mesh quality at the granularity of individual faces, resolving localized errors while preserving global coherence. Experiment results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\\% and improves Topology Score (TS) by 3.8\\% over pre-trained models, while outperforming global DPO methods with a 17.4\\% HD reduction and 4.9\\% TS gain. These results demonstrate Mesh-RFT’s ability to improve geometric integrity and topological regularity, achieving new state-of-the-art performance in production-ready mesh generation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "uL7lCOHtiZ@OpenReview",
      "index": 646,
      "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank",
      "authors": [
        "Tianhe Wu",
        "Jian Zou",
        "Jie Liang",
        "Lei Zhang",
        "Kede Ma"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "visualquality",
        "iqa",
        "quality",
        "reasoning",
        "image",
        "induced",
        "reinforcement",
        "assessment",
        "rank",
        "thurstone"
      ],
      "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computation has not been thoroughly explored in the context of image quality assessment (IQA), a task depending critically on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uL7lCOHtiZ"
        ],
        "venue": [
          "/venue/uL7lCOHtiZ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uL7lCOHtiZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=uL7lCOHtiZ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Tianhe Wu , Jian Zou , Jie Liang , Lei Zhang , Kede Ma DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computation has not been thoroughly explored in the context of image quality assessment (IQA), a task depending critically on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "n8AvXKcCeR@OpenReview",
      "index": 647,
      "title": "GenColor: Generative and Expressive Color Enhancement with Pixel-Perfect Texture Preservation",
      "authors": [
        "Yi Dong",
        "Yuxi Wang",
        "Xianhui Lin",
        "Wenqi Ouyang",
        "Zhiqi Shen",
        "Peiran Ren",
        "Ruoxi Fan",
        "Rynson W. H. Lau"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gencolor",
        "texture",
        "color",
        "preservation",
        "enhancement",
        "expressive",
        "pixel",
        "artisan",
        "colorists",
        "perfect"
      ],
      "summary": "Color enhancement is a crucial yet challenging task in digital photography. It demands methods that are (i) expressive enough for fine-grained adjustments, (ii) adaptable to diverse inputs, and (iii) able to preserve texture. Existing approaches typically fall short in at least one of these aspects, yielding unsatisfactory results. We propose GenColor, a novel diffusion-based framework for sophisticated, texture-preserving color enhancement. GenColor reframes the task as conditional image generation. Leveraging ControlNet and a tailored training scheme, it learns advanced color transformations that adapt to diverse lighting and content. We train GenColor on ARTISAN, our newly collected large-scale dataset of 1.2M high-quality photographs specifically curated for enhancement tasks. To overcome texture preservation limitations inherent in diffusion models, we introduce a color-transfer network with a novel degradation scheme that simulates texture–color relationships. This network achieves pixel-perfect texture preservation while enabling fine-grained color matching with the diffusion-generated reference images. Extensive experiments show that GenColor produces visually compelling results comparable to those of expert colorists and surpasses state-of-the-art methods in both subjective and objective evaluations. We have released the code and dataset.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=n8AvXKcCeR"
        ],
        "venue": [
          "/venue/n8AvXKcCeR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=n8AvXKcCeR"
        ],
        "detail": [
          "https://openreview.net/forum?id=n8AvXKcCeR"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "GenColor: Generative and Expressive Color Enhancement with Pixel-Perfect Texture Preservation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Yi Dong , Yuxi Wang , Xianhui Lin , Wenqi Ouyang , Zhiqi Shen , Peiran Ren , Ruoxi Fan , Rynson W. H. Lau Color enhancement is a crucial yet challenging task in digital photography. It demands methods that are (i) expressive enough for fine-grained adjustments, (ii) adaptable to diverse inputs, and (iii) able to preserve texture. Existing approaches typically fall short in at least one of these aspects, yielding unsatisfactory results. We propose GenColor, a novel diffusion-based framework for sophisticated, texture-preserving color enhancement. GenColor reframes the task as conditional image generation. Leveraging ControlNet and a tailored training scheme, it learns advanced color transformations that adapt to diverse lighting and content. We train GenColor on ARTISAN, our newly collected large-scale dataset of 1.2M high-quality photographs specifically curated for enhancement tasks. To overcome texture preservation limitations inherent in diffusion models, we introduce a color-transfer network with a novel degradation scheme that simulates texture–color relationships. This network achieves pixel-perfect texture preservation while enabling fine-grained color matching with the diffusion-generated reference images. Extensive experiments show that GenColor produces visually compelling results comparable to those of expert colorists and surpasses state-of-the-art methods in both subjective and objective evaluations. We have released the code and dataset. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "mmIAp3cVS0@OpenReview",
      "index": 648,
      "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
      "authors": [
        "Guibin Zhang",
        "Muxin Fu",
        "Kun Wang",
        "Guancheng Wan",
        "Miao Yu",
        "Shuicheng YAN"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "memory",
        "mas",
        "agent",
        "llm",
        "upon",
        "trajectories",
        "trial",
        "hierarchical",
        "execution",
        "alarmed"
      ],
      "summary": "Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both \\textit{high-level, generalizable insights} that enable the system to leverage cross-trial knowledge, and \\textit{fine-grained, condensed interaction trajectories} that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to 20.89 20.89 and 10.12 10.12 , respectively, without any modifications to the original frameworks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mmIAp3cVS0"
        ],
        "venue": [
          "/venue/mmIAp3cVS0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mmIAp3cVS0"
        ],
        "detail": [
          "https://openreview.net/forum?id=mmIAp3cVS0"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 8
      },
      "raw_excerpt": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems [PDF 7 ] [Copy] [Kimi 8 ] [REL] Authors : Guibin Zhang , Muxin Fu , Kun Wang , Guancheng Wan , Miao Yu , Shuicheng YAN Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both \\textit{high-level, generalizable insights} that enable the system to leverage cross-trial knowledge, and \\textit{fine-grained, condensed interaction trajectories} that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to 20.89 20.89 and 10.12 10.12 , respectively, without any modifications to the original frameworks. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fCirUh6FRb@OpenReview",
      "index": 649,
      "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
      "authors": [
        "Shuang Zeng",
        "Xinyuan Chang",
        "Mengwei Xie",
        "Xinran Liu",
        "Yifan Bai",
        "Zheng Pan",
        "Mu Xu",
        "Xing Wei"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "cot",
        "fsdrive",
        "vlas",
        "visual",
        "future",
        "spatio",
        "frame",
        "temporal",
        "vla",
        "futuresightdrive"
      ],
      "summary": "Vision–Language–Action (VLA) models are increasingly used for end-to-end driving due to their world knowledge and reasoning ability. Most prior work, however, inserts textual chains-of-thought (CoT) as intermediate steps tailored to the current scene. Such symbolic compressions can blur spatio-temporal relations and discard fine visual cues, creating a cross-modal gap between perception and planning. We propose FSDrive, a visual spatio-temporal CoT framework that enables VLAs to think in images. The model first acts as a world model to generate a unified future frame that overlays coarse but physically-plausible priors—future lane dividers and 3D boxes—on the predicted future image. This unified frame serves as the visual CoT, capturing both spatial structure and temporal evolution. The same VLA then functions as an inverse-dynamics model, planning trajectories from current observations and the visual CoT. To equip VLAs with image generation while preserving understanding, we introduce a unified pre-training paradigm that expands the vocabulary to include visual tokens and jointly optimizes VQA (for semantics) and future-frame prediction (for dynamics). A progressive easy-to-hard scheme first predicts lane/box priors to enforce physical constraints, then completes full future frames for fine details. On nuScenes and NAVSIM, FSDrive improves trajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics, and attains competitive FID for future-frame generation despite using lightweight autoregression. It also advances scene understanding on DriveLM. Together, these results indicate that visual CoT narrows the cross-modal gap and yields safer, more anticipatory planning. Code is available at https://github.com/MIV-XJTU/FSDrive.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fCirUh6FRb"
        ],
        "venue": [
          "/venue/fCirUh6FRb@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fCirUh6FRb"
        ],
        "detail": [
          "https://openreview.net/forum?id=fCirUh6FRb"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Shuang Zeng , Xinyuan Chang , Mengwei Xie , Xinran Liu , Yifan Bai , Zheng Pan , Mu Xu , Xing Wei Vision–Language–Action (VLA) models are increasingly used for end-to-end driving due to their world knowledge and reasoning ability. Most prior work, however, inserts textual chains-of-thought (CoT) as intermediate steps tailored to the current scene. Such symbolic compressions can blur spatio-temporal relations and discard fine visual cues, creating a cross-modal gap between perception and planning. We propose FSDrive, a visual spatio-temporal CoT framework that enables VLAs to think in images. The model first acts as a world model to generate a unified future frame that overlays coarse but physically-plausible priors—future lane dividers and 3D boxes—on the predicted future image. This unified frame serves as the visual CoT, capturing both spatial structure and temporal evolution. The same VLA then functions as an inverse-dynamics model, planning trajectories from current observations and the visual CoT. To equip VLAs with image generation while preserving understanding, we introduce a unified pre-training paradigm that expands the vocabulary to include visual tokens and jointly optimizes VQA (for semantics) and future-frame prediction (for dynamics). A progressive easy-to-hard scheme first predicts lane/box priors to enforce physical constraints, then completes full future frames for fine details. On nuScenes and NAVSIM, FSDrive improves trajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics, and attains competitive FID for future-frame generation despite using lightweight autoregression. It also advances scene understanding on DriveLM. Together, these results indicate that visual CoT narrows the cross-modal gap and yields safer, more anticipatory planning. Code is available at https://github.com/MIV-XJTU/FSDrive. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "ls5L4IMEwt@OpenReview",
      "index": 650,
      "title": "E2Former: An Efficient and Equivariant Transformer with Linear-Scaling Tensor Products",
      "authors": [
        "Yunyang Li",
        "Lin Huang",
        "Zhihao Ding",
        "Xinran Wei",
        "Chu Wang",
        "Han Yang",
        "Zun Wang",
        "Chang Liu",
        "Yu Shi",
        "Peiran Jin",
        "Tao Qin",
        "Mark Gerstein",
        "Jia Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "e2former",
        "egnns",
        "equivariant",
        "wigner",
        "conv",
        "transformer",
        "tensor",
        "products",
        "computational",
        "mathcal"
      ],
      "summary": "Equivariant Graph Neural Networks (EGNNs) have demonstrated significant success in modeling microscale systems, including those in chemistry, biology and materials science. However, EGNNs face substantial computational challenges due to the high cost of constructing edge features via spherical tensor products, making them almost impractical for large-scale systems. To address this limitation, we introduce E2Former, an equivariant and efficient transformer architecture that incorporates a Wigner 6 j 6 j convolution (Wigner 6 j 6 j Conv). By shifting the computational burden from edges to nodes, Wigner 6 j 6 j Conv reduces the complexity from O ( | E | ) O ( | E | ) to O ( | V | ) O ( | V | ) while preserving both the model's expressive power and rotational equivariance. We show that this approach achieves a 7x–30x speedup compared to conventional S O ( 3 ) S O ( 3 ) convolutions. Furthermore, our empirical results demonstrate that the derived E2Former mitigates the computational challenges of existing approaches without compromising the ability to capture detailed geometric information. This development could suggest a promising direction for scalable molecular modeling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ls5L4IMEwt"
        ],
        "venue": [
          "/venue/ls5L4IMEwt@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ls5L4IMEwt"
        ],
        "detail": [
          "https://openreview.net/forum?id=ls5L4IMEwt"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "E2Former: An Efficient and Equivariant Transformer with Linear-Scaling Tensor Products [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yunyang Li , Lin Huang , Zhihao Ding , Xinran Wei , Chu Wang , Han Yang , Zun Wang , Chang Liu , Yu Shi , Peiran Jin , Tao Qin , Mark Gerstein , Jia Zhang Equivariant Graph Neural Networks (EGNNs) have demonstrated significant success in modeling microscale systems, including those in chemistry, biology and materials science. However, EGNNs face substantial computational challenges due to the high cost of constructing edge features via spherical tensor products, making them almost impractical for large-scale systems. To address this limitation, we introduce E2Former, an equivariant and efficient transformer architecture that incorporates a Wigner 6 j 6 j convolution (Wigner 6 j 6 j Conv). By shifting the computational burden from edges to nodes, Wigner 6 j 6 j Conv reduces the complexity from O ( | E | ) O ( | E | ) to O ( | V | ) O ( | V | ) while preserving both the model's expressive power and rotational equivariance. We show that this approach achieves a 7x–30x speedup compared to conventional S O ( 3 ) S O ( 3 ) convolutions. Furthermore, our empirical results demonstrate that the derived E2Former mitigates the computational challenges of existing approaches without compromising the ability to capture detailed geometric information. This development could suggest a promising direction for scalable molecular modeling. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "sm2e1SnMK4@OpenReview",
      "index": 651,
      "title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler",
      "authors": [
        "Zixuan Hu",
        "Li Shen",
        "Zhenyi Wang",
        "Yongxian Wei",
        "Dacheng Tao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "defense",
        "tuning",
        "scheduler",
        "bayesian",
        "harmful",
        "fine",
        "bds",
        "attack",
        "posterior",
        "data"
      ],
      "summary": "Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service for large language models. Existing defense strategies preemptively build robustness via attack simulation but suffer from fundamental limitations: (i) the infeasibility of extending attack simulations beyond bounded threat models due to the inherent difficulty of anticipating unknown attacks, and (ii) limited adaptability to varying attack settings, as simulation fails to capture their variability and complexity. To address these challenges, we propose Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with no need for attack simulation. BDS formulates harmful fine-tuning defense as a Bayesian inference problem, learning the posterior distribution of each data point's safety attribute, conditioned on the fine-tuning and alignment datasets. The fine-tuning process is then constrained by weighting data with their safety attributes sampled from the posterior, thus mitigating the influence of harmful data. By leveraging the post hoc nature of Bayesian inference, the posterior is conditioned on the fine-tuning dataset, enabling BDS to tailor its defense to the specific dataset, thereby achieving adaptive defense. Furthermore, we introduce a neural scheduler based on amortized Bayesian learning, enabling efficient transfer to new data without retraining. Comprehensive results across diverse attack and defense settings demonstrate the state-of-the-art performance of our approach. Code is available at https://github.com/Egg-Hu/Bayesian-Data-Scheduler.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sm2e1SnMK4"
        ],
        "venue": [
          "/venue/sm2e1SnMK4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sm2e1SnMK4"
        ],
        "detail": [
          "https://openreview.net/forum?id=sm2e1SnMK4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zixuan Hu , Li Shen , Zhenyi Wang , Yongxian Wei , Dacheng Tao Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service for large language models. Existing defense strategies preemptively build robustness via attack simulation but suffer from fundamental limitations: (i) the infeasibility of extending attack simulations beyond bounded threat models due to the inherent difficulty of anticipating unknown attacks, and (ii) limited adaptability to varying attack settings, as simulation fails to capture their variability and complexity. To address these challenges, we propose Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with no need for attack simulation. BDS formulates harmful fine-tuning defense as a Bayesian inference problem, learning the posterior distribution of each data point's safety attribute, conditioned on the fine-tuning and alignment datasets. The fine-tuning process is then constrained by weighting data with their safety attributes sampled from the posterior, thus mitigating the influence of harmful data. By leveraging the post hoc nature of Bayesian inference, the posterior is conditioned on the fine-tuning dataset, enabling BDS to tailor its defense to the specific dataset, thereby achieving adaptive defense. Furthermore, we introduce a neural scheduler based on amortized Bayesian learning, enabling efficient transfer to new data without retraining. Comprehensive results across diverse attack and defense settings demonstrate the state-of-the-art performance of our approach. Code is available at https://github.com/Egg-Hu/Bayesian-Data-Scheduler. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oIpRvQkrH9@OpenReview",
      "index": 652,
      "title": "Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection",
      "authors": [
        "Yu Guo",
        "Shengfeng He",
        "Yuxu Lu",
        "Haonan An",
        "Yihang Tao",
        "Huilin Zhu",
        "Jingxian Liu",
        "Yuguang Fang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "maritime",
        "neptune",
        "generation",
        "object",
        "generative",
        "gy65896",
        "detection",
        "active",
        "synthetic",
        "aquatic"
      ],
      "summary": "Maritime object detection is essential for navigation safety, surveillance, and autonomous operations, yet constrained by two key challenges: the scarcity of annotated maritime data and poor generalization across various maritime attributes (e.g., object category, viewpoint, location, and imaging environment). To address these challenges, we propose Neptune-X, a data-centric generative-selection framework that enhances training effectiveness by leveraging synthetic data generation with task-aware sample selection. From the generation perspective, we develop X-to-Maritime, a multi-modality-conditioned generative model that synthesizes diverse and realistic maritime scenes. A key component is the Bidirectional Object-Water Attention module, which captures boundary interactions between objects and their aquatic surroundings to improve visual fidelity. To further improve downstream tasking performance, we propose Attribute-correlated Active Sampling, which dynamically selects synthetic samples based on their task relevance. To support robust benchmarking, we construct the Maritime Generation Dataset, the first dataset tailored for generative maritime learning, encompassing a wide range of semantic conditions. Extensive experiments demonstrate that our approach sets a new benchmark in maritime scene synthesis, significantly improving detection accuracy, particularly in challenging and previously underrepresented settings. The code is available at https://github.com/gy65896/Neptune-X.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oIpRvQkrH9"
        ],
        "venue": [
          "/venue/oIpRvQkrH9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oIpRvQkrH9"
        ],
        "detail": [
          "https://openreview.net/forum?id=oIpRvQkrH9"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yu Guo , Shengfeng He , Yuxu Lu , Haonan An , Yihang Tao , Huilin Zhu , Jingxian Liu , Yuguang Fang Maritime object detection is essential for navigation safety, surveillance, and autonomous operations, yet constrained by two key challenges: the scarcity of annotated maritime data and poor generalization across various maritime attributes (e.g., object category, viewpoint, location, and imaging environment). To address these challenges, we propose Neptune-X, a data-centric generative-selection framework that enhances training effectiveness by leveraging synthetic data generation with task-aware sample selection. From the generation perspective, we develop X-to-Maritime, a multi-modality-conditioned generative model that synthesizes diverse and realistic maritime scenes. A key component is the Bidirectional Object-Water Attention module, which captures boundary interactions between objects and their aquatic surroundings to improve visual fidelity. To further improve downstream tasking performance, we propose Attribute-correlated Active Sampling, which dynamically selects synthetic samples based on their task relevance. To support robust benchmarking, we construct the Maritime Generation Dataset, the first dataset tailored for generative maritime learning, encompassing a wide range of semantic conditions. Extensive experiments demonstrate that our approach sets a new benchmark in maritime scene synthesis, significantly improving detection accuracy, particularly in challenging and previously underrepresented settings. The code is available at https://github.com/gy65896/Neptune-X. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2hgHyoyVWj@OpenReview",
      "index": 653,
      "title": "AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping",
      "authors": [
        "Haonan Dong",
        "Wenhao Zhu",
        "Guojie Song",
        "Liang Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lora",
        "rank",
        "textbf",
        "ourmethod",
        "peft",
        "bottleneck",
        "tuning",
        "fontfamily",
        "selectfont",
        "aurora"
      ],
      "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA faces an inherent low-rank bottleneck: narrowing its performance gap with full fine-tuning requires increasing the rank of its parameter matrix, resulting in significant parameter overhead. Recent linear LoRA variants have attempted to enhance expressiveness by introducing additional linear mappings; however, their composition remains inherently linear and fails to fundamentally improve LoRA’s representational capacity. To address this limitation, we propose \\ourmethod, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear projectors to capture \\emph{fixed} and \\emph{learnable} nonlinearities. This combination forms an {\\fontfamily{lmtt}\\selectfont \\textbf{MLP-like structure}} with a compressed rank, enabling flexible and precise approximation of diverse target functions while theoretically guaranteeing lower approximation errors and bounded gradients. Extensive experiments on 22 datasets and 6 pretrained models demonstrate that \\ourmethod: (\\textbf{I}) not only matches or surpasses full fine-tuning performance with only 6.18 % ∼ 25 % 6.18 % ∼ 25 % of LoRA’s parameters but also (\\textbf{II}) outperforms state-of-the-art PEFT methods by up to 10.88 % 10.88 % in both NLP and CV tasks, and \\textbf{(III)} exhibits robust performance across various rank configurations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2hgHyoyVWj"
        ],
        "venue": [
          "/venue/2hgHyoyVWj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2hgHyoyVWj"
        ],
        "detail": [
          "https://openreview.net/forum?id=2hgHyoyVWj"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Haonan Dong , Wenhao Zhu , Guojie Song , Liang Wang Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA faces an inherent low-rank bottleneck: narrowing its performance gap with full fine-tuning requires increasing the rank of its parameter matrix, resulting in significant parameter overhead. Recent linear LoRA variants have attempted to enhance expressiveness by introducing additional linear mappings; however, their composition remains inherently linear and fails to fundamentally improve LoRA’s representational capacity. To address this limitation, we propose \\ourmethod, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear projectors to capture \\emph{fixed} and \\emph{learnable} nonlinearities. This combination forms an {\\fontfamily{lmtt}\\selectfont \\textbf{MLP-like structure}} with a compressed rank, enabling flexible and precise approximation of diverse target functions while theoretically guaranteeing lower approximation errors and bounded gradients. Extensive experiments on 22 datasets and 6 pretrained models demonstrate that \\ourmethod: (\\textbf{I}) not only matches or surpasses full fine-tuning performance with only 6.18 % ∼ 25 % 6.18 % ∼ 25 % of LoRA’s parameters but also (\\textbf{II}) outperforms state-of-the-art PEFT methods by up to 10.88 % 10.88 % in both NLP and CV tasks, and \\textbf{(III)} exhibits robust performance across various rank configurations. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Wbc3PutCyQ@OpenReview",
      "index": 654,
      "title": "Puppeteer: Rig and Animate Your 3D Models",
      "authors": [
        "Chaoyue Song",
        "Xiu Li",
        "Fan Yang",
        "Zhongcong Xu",
        "Jiacheng Wei",
        "Fayao Liu",
        "Jiashi Feng",
        "Guosheng Lin",
        "Jianfeng Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "rigging",
        "puppeteer",
        "skeletal",
        "animation",
        "skinning",
        "animations",
        "animate",
        "content",
        "joint",
        "assets"
      ],
      "summary": "Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present \\textbf{Puppeteer}, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Wbc3PutCyQ"
        ],
        "venue": [
          "/venue/Wbc3PutCyQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Wbc3PutCyQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=Wbc3PutCyQ"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Puppeteer: Rig and Animate Your 3D Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Chaoyue Song , Xiu Li , Fan Yang , Zhongcong Xu , Jiacheng Wei , Fayao Liu , Jiashi Feng , Guosheng Lin , Jianfeng Zhang Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present \\textbf{Puppeteer}, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "fxERuSBpfQ@OpenReview",
      "index": 655,
      "title": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras",
      "authors": [
        "Lingdong Kong",
        "Dongyue Lu",
        "Alan Liang",
        "Rong Li",
        "Yuhao Dong",
        "Tianshuai Hu",
        "Lai Xing Ng",
        "Wei Tsang Ooi",
        "Benoit R Cottereau"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "talk2event",
        "event",
        "grounding",
        "attribute",
        "cameras",
        "moee",
        "language",
        "perception",
        "grounded",
        "understanding"
      ],
      "summary": "Event cameras offer microsecond-level latency and robustness to motion blur, making them ideal for understanding dynamic environments. Yet, connecting these asynchronous streams to human language remains an open challenge. We introduce Talk2Event, the first large-scale benchmark for language-driven object grounding in event-based perception. Built from real-world driving data, Talk2Event provides over 30,000 validated referring expressions, each enriched with four grounding attributes -- appearance, status, relation to viewer, and relation to other objects -- bridging spatial, temporal, and relational reasoning. To fully exploit these cues, we propose EventRefer, an attribute-aware grounding framework that dynamically fuses multi-attribute representations through a Mixture of Event-Attribute Experts (MoEE). Our method adapts to different modalities and scene dynamics, achieving consistent gains over state-of-the-art baselines in event-only, frame-only, and event-frame fusion settings. We hope our dataset and approach will establish a foundation for advancing multimodal, temporally-aware, and language-driven perception in real-world robotics and autonomy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fxERuSBpfQ"
        ],
        "venue": [
          "/venue/fxERuSBpfQ@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fxERuSBpfQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=fxERuSBpfQ"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Lingdong Kong , Dongyue Lu , Alan Liang , Rong Li , Yuhao Dong , Tianshuai Hu , Lai Xing Ng , Wei Tsang Ooi , Benoit R Cottereau Event cameras offer microsecond-level latency and robustness to motion blur, making them ideal for understanding dynamic environments. Yet, connecting these asynchronous streams to human language remains an open challenge. We introduce Talk2Event, the first large-scale benchmark for language-driven object grounding in event-based perception. Built from real-world driving data, Talk2Event provides over 30,000 validated referring expressions, each enriched with four grounding attributes -- appearance, status, relation to viewer, and relation to other objects -- bridging spatial, temporal, and relational reasoning. To fully exploit these cues, we propose EventRefer, an attribute-aware grounding framework that dynamically fuses multi-attribute representations through a Mixture of Event-Attribute Experts (MoEE). Our method adapts to different modalities and scene dynamics, achieving consistent gains over state-of-the-art baselines in event-only, frame-only, and event-frame fusion settings. We hope our dataset and approach will establish a foundation for advancing multimodal, temporally-aware, and language-driven perception in real-world robotics and autonomy. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "JxBA9OJExP@OpenReview",
      "index": 656,
      "title": "DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing",
      "authors": [
        "Chenxi Xie",
        "Minghan Li",
        "Shuai Li",
        "Yuhui Wu",
        "Qiaosi Yi",
        "Lei Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dnaedit",
        "dna",
        "editing",
        "timestep",
        "noise",
        "mvg",
        "rectified",
        "methods",
        "guided",
        "latent"
      ],
      "summary": "Leveraging the powerful generation capability of large-scale pretrained text-to-image models, training-free methods have demonstrated impressive image editing results. Conventional diffusion-based methods, as well as recent rectified flow (RF)-based methods, typically reverse synthesis trajectories by gradually adding noise to clean images, during which the noisy latent at the current timestep is used to approximate that at the next timesteps, introducing accumulated drift and degrading reconstruction accuracy. Considering the fact that in RF the noisy latent is estimated through direct interpolation between Gaussian noises and clean images at each timestep, we propose Direct Noise Alignment (DNA), which directly refines the desired Gaussian noise in the noise domain, significantly reducing the error accumulation in previous methods. Specifically, DNA estimates the velocity field of the interpolated noised latent at each timestep and adjusts the Gaussian noise by computing the difference between the predicted and expected velocity field. We validate the effectiveness of DNA and reveal its relationship with existing RF-based inversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG) to control the target prompt-guided generation process, balancing image background preservation and target object editability. DNA and MVG collectively constitute our proposed method, namely DNAEdit. Finally, we introduce DNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced image editing models. Experimental results demonstrate that our DNAEdit achieves superior performance to state-of-the-art text-guided editing methods. Our code, model, and benchmark will be made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JxBA9OJExP"
        ],
        "venue": [
          "/venue/JxBA9OJExP@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JxBA9OJExP"
        ],
        "detail": [
          "https://openreview.net/forum?id=JxBA9OJExP"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Chenxi Xie , Minghan Li , Shuai Li , Yuhui Wu , Qiaosi Yi , Lei Zhang Leveraging the powerful generation capability of large-scale pretrained text-to-image models, training-free methods have demonstrated impressive image editing results. Conventional diffusion-based methods, as well as recent rectified flow (RF)-based methods, typically reverse synthesis trajectories by gradually adding noise to clean images, during which the noisy latent at the current timestep is used to approximate that at the next timesteps, introducing accumulated drift and degrading reconstruction accuracy. Considering the fact that in RF the noisy latent is estimated through direct interpolation between Gaussian noises and clean images at each timestep, we propose Direct Noise Alignment (DNA), which directly refines the desired Gaussian noise in the noise domain, significantly reducing the error accumulation in previous methods. Specifically, DNA estimates the velocity field of the interpolated noised latent at each timestep and adjusts the Gaussian noise by computing the difference between the predicted and expected velocity field. We validate the effectiveness of DNA and reveal its relationship with existing RF-based inversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG) to control the target prompt-guided generation process, balancing image background preservation and target object editability. DNA and MVG collectively constitute our proposed method, namely DNAEdit. Finally, we introduce DNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced image editing models. Experimental results demonstrate that our DNAEdit achieves superior performance to state-of-the-art text-guided editing methods. Our code, model, and benchmark will be made publicly available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yHJRI6rzaA@OpenReview",
      "index": 657,
      "title": "Alligat0R: Pre-Training through Covisibility Segmentation for Relative Camera Pose Regression",
      "authors": [
        "Thibaut Loiseau",
        "Guillaume Bourmaud",
        "Vincent Lepetit"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "covisible",
        "covisibility",
        "alligat0r",
        "cub3",
        "croco",
        "pre",
        "pose",
        "regression",
        "training",
        "view"
      ],
      "summary": "Pre-training techniques have greatly advanced computer vision, with CroCo’s cross-view completion approach yielding impressive results in tasks like 3D reconstruction and pose regression. However, cross-view completion is ill-posed in non-covisible regions, limiting its effectiveness. We introduce Alligat0R, a novel pre-training approach that replaces cross-view learning with a covisibility segmentation task. Our method predicts whether each pixel in one image is covisible in the second image, occluded, or outside the field of view, making the pre-training effective in both covisible and non-covisible regions, and provides interpretable predictions. To support this, we present Cub3, a large-scale dataset with 5M image pairs and dense covisibility annotations derived from the nuScenes and ScanNet datasets. Cub3 includes diverse scenarios with varying degrees of overlap. The experiments show that our novel pre-training method Alligat0R significantly outperforms CroCo in relative pose regression. Alligat0R and Cub3 will be made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yHJRI6rzaA"
        ],
        "venue": [
          "/venue/yHJRI6rzaA@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yHJRI6rzaA"
        ],
        "detail": [
          "https://openreview.net/forum?id=yHJRI6rzaA"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Alligat0R: Pre-Training through Covisibility Segmentation for Relative Camera Pose Regression [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Thibaut Loiseau , Guillaume Bourmaud , Vincent Lepetit Pre-training techniques have greatly advanced computer vision, with CroCo’s cross-view completion approach yielding impressive results in tasks like 3D reconstruction and pose regression. However, cross-view completion is ill-posed in non-covisible regions, limiting its effectiveness. We introduce Alligat0R, a novel pre-training approach that replaces cross-view learning with a covisibility segmentation task. Our method predicts whether each pixel in one image is covisible in the second image, occluded, or outside the field of view, making the pre-training effective in both covisible and non-covisible regions, and provides interpretable predictions. To support this, we present Cub3, a large-scale dataset with 5M image pairs and dense covisibility annotations derived from the nuScenes and ScanNet datasets. Cub3 includes diverse scenarios with varying degrees of overlap. The experiments show that our novel pre-training method Alligat0R significantly outperforms CroCo in relative pose regression. Alligat0R and Cub3 will be made publicly available. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "UnslcaZSnb@OpenReview",
      "index": 658,
      "title": "DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling",
      "authors": [
        "Yuang Ai",
        "Qihang Fan",
        "Xuefeng Hu",
        "Zhenheng Yang",
        "Ran He",
        "Huaibo Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "dico",
        "dit",
        "diffusion",
        "convnets",
        "convnet",
        "revitalizing",
        "256",
        "times",
        "512",
        "generation"
      ],
      "summary": "Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns—highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet generation benchmarks, DiCo-XL achieves an FID of 2.05 at 256 × × 256 resolution and 2.53 at 512 × × 512, with a **2.7 × × ** and **3.1 × × ** speedup over DiT-XL/2, respectively. Furthermore, experimental results on MS-COCO demonstrate that the purely convolutional DiCo exhibits strong potential for text-to-image generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UnslcaZSnb"
        ],
        "venue": [
          "/venue/UnslcaZSnb@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UnslcaZSnb"
        ],
        "detail": [
          "https://openreview.net/forum?id=UnslcaZSnb"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Yuang Ai , Qihang Fan , Xuefeng Hu , Zhenheng Yang , Ran He , Huaibo Huang Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns—highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet generation benchmarks, DiCo-XL achieves an FID of 2.05 at 256 × × 256 resolution and 2.53 at 512 × × 512, with a **2.7 × × ** and **3.1 × × ** speedup over DiT-XL/2, respectively. Furthermore, experimental results on MS-COCO demonstrate that the purely convolutional DiCo exhibits strong potential for text-to-image generation. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "BirE0jYKt0@OpenReview",
      "index": 659,
      "title": "Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective",
      "authors": [
        "Xingjian Wu",
        "Xiangfei Qiu",
        "Hanyin Cheng",
        "Zhengyu Li",
        "Jilin Hu",
        "Chenjuan Guo",
        "Bin Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "srs",
        "srsnet",
        "forecasting",
        "selective",
        "patching",
        "series",
        "patches",
        "representation",
        "patch",
        "contextual"
      ],
      "summary": "Time Series Forecasting has made significant progress with the help of Patching technique, which partitions time series into multiple patches to effectively retain contextual semantic information into a representation space beneficial for modeling long-term dependencies. However, conventional patching partitions a time series into adjacent patches, which causes a fixed representation space, thus resulting in insufficiently expressful representations. In this paper, we pioneer the exploration of constructing a selective representation space to flexibly include the most informative patches for forecasting. Specifically, we propose the Selective Representation Space (SRS) module, which utilizes the learnable Selective Patching and Dynamic Reassembly techniques to adaptively select and shuffle the patches from the contextual time series, aiming at fully exploiting the information of contextual time series to enhance the forecasting performance of patch-based models. To demonstrate the effectiveness of SRS module, we propose a simple yet effective SRSNet consisting of SRS and an MLP head, which achieves state-of-the-art performance on real-world datasets from multiple domains. Furthermore, as a novel plugin-and-play module, SRS can also enhance the performance of existing patch-based models. The resources are available at https://github.com/decisionintelligence/SRSNet.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BirE0jYKt0"
        ],
        "venue": [
          "/venue/BirE0jYKt0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BirE0jYKt0"
        ],
        "detail": [
          "https://openreview.net/forum?id=BirE0jYKt0"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Xingjian Wu , Xiangfei Qiu , Hanyin Cheng , Zhengyu Li , Jilin Hu , Chenjuan Guo , Bin Yang Time Series Forecasting has made significant progress with the help of Patching technique, which partitions time series into multiple patches to effectively retain contextual semantic information into a representation space beneficial for modeling long-term dependencies. However, conventional patching partitions a time series into adjacent patches, which causes a fixed representation space, thus resulting in insufficiently expressful representations. In this paper, we pioneer the exploration of constructing a selective representation space to flexibly include the most informative patches for forecasting. Specifically, we propose the Selective Representation Space (SRS) module, which utilizes the learnable Selective Patching and Dynamic Reassembly techniques to adaptively select and shuffle the patches from the contextual time series, aiming at fully exploiting the information of contextual time series to enhance the forecasting performance of patch-based models. To demonstrate the effectiveness of SRS module, we propose a simple yet effective SRSNet consisting of SRS and an MLP head, which achieves state-of-the-art performance on real-world datasets from multiple domains. Furthermore, as a novel plugin-and-play module, SRS can also enhance the performance of existing patch-based models. The resources are available at https://github.com/decisionintelligence/SRSNet. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "oaWpRaZ4jj@OpenReview",
      "index": 660,
      "title": "Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes",
      "authors": [
        "Haonan Wang",
        "Hanyu Zhou",
        "Haoyue Liu",
        "Luxin Yan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "frame",
        "event",
        "appearance",
        "boundary",
        "scenes",
        "flow",
        "completeness",
        "fusion",
        "abflow",
        "diffusion"
      ],
      "summary": "Optical flow estimation has achieved promising results in conventional scenes but faces challenges in high-speed and low-light scenes, which suffer from motion blur and insufficient illumination. These conditions lead to weakened texture and amplified noise and deteriorate the appearance saturation and boundary completeness of frame cameras, which are necessary for motion feature matching. In degraded scenes, the frame camera provides dense appearance saturation but sparse boundary completeness due to its long imaging time and low dynamic range. In contrast, the event camera offers sparse appearance saturation, while its short imaging time and high dynamic range gives rise to dense boundary completeness. Traditionally, existing methods utilize feature fusion or domain adaptation to introduce event to improve boundary completeness. However, the appearance features are still deteriorated, which severely affects the mostly adopted discriminative models that learn the mapping from visual features to motion fields and generative models that generate motion fields based on given visual features. So we introduce diffusion models that learn the mapping from noising flow to clear flow, which is not affected by the deteriorated visual features. Therefore, we propose a novel optical flow estimation framework Diff-ABFlow based on diffusion models with frame-event appearance-boundary fusion. Inspired by the appearance-boundary complementarity of frame and event, we propose an Attention-Guided Appearance-Boundary Fusion module to fuse frame and event. Based on diffusion models, we propose a Multi-Condition Iterative Denoising Decoder. Our proposed method can effectively utilize the respective advantages of frame and event, and shows great robustness to degraded input. In addition, we propose a dual-modal optical flow dataset for generalization experiments. Extensive experiments have verified the superiority of our proposed method. The code is released at <https://github.com/Haonan-Wang-aurora/Diff-ABFlow>.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oaWpRaZ4jj"
        ],
        "venue": [
          "/venue/oaWpRaZ4jj@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oaWpRaZ4jj"
        ],
        "detail": [
          "https://openreview.net/forum?id=oaWpRaZ4jj"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes [PDF ] [Copy] [Kimi ] [REL] Authors : Haonan Wang , Hanyu Zhou , Haoyue Liu , Luxin Yan Optical flow estimation has achieved promising results in conventional scenes but faces challenges in high-speed and low-light scenes, which suffer from motion blur and insufficient illumination. These conditions lead to weakened texture and amplified noise and deteriorate the appearance saturation and boundary completeness of frame cameras, which are necessary for motion feature matching. In degraded scenes, the frame camera provides dense appearance saturation but sparse boundary completeness due to its long imaging time and low dynamic range. In contrast, the event camera offers sparse appearance saturation, while its short imaging time and high dynamic range gives rise to dense boundary completeness. Traditionally, existing methods utilize feature fusion or domain adaptation to introduce event to improve boundary completeness. However, the appearance features are still deteriorated, which severely affects the mostly adopted discriminative models that learn the mapping from visual features to motion fields and generative models that generate motion fields based on given visual features. So we introduce diffusion models that learn the mapping from noising flow to clear flow, which is not affected by the deteriorated visual features. Therefore, we propose a novel optical flow estimation framework Diff-ABFlow based on diffusion models with frame-event appearance-boundary fusion. Inspired by the appearance-boundary complementarity of frame and event, we propose an Attention-Guided Appearance-Boundary Fusion module to fuse frame and event. Based on diffusion models, we propose a Multi-Condition Iterative Denoising Decoder. Our proposed method can effectively utilize the respective advantages of frame and event, and shows great robustness to degraded input. In addition, we propose a dual-modal optical flow dataset for generalization experiments. Extensive experiments have verified the superiority of our proposed method. The code is released at <https://github.com/Haonan-Wang-aurora/Diff-ABFlow>. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "9XCyUFsm1H@OpenReview",
      "index": 661,
      "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers",
      "authors": [
        "Ziqiao Peng",
        "Jiwen Liu",
        "Haoxian Zhang",
        "Xiaoqiang Liu",
        "Songlin Tang",
        "Pengfei Wan",
        "Di ZHANG",
        "Hongyan Liu",
        "Jun He"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "lip",
        "omnisync",
        "synchronization",
        "audio",
        "identity",
        "sync",
        "video",
        "conditioning",
        "editing",
        "lipsync"
      ],
      "summary": "Lip synchronization is the task of aligning a speaker’s lip movements in video with corresponding speech audio, and it is essential for creating realistic, expressive video content. However, existing methods often rely on reference frames and masked-frame inpainting, which limit their robustness to identity consistency, pose variations, facial occlusions, and stylized content. In addition, since audio signals provide weaker conditioning than visual cues, lip shape leakage from the original video will affect lip sync quality. In this paper, we present OmniSync, a universal lip synchronization framework for diverse visual scenarios. Our approach introduces a mask-free training paradigm using Diffusion Transformer models for direct frame editing without explicit masks, enabling unlimited-duration inference while maintaining natural facial dynamics and preserving character identity. During inference, we propose a flow-matching-based progressive noise initialization to ensure pose and identity consistency, while allowing precise mouth-region editing. To address the weak conditioning signal of audio, we develop a Dynamic Spatiotemporal Classifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance strength over time and space. We also establish the AIGC-LipSync Benchmark, the first evaluation suite for lip synchronization in diverse AI-generated videos. Extensive experiments demonstrate that OmniSync significantly outperforms prior methods in both visual quality and lip sync accuracy, achieving superior results in both real-world and AI-generated videos.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9XCyUFsm1H"
        ],
        "venue": [
          "/venue/9XCyUFsm1H@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9XCyUFsm1H"
        ],
        "detail": [
          "https://openreview.net/forum?id=9XCyUFsm1H"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ziqiao Peng , Jiwen Liu , Haoxian Zhang , Xiaoqiang Liu , Songlin Tang , Pengfei Wan , Di ZHANG , Hongyan Liu , Jun He Lip synchronization is the task of aligning a speaker’s lip movements in video with corresponding speech audio, and it is essential for creating realistic, expressive video content. However, existing methods often rely on reference frames and masked-frame inpainting, which limit their robustness to identity consistency, pose variations, facial occlusions, and stylized content. In addition, since audio signals provide weaker conditioning than visual cues, lip shape leakage from the original video will affect lip sync quality. In this paper, we present OmniSync, a universal lip synchronization framework for diverse visual scenarios. Our approach introduces a mask-free training paradigm using Diffusion Transformer models for direct frame editing without explicit masks, enabling unlimited-duration inference while maintaining natural facial dynamics and preserving character identity. During inference, we propose a flow-matching-based progressive noise initialization to ensure pose and identity consistency, while allowing precise mouth-region editing. To address the weak conditioning signal of audio, we develop a Dynamic Spatiotemporal Classifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance strength over time and space. We also establish the AIGC-LipSync Benchmark, the first evaluation suite for lip synchronization in diverse AI-generated videos. Extensive experiments demonstrate that OmniSync significantly outperforms prior methods in both visual quality and lip sync accuracy, achieving superior results in both real-world and AI-generated videos. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "U88JlpY0vR@OpenReview",
      "index": 662,
      "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
      "authors": [
        "Jinkun Hao",
        "Naifu Liang",
        "Zhen Luo",
        "Xudong XU",
        "Weipeng Zhong",
        "Ran Yi",
        "Yichen Jin",
        "Zhaoyang Lyu",
        "Feng Zheng",
        "Lizhuang Ma",
        "Jiangmiao Pang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "tabletop",
        "mesatask",
        "scenes",
        "task",
        "layouts",
        "reasoning",
        "textbf",
        "scene",
        "generation",
        "spatial"
      ],
      "summary": "The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce \\textbf{MesaTask-10K}, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with \\emph{manually crafted layouts} that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a \\textbf{Spatial Reasoning Chain} that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present \\textbf{MesaTask}, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=U88JlpY0vR"
        ],
        "venue": [
          "/venue/U88JlpY0vR@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=U88JlpY0vR"
        ],
        "detail": [
          "https://openreview.net/forum?id=U88JlpY0vR"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning [PDF ] [Copy] [Kimi ] [REL] Authors : Jinkun Hao , Naifu Liang , Zhen Luo , Xudong XU , Weipeng Zhong , Ran Yi , Yichen Jin , Zhaoyang Lyu , Feng Zheng , Lizhuang Ma , Jiangmiao Pang The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce \\textbf{MesaTask-10K}, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with \\emph{manually crafted layouts} that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a \\textbf{Spatial Reasoning Chain} that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present \\textbf{MesaTask}, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "mSiN7i0BYH@OpenReview",
      "index": 663,
      "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
      "authors": [
        "Xun Huang",
        "Zhengqi Li",
        "Guande He",
        "Mingyuan Zhou",
        "Eli Shechtman"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "video",
        "autoregressive",
        "forcing",
        "self",
        "diffusion",
        "truth",
        "outputs",
        "frames",
        "generation",
        "bridging"
      ],
      "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mSiN7i0BYH"
        ],
        "venue": [
          "/venue/mSiN7i0BYH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mSiN7i0BYH"
        ],
        "detail": [
          "https://openreview.net/forum?id=mSiN7i0BYH"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Xun Huang , Zhengqi Li , Guande He , Mingyuan Zhou , Eli Shechtman We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "hQhAPGCtPo@OpenReview",
      "index": 664,
      "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
      "authors": [
        "Canyu Zhao",
        "Yanlong Sun",
        "Mingyu Liu",
        "Huanyi Zheng",
        "Muzhi Zhu",
        "Zhiyue Zhao",
        "Hao Chen",
        "Tong He",
        "Chunhua Shen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "diception",
        "generalist",
        "tasks",
        "perception",
        "diffusion",
        "model",
        "visual",
        "trained",
        "images",
        "pixel"
      ],
      "summary": "This paper's primary objective is to develop a robust generalist perception model capable of addressing multiple tasks under constraints of computational resources and limited training data. We leverage text-to-image diffusion models pre-trained on billions of images and successfully introduce our DICEPTION, a visual generalist model. Exhaustive evaluations demonstrate that DICEPTION effectively tackles diverse perception tasks, even achieving performance comparable to SOTA single-task specialist models. Specifically, we achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs.\\ 1B pixel-level annotated images). We designed comprehensive experiments on architectures and input paradigms, demonstrating that the key to successfully re-purposing a single diffusion model for multiple perception tasks lies in maximizing the preservation of the pre-trained model's prior knowledge. Consequently, DICEPTION can be trained with substantially lower computational costs than conventional models requiring training from scratch. Furthermore, adapting DICEPTION to novel tasks is highly efficient, necessitating fine-tuning on as few as 50 images and approximately 1% of its parameters. Finally, we demonstrate that a subtle application of classifier-free guidance can improve the model's performance on depth and normal estimation. We also show that pixel-aligned training, as is characteristic of perception tasks, significantly enhances the model's ability to preserve fine details. DICEPTION offers valuable insights and presents a promising direction for the development of advanced diffusion-based visual generalist models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hQhAPGCtPo"
        ],
        "venue": [
          "/venue/hQhAPGCtPo@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hQhAPGCtPo"
        ],
        "detail": [
          "https://openreview.net/forum?id=hQhAPGCtPo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Canyu Zhao , Yanlong Sun , Mingyu Liu , Huanyi Zheng , Muzhi Zhu , Zhiyue Zhao , Hao Chen , Tong He , Chunhua Shen This paper's primary objective is to develop a robust generalist perception model capable of addressing multiple tasks under constraints of computational resources and limited training data. We leverage text-to-image diffusion models pre-trained on billions of images and successfully introduce our DICEPTION, a visual generalist model. Exhaustive evaluations demonstrate that DICEPTION effectively tackles diverse perception tasks, even achieving performance comparable to SOTA single-task specialist models. Specifically, we achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs.\\ 1B pixel-level annotated images). We designed comprehensive experiments on architectures and input paradigms, demonstrating that the key to successfully re-purposing a single diffusion model for multiple perception tasks lies in maximizing the preservation of the pre-trained model's prior knowledge. Consequently, DICEPTION can be trained with substantially lower computational costs than conventional models requiring training from scratch. Furthermore, adapting DICEPTION to novel tasks is highly efficient, necessitating fine-tuning on as few as 50 images and approximately 1% of its parameters. Finally, we demonstrate that a subtle application of classifier-free guidance can improve the model's performance on depth and normal estimation. We also show that pixel-aligned training, as is characteristic of perception tasks, significantly enhances the model's ability to preserve fine details. DICEPTION offers valuable insights and presents a promising direction for the development of advanced diffusion-based visual generalist models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nIFFMrDQ5w@OpenReview",
      "index": 665,
      "title": "Variational Learning Finds Flatter Solutions at the Edge of Stability",
      "authors": [
        "Avrajit Ghosh",
        "Bai Cong",
        "Rio Yokota",
        "Saiprasad Ravishankar",
        "Rongrong Wang",
        "Molei Tao",
        "Mohammad Emtiyaz Khan",
        "Thomas Möllenhoff"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "eos",
        "flatter",
        "variational",
        "posterior",
        "solutions",
        "edge",
        "stability",
        "implicit",
        "regularization",
        "finds"
      ],
      "summary": "Variational Learning (VL) has recently gained popularity for training deep neural networks. Part of its empirical success can be explained by theories such as PAC-Bayes bounds, minimum description length and marginal likelihood, but little has been done to unravel the implicit regularization in play. Here, we analyze the implicit regularization of VL through the Edge of Stability (EoS) framework. EoS has previously been used to show that gradient descent can find flat solutions and we extend this result to show that VL can find even flatter solutions. This result is obtained by controlling the shape of the variational posterior as well as the number of posterior samples used during training. The derivation follows in a similar fashion as in the standard EoS literature for deep learning, by first deriving a result for a quadratic problem and then extending it to deep neural networks. We empirically validate these findings on a wide variety of large networks, such as ResNet and ViT, to find that the theoretical results closely match the empirical ones. Ours is the first work to analyze the EoS dynamics of~VL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nIFFMrDQ5w"
        ],
        "venue": [
          "/venue/nIFFMrDQ5w@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nIFFMrDQ5w"
        ],
        "detail": [
          "https://openreview.net/forum?id=nIFFMrDQ5w"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Variational Learning Finds Flatter Solutions at the Edge of Stability [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Avrajit Ghosh , Bai Cong , Rio Yokota , Saiprasad Ravishankar , Rongrong Wang , Molei Tao , Mohammad Emtiyaz Khan , Thomas Möllenhoff Variational Learning (VL) has recently gained popularity for training deep neural networks. Part of its empirical success can be explained by theories such as PAC-Bayes bounds, minimum description length and marginal likelihood, but little has been done to unravel the implicit regularization in play. Here, we analyze the implicit regularization of VL through the Edge of Stability (EoS) framework. EoS has previously been used to show that gradient descent can find flat solutions and we extend this result to show that VL can find even flatter solutions. This result is obtained by controlling the shape of the variational posterior as well as the number of posterior samples used during training. The derivation follows in a similar fashion as in the standard EoS literature for deep learning, by first deriving a result for a quadratic problem and then extending it to deep neural networks. We empirically validate these findings on a wide variety of large networks, such as ResNet and ViT, to find that the theoretical results closely match the empirical ones. Ours is the first work to analyze the EoS dynamics of~VL. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "PWdWmw9wh0@OpenReview",
      "index": 666,
      "title": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization",
      "authors": [
        "Yanhao Jia",
        "Ji Xie",
        "S Jivaganesh",
        "Li Hao",
        "Xu Wu",
        "Mengmi Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sound",
        "conflict",
        "auditory",
        "multimodal",
        "ssl",
        "audio",
        "bias",
        "modality",
        "conflicting",
        "hearing"
      ],
      "summary": "Imagine hearing a dog bark and instinctively turning toward the sound—only to find a parked car, while a silent dog sits nearby. Such moments of sensory conflict challenge perception, yet humans flexibly resolve these discrepancies, prioritizing auditory cues over misleading visuals to accurately localize sounds. Despite the rapid advancement of multimodal AI models that integrate vision and sound, little is known about how these systems handle cross-modal conflicts or whether they favor one modality over another. Here, we systematically and quantitatively examine modality bias and conflict resolution in AI models for Sound Source Localization (SSL). We evaluate a wide range of state-of-the-art multimodal models and compare them against human performance in psychophysics experiments spanning six audiovisual conditions, including congruent, conflicting, and absent visual and audio cues. Our results reveal that humans consistently outperform AI in SSL and exhibit greater robustness to conflicting or absent visual information by effectively prioritizing auditory signals. In contrast, AI shows a pronounced bias toward vision, often failing to suppress irrelevant or conflicting visual input, leading to chance-level performance. To bridge this gap, we present EchoPin, a neuroscience-inspired multimodal model for SSL that emulates human auditory perception. The model is trained on our carefully curated AudioCOCO dataset, in which stereo audio signals are first rendered using a physics-based 3D simulator, then filtered with Head-Related Transfer Functions (HRTFs) to capture pinnae, head, and torso effects, and finally transformed into cochleagram representations that mimic cochlear processing. To eliminate existing biases in standard benchmark datasets, we carefully controlled the vocal object sizes, semantics, and spatial locations in the corresponding images of AudioCOCO. EchoPin outperforms existing models trained on standard audio-visual datasets. Remarkably, consistent with neuroscience findings, it exhibits a human-like localization bias, favoring horizontal (left–right) precision over vertical (up–down) precision. This asymmetry likely arises from HRTF-shaped and cochlear-modulated stereo audio and the lateral placement of human ears, highlighting how sensory input quality and physical structure jointly shape precision of multimodal representations. All code, data, and models are available \\href{https://github.com/CuriseJia/SSHS}{here}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PWdWmw9wh0"
        ],
        "venue": [
          "/venue/PWdWmw9wh0@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PWdWmw9wh0"
        ],
        "detail": [
          "https://openreview.net/forum?id=PWdWmw9wh0"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yanhao Jia , Ji Xie , S Jivaganesh , Li Hao , Xu Wu , Mengmi Zhang Imagine hearing a dog bark and instinctively turning toward the sound—only to find a parked car, while a silent dog sits nearby. Such moments of sensory conflict challenge perception, yet humans flexibly resolve these discrepancies, prioritizing auditory cues over misleading visuals to accurately localize sounds. Despite the rapid advancement of multimodal AI models that integrate vision and sound, little is known about how these systems handle cross-modal conflicts or whether they favor one modality over another. Here, we systematically and quantitatively examine modality bias and conflict resolution in AI models for Sound Source Localization (SSL). We evaluate a wide range of state-of-the-art multimodal models and compare them against human performance in psychophysics experiments spanning six audiovisual conditions, including congruent, conflicting, and absent visual and audio cues. Our results reveal that humans consistently outperform AI in SSL and exhibit greater robustness to conflicting or absent visual information by effectively prioritizing auditory signals. In contrast, AI shows a pronounced bias toward vision, often failing to suppress irrelevant or conflicting visual input, leading to chance-level performance. To bridge this gap, we present EchoPin, a neuroscience-inspired multimodal model for SSL that emulates human auditory perception. The model is trained on our carefully curated AudioCOCO dataset, in which stereo audio signals are first rendered using a physics-based 3D simulator, then filtered with Head-Related Transfer Functions (HRTFs) to capture pinnae, head, and torso effects, and finally transformed into cochleagram representations that mimic cochlear processing. To eliminate existing biases in standard benchmark datasets, we carefully controlled the vocal object sizes, semantics, and spatial locations in the corresponding images of AudioCOCO. EchoPin outperforms existing models trained on standard audio-visual datasets. Remarkably, consistent with neuroscience findings, it exhibits a human-like localization bias, favoring horizontal (left–right) precision over vertical (up–down) precision. This asymmetry likely arises from HRTF-shaped and cochlear-modulated stereo audio and the lateral placement of human ears, highlighting how sensory input quality and physical structure jointly shape precision of multimodal representations. All code, data, and models are available \\href{https://github.com/CuriseJia/SSHS}{here}. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "btm5Z5Vu8G@OpenReview",
      "index": 667,
      "title": "ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition",
      "authors": [
        "Daolang Huang",
        "Xinyi Wen",
        "Ayush Bharti",
        "Samuel Kaski",
        "Luigi Acerbi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "aline",
        "inference",
        "amortized",
        "bayesian",
        "active",
        "informative",
        "strategically",
        "instant",
        "amortization",
        "acquisition"
      ],
      "summary": "Many critical applications, from autonomous scientific discovery to personalized medicine, demand systems that can both strategically acquire the most informative data and instantaneously perform inference based upon it. While amortized methods for Bayesian inference and experimental design offer part of the solution, neither approach is optimal in the most general and challenging task, where new data needs to be collected for instant inference. To tackle this issue, we introduce the Amortized Active Learning and Inference Engine (ALINE), a unified framework for amortized Bayesian inference and active data acquisition. ALINE leverages a transformer architecture trained via reinforcement learning with a reward based on self-estimated information gain provided by its own integrated inference component. This allows it to strategically query informative data points while simultaneously refining its predictions. Moreover, ALINE can selectively direct its querying strategy towards specific subsets of model parameters or designated predictive tasks, optimizing for posterior estimation, data prediction, or a mixture thereof. Empirical results on regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model with selectively targeted parameters demonstrate that ALINE delivers both instant and accurate inference along with efficient selection of informative points.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=btm5Z5Vu8G"
        ],
        "venue": [
          "/venue/btm5Z5Vu8G@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=btm5Z5Vu8G"
        ],
        "detail": [
          "https://openreview.net/forum?id=btm5Z5Vu8G"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Daolang Huang , Xinyi Wen , Ayush Bharti , Samuel Kaski , Luigi Acerbi Many critical applications, from autonomous scientific discovery to personalized medicine, demand systems that can both strategically acquire the most informative data and instantaneously perform inference based upon it. While amortized methods for Bayesian inference and experimental design offer part of the solution, neither approach is optimal in the most general and challenging task, where new data needs to be collected for instant inference. To tackle this issue, we introduce the Amortized Active Learning and Inference Engine (ALINE), a unified framework for amortized Bayesian inference and active data acquisition. ALINE leverages a transformer architecture trained via reinforcement learning with a reward based on self-estimated information gain provided by its own integrated inference component. This allows it to strategically query informative data points while simultaneously refining its predictions. Moreover, ALINE can selectively direct its querying strategy towards specific subsets of model parameters or designated predictive tasks, optimizing for posterior estimation, data prediction, or a mixture thereof. Empirical results on regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model with selectively targeted parameters demonstrate that ALINE delivers both instant and accurate inference along with efficient selection of informative points. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "11fe8wKkmk@OpenReview",
      "index": 668,
      "title": "Fully Autonomous Neuromorphic Navigation and Dynamic Obstacle Avoidance",
      "authors": [
        "Xiaochen Shang",
        "Luo Pengwei",
        "Xinning Wang",
        "Jiayue Zhao",
        "Huilin Ge",
        "Bo Dong",
        "Xin Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "avoidance",
        "navigation",
        "obstacle",
        "neuromorphic",
        "unmanned",
        "onboard",
        "autonomous",
        "aerial",
        "vehicles",
        "latency"
      ],
      "summary": "Unmanned aerial vehicles could accurately accomplish complex navigation and obstacle avoidance tasks under external control. However, enabling unmanned aerial vehicles (UAVs) to rely solely on onboard computation and sensing for real-time navigation and dynamic obstacle avoidance remains a significant challenge due to stringent latency and energy constraints. Inspired by the efficiency of biological systems, we propose a fully neuromorphic framework achieving end-to-end obstacle avoidance during navigation with an overall latency of just 2.3 milliseconds. Specifically, our bio-inspired approach enables accurate moving object detection and avoidance without requiring target recognition or trajectory computation. Additionally, we introduce the first monocular event-based pose correction dataset with over 50,000 paired and labeled event streams. We validate our system on an autonomous quadrotor using only onboard resources, demonstrating reliable navigation and avoidance of diverse obstacles moving at speeds up to 10 m/s.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=11fe8wKkmk"
        ],
        "venue": [
          "/venue/11fe8wKkmk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=11fe8wKkmk"
        ],
        "detail": [
          "https://openreview.net/forum?id=11fe8wKkmk"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Fully Autonomous Neuromorphic Navigation and Dynamic Obstacle Avoidance [PDF ] [Copy] [Kimi ] [REL] Authors : Xiaochen Shang , Luo Pengwei , Xinning Wang , Jiayue Zhao , Huilin Ge , Bo Dong , Xin Yang Unmanned aerial vehicles could accurately accomplish complex navigation and obstacle avoidance tasks under external control. However, enabling unmanned aerial vehicles (UAVs) to rely solely on onboard computation and sensing for real-time navigation and dynamic obstacle avoidance remains a significant challenge due to stringent latency and energy constraints. Inspired by the efficiency of biological systems, we propose a fully neuromorphic framework achieving end-to-end obstacle avoidance during navigation with an overall latency of just 2.3 milliseconds. Specifically, our bio-inspired approach enables accurate moving object detection and avoidance without requiring target recognition or trajectory computation. Additionally, we introduce the first monocular event-based pose correction dataset with over 50,000 paired and labeled event streams. We validate our system on an autonomous quadrotor using only onboard resources, demonstrating reliable navigation and avoidance of diverse obstacles moving at speeds up to 10 m/s. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "RbdLnwEEjk@OpenReview",
      "index": 669,
      "title": "Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks",
      "authors": [
        "Huanming Shen",
        "Baizhou Huang",
        "Xiaojun Wan"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "scrubbing",
        "watermark",
        "spoofing",
        "attacks",
        "hearum",
        "seekwm",
        "resilience",
        "xture",
        "quivalent",
        "window"
      ],
      "summary": "Watermarking is a promising defense against the misuse of large language models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks. This vulnerability stems from an inherent trade-off governed by watermark window size: smaller windows resist scrubbing better but are easier to reverse-engineer, enabling low-cost statistics-based spoofing attacks. This work expands the trade-off boundary by introducing a novel mechanism, equivalent texture keys, where multiple tokens within a watermark window can independently support the detection. Based on the redundancy, we propose a watermark scheme with **S**ub-vocabulary decomposed **E**quivalent t**E**xture **K**ey (**SEEK**). It achieves a Pareto improvement, increasing the resilience against scrubbing attacks without compromising robustness to spoofing. Our code will be available at [https://github.com/Hearum/SeekWM](https://github.com/Hearum/SeekWM).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RbdLnwEEjk"
        ],
        "venue": [
          "/venue/RbdLnwEEjk@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RbdLnwEEjk"
        ],
        "detail": [
          "https://openreview.net/forum?id=RbdLnwEEjk"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Huanming Shen , Baizhou Huang , Xiaojun Wan Watermarking is a promising defense against the misuse of large language models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks. This vulnerability stems from an inherent trade-off governed by watermark window size: smaller windows resist scrubbing better but are easier to reverse-engineer, enabling low-cost statistics-based spoofing attacks. This work expands the trade-off boundary by introducing a novel mechanism, equivalent texture keys, where multiple tokens within a watermark window can independently support the detection. Based on the redundancy, we propose a watermark scheme with **S**ub-vocabulary decomposed **E**quivalent t**E**xture **K**ey (**SEEK**). It achieves a Pareto improvement, increasing the resilience against scrubbing attacks without compromising robustness to spoofing. Our code will be available at [https://github.com/Hearum/SeekWM](https://github.com/Hearum/SeekWM). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "lwOV2ACEK9@OpenReview",
      "index": 670,
      "title": "Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search",
      "authors": [
        "Huanjin Yao",
        "Jiaxing Huang",
        "Wenhao Wu",
        "Jingyi Zhang",
        "Yibo Wang",
        "Shunyu Liu",
        "Yingjie Wang",
        "YuXin Song",
        "Haocheng Feng",
        "Li Shen",
        "Dacheng Tao"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "mulberry",
        "reasoning",
        "comcts",
        "collective",
        "260k",
        "mllm",
        "tree",
        "search",
        "mllms",
        "reflection"
      ],
      "summary": "In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code is available at https://github.com/HJYao00/Mulberry.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lwOV2ACEK9"
        ],
        "venue": [
          "/venue/lwOV2ACEK9@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lwOV2ACEK9"
        ],
        "detail": [
          "https://openreview.net/forum?id=lwOV2ACEK9"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [PDF ] [Copy] [Kimi ] [REL] Authors : Huanjin Yao , Jiaxing Huang , Wenhao Wu , Jingyi Zhang , Yibo Wang , Shunyu Liu , Yingjie Wang , YuXin Song , Haocheng Feng , Li Shen , Dacheng Tao In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code is available at https://github.com/HJYao00/Mulberry. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "2uKVyGq5zK@OpenReview",
      "index": 671,
      "title": "ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search",
      "authors": [
        "Mengdi Liu",
        "Xiaoxue Cheng",
        "Zhangyang Gao",
        "Hong Chang",
        "Cheng Tan",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "protinvtree",
        "protein",
        "sequences",
        "folding",
        "deliberate",
        "inverse",
        "reward",
        "diverse",
        "proteininvbench",
        "search"
      ],
      "summary": "Designing protein sequences that fold into a target 3D structure—known as protein inverse folding—is a fundamental challenge in protein engineering. While recent deep learning methods have achieved impressive performance by recovering native sequences, they often overlook the one-to-many nature of the problem: multiple diverse sequences can fold into the same structure. This motivates the need for a generative model capable of designing diverse sequences while preserving structural consistency. To address this trade-off, we introduce ProtInvTree, the first reward-guided tree-search framework for protein inverse folding. ProtInvTree reformulates sequence generation as a deliberate, step-wise decision-making process, enabling the exploration of multiple design paths and exploitation of promising candidates through self-evaluation, lookahead, and backtracking. We propose a two-stage focus-and-grounding action mechanism that decouples position selection and residue generation. To efficiently evaluate intermediate states, we introduce a jumpy denoising strategy that avoids full rollouts. Built upon pretrained protein language models, ProtInvTree supports flexible test-time scaling by adjusting the search depth and breadth without retraining. Empirically, ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks, generating structurally consistent yet diverse sequences, including those far from the native ground truth. The code is available at https://github.com/A4Bio/ProteinInvBench/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2uKVyGq5zK"
        ],
        "venue": [
          "/venue/2uKVyGq5zK@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2uKVyGq5zK"
        ],
        "detail": [
          "https://openreview.net/forum?id=2uKVyGq5zK"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Mengdi Liu , Xiaoxue Cheng , Zhangyang Gao , Hong Chang , Cheng Tan , Shiguang Shan , Xilin Chen Designing protein sequences that fold into a target 3D structure—known as protein inverse folding—is a fundamental challenge in protein engineering. While recent deep learning methods have achieved impressive performance by recovering native sequences, they often overlook the one-to-many nature of the problem: multiple diverse sequences can fold into the same structure. This motivates the need for a generative model capable of designing diverse sequences while preserving structural consistency. To address this trade-off, we introduce ProtInvTree, the first reward-guided tree-search framework for protein inverse folding. ProtInvTree reformulates sequence generation as a deliberate, step-wise decision-making process, enabling the exploration of multiple design paths and exploitation of promising candidates through self-evaluation, lookahead, and backtracking. We propose a two-stage focus-and-grounding action mechanism that decouples position selection and residue generation. To efficiently evaluate intermediate states, we introduce a jumpy denoising strategy that avoids full rollouts. Built upon pretrained protein language models, ProtInvTree supports flexible test-time scaling by adjusting the search depth and breadth without retraining. Empirically, ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks, generating structurally consistent yet diverse sequences, including those far from the native ground truth. The code is available at https://github.com/A4Bio/ProteinInvBench/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "3KtPujOw5z@OpenReview",
      "index": 672,
      "title": "On the Value of Cross-Modal Misalignment in Multimodal Representation Learning",
      "authors": [
        "Yichao Cai",
        "Yuhang Liu",
        "Erdun Gao",
        "Tianjiao Jiang",
        "Zhen Zhang",
        "Anton van den Hengel",
        "Javen Qinfeng Shi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "misalignment",
        "multimodal",
        "mmcl",
        "modal",
        "cross",
        "text",
        "representation",
        "pairs",
        "representations",
        "semantic"
      ],
      "summary": "Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit cross-modal misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize cross-modal misalignment by introducing two specific mechanisms: Selection bias, where some semantic variables are absent in the text, and perturbation bias, where semantic variables are altered—both leading to misalignment in data pairs. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings via extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of cross-modal misalignment on multimodal representation learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3KtPujOw5z"
        ],
        "venue": [
          "/venue/3KtPujOw5z@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3KtPujOw5z"
        ],
        "detail": [
          "https://openreview.net/forum?id=3KtPujOw5z"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "On the Value of Cross-Modal Misalignment in Multimodal Representation Learning [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Yichao Cai , Yuhang Liu , Erdun Gao , Tianjiao Jiang , Zhen Zhang , Anton van den Hengel , Javen Qinfeng Shi Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit cross-modal misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize cross-modal misalignment by introducing two specific mechanisms: Selection bias, where some semantic variables are absent in the text, and perturbation bias, where semantic variables are altered—both leading to misalignment in data pairs. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings via extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of cross-modal misalignment on multimodal representation learning. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "nlQRra0OLH@OpenReview",
      "index": 673,
      "title": "A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking",
      "authors": [
        "Zixiang Zhao",
        "Haowen Bai",
        "Bingxin Ke",
        "Yukun Cui",
        "Lilun Deng",
        "Yulun Zhang",
        "Kai Zhang",
        "Konrad Schindler"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "fusion",
        "video",
        "bench",
        "unified",
        "univf",
        "vfbench",
        "multi",
        "frame",
        "temporal",
        "benchmarking"
      ],
      "summary": "The real world is dynamic, yet most image fusion methods process static frames independently, ignoring temporal correlations in videos and leading to flickering and temporal inconsistency. To address this, we propose Unified Video Fusion (UniVF), a novel and unified framework for video fusion that leverages multi-frame learning and optical flow-based feature warping for informative, temporally coherent video fusion. To support its development, we also introduce Video Fusion Benchmark (VF-Bench), the first comprehensive benchmark covering four video fusion tasks: multi-exposure, multi-focus, infrared-visible, and medical fusion. VF-Bench provides high-quality, well-aligned video pairs obtained through synthetic data generation and rigorous curation from existing datasets, with a unified evaluation protocol that jointly assesses the spatial quality and temporal consistency of video fusion. Extensive experiments show that UniVF achieves state-of-the-art results across all tasks on VF-Bench. Project page: [vfbench.github.io](https://vfbench.github.io).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nlQRra0OLH"
        ],
        "venue": [
          "/venue/nlQRra0OLH@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nlQRra0OLH"
        ],
        "detail": [
          "https://openreview.net/forum?id=nlQRra0OLH"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking [PDF ] [Copy] [Kimi ] [REL] Authors : Zixiang Zhao , Haowen Bai , Bingxin Ke , Yukun Cui , Lilun Deng , Yulun Zhang , Kai Zhang , Konrad Schindler The real world is dynamic, yet most image fusion methods process static frames independently, ignoring temporal correlations in videos and leading to flickering and temporal inconsistency. To address this, we propose Unified Video Fusion (UniVF), a novel and unified framework for video fusion that leverages multi-frame learning and optical flow-based feature warping for informative, temporally coherent video fusion. To support its development, we also introduce Video Fusion Benchmark (VF-Bench), the first comprehensive benchmark covering four video fusion tasks: multi-exposure, multi-focus, infrared-visible, and medical fusion. VF-Bench provides high-quality, well-aligned video pairs obtained through synthetic data generation and rigorous curation from existing datasets, with a unified evaluation protocol that jointly assesses the spatial quality and temporal consistency of video fusion. Extensive experiments show that UniVF achieves state-of-the-art results across all tasks on VF-Bench. Project page: [vfbench.github.io](https://vfbench.github.io). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "IupCqXiiOE@OpenReview",
      "index": 674,
      "title": "Multi-agent Markov Entanglement",
      "authors": [
        "Shuze Chen",
        "Tianyi Peng"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "decomposition",
        "entanglement",
        "markov",
        "agent",
        "value",
        "ldots",
        "entangled",
        "measure",
        "multi",
        "quantum"
      ],
      "summary": "Value decomposition has long been a fundamental technique in multi-agent reinforcement learning and dynamic programming. Specifically, the value function of a global state ( s 1 , s 2 , … , s N ) ( s 1 , s 2 , … , s N ) is often approximated as the sum of local functions: V ( s 1 , s 2 , … , s N ) ≈ ∑ N i = 1 V i ( s i ) V ( s 1 , s 2 , … , s N ) ≈ ∑ i = 1 N V i ( s i ) . This approach has found various applications in modern RL systems. However, the theoretical justification for why this decomposition works so effectively remains underexplored. In this paper, we uncover the underlying mathematical structure that enables value decomposition. We demonstrate that a Markov decision process (MDP) permits value decomposition *if and only if* its transition matrix is not \"entangled\"—a concept analogous to quantum entanglement in quantum physics. Drawing inspiration from how physicists measure quantum entanglement, we introduce how to measure the \"Markov entanglement\" and show that this measure can be used to bound the decomposition error in general multi-agent MDPs. Using the concept of Markov entanglement, we proved that a widely-used class of policies, the index policy, is weakly-entangled and enjoys a sublinear O ( N − − √ ) O ( N ) scale of decomposition error for N N -agent systems. Finally, we show Markov entanglement can be efficiently estimated, guiding practitioners on the feasibility of value decomposition.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IupCqXiiOE"
        ],
        "venue": [
          "/venue/IupCqXiiOE@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IupCqXiiOE"
        ],
        "detail": [
          "https://openreview.net/forum?id=IupCqXiiOE"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Multi-agent Markov Entanglement [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Shuze Chen , Tianyi Peng Value decomposition has long been a fundamental technique in multi-agent reinforcement learning and dynamic programming. Specifically, the value function of a global state ( s 1 , s 2 , … , s N ) ( s 1 , s 2 , … , s N ) is often approximated as the sum of local functions: V ( s 1 , s 2 , … , s N ) ≈ ∑ N i = 1 V i ( s i ) V ( s 1 , s 2 , … , s N ) ≈ ∑ i = 1 N V i ( s i ) . This approach has found various applications in modern RL systems. However, the theoretical justification for why this decomposition works so effectively remains underexplored. In this paper, we uncover the underlying mathematical structure that enables value decomposition. We demonstrate that a Markov decision process (MDP) permits value decomposition *if and only if* its transition matrix is not \"entangled\"—a concept analogous to quantum entanglement in quantum physics. Drawing inspiration from how physicists measure quantum entanglement, we introduce how to measure the \"Markov entanglement\" and show that this measure can be used to bound the decomposition error in general multi-agent MDPs. Using the concept of Markov entanglement, we proved that a widely-used class of policies, the index policy, is weakly-entangled and enjoys a sublinear O ( N − − √ ) O ( N ) scale of decomposition error for N N -agent systems. Finally, we show Markov entanglement can be efficiently estimated, guiding practitioners on the feasibility of value decomposition. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "plpAecfkf4@OpenReview",
      "index": 675,
      "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving",
      "authors": [
        "Haiming Zhang",
        "Yiyao Zhu",
        "Wending Zhou",
        "Xu Yan",
        "Yingjie CAI",
        "Bingbing Liu",
        "Shuguang Cui",
        "Zhen Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "sqs",
        "splatting",
        "query",
        "spms",
        "queries",
        "perception",
        "occupancy",
        "pre",
        "sparse",
        "autonomous"
      ],
      "summary": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes explicit dense BEV or volumetric construction, enabling highly efficient computation and accelerated inference. In this paper, we introduce SQS, a novel query-based splatting pre-training specifically designed to advance SPMs in autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian representations from sparse queries during pre-training, leveraging self-supervised splatting to learn fine-grained contextual features through the reconstruction of multi-view images and depth maps. During fine-tuning, the pre-trained Gaussian queries are seamlessly integrated into downstream networks via query interaction mechanisms that explicitly connect pre-trained queries with task-specific queries, effectively accommodating the diverse requirements of occupancy prediction and 3D object detection. Extensive experiments on autonomous driving benchmarks demonstrate that SQS delivers considerable performance gains across multiple query-based 3D perception tasks, notably in occupancy prediction and 3D object detection, outperforming prior state-of-the-art pre-training approaches by a significant margin (i.e., +1.3 mIoU on occupancy prediction and +1.0 NDS on 3D detection).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=plpAecfkf4"
        ],
        "venue": [
          "/venue/plpAecfkf4@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=plpAecfkf4"
        ],
        "detail": [
          "https://openreview.net/forum?id=plpAecfkf4"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Haiming Zhang , Yiyao Zhu , Wending Zhou , Xu Yan , Yingjie CAI , Bingbing Liu , Shuguang Cui , Zhen Li Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes explicit dense BEV or volumetric construction, enabling highly efficient computation and accelerated inference. In this paper, we introduce SQS, a novel query-based splatting pre-training specifically designed to advance SPMs in autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian representations from sparse queries during pre-training, leveraging self-supervised splatting to learn fine-grained contextual features through the reconstruction of multi-view images and depth maps. During fine-tuning, the pre-trained Gaussian queries are seamlessly integrated into downstream networks via query interaction mechanisms that explicitly connect pre-trained queries with task-specific queries, effectively accommodating the diverse requirements of occupancy prediction and 3D object detection. Extensive experiments on autonomous driving benchmarks demonstrate that SQS delivers considerable performance gains across multiple query-based 3D perception tasks, notably in occupancy prediction and 3D object detection, outperforming prior state-of-the-art pre-training approaches by a significant margin (i.e., +1.3 mIoU on occupancy prediction and +1.0 NDS on 3D detection). Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "5NkfjxMpWe@OpenReview",
      "index": 676,
      "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding",
      "authors": [
        "Jang Hyun Cho",
        "Andrea Madotto",
        "Effrosyni Mavroudi",
        "Triantafyllos Afouras",
        "Tushar Nagarajan",
        "Muhammad Maaz",
        "Yale Song",
        "Tengyu Ma",
        "Shuming Hu",
        "Suyog Jain",
        "Miguel Martin",
        "Huiyu Wang",
        "Hanoona Abdul Rasheed",
        "Peize Sun",
        "Po-Yao Huang",
        "Daniel Bolya",
        "Nikhila Ravi",
        "Shashank Jain",
        "Tammy Stark",
        "Seungwhan Moon",
        "Babak Damavandi",
        "Vivian Lee",
        "Andrew Westbury",
        "Salman Khan",
        "Philipp Kraehenbuehl",
        "Piotr Dollar",
        "Lorenzo Torresani",
        "Kristen Grauman",
        "Christoph Feichtenhofer"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "video",
        "understanding",
        "plm",
        "perceptionlm",
        "data",
        "reproducible",
        "models",
        "research",
        "training",
        "distillation"
      ],
      "summary": "Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM–VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about ''what'', ''where'', ''when'', and ''how'' of a video. We make our work fully reproducible by providing data, training recipes, code & models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5NkfjxMpWe"
        ],
        "venue": [
          "/venue/5NkfjxMpWe@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5NkfjxMpWe"
        ],
        "detail": [
          "https://openreview.net/forum?id=5NkfjxMpWe"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Jang Hyun Cho , Andrea Madotto , Effrosyni Mavroudi , Triantafyllos Afouras , Tushar Nagarajan , Muhammad Maaz , Yale Song , Tengyu Ma , Shuming Hu , Suyog Jain , Miguel Martin , Huiyu Wang , Hanoona Abdul Rasheed , Peize Sun , Po-Yao Huang , Daniel Bolya , Nikhila Ravi , Shashank Jain , Tammy Stark , Seungwhan Moon , Babak Damavandi , Vivian Lee , Andrew Westbury , Salman Khan , Philipp Kraehenbuehl , Piotr Dollar , Lorenzo Torresani , Kristen Grauman , Christoph Feichtenhofer Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM–VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about ''what'', ''where'', ''when'', and ''how'' of a video. We make our work fully reproducible by providing data, training recipes, code & models. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "iUjGNJzrF1@OpenReview",
      "index": 677,
      "title": "Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?",
      "authors": [
        "Hyeong Kyu Choi",
        "Jerry Zhu",
        "Sharon Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "debate",
        "mad",
        "vote",
        "agent",
        "voting",
        "majority",
        "alone",
        "belief",
        "deeplearning",
        "wisc"
      ],
      "summary": "Multi-Agent Debate (MAD) has emerged as a promising paradigm for improving the performance of large language models through collaborative reasoning. Despite recent advances, the key factors driving MAD’s effectiveness remain unclear. In this work, we disentangle MAD into two key components–Majority Voting and inter-agent Debate–and assess their respective contributions. Through extensive experiments across seven NLP benchmarks, we find that Majority Voting alone accounts for most of the performance gains typically attributed to MAD. To explain this, we propose a theoretical framework that models debate as a stochastic process. We prove that it induces a martingale over agents’ belief trajectories, implying that debate alone does not improve expected correctness. Guided by these insights, we demonstrate that targeted interventions, by biasing the belief update toward correction, can meaningfully enhance debate effectiveness. Overall, our findings suggest that while MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings. Code is released in https://github.com/deeplearning-wisc/debate-or-vote.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=iUjGNJzrF1"
        ],
        "venue": [
          "/venue/iUjGNJzrF1@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=iUjGNJzrF1"
        ],
        "detail": [
          "https://openreview.net/forum?id=iUjGNJzrF1"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models? [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Hyeong Kyu Choi , Jerry Zhu , Sharon Li Multi-Agent Debate (MAD) has emerged as a promising paradigm for improving the performance of large language models through collaborative reasoning. Despite recent advances, the key factors driving MAD’s effectiveness remain unclear. In this work, we disentangle MAD into two key components–Majority Voting and inter-agent Debate–and assess their respective contributions. Through extensive experiments across seven NLP benchmarks, we find that Majority Voting alone accounts for most of the performance gains typically attributed to MAD. To explain this, we propose a theoretical framework that models debate as a stochastic process. We prove that it induces a martingale over agents’ belief trajectories, implying that debate alone does not improve expected correctness. Guided by these insights, we demonstrate that targeted interventions, by biasing the belief update toward correction, can meaningfully enhance debate effectiveness. Overall, our findings suggest that while MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings. Code is released in https://github.com/deeplearning-wisc/debate-or-vote. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "lMhNrt0Bnm@OpenReview",
      "index": 678,
      "title": "VoxDet: Rethinking 3D Semantic Scene Completion as Dense Object Detection",
      "authors": [
        "Wuyang Li",
        "Zhu Yu",
        "Alexandre Alahi"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "voxdet",
        "voxel",
        "ssc",
        "instance",
        "dense",
        "completion",
        "level",
        "offset",
        "labels",
        "scene"
      ],
      "summary": "Semantic Scene Completion (SSC) aims to reconstruct the 3D geometry and semantics of the surrounding environment. With dense voxel labels, prior works typically formulate SSC as a *dense segmentation task*, independently classifying each voxel. However, this paradigm neglects critical instance-centric discriminability, leading to instance-level incompleteness and adjacent ambiguities. To address this, we highlight a \"free lunch\" of SSC labels: the voxel-level class label has implicitly told the instance-level insight, which is ever-overlooked by the community. Motivated by this observation, we first introduce a training-free **Voxel-to-Instance (VoxNT) trick**: a simple yet effective method that freely converts voxel-level class labels into instance-level offset labels. Building on this, we further propose **VoxDet**, an instance-centric framework that reformulates the voxel-level SSC as *dense object detection* by decoupling it into two sub-tasks: offset regression and semantic prediction. Specifically, based on the lifted 3D volume, VoxDet first uses (a) Spatially-decoupled Voxel Encoder to generate disentangled feature volumes for the two sub-tasks, which learn task-specific spatial deformation in the densely projected tri-perceptive space. Then, we deploy (b) Task-decoupled Dense Predictor to address SSC via dense detection. Here, we first regress a 4D offset field to estimate distances (6 directions) between voxels and the corresponding object boundaries in the voxel space. The regressed offsets are then used to guide the instance-level aggregation in the classification branch, achieving instance-aware scene completion. VoxDet can be deployed on both camera and LiDAR input and jointly achieves state-of-the-art results on both benchmarks, which gives 63.0 IoU on the SemanticKITTI test set, **ranking 1 s t s t ** on the online leaderboard.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lMhNrt0Bnm"
        ],
        "venue": [
          "/venue/lMhNrt0Bnm@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lMhNrt0Bnm"
        ],
        "detail": [
          "https://openreview.net/forum?id=lMhNrt0Bnm"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "VoxDet: Rethinking 3D Semantic Scene Completion as Dense Object Detection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Wuyang Li , Zhu Yu , Alexandre Alahi Semantic Scene Completion (SSC) aims to reconstruct the 3D geometry and semantics of the surrounding environment. With dense voxel labels, prior works typically formulate SSC as a *dense segmentation task*, independently classifying each voxel. However, this paradigm neglects critical instance-centric discriminability, leading to instance-level incompleteness and adjacent ambiguities. To address this, we highlight a \"free lunch\" of SSC labels: the voxel-level class label has implicitly told the instance-level insight, which is ever-overlooked by the community. Motivated by this observation, we first introduce a training-free **Voxel-to-Instance (VoxNT) trick**: a simple yet effective method that freely converts voxel-level class labels into instance-level offset labels. Building on this, we further propose **VoxDet**, an instance-centric framework that reformulates the voxel-level SSC as *dense object detection* by decoupling it into two sub-tasks: offset regression and semantic prediction. Specifically, based on the lifted 3D volume, VoxDet first uses (a) Spatially-decoupled Voxel Encoder to generate disentangled feature volumes for the two sub-tasks, which learn task-specific spatial deformation in the densely projected tri-perceptive space. Then, we deploy (b) Task-decoupled Dense Predictor to address SSC via dense detection. Here, we first regress a 4D offset field to estimate distances (6 directions) between voxels and the corresponding object boundaries in the voxel space. The regressed offsets are then used to guide the instance-level aggregation in the classification branch, achieving instance-aware scene completion. VoxDet can be deployed on both camera and LiDAR input and jointly achieves state-of-the-art results on both benchmarks, which gives 63.0 IoU on the SemanticKITTI test set, **ranking 1 s t s t ** on the online leaderboard. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "lv4zLWzOi2@OpenReview",
      "index": 679,
      "title": "Approximate Domain Unlearning for Vision-Language Models",
      "authors": [
        "Kodai Kawamura",
        "Yuta Goto",
        "Rintaro Yanagi",
        "Hirokatsu Kataoka",
        "Go Irie"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "unlearning",
        "domain",
        "vlms",
        "adu",
        "approximate",
        "cars",
        "recognize",
        "vlm",
        "domains",
        "vision"
      ],
      "summary": "Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific target downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on {\\em class unlearning}, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize {\\em real} cars, while avoiding misrecognition of {\\em illustrated} cars depicted in roadside advertisements as {\\em real} cars, which could be hazardous. In this paper, we introduce {\\em Approximate Domain Unlearning (ADU)}, a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., {\\em illustration}) while preserving accuracy for other domains (e.g., {\\em real}). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments on four multi-domain benchmark datasets demonstrate that our approach significantly outperforms strong baselines built upon state-of-the-art VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code : https://kodaikawamura.github.io/Domain_Unlearning/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lv4zLWzOi2"
        ],
        "venue": [
          "/venue/lv4zLWzOi2@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lv4zLWzOi2"
        ],
        "detail": [
          "https://openreview.net/forum?id=lv4zLWzOi2"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Approximate Domain Unlearning for Vision-Language Models [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Kodai Kawamura , Yuta Goto , Rintaro Yanagi , Hirokatsu Kataoka , Go Irie Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific target downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on {\\em class unlearning}, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize {\\em real} cars, while avoiding misrecognition of {\\em illustrated} cars depicted in roadside advertisements as {\\em real} cars, which could be hazardous. In this paper, we introduce {\\em Approximate Domain Unlearning (ADU)}, a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., {\\em illustration}) while preserving accuracy for other domains (e.g., {\\em real}). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments on four multi-domain benchmark datasets demonstrate that our approach significantly outperforms strong baselines built upon state-of-the-art VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code : https://kodaikawamura.github.io/Domain_Unlearning/. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "yPC9zmkQgG@OpenReview",
      "index": 680,
      "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning",
      "authors": [
        "Jianyang Gu",
        "Samuel Stevens",
        "Elizabeth G Campolongo",
        "Matthew J Thompson",
        "Net Zhang",
        "Jiaman Wu",
        "Andrei Kopanev",
        "Zheda Mai",
        "Alexander E. White",
        "James Balhoff",
        "Wasla Dahdul",
        "Daniel Rubenstein",
        "Hilmar Lapp",
        "Tanya Berger-Wolf",
        "Wei-Lun Chao",
        "Yu Su"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "bioclip",
        "emergent",
        "treeoflife",
        "species",
        "contrastive",
        "200m",
        "biological",
        "embedding",
        "properties",
        "hierarchical"
      ],
      "summary": "Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yPC9zmkQgG"
        ],
        "venue": [
          "/venue/yPC9zmkQgG@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yPC9zmkQgG"
        ],
        "detail": [
          "https://openreview.net/forum?id=yPC9zmkQgG"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Jianyang Gu , Samuel Stevens , Elizabeth G Campolongo , Matthew J Thompson , Net Zhang , Jiaman Wu , Andrei Kopanev , Zheda Mai , Alexander E. White , James Balhoff , Wasla Dahdul , Daniel Rubenstein , Hilmar Lapp , Tanya Berger-Wolf , Wei-Lun Chao , Yu Su Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "h3dbocj7po@OpenReview",
      "index": 681,
      "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
      "authors": [
        "Enjun Du",
        "Xunkai Li",
        "Tian Jin",
        "Zhihan Zhang",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "graphmaster",
        "synthesis",
        "graph",
        "gfms",
        "foundation",
        "environments",
        "llm",
        "limited",
        "data",
        "agents"
      ],
      "summary": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes—a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce \\textbf{GraphMaster}—the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited “Sub” variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=h3dbocj7po"
        ],
        "venue": [
          "/venue/h3dbocj7po@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=h3dbocj7po"
        ],
        "detail": [
          "https://openreview.net/forum?id=h3dbocj7po"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Enjun Du , Xunkai Li , Tian Jin , Zhihan Zhang , Rong-Hua Li , Guoren Wang The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes—a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce \\textbf{GraphMaster}—the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited “Sub” variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "Bds54EfR9x@OpenReview",
      "index": 682,
      "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning",
      "authors": [
        "Weiqi Li",
        "Xuanyu Zhang",
        "Shijie Zhao",
        "Yabin ZHANG",
        "Junlin Li",
        "Li zhang",
        "Jian Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "degradation",
        "insight",
        "quality",
        "iqa",
        "perception",
        "image",
        "reasoning",
        "grpo",
        "understanding",
        "visual"
      ],
      "summary": "Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods on both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization and superior comparison reasoning capability. The code and models are available at https://github.com/bytedance/Q-Insight.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Bds54EfR9x"
        ],
        "venue": [
          "/venue/Bds54EfR9x@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Bds54EfR9x"
        ],
        "detail": [
          "https://openreview.net/forum?id=Bds54EfR9x"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Weiqi Li , Xuanyu Zhang , Shijie Zhao , Yabin ZHANG , Junlin Li , Li zhang , Jian Zhang Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods on both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization and superior comparison reasoning capability. The code and models are available at https://github.com/bytedance/Q-Insight. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "7pufO0SJAC@OpenReview",
      "index": 683,
      "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
      "authors": [
        "Noam Razin",
        "Zixuan Wang",
        "Hubert Strauss",
        "Stanley Wei",
        "Jason D. Lee",
        "Sanjeev Arora"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "variance",
        "accuracy",
        "model",
        "optimization",
        "rlhf",
        "induce",
        "teacher",
        "makes",
        "perspective"
      ],
      "summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically depends on the quality of the reward model. However, while this quality is primarily evaluated through accuracy, it remains unclear whether accuracy fully captures what makes a reward model an effective teacher. We address this question from an optimization perspective. First, we prove that regardless of how accurate a reward model is, if it induces low reward variance, then the RLHF objective suffers from a flat landscape. Consequently, even a perfectly accurate reward model can lead to extremely slow optimization, underperforming less accurate models that induce higher reward variance. We additionally show that a reward model that works well for one language model can induce low reward variance, and thus a flat objective landscape, for another. These results establish a fundamental limitation of evaluating reward models solely based on accuracy or independently of the language model they guide. Experiments using models of up to 8B parameters corroborate our theory, demonstrating the interplay between reward variance, accuracy, and reward maximization rate. Overall, our findings highlight that beyond accuracy, a reward model needs to induce sufficient variance for efficient optimization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7pufO0SJAC"
        ],
        "venue": [
          "/venue/7pufO0SJAC@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7pufO0SJAC"
        ],
        "detail": [
          "https://openreview.net/forum?id=7pufO0SJAC"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "What Makes a Reward Model a Good Teacher? An Optimization Perspective [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Noam Razin , Zixuan Wang , Hubert Strauss , Stanley Wei , Jason D. Lee , Sanjeev Arora The success of Reinforcement Learning from Human Feedback (RLHF) critically depends on the quality of the reward model. However, while this quality is primarily evaluated through accuracy, it remains unclear whether accuracy fully captures what makes a reward model an effective teacher. We address this question from an optimization perspective. First, we prove that regardless of how accurate a reward model is, if it induces low reward variance, then the RLHF objective suffers from a flat landscape. Consequently, even a perfectly accurate reward model can lead to extremely slow optimization, underperforming less accurate models that induce higher reward variance. We additionally show that a reward model that works well for one language model can induce low reward variance, and thus a flat objective landscape, for another. These results establish a fundamental limitation of evaluating reward models solely based on accuracy or independently of the language model they guide. Experiments using models of up to 8B parameters corroborate our theory, demonstrating the interplay between reward variance, accuracy, and reward maximization rate. Overall, our findings highlight that beyond accuracy, a reward model needs to induce sufficient variance for efficient optimization. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "slVqJAI5sT@OpenReview",
      "index": 684,
      "title": "Ψ Ψ -Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models",
      "authors": [
        "TaeHoon Yoon",
        "Yunhong Min",
        "Kyeongmin Yeo",
        "Minhyuk Sung"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "smc",
        "pcnl",
        "alignment",
        "sampling",
        "posterior",
        "sampler",
        "score",
        "psi",
        "inference"
      ],
      "summary": "We introduce Ψ Ψ -Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank–Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=slVqJAI5sT"
        ],
        "venue": [
          "/venue/slVqJAI5sT@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=slVqJAI5sT"
        ],
        "detail": [
          "https://openreview.net/forum?id=slVqJAI5sT"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Ψ Ψ -Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models [PDF ] [Copy] [Kimi ] [REL] Authors : TaeHoon Yoon , Yunhong Min , Kyeongmin Yeo , Minhyuk Sung We introduce Ψ Ψ -Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank–Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "OaNbl9b56B@OpenReview",
      "index": 685,
      "title": "Do-PFN: In-Context Learning for Causal Effect Estimation",
      "authors": [
        "Jake Robertson",
        "Arik Reuter",
        "Siyuan Guo",
        "Noah Hollmann",
        "Frank Hutter",
        "Bernhard Schölkopf"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "causal",
        "pfns",
        "synthetic",
        "estimation",
        "interventional",
        "pfn",
        "tabular",
        "unconfoundedness",
        "effect",
        "context"
      ],
      "summary": "Causal effect estimation is critical to a range of scientific disciplines. Existing methods for this task either require interventional data, knowledge about the ground-truth causal graph, or rely on assumptions such as unconfoundedness, restricting their applicability in real-world settings. In the domain of tabular machine learning, Prior-data fitted networks (PFNs) have achieved state-of-the-art predictive performance, having been pre-trained on synthetic causal data to solve tabular prediction problems via in-context learning. To assess whether this can be transferred to the problem of causal effect estimation, we pre-train PFNs on synthetic data drawn from a wide variety of causal structures, including interventions, to predict interventional outcomes given observational data. Through extensive experiments in synthetic and semi-synthetic settings, we show that our approach allows for the accurate estimation of causal effects without knowledge of the underlying causal graph.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OaNbl9b56B"
        ],
        "venue": [
          "/venue/OaNbl9b56B@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OaNbl9b56B"
        ],
        "detail": [
          "https://openreview.net/forum?id=OaNbl9b56B"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Do-PFN: In-Context Learning for Causal Effect Estimation [PDF ] [Copy] [Kimi ] [REL] Authors : Jake Robertson , Arik Reuter , Siyuan Guo , Noah Hollmann , Frank Hutter , Bernhard Schölkopf Causal effect estimation is critical to a range of scientific disciplines. Existing methods for this task either require interventional data, knowledge about the ground-truth causal graph, or rely on assumptions such as unconfoundedness, restricting their applicability in real-world settings. In the domain of tabular machine learning, Prior-data fitted networks (PFNs) have achieved state-of-the-art predictive performance, having been pre-trained on synthetic causal data to solve tabular prediction problems via in-context learning. To assess whether this can be transferred to the problem of causal effect estimation, we pre-train PFNs on synthetic data drawn from a wide variety of causal structures, including interventions, to predict interventional outcomes given observational data. Through extensive experiments in synthetic and semi-synthetic settings, we show that our approach allows for the accurate estimation of causal effects without knowledge of the underlying causal graph. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "T0CiI4gDFB@OpenReview",
      "index": 686,
      "title": "ReSim: Reliable World Simulation for Autonomous Driving",
      "authors": [
        "Jiazhi Yang",
        "Kashyap Chitta",
        "Shenyuan Gao",
        "Long Chen",
        "Yuqian Shao",
        "Xiaosong Jia",
        "Hongyang Li",
        "Andreas Geiger",
        "Xiangyu Yue",
        "Li Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "resim",
        "driving",
        "expert",
        "world",
        "hazardous",
        "fidelity",
        "actions",
        "simulation",
        "controllability",
        "navsim"
      ],
      "summary": "How can we reliably simulate future driving scenarios under a wide range of ego driving behaviors? Recent driving world models, developed exclusively on real-world driving data composed mainly of safe expert trajectories, struggle to follow hazardous or non-expert behaviors, which are rare in such data. This limitation restricts their applicability to tasks such as policy evaluation. In this work, we address this challenge by enriching real-world human demonstrations with diverse non-expert data collected from a driving simulator (e.g., CARLA), and building a controllable world model trained on this heterogeneous corpus. Starting with a video generator featuring diffusion transformer architecture, we devise several strategies to effectively integrate conditioning signals and improve prediction controllability and fidelity. The resulting model, ReSim, enables Reliable Simulation of diverse open-world driving scenarios under various actions, including hazardous non-expert ones. To close the gap between high-fidelity simulation and applications that require reward signals to judge different actions, we introduce a Video2Reward module that estimates reward from ReSim’s simulated future. Our ReSim paradigm achieves up to 44% higher visual fidelity, improves controllability for both expert and non-expert actions by over 50%, and boosts planning and policy selection performance on NAVSIM by 2% and 25%, respectively.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=T0CiI4gDFB"
        ],
        "venue": [
          "/venue/T0CiI4gDFB@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=T0CiI4gDFB"
        ],
        "detail": [
          "https://openreview.net/forum?id=T0CiI4gDFB"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "ReSim: Reliable World Simulation for Autonomous Driving [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jiazhi Yang , Kashyap Chitta , Shenyuan Gao , Long Chen , Yuqian Shao , Xiaosong Jia , Hongyang Li , Andreas Geiger , Xiangyu Yue , Li Chen How can we reliably simulate future driving scenarios under a wide range of ego driving behaviors? Recent driving world models, developed exclusively on real-world driving data composed mainly of safe expert trajectories, struggle to follow hazardous or non-expert behaviors, which are rare in such data. This limitation restricts their applicability to tasks such as policy evaluation. In this work, we address this challenge by enriching real-world human demonstrations with diverse non-expert data collected from a driving simulator (e.g., CARLA), and building a controllable world model trained on this heterogeneous corpus. Starting with a video generator featuring diffusion transformer architecture, we devise several strategies to effectively integrate conditioning signals and improve prediction controllability and fidelity. The resulting model, ReSim, enables Reliable Simulation of diverse open-world driving scenarios under various actions, including hazardous non-expert ones. To close the gap between high-fidelity simulation and applications that require reward signals to judge different actions, we introduce a Video2Reward module that estimates reward from ReSim’s simulated future. Our ReSim paradigm achieves up to 44% higher visual fidelity, improves controllability for both expert and non-expert actions by over 50%, and boosts planning and policy selection performance on NAVSIM by 2% and 25%, respectively. Subject : NeurIPS.2025 - Spotlight"
    },
    {
      "paper_id": "8OGTkEJrmb@OpenReview",
      "index": 687,
      "title": "Differentiable Sparsity via D D -Gating: Simple and Versatile Structured Penalization",
      "authors": [
        "Chris Kolb",
        "Laetitia Frost",
        "Bernd Bischl",
        "David Rügamer"
      ],
      "subjects": [
        "NeurIPS.2025 - Spotlight"
      ],
      "keywords": [
        "gating",
        "structured",
        "sparsity",
        "penalization",
        "differentiable",
        "pruning",
        "overparameterization",
        "minimum",
        "sparse",
        "versatile"
      ],
      "summary": "Structured sparsity regularization offers a principled way to compact neural networks, but its non-differentiability breaks compatibility with conventional stochastic gradient descent and requires either specialized optimizers or additional post-hoc pruning without formal guarantees. In this work, we propose D D -Gating, a fully differentiable structured overparameterization that splits each group of weights into a primary weight vector and multiple scalar gating factors. We prove that any local minimum under D D -Gating is also a local minimum using non-smooth structured L 2 , 2 / D L 2 , 2 / D penalization, and further show that the D D -Gating objective converges at least exponentially fast to the L 2 , 2 / D L 2 , 2 / D –regularized loss in the gradient flow limit. Together, our results show that D D -Gating is theoretically equivalent to solving the original group sparsity problem, yet induces distinct learning dynamics that evolve from a non-sparse regime into sparse optimization. We validate our theory across vision, language, and tabular tasks, where D D -Gating consistently delivers strong performance–sparsity tradeoffs and outperforms both direct optimization of structured penalties and conventional pruning baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8OGTkEJrmb"
        ],
        "venue": [
          "/venue/8OGTkEJrmb@OpenReview",
          "/venue/NeurIPS.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8OGTkEJrmb"
        ],
        "detail": [
          "https://openreview.net/forum?id=8OGTkEJrmb"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Differentiable Sparsity via D D -Gating: Simple and Versatile Structured Penalization [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Chris Kolb , Laetitia Frost , Bernd Bischl , David Rügamer Structured sparsity regularization offers a principled way to compact neural networks, but its non-differentiability breaks compatibility with conventional stochastic gradient descent and requires either specialized optimizers or additional post-hoc pruning without formal guarantees. In this work, we propose D D -Gating, a fully differentiable structured overparameterization that splits each group of weights into a primary weight vector and multiple scalar gating factors. We prove that any local minimum under D D -Gating is also a local minimum using non-smooth structured L 2 , 2 / D L 2 , 2 / D penalization, and further show that the D D -Gating objective converges at least exponentially fast to the L 2 , 2 / D L 2 , 2 / D –regularized loss in the gradient flow limit. Together, our results show that D D -Gating is theoretically equivalent to solving the original group sparsity problem, yet induces distinct learning dynamics that evolve from a non-sparse regime into sparse optimization. We validate our theory across vision, language, and tabular tasks, where D D -Gating consistently delivers strong performance–sparsity tradeoffs and outperforms both direct optimization of structured penalties and conventional pruning baselines. Subject : NeurIPS.2025 - Spotlight"
    }
  ]
}