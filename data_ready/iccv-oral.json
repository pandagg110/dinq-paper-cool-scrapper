{
  "source_html": "html\\iccv-oral.html",
  "paper_count": 64,
  "conference": "iccv",
  "year": 2025,
  "status": "oral",
  "papers": [
    {
      "paper_id": "Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF",
      "index": 1,
      "title": "Token Activation Map to Visually Explain Multimodal LLMs",
      "authors": [
        "Yi Li",
        "Hualiang Wang",
        "Xinpeng Ding",
        "Haonan Wang",
        "Xiaomeng Li"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "tam",
        "mllms",
        "tokens",
        "activation",
        "token",
        "visualization",
        "mllm",
        "multimodal",
        "map",
        "redundant"
      ],
      "summary": "Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available at github.com/xmed-lab/TAM.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 72,
        "kimi": 66
      },
      "raw_excerpt": "Token Activation Map to Visually Explain Multimodal LLMs [PDF 72 ] [Copy] [Kimi 66 ] [REL] Authors : Yi Li , Hualiang Wang , Xinpeng Ding , Haonan Wang , Xiaomeng Li Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available at github.com/xmed-lab/TAM. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF",
      "index": 2,
      "title": "NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping",
      "authors": [
        "Tianyi Wang",
        "Shuaicheng Niu",
        "Harry Cheng",
        "Xiao Zhang",
        "Yinglong Wang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "swapping",
        "identity",
        "proactive",
        "deepfake",
        "face",
        "nullswap",
        "cloaking",
        "identities",
        "images",
        "source"
      ],
      "summary": "Suffering from performance bottlenecks in passively detecting high-quality Deepfake images due to the advancement of generative models, proactive perturbations offer a promising approach to disabling Deepfake manipulations by inserting signals into benign images. However, existing proactive perturbation approaches remain unsatisfactory in several aspects: 1) visual degradation due to direct element-wise addition; 2) limited effectiveness against face swapping manipulation; 3) unavoidable reliance on white- and grey-box settings to involve generative models during training. In this study, we analyze the essence of Deepfake face swapping and argue the necessity of protecting source identities rather than target images, and we propose NullSwap, a novel proactive defense approach that cloaks source image identities and nullifies face swapping under a pure black-box scenario. We design an Identity Extraction module to obtain facial identity features from the source image, while a Perturbation Block is then devised to generate identity-guided perturbations accordingly. Meanwhile, a Feature Block extracts shallow-level image features, which are then fused with the perturbation in the Cloaking Block for image reconstruction. Furthermore, to ensure adaptability across different identity extractors in face swapping algorithms, we propose Dynamic Loss Weighting to adaptively balance identity losses. Experiments demonstrate the outstanding ability of our approach to fool various identity recognition models, outperforming state-of-the-art proactive perturbations in preventing face swapping models from generating images with correct source identities.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 14
      },
      "raw_excerpt": "NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping [PDF 18 ] [Copy] [Kimi 14 ] [REL] Authors : Tianyi Wang , Shuaicheng Niu , Harry Cheng , Xiao Zhang , Yinglong Wang Suffering from performance bottlenecks in passively detecting high-quality Deepfake images due to the advancement of generative models, proactive perturbations offer a promising approach to disabling Deepfake manipulations by inserting signals into benign images. However, existing proactive perturbation approaches remain unsatisfactory in several aspects: 1) visual degradation due to direct element-wise addition; 2) limited effectiveness against face swapping manipulation; 3) unavoidable reliance on white- and grey-box settings to involve generative models during training. In this study, we analyze the essence of Deepfake face swapping and argue the necessity of protecting source identities rather than target images, and we propose NullSwap, a novel proactive defense approach that cloaks source image identities and nullifies face swapping under a pure black-box scenario. We design an Identity Extraction module to obtain facial identity features from the source image, while a Perturbation Block is then devised to generate identity-guided perturbations accordingly. Meanwhile, a Feature Block extracts shallow-level image features, which are then fused with the perturbation in the Cloaking Block for image reconstruction. Furthermore, to ensure adaptability across different identity extractors in face swapping algorithms, we propose Dynamic Loss Weighting to adaptively balance identity losses. Experiments demonstrate the outstanding ability of our approach to fool various identity recognition models, outperforming state-of-the-art proactive perturbations in preventing face swapping models from generating images with correct source identities. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF",
      "index": 3,
      "title": "CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation",
      "authors": [
        "Dengke Zhang",
        "Fagui Liu",
        "Quan Tang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "corrclip",
        "patch",
        "correlations",
        "segmentation",
        "clip",
        "sam",
        "semantic",
        "vocabulary",
        "inter",
        "masks"
      ],
      "summary": "Open-vocabulary semantic segmentation aims to assign semantic labels to each pixel without being constrained by a predefined set of categories. While Contrastive Language-Image Pre-training (CLIP) excels in zero-shot classification, it struggles to align image patches with category embeddings because of its incoherent patch correlations. This study reveals that inter-class correlations are the main reason for impairing CLIP's segmentation performance. Accordingly, we propose CorrCLIP, which reconstructs the scope and value of patch correlations. Specifically, CorrCLIP leverages the Segment Anything Model (SAM) to define the scope of patch interactions, reducing inter-class correlations. To mitigate the problem that SAM-generated masks may contain patches belonging to different classes, CorrCLIP incorporates self-supervised models to compute coherent similarity values, suppressing the weight of inter-class correlations. Additionally, we introduce two additional branches to strengthen patch features' spatial details and semantic representation. Finally, we update segmentation maps with SAM-generated masks to improve spatial consistency. Based on the improvement across patch correlations, feature representations, and segmentation maps, CorrCLIP achieves superior performance across eight benchmarks. Codes are available at: https://github.com/zdk258/CorrCLIP.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 23,
        "kimi": 23
      },
      "raw_excerpt": "CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation [PDF 23 ] [Copy] [Kimi 23 ] [REL] Authors : Dengke Zhang , Fagui Liu , Quan Tang Open-vocabulary semantic segmentation aims to assign semantic labels to each pixel without being constrained by a predefined set of categories. While Contrastive Language-Image Pre-training (CLIP) excels in zero-shot classification, it struggles to align image patches with category embeddings because of its incoherent patch correlations. This study reveals that inter-class correlations are the main reason for impairing CLIP's segmentation performance. Accordingly, we propose CorrCLIP, which reconstructs the scope and value of patch correlations. Specifically, CorrCLIP leverages the Segment Anything Model (SAM) to define the scope of patch interactions, reducing inter-class correlations. To mitigate the problem that SAM-generated masks may contain patches belonging to different classes, CorrCLIP incorporates self-supervised models to compute coherent similarity values, suppressing the weight of inter-class correlations. Additionally, we introduce two additional branches to strengthen patch features' spatial details and semantic representation. Finally, we update segmentation maps with SAM-generated masks to improve spatial consistency. Based on the improvement across patch correlations, feature representations, and segmentation maps, CorrCLIP achieves superior performance across eight benchmarks. Codes are available at: https://github.com/zdk258/CorrCLIP. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF",
      "index": 4,
      "title": "MaskControl: Spatio-Temporal Control for Masked Motion Synthesis",
      "authors": [
        "Ekkasit Pinyoanuntapong",
        "Muhammad Saleem",
        "Korrawe Karunratanakul",
        "Pu Wang",
        "Hongfei Xue",
        "Chen Chen",
        "Chuan Guo",
        "Junli Cao",
        "Jian Ren",
        "Sergey Tulyakov"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "maskcontrol",
        "logits",
        "motion",
        "control",
        "masked",
        "regularizer",
        "joint",
        "generation",
        "align",
        "positions"
      ],
      "summary": "Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, Logits Regularizer implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, Logit Optimization explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce Differentiable Expectation Sampling (DES) to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by 77%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://anonymous-ai-agent.github.io/CAM",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 11
      },
      "raw_excerpt": "MaskControl: Spatio-Temporal Control for Masked Motion Synthesis [PDF 19 ] [Copy] [Kimi 11 ] [REL] Authors : Ekkasit Pinyoanuntapong , Muhammad Saleem , Korrawe Karunratanakul , Pu Wang , Hongfei Xue , Chen Chen , Chuan Guo , Junli Cao , Jian Ren , Sergey Tulyakov Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, Logits Regularizer implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, Logit Optimization explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce Differentiable Expectation Sampling (DES) to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by 77%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://anonymous-ai-agent.github.io/CAM Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF",
      "index": 5,
      "title": "Removing Cost Volumes from Optical Flow Estimators",
      "authors": [
        "Simon Kiefhaber",
        "Stefan Roth",
        "Simone Schaub-Meyer"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "volumes",
        "cost",
        "removing",
        "estimators",
        "optical",
        "flow",
        "frames",
        "memory",
        "raft",
        "2xfaster"
      ],
      "summary": "Cost volumes are used in every modern optical flow estimator, but due to their computational and space complexity, they are often a limiting factor regarding both processing speed and the resolution of input frames. Motivated by our empirical observation that cost volumes lose their importance once all other network parts of, e.g., a RAFT-based pipeline have been sufficiently trained, we introduce a training strategy that allows removing the cost volume from optical flow estimators throughout training. This leads to significantly improved inference speed and reduced memory requirements. Using our training strategy, we create three different models covering different compute budgets. Our most accurate model reaches state-of-the-art accuracy while being 1.2xfaster and having a 6xlower memory footprint than comparable models; our fastest model is capable of processing Full HD frames at 20\\,\\mathrm FPS using only 500\\,\\mathrm MB of GPU memory.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 12
      },
      "raw_excerpt": "Removing Cost Volumes from Optical Flow Estimators [PDF 16 ] [Copy] [Kimi 12 ] [REL] Authors : Simon Kiefhaber , Stefan Roth , Simone Schaub-Meyer Cost volumes are used in every modern optical flow estimator, but due to their computational and space complexity, they are often a limiting factor regarding both processing speed and the resolution of input frames. Motivated by our empirical observation that cost volumes lose their importance once all other network parts of, e.g., a RAFT-based pipeline have been sufficiently trained, we introduce a training strategy that allows removing the cost volume from optical flow estimators throughout training. This leads to significantly improved inference speed and reduced memory requirements. Using our training strategy, we create three different models covering different compute budgets. Our most accurate model reaches state-of-the-art accuracy while being 1.2xfaster and having a 6xlower memory footprint than comparable models; our fastest model is capable of processing Full HD frames at 20\\,\\mathrm FPS using only 500\\,\\mathrm MB of GPU memory. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF",
      "index": 6,
      "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering",
      "authors": [
        "Xiaohang Zhan",
        "Dingming Liu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "occlusion",
        "image",
        "rendering",
        "objects",
        "larender",
        "relationships",
        "control",
        "generation",
        "latent",
        "opacities"
      ],
      "summary": "We propose a novel training-free image generation algorithm that precisely controls the occlusion relationships between objects in an image. Existing image generation methods typically rely on prompts to influence occlusion, which often lack precision. While layout-to-image methods provide control over object locations, they fail to address occlusion relationships explicitly. Given a pre-trained image diffusion model, our method leverages volume rendering principles to \"render\" the scene in latent space, guided by occlusion relationships and the estimated transmittance of objects. This approach does not require retraining or fine-tuning the image diffusion model, yet it enables accurate occlusion control due to its physics-grounded foundation. In extensive experiments, our method significantly outperforms existing approaches in terms of occlusion accuracy. Furthermore, we demonstrate that by adjusting the opacities of objects or concepts during rendering, our method can achieve a variety of effects, such as altering the transparency of objects, the density of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the intensity of light, and the strength of lens effects, etc.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 28,
        "kimi": 12
      },
      "raw_excerpt": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering [PDF 28 ] [Copy] [Kimi 12 ] [REL] Authors : Xiaohang Zhan , Dingming Liu We propose a novel training-free image generation algorithm that precisely controls the occlusion relationships between objects in an image. Existing image generation methods typically rely on prompts to influence occlusion, which often lack precision. While layout-to-image methods provide control over object locations, they fail to address occlusion relationships explicitly. Given a pre-trained image diffusion model, our method leverages volume rendering principles to \"render\" the scene in latent space, guided by occlusion relationships and the estimated transmittance of objects. This approach does not require retraining or fine-tuning the image diffusion model, yet it enables accurate occlusion control due to its physics-grounded foundation. In extensive experiments, our method significantly outperforms existing approaches in terms of occlusion accuracy. Furthermore, we demonstrate that by adjusting the opacities of objects or concepts during rendering, our method can achieve a variety of effects, such as altering the transparency of objects, the density of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the intensity of light, and the strength of lens effects, etc. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF",
      "index": 7,
      "title": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability",
      "authors": [
        "Seungju Yoo",
        "Hyuk Kwon",
        "Joong-Won Hwang",
        "Kibok Lee"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "autoeval",
        "boxes",
        "pcr",
        "reliability",
        "nms",
        "consistency",
        "evaluation",
        "object",
        "detection",
        "automated"
      ],
      "summary": "Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 10
      },
      "raw_excerpt": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability [PDF 16 ] [Copy] [Kimi 10 ] [REL] Authors : Seungju Yoo , Hyuk Kwon , Joong-Won Hwang , Kibok Lee Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF",
      "index": 8,
      "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models",
      "authors": [
        "Haiwen Huang",
        "Anpei Chen",
        "Volodymyr Havrylov",
        "Andreas Geiger",
        "Dan Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "upsampler",
        "feature",
        "loftup",
        "upsampling",
        "resolution",
        "coordinate",
        "foundation",
        "vision",
        "vfms",
        "vfm"
      ],
      "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers a promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce a coordinate-based cross-attention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudo-groundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/andrehuang/loftup.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 8
      },
      "raw_excerpt": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models [PDF 16 ] [Copy] [Kimi 8 ] [REL] Authors : Haiwen Huang , Anpei Chen , Volodymyr Havrylov , Andreas Geiger , Dan Zhang Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers a promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce a coordinate-based cross-attention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudo-groundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/andrehuang/loftup. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF",
      "index": 9,
      "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing",
      "authors": [
        "Federico Girella",
        "Davide Talon",
        "Ziyue Liu",
        "Zanxi Ruan",
        "Yiming Wang",
        "Marco Cristani"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "sketch",
        "lots",
        "fashion",
        "text",
        "conditioning",
        "localized",
        "generation",
        "sketches",
        "image",
        "fashionpedia"
      ],
      "summary": "Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 5
      },
      "raw_excerpt": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing [PDF 14 ] [Copy] [Kimi 5 ] [REL] Authors : Federico Girella , Davide Talon , Ziyue Liu , Zanxi Ruan , Yiming Wang , Marco Cristani Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF",
      "index": 10,
      "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
      "authors": [
        "Yi Wang",
        "Zhitong Xiong",
        "Chenying Liu",
        "Adam J. Stewart",
        "Thomas Dujardin",
        "Nikolaos Ioannis Bountos",
        "Angelos Zavras",
        "Franziska Gerken",
        "Ioannis Papoutsis",
        "Laura Leal-Taixé",
        "Xiao Xiang Zhu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "copernicus",
        "foundation",
        "earth",
        "sentinel",
        "metadata",
        "xlab",
        "unified",
        "spectral",
        "towards",
        "unlocked"
      ],
      "summary": "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes at https://github.com/zhu-xlab/Copernicus-FM.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 6
      },
      "raw_excerpt": "Towards a Unified Copernicus Foundation Model for Earth Vision [PDF 10 ] [Copy] [Kimi 6 ] [REL] Authors : Yi Wang , Zhitong Xiong , Chenying Liu , Adam J. Stewart , Thomas Dujardin , Nikolaos Ioannis Bountos , Angelos Zavras , Franziska Gerken , Ioannis Papoutsis , Laura Leal-Taixé , Xiao Xiang Zhu Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes at https://github.com/zhu-xlab/Copernicus-FM. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF",
      "index": 11,
      "title": "E-SAM: Training-Free Segment Every Entity Model",
      "authors": [
        "Weiming Zhang",
        "Dingwen Xiao",
        "Lei Chen",
        "Lin Wang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "sam",
        "masks",
        "entity",
        "segmentation",
        "amg",
        "emr",
        "mask",
        "level",
        "outputs",
        "segment"
      ],
      "summary": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities within an image without the need for predefined class labels. This characteristic makes ES well-suited to open-world applications with adaptation to diverse and dynamically changing environments, where new and previously unseen entities may appear frequently. Existing ES methods either require large annotated datasets or high training costs, limiting their scalability and adaptability. Recently, the Segment Anything Model (SAM), especially in its Automatic Mask Generation (AMG) mode, has shown potential for holistic image segmentation. However, it struggles with over-segmentation and under-segmentation, making it less effective for ES. In this paper, we introduce E-SAM, a novel training-free framework that exhibits exceptional ES capability. Specifically, we first propose Multi-level Mask Generation (MMG) that hierarchically processes SAM's AMG outputs to generate reliable object-level masks while preserving fine details at other levels. Entity-level Mask Refinement (EMR) then refines these object-level masks into accurate entity-level masks. That is, it separates overlapping masks to address the redundancy issues inherent in SAM's outputs and merges similar masks by evaluating entity-level consistency. Lastly, Under-Segmentation Refinement (USR) addresses under-segmentation by generating additional high-confidence masks fused with EMR outputs to produce the final ES map. These three modules are seamlessly optimized to achieve the best ES without additional training overhead. Extensive experiments demonstrate that E-SAM achieves state-of-the-art performance compared to prior ES methods, demonstrating a significant improvement by +30.1 on benchmark metrics.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 23,
        "kimi": 7
      },
      "raw_excerpt": "E-SAM: Training-Free Segment Every Entity Model [PDF 23 ] [Copy] [Kimi 7 ] [REL] Authors : Weiming Zhang , Dingwen Xiao , Lei Chen , Lin Wang Entity Segmentation (ES) aims at identifying and segmenting distinct entities within an image without the need for predefined class labels. This characteristic makes ES well-suited to open-world applications with adaptation to diverse and dynamically changing environments, where new and previously unseen entities may appear frequently. Existing ES methods either require large annotated datasets or high training costs, limiting their scalability and adaptability. Recently, the Segment Anything Model (SAM), especially in its Automatic Mask Generation (AMG) mode, has shown potential for holistic image segmentation. However, it struggles with over-segmentation and under-segmentation, making it less effective for ES. In this paper, we introduce E-SAM, a novel training-free framework that exhibits exceptional ES capability. Specifically, we first propose Multi-level Mask Generation (MMG) that hierarchically processes SAM's AMG outputs to generate reliable object-level masks while preserving fine details at other levels. Entity-level Mask Refinement (EMR) then refines these object-level masks into accurate entity-level masks. That is, it separates overlapping masks to address the redundancy issues inherent in SAM's outputs and merges similar masks by evaluating entity-level consistency. Lastly, Under-Segmentation Refinement (USR) addresses under-segmentation by generating additional high-confidence masks fused with EMR outputs to produce the final ES map. These three modules are seamlessly optimized to achieve the best ES without additional training overhead. Extensive experiments demonstrate that E-SAM achieves state-of-the-art performance compared to prior ES methods, demonstrating a significant improvement by +30.1 on benchmark metrics. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF",
      "index": 12,
      "title": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)",
      "authors": [
        "Lennart Bastian",
        "Mohammad Rashed",
        "Nassir Navab",
        "Tolga Birdal"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "conservative",
        "extrapolation",
        "trajectories",
        "noisy",
        "savitzky",
        "conservation",
        "unknown",
        "forces",
        "golay",
        "forecasting"
      ],
      "summary": "Modeling the rotation of moving objects is a fundamental task in computer vision, yet SO(3) extrapolation still presents numerous challenges: (1) unknown quantities such as the moment of inertia complicate dynamics, (2) the presence of external forces and torques can lead to non-conservative kinematics, and (3) estimating evolving state trajectories under sparse, noisy observations requires robustness. We propose modeling trajectories of noisy pose estimates on the manifold of 3D rotations in a physically and geometrically meaningful way by leveraging Neural Controlled Differential Equations guided with SO(3) Savitzky-Golay paths. Existing extrapolation methods often rely on energy conservation or constant velocity assumptions, limiting their applicability in real-world scenarios involving non-conservative forces. In contrast, our approach is agnostic to energy and momentum conservation while being robust to input noise, making it applicable to complex, non-inertial systems. Our approach is easily integrated as a module in existing pipelines and generalizes well to trajectories with unknown physical parameters. By learning to approximate object dynamics from noisy states during training, our model attains robust extrapolation capabilities in simulation and various real-world settings.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3) [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Lennart Bastian , Mohammad Rashed , Nassir Navab , Tolga Birdal Modeling the rotation of moving objects is a fundamental task in computer vision, yet SO(3) extrapolation still presents numerous challenges: (1) unknown quantities such as the moment of inertia complicate dynamics, (2) the presence of external forces and torques can lead to non-conservative kinematics, and (3) estimating evolving state trajectories under sparse, noisy observations requires robustness. We propose modeling trajectories of noisy pose estimates on the manifold of 3D rotations in a physically and geometrically meaningful way by leveraging Neural Controlled Differential Equations guided with SO(3) Savitzky-Golay paths. Existing extrapolation methods often rely on energy conservation or constant velocity assumptions, limiting their applicability in real-world scenarios involving non-conservative forces. In contrast, our approach is agnostic to energy and momentum conservation while being robust to input noise, making it applicable to complex, non-inertial systems. Our approach is easily integrated as a module in existing pipelines and generalizes well to trajectories with unknown physical parameters. By learning to approximate object dynamics from noisy states during training, our model attains robust extrapolation capabilities in simulation and various real-world settings. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF",
      "index": 13,
      "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases",
      "authors": [
        "Shuai Tan",
        "Bill Gong",
        "Bin Ji",
        "Ye Pan"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "identity",
        "talking",
        "fixtalk",
        "head",
        "leakage",
        "quality",
        "generation",
        "taming",
        "extreme",
        "rendering"
      ],
      "summary": "Talking head generation is gaining significant importance across various domains, with a growing demand for high-quality rendering. However, existing methods often suffer from identity leakage (IL) and rendering artifacts (RA), particularly in extreme cases. Through an in-depth analysis of previous approaches, we identify two key insights: (1) IL arises from identity information embedded within motion features, and (2) this identity information can be leveraged to address RA. Building on these findings, this paper introduces FixTalk, a novel framework designed to simultaneously resolve both issues for high-quality talking head generation. Firstly, we propose an Enhanced Motion Indicator (EMI) to effectively decouple identity information from motion features, mitigating the impact of IL on generated talking heads. To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes the leaked identity information to supplement missing details, thus fixing the artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates IL and RA, achieving superior performance compared to state-of-the-art methods.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 8
      },
      "raw_excerpt": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases [PDF 12 ] [Copy] [Kimi 8 ] [REL] Authors : Shuai Tan , Bill Gong , Bin Ji , Ye Pan Talking head generation is gaining significant importance across various domains, with a growing demand for high-quality rendering. However, existing methods often suffer from identity leakage (IL) and rendering artifacts (RA), particularly in extreme cases. Through an in-depth analysis of previous approaches, we identify two key insights: (1) IL arises from identity information embedded within motion features, and (2) this identity information can be leveraged to address RA. Building on these findings, this paper introduces FixTalk, a novel framework designed to simultaneously resolve both issues for high-quality talking head generation. Firstly, we propose an Enhanced Motion Indicator (EMI) to effectively decouple identity information from motion features, mitigating the impact of IL on generated talking heads. To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes the leaked identity information to supplement missing details, thus fixing the artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates IL and RA, achieving superior performance compared to state-of-the-art methods. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF",
      "index": 14,
      "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
      "authors": [
        "Mark Yu",
        "Wenbo Hu",
        "Jinbo Xing",
        "Ying Shan"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "videos",
        "monocular",
        "trajectorycrafter",
        "view",
        "camera",
        "redirecting",
        "diffusion",
        "redirect",
        "trajectories",
        "transformations"
      ],
      "summary": "We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method. Code and pre-trained model will be released.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models_ICCV_2025_paper.html",
          "/venue/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Oral",
          "/venue/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 9
      },
      "raw_excerpt": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models [PDF 15 ] [Copy] [Kimi 9 ] [REL] Authors : Mark Yu , Wenbo Hu , Jinbo Xing , Ying Shan We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method. Code and pre-trained model will be released. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF",
      "index": 15,
      "title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models",
      "authors": [
        "Vladimir Kulikov",
        "Matan Kleiner",
        "Inbar Huberman-Spiegelglas",
        "Tomer Michaeli"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "inversion",
        "flowedit",
        "editing",
        "text",
        "t2i",
        "pre",
        "trained",
        "flow",
        "free",
        "intervene"
      ],
      "summary": "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 8
      },
      "raw_excerpt": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models [PDF 18 ] [Copy] [Kimi 8 ] [REL] Authors : Vladimir Kulikov , Matan Kleiner , Inbar Huberman-Spiegelglas , Tomer Michaeli Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF",
      "index": 16,
      "title": "Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval",
      "authors": [
        "Ziwei Wang",
        "Sameera Ramasinghe",
        "Chenchen Xu",
        "Julien Monteil",
        "Loris Bazzani",
        "Thalaiyasingam Ajanthan"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "hierarchies",
        "visual",
        "retrieval",
        "hierarchical",
        "image",
        "hyperbolic",
        "hierarchy",
        "similarity",
        "metrics",
        "transcends"
      ],
      "summary": "Structuring latent representations in a hierarchical manner enables models to learn patterns at multiple levels of abstraction. However, most prevalent image understanding models focus on visual similarity, and learning visual hierarchies is relatively unexplored. In this work, for the first time, we introduce a learning paradigm that can encode user-defined multi-level complex visual hierarchies in hyperbolic space without requiring explicit hierarchical labels. As a concrete example, first, we define a part-based image hierarchy using object-level annotations within and across images. Then, we introduce an approach to enforce the hierarchy using contrastive loss with pairwise entailment metrics. Finally, we discuss new evaluation metrics to effectively measure hierarchical image retrieval. Encoding these complex relationships ensures that the learned representations capture semantic and structural information that transcends mere visual similarity. Experiments in part-based image retrieval show significant improvements in hierarchical retrieval tasks, demonstrating the capability of our model in capturing visual hierarchies.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 10
      },
      "raw_excerpt": "Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval [PDF 16 ] [Copy] [Kimi 10 ] [REL] Authors : Ziwei Wang , Sameera Ramasinghe , Chenchen Xu , Julien Monteil , Loris Bazzani , Thalaiyasingam Ajanthan Structuring latent representations in a hierarchical manner enables models to learn patterns at multiple levels of abstraction. However, most prevalent image understanding models focus on visual similarity, and learning visual hierarchies is relatively unexplored. In this work, for the first time, we introduce a learning paradigm that can encode user-defined multi-level complex visual hierarchies in hyperbolic space without requiring explicit hierarchical labels. As a concrete example, first, we define a part-based image hierarchy using object-level annotations within and across images. Then, we introduce an approach to enforce the hierarchy using contrastive loss with pairwise entailment metrics. Finally, we discuss new evaluation metrics to effectively measure hierarchical image retrieval. Encoding these complex relationships ensures that the learned representations capture semantic and structural information that transcends mere visual similarity. Experiments in part-based image retrieval show significant improvements in hierarchical retrieval tasks, demonstrating the capability of our model in capturing visual hierarchies. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF",
      "index": 17,
      "title": "Dynamic Typography: Bringing Text to Life via Video Diffusion Prior",
      "authors": [
        "Zichen Liu",
        "Yihao Meng",
        "Hao Ouyang",
        "Yue Yu",
        "Bolin Zhao",
        "Daniel Cohen-Or",
        "Huamin Qu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "animation",
        "typography",
        "text",
        "video",
        "animations",
        "canonical",
        "dynamic",
        "prompts",
        "infuses",
        "bringing"
      ],
      "summary": "Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives. Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation. We present an automated text animation scheme, termed \"Dynamic Typography\", which deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts. The animation is represented by a canonical field that aggregates the semantic content in a canonical shape and a deformation field that applies per-frame motion to deform the canonical shape. Two fields are jointly optimized by the priors from a large pretrained text-to-video diffusion model using score-distillation loss with designed regularization, encouraging the video coherence with the intended textual concept while maintaining legibility and structural integrity throughout the animation process. We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our methodology over baselines. Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior_ICCV_2025_paper.html",
          "/venue/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Oral",
          "/venue/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 6
      },
      "raw_excerpt": "Dynamic Typography: Bringing Text to Life via Video Diffusion Prior [PDF 16 ] [Copy] [Kimi 6 ] [REL] Authors : Zichen Liu , Yihao Meng , Hao Ouyang , Yue Yu , Bolin Zhao , Daniel Cohen-Or , Huamin Qu Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives. Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation. We present an automated text animation scheme, termed \"Dynamic Typography\", which deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts. The animation is represented by a canonical field that aggregates the semantic content in a canonical shape and a deformation field that applies per-frame motion to deform the canonical shape. Two fields are jointly optimized by the priors from a large pretrained text-to-video diffusion model using score-distillation loss with designed regularization, encouraging the video coherence with the intended textual concept while maintaining legibility and structural integrity throughout the animation process. We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our methodology over baselines. Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF",
      "index": 18,
      "title": "Uncalibrated Structure from Motion on a Sphere",
      "authors": [
        "Jonathan Ventura",
        "Viktor Larsson",
        "Fredrik Kahl"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "motion",
        "spherical",
        "uncalibrated",
        "camera",
        "jonathanventura",
        "focal",
        "outstretched",
        "sphere",
        "hemi",
        "structure"
      ],
      "summary": "Spherical motion is a special case of camera motion where the camera moves on the imaginary surface of a sphere with the optical axis normal to the surface. Common sources of spherical motion are a person capturing a stereo panorama with a phone held in an outstretched hand, or a hemi-spherical camera rig used for multi-view scene capture. However, traditional structure-from-motion pipelines tend to fail on spherical camera motion sequences, especially when the camera is facing outward. Building upon prior work addressing the calibrated case, we explore uncalibrated reconstruction from spherical motion, assuming a fixed but unknown focal length parameter. We show that, although two-view spherical motion is always a critical case, self-calibration is possible from three or more views. Through analysis of the relationship between focal length and spherical relative pose, we devise a global structure-from-motion approach for uncalibrated reconstruction. We demonstrate the effectiveness of our approach on real-world captures in various settings, even when the camera motion deviates from perfect spherical motion. Code and data for our method are available at https://github.com/jonathanventura/spherical-sfm.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 7
      },
      "raw_excerpt": "Uncalibrated Structure from Motion on a Sphere [PDF 16 ] [Copy] [Kimi 7 ] [REL] Authors : Jonathan Ventura , Viktor Larsson , Fredrik Kahl Spherical motion is a special case of camera motion where the camera moves on the imaginary surface of a sphere with the optical axis normal to the surface. Common sources of spherical motion are a person capturing a stereo panorama with a phone held in an outstretched hand, or a hemi-spherical camera rig used for multi-view scene capture. However, traditional structure-from-motion pipelines tend to fail on spherical camera motion sequences, especially when the camera is facing outward. Building upon prior work addressing the calibrated case, we explore uncalibrated reconstruction from spherical motion, assuming a fixed but unknown focal length parameter. We show that, although two-view spherical motion is always a critical case, self-calibration is possible from three or more views. Through analysis of the relationship between focal length and spherical relative pose, we devise a global structure-from-motion approach for uncalibrated reconstruction. We demonstrate the effectiveness of our approach on real-world captures in various settings, even when the camera motion deviates from perfect spherical motion. Code and data for our method are available at https://github.com/jonathanventura/spherical-sfm. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF",
      "index": 19,
      "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors",
      "authors": [
        "Derong Jin",
        "Ruohan Gao"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "acoustic",
        "room",
        "dar",
        "rendering",
        "differentiable",
        "audio",
        "visual",
        "physics",
        "view",
        "environments"
      ],
      "summary": "An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Derong Jin , Ruohan Gao An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF",
      "index": 20,
      "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space",
      "authors": [
        "David G. Shatwell",
        "Ishan Rajendrakumar Dave",
        "Sirnam Swetha",
        "Mubarak Shah"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "geo",
        "loc",
        "timestamp",
        "cyclical",
        "retrieval",
        "location",
        "cues",
        "joint",
        "embedding",
        "prediction"
      ],
      "summary": "Timestamp prediction aims to determine when an image was captured using only visual information, supporting applications such as metadata correction, retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely on cues like brightness, hue, and shadow positioning, while seasonal changes and weather inform date estimation. However, these visual cues significantly depend on geographic context, closely linking timestamp prediction to geo-localization. To address this interdependence, we introduce GT-Loc, a novel retrieval-based method that jointly predicts the capture time (hour and month) and geo-location (GPS coordinates) of an image. Our approach employs separate encoders for images, time, and location, aligning their embeddings within a shared high-dimensional feature space. Recognizing the cyclical nature of time, instead of conventional contrastive learning with hard positives and negatives, we propose a temporal metric-learning objective providing soft targets by modeling pairwise time differences over a cyclical toroidal surface. We present new benchmarks demonstrating that our joint optimization surpasses previous time prediction methods, even those using the ground-truth geo-location as an input during inference. Additionally, our approach achieves competitive results on standard geo-localization tasks, and the unified embedding space facilitates compositional and text-based image retrieval.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 5
      },
      "raw_excerpt": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space [PDF 15 ] [Copy] [Kimi 5 ] [REL] Authors : David G. Shatwell , Ishan Rajendrakumar Dave , Sirnam Swetha , Mubarak Shah Timestamp prediction aims to determine when an image was captured using only visual information, supporting applications such as metadata correction, retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely on cues like brightness, hue, and shadow positioning, while seasonal changes and weather inform date estimation. However, these visual cues significantly depend on geographic context, closely linking timestamp prediction to geo-localization. To address this interdependence, we introduce GT-Loc, a novel retrieval-based method that jointly predicts the capture time (hour and month) and geo-location (GPS coordinates) of an image. Our approach employs separate encoders for images, time, and location, aligning their embeddings within a shared high-dimensional feature space. Recognizing the cyclical nature of time, instead of conventional contrastive learning with hard positives and negatives, we propose a temporal metric-learning objective providing soft targets by modeling pairwise time differences over a cyclical toroidal surface. We present new benchmarks demonstrating that our joint optimization surpasses previous time prediction methods, even those using the ground-truth geo-location as an input during inference. Additionally, our approach achieves competitive results on standard geo-localization tasks, and the unified embedding space facilitates compositional and text-based image retrieval. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF",
      "index": 21,
      "title": "Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos",
      "authors": [
        "Yi Chen",
        "Yuying Ge",
        "Weiliang Tang",
        "Yizhuo Li",
        "Yixiao Ge",
        "Mingyu Ding",
        "Ying Shan",
        "Xihui Liu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "moto",
        "motion",
        "robot",
        "token",
        "manipulation",
        "latent",
        "pre",
        "language",
        "gpt",
        "bridging"
      ],
      "summary": "Recent developments in Large Language Models (LLMs) pre-trained on extensive corpora have shown significant success in various natural language processing (NLP) tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks.Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood.To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 4
      },
      "raw_excerpt": "Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos [PDF 10 ] [Copy] [Kimi 4 ] [REL] Authors : Yi Chen , Yuying Ge , Weiliang Tang , Yizhuo Li , Yixiao Ge , Mingyu Ding , Ying Shan , Xihui Liu Recent developments in Large Language Models (LLMs) pre-trained on extensive corpora have shown significant success in various natural language processing (NLP) tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks.Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood.To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF",
      "index": 22,
      "title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
      "authors": [
        "Alexander Mai",
        "Peter Hedman",
        "George Kopanas",
        "Dor Verbin",
        "David Futschik",
        "Qiangeng Xu",
        "Falko Kuester",
        "Jonathan T. Barron",
        "Yinda Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "ever",
        "volumetric",
        "ellipsoid",
        "zip",
        "ray",
        "rendering",
        "raytraced",
        "nerf",
        "rtx4090",
        "popping"
      ],
      "summary": "We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time 3D reconstruction.EVER accurately blends an unlimited number of overlapping primitives together in 3D space, eliminating the popping artifacts that 3D Gaussian Splatting (3DGS) and other related methods exhibit.EVER represents a radiance field as a set of constant-density volumetric ellipsoids, which are raytraced by intersecting each primitive twice (once upon ray entrance and another on ray exit) and accumulating the derivatives of the densities and colors along the ray.Because EVER is built around ray tracing, it also enables effects such as defocus blur and fish-eye camera distortion, while still achieving frame rates of 30 FPS at 720p on an NVIDIA RTX4090. We show that our method is more accurate on the challenging large-scale scenes from the Zip-NeRF dataset, where it achieves state of the art SSIM, even higher than Zip-NeRF.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 5
      },
      "raw_excerpt": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis [PDF 10 ] [Copy] [Kimi 5 ] [REL] Authors : Alexander Mai , Peter Hedman , George Kopanas , Dor Verbin , David Futschik , Qiangeng Xu , Falko Kuester , Jonathan T. Barron , Yinda Zhang We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time 3D reconstruction.EVER accurately blends an unlimited number of overlapping primitives together in 3D space, eliminating the popping artifacts that 3D Gaussian Splatting (3DGS) and other related methods exhibit.EVER represents a radiance field as a set of constant-density volumetric ellipsoids, which are raytraced by intersecting each primitive twice (once upon ray entrance and another on ray exit) and accumulating the derivatives of the densities and colors along the ray.Because EVER is built around ray tracing, it also enables effects such as defocus blur and fish-eye camera distortion, while still achieving frame rates of 30 FPS at 720p on an NVIDIA RTX4090. We show that our method is more accurate on the challenging large-scale scenes from the Zip-NeRF dataset, where it achieves state of the art SSIM, even higher than Zip-NeRF. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF",
      "index": 23,
      "title": "Multi-View 3D Point Tracking",
      "authors": [
        "Frano Rajič",
        "Haofei Xu",
        "Marko Mihajlovic",
        "Siyuan Li",
        "Irem Demir",
        "Emircan Gündoğdu",
        "Lei Ke",
        "Sergey Prokudin",
        "Marc Pollefeys",
        "Siyu Tang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "view",
        "multi",
        "tracker",
        "camera",
        "tracking",
        "occlusion",
        "correspondences",
        "alongside",
        "kubric",
        "0cm"
      ],
      "summary": "We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks--Panoptic Studio and DexYCB--achieving median trajectory errors of 3.1 cm and 2.0cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page: https://ethz-vlg.github.io/mvtracker.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Rajic_Multi-View_3D_Point_Tracking_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Rajic_Multi-View_3D_Point_Tracking_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Multi-View 3D Point Tracking [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Frano Rajič , Haofei Xu , Marko Mihajlovic , Siyuan Li , Irem Demir , Emircan Gündoğdu , Lei Ke , Sergey Prokudin , Marc Pollefeys , Siyu Tang We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks--Panoptic Studio and DexYCB--achieving median trajectory errors of 3.1 cm and 2.0cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page: https://ethz-vlg.github.io/mvtracker. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF",
      "index": 24,
      "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching",
      "authors": [
        "Chengtang Yao",
        "Lidong Yu",
        "Zhidan Liu",
        "Jiaxi Zeng",
        "Yuwei Wu",
        "Yunde Jia"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "monocular",
        "stereo",
        "fusion",
        "disparity",
        "matching",
        "depth",
        "optima",
        "vfm",
        "ill",
        "posed"
      ],
      "summary": "The matching formulation makes it naturally hard for the stereo matching to handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing monocular priors has been proven helpful for ill-posed matching, but the biased monocular prior learned from small stereo datasets constrains the generalization. Recently, stereo matching has progressed by leveraging the unbiased monocular prior from the vision foundation model (VFM) to improve the generalization in ill-posed regions. We dive into the fusion process and observe three main problems limiting the fusion of the VFM monocular prior. The first problem is the misalignment between affine-invariant relative monocular depth and absolute depth of disparity. Besides, when we use the monocular feature in an iterative update structure, the over-confidence in the disparity update leads to local optima results. A direct fusion of a monocular depth map could alleviate the local optima problem, but noisy disparity results computed at the first several iterations will misguide the fusion. In this paper, we propose a binary local ordering map to guide the fusion, which converts the depth map into a binary relative format, unifying the relative and absolute depth representation. The computed local ordering map is also used to re-weight the initial disparity update, resolving the local optima and noisy problem. In addition, we formulate the final direct fusion of monocular depth to the disparity as a registration problem, where a pixel-wise linear regression module can globally and adaptively align them. Our method fully exploits the monocular prior to support stereo matching results effectively and efficiently. We significantly improve the performance from the experiments when generalizing from SceneFlow to Middlebury and Booster datasets while barely reducing the efficiency.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Chengtang Yao , Lidong Yu , Zhidan Liu , Jiaxi Zeng , Yuwei Wu , Yunde Jia The matching formulation makes it naturally hard for the stereo matching to handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing monocular priors has been proven helpful for ill-posed matching, but the biased monocular prior learned from small stereo datasets constrains the generalization. Recently, stereo matching has progressed by leveraging the unbiased monocular prior from the vision foundation model (VFM) to improve the generalization in ill-posed regions. We dive into the fusion process and observe three main problems limiting the fusion of the VFM monocular prior. The first problem is the misalignment between affine-invariant relative monocular depth and absolute depth of disparity. Besides, when we use the monocular feature in an iterative update structure, the over-confidence in the disparity update leads to local optima results. A direct fusion of a monocular depth map could alleviate the local optima problem, but noisy disparity results computed at the first several iterations will misguide the fusion. In this paper, we propose a binary local ordering map to guide the fusion, which converts the depth map into a binary relative format, unifying the relative and absolute depth representation. The computed local ordering map is also used to re-weight the initial disparity update, resolving the local optima and noisy problem. In addition, we formulate the final direct fusion of monocular depth to the disparity as a registration problem, where a pixel-wise linear regression module can globally and adaptively align them. Our method fully exploits the monocular prior to support stereo matching results effectively and efficiently. We significantly improve the performance from the experiments when generalizing from SceneFlow to Middlebury and Booster datasets while barely reducing the efficiency. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF",
      "index": 25,
      "title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars",
      "authors": [
        "Byungjun Kim",
        "Shunsuke Saito",
        "Giljoo Nam",
        "Tomas Simon",
        "Jason Saragih",
        "Hanbyul Joo",
        "Junxuan Li"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "hair",
        "avatars",
        "compositionality",
        "hairless",
        "face",
        "head",
        "prior",
        "haircup",
        "compositional",
        "holistic"
      ],
      "summary": "We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of original and synthetic hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a data-efficient manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 7
      },
      "raw_excerpt": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars [PDF 9 ] [Copy] [Kimi 7 ] [REL] Authors : Byungjun Kim , Shunsuke Saito , Giljoo Nam , Tomas Simon , Jason Saragih , Hanbyul Joo , Junxuan Li We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of original and synthetic hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a data-efficient manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF",
      "index": 26,
      "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
      "authors": [
        "Jianhong Bai",
        "Menghan Xia",
        "Xiao Fu",
        "Xintao Wang",
        "Lianrui Mu",
        "Jinwen Cao",
        "Zuozhu Liu",
        "Haoji Hu",
        "Xiang Bai",
        "Pengfei Wan",
        "Di Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "video",
        "camera",
        "recammaster",
        "generative",
        "rendering",
        "controlled",
        "filming",
        "trajectories",
        "unreal",
        "diverse"
      ],
      "summary": "Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through an elegant yet powerful video conditioning mechanism--an aspect often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments show that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Our code and dataset are publicly available.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video_ICCV_2025_paper.html",
          "/venue/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Oral",
          "/venue/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 4
      },
      "raw_excerpt": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video [PDF 10 ] [Copy] [Kimi 4 ] [REL] Authors : Jianhong Bai , Menghan Xia , Xiao Fu , Xintao Wang , Lianrui Mu , Jinwen Cao , Zuozhu Liu , Haoji Hu , Xiang Bai , Pengfei Wan , Di Zhang Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through an elegant yet powerful video conditioning mechanism--an aspect often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments show that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Our code and dataset are publicly available. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF",
      "index": 27,
      "title": "RePoseD: Efficient Relative Pose Estimation With Known Depth Information",
      "authors": [
        "Yaqing Ding",
        "Viktor Kocur",
        "Václav Vávra",
        "Zuzana Berger Haladová",
        "Jian Yang",
        "Torsten Sattler",
        "Zuzana Kukelova"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "depth",
        "solvers",
        "mdes",
        "cameras",
        "pose",
        "monocular",
        "unknown",
        "reposed",
        "relative",
        "kocurvik"
      ],
      "summary": "Recent advances in monocular depth estimation methods (MDE) and their improved accuracy open new possibilities for their applications. In this paper, we investigate how monocular depth estimates can be used for relative pose estimation. In particular, we are interested in answering the question whether using MDEs improves results over traditional point-based methods. We propose a novel framework for estimating the relative pose of two cameras from point correspondences with associated monocular depths. Since depth predictions are typically defined up to an unknown scale or even both unknown scale and shift parameters, our solvers jointly estimate the scale or both the scale and shift parameters along with the relative pose. We derive efficient solvers considering different types of depths for three camera configurations: (1) two calibrated cameras, (2) two cameras with an unknown shared focal length, and (3) two cameras with unknown different focal lengths. Our new solvers outperform state-of-the-art depth-aware solvers in terms of speed and accuracy. In extensive real experiments on multiple datasets and with various MDEs, we discuss which depth-aware solvers are preferable in which situation. The code is available at https://github.com/kocurvik/mdrp.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 6
      },
      "raw_excerpt": "RePoseD: Efficient Relative Pose Estimation With Known Depth Information [PDF 8 ] [Copy] [Kimi 6 ] [REL] Authors : Yaqing Ding , Viktor Kocur , Václav Vávra , Zuzana Berger Haladová , Jian Yang , Torsten Sattler , Zuzana Kukelova Recent advances in monocular depth estimation methods (MDE) and their improved accuracy open new possibilities for their applications. In this paper, we investigate how monocular depth estimates can be used for relative pose estimation. In particular, we are interested in answering the question whether using MDEs improves results over traditional point-based methods. We propose a novel framework for estimating the relative pose of two cameras from point correspondences with associated monocular depths. Since depth predictions are typically defined up to an unknown scale or even both unknown scale and shift parameters, our solvers jointly estimate the scale or both the scale and shift parameters along with the relative pose. We derive efficient solvers considering different types of depths for three camera configurations: (1) two calibrated cameras, (2) two cameras with an unknown shared focal length, and (3) two cameras with unknown different focal lengths. Our new solvers outperform state-of-the-art depth-aware solvers in terms of speed and accuracy. In extensive real experiments on multiple datasets and with various MDEs, we discuss which depth-aware solvers are preferable in which situation. The code is available at https://github.com/kocurvik/mdrp. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF",
      "index": 28,
      "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
      "authors": [
        "Xianglong He",
        "Zi-Xin Zou",
        "Chia-Hao Chen",
        "Yuan-Chen Guo",
        "Ding Liang",
        "Chun Yuan",
        "Wanli Ouyang",
        "Yan-Pei Cao",
        "Yangguang Li"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "sparseflex",
        "rendering",
        "reconstruction",
        "resolution",
        "shape",
        "mesh",
        "topology",
        "interiors",
        "high",
        "arbitrary"
      ],
      "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a 82% reduction in Chamfer Distance and a 88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 4
      },
      "raw_excerpt": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling [PDF 7 ] [Copy] [Kimi 4 ] [REL] Authors : Xianglong He , Zi-Xin Zou , Chia-Hao Chen , Yuan-Chen Guo , Ding Liang , Chun Yuan , Wanli Ouyang , Yan-Pei Cao , Yangguang Li Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a 82% reduction in Chamfer Distance and a 88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF",
      "index": 29,
      "title": "RayZer: A Self-supervised Large View Synthesis Model",
      "authors": [
        "Hanwen Jiang",
        "Hao Tan",
        "Peng Wang",
        "Haian Jin",
        "Yue Zhao",
        "Sai Bi",
        "Kai Zhang",
        "Fujun Luan",
        "Kalyan Sunkavalli",
        "Qixing Huang",
        "Georgios Pavlakos"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "rayzer",
        "camera",
        "scene",
        "self",
        "supervised",
        "view",
        "awareness",
        "synthesis",
        "supervision",
        "poses"
      ],
      "summary": "We present RayZer, a self-supervised multi-view 3D Vision model trained without any 3D supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. During training, RayZer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing RayZer to be trained with 2D image supervision. The emerging 3D awareness of RayZer is attributed to two key factors. First, we design a self-supervised framework, which achieves 3D-aware auto-encoding of input images by disentangling camera and scene representations. Second, we design a transformer-based model in which the only 3D prior is the ray structure, connecting camera, pixel, and scene simultaneously. RayZer demonstrates comparable or even superior novel view synthesis performance than \"oracle\" methods that rely on pose annotations in both training and testing.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 8
      },
      "raw_excerpt": "RayZer: A Self-supervised Large View Synthesis Model [PDF 12 ] [Copy] [Kimi 8 ] [REL] Authors : Hanwen Jiang , Hao Tan , Peng Wang , Haian Jin , Yue Zhao , Sai Bi , Kai Zhang , Fujun Luan , Kalyan Sunkavalli , Qixing Huang , Georgios Pavlakos We present RayZer, a self-supervised multi-view 3D Vision model trained without any 3D supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. During training, RayZer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing RayZer to be trained with 2D image supervision. The emerging 3D awareness of RayZer is attributed to two key factors. First, we design a self-supervised framework, which achieves 3D-aware auto-encoding of input images by disentangling camera and scene representations. Second, we design a transformer-based model in which the only 3D prior is the ray structure, connecting camera, pixel, and scene simultaneously. RayZer demonstrates comparable or even superior novel view synthesis performance than \"oracle\" methods that rely on pose annotations in both training and testing. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF",
      "index": 30,
      "title": "Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction",
      "authors": [
        "Weirong Chen",
        "Ganlin Zhang",
        "Felix Wimbauer",
        "Rui Wang",
        "Nikita Araslanov",
        "Andrea Vedaldi",
        "Daniel Cremers"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "adjustment",
        "bundle",
        "motion",
        "dynamic",
        "camera",
        "elements",
        "track",
        "slam",
        "reconstructions",
        "tracker"
      ],
      "summary": "Traditional SLAM systems, which rely on bundle adjustment, struggle with the highly dynamic scenes commonly found in casual videos. Such videos entangle the motion of dynamic elements, undermining the assumption of static environments required by traditional systems. Existing techniques either filter out dynamic elements or model their motion independently. However, the former often results in incomplete reconstructions, while the latter can lead to inconsistent motion estimates. Taking a novel approach, this work leverages a 3D point tracker to separate camera-induced motion from the observed motion of dynamic objects. By considering only the camera-induced component, bundle adjustment can operate reliably on all scene elements. We further ensure depth consistency across video frames with lightweight post-processing based on scale maps. Our framework combines the core of traditional SLAM---bundle adjustment---with a robust learning-based 3D tracker. Integrating motion decomposition, bundle adjustment, and depth refinement, our unified framework, BA-Track, accurately tracks camera motion and produces temporally coherent and scale-consistent dense reconstructions, accommodating both static and dynamic elements. Our experiments on challenging datasets reveal significant improvements in camera pose estimation and 3D reconstruction accuracy.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Weirong Chen , Ganlin Zhang , Felix Wimbauer , Rui Wang , Nikita Araslanov , Andrea Vedaldi , Daniel Cremers Traditional SLAM systems, which rely on bundle adjustment, struggle with the highly dynamic scenes commonly found in casual videos. Such videos entangle the motion of dynamic elements, undermining the assumption of static environments required by traditional systems. Existing techniques either filter out dynamic elements or model their motion independently. However, the former often results in incomplete reconstructions, while the latter can lead to inconsistent motion estimates. Taking a novel approach, this work leverages a 3D point tracker to separate camera-induced motion from the observed motion of dynamic objects. By considering only the camera-induced component, bundle adjustment can operate reliably on all scene elements. We further ensure depth consistency across video frames with lightweight post-processing based on scale maps. Our framework combines the core of traditional SLAM---bundle adjustment---with a robust learning-based 3D tracker. Integrating motion decomposition, bundle adjustment, and depth refinement, our unified framework, BA-Track, accurately tracks camera motion and produces temporally coherent and scale-consistent dense reconstructions, accommodating both static and dynamic elements. Our experiments on challenging datasets reveal significant improvements in camera pose estimation and 3D reconstruction accuracy. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF",
      "index": 31,
      "title": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis",
      "authors": [
        "Chen Zhao",
        "Xuan Wang",
        "Tong Zhang",
        "Saqib Javed",
        "Mathieu Salzmann"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "3dgs",
        "ensembling",
        "splatting",
        "nvs",
        "perturbed",
        "sigma",
        "self",
        "gaussian",
        "view",
        "segs"
      ],
      "summary": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A \\Delta-model and a \\Sigma-model are jointly trained on the available images. The \\Delta-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the \\Sigma-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the \\Sigma-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: https://sailor-z.github.io/projects/SEGS.html.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 5
      },
      "raw_excerpt": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis [PDF 10 ] [Copy] [Kimi 5 ] [REL] Authors : Chen Zhao , Xuan Wang , Tong Zhang , Saqib Javed , Mathieu Salzmann 3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A \\Delta-model and a \\Sigma-model are jointly trained on the available images. The \\Delta-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the \\Sigma-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the \\Sigma-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: https://sailor-z.github.io/projects/SEGS.html. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF",
      "index": 32,
      "title": "Importance-Based Token Merging for Efficient Image and Video Generation",
      "authors": [
        "Haoyu Wu",
        "Jingyi Xu",
        "Hieu Le",
        "Dimitris Samaras"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "token",
        "merging",
        "tokens",
        "importance",
        "generation",
        "animatediff",
        "video",
        "zero123",
        "pixart",
        "image"
      ],
      "summary": "Token merging can effectively accelerate various vision systems by processing groups of similar tokens only once and sharing the results across them. However, existing token grouping methods are often ad hoc and random, disregarding the actual content of the samples. We show that preserving high-information tokens during merging--those essential for semantic fidelity and structural details--significantly improves sample quality, producing finer details and more coherent, realistic generations. To do so, we propose an importance-based token merging method that prioritizes the most critical tokens in computational resource allocation, leveraging readily available importance scores, such as those from classifier-free guidance in diffusion models. Experiments show that our approach significantly outperforms baseline methods across multiple applications, including text-to-image synthesis, multi-view image generation, and video generation with various model architectures such as Stable Diffusion, Zero123++, AnimateDiff, or PixArt-\\alpha.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation_ICCV_2025_paper.html",
          "/venue/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Oral",
          "/venue/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 7
      },
      "raw_excerpt": "Importance-Based Token Merging for Efficient Image and Video Generation [PDF 17 ] [Copy] [Kimi 7 ] [REL] Authors : Haoyu Wu , Jingyi Xu , Hieu Le , Dimitris Samaras Token merging can effectively accelerate various vision systems by processing groups of similar tokens only once and sharing the results across them. However, existing token grouping methods are often ad hoc and random, disregarding the actual content of the samples. We show that preserving high-information tokens during merging--those essential for semantic fidelity and structural details--significantly improves sample quality, producing finer details and more coherent, realistic generations. To do so, we propose an importance-based token merging method that prioritizes the most critical tokens in computational resource allocation, leveraging readily available importance scores, such as those from classifier-free guidance in diffusion models. Experiments show that our approach significantly outperforms baseline methods across multiple applications, including text-to-image synthesis, multi-view image generation, and video generation with various model architectures such as Stable Diffusion, Zero123++, AnimateDiff, or PixArt-\\alpha. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF",
      "index": 33,
      "title": "Knowledge Distillation for Learned Image Compression",
      "authors": [
        "Yunuo Chen",
        "Zezheng Lyu",
        "Bing He",
        "Ning Cao",
        "Gang Chen",
        "Guo Lu",
        "Wenjun Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "lic",
        "student",
        "distillation",
        "knowledge",
        "teacher",
        "kdic",
        "learned",
        "compression",
        "image",
        "stage"
      ],
      "summary": "Recently, learned image compression (LIC) models have achieved remarkable rate-distortion (RD) performance, yet their high computational complexity severely limits practical deployment. To overcome this challenge, we propose a novel Stage-wise Modular Distillation framework, SMoDi, which efficiently compresses LIC models while preserving RD performance. This framework treats each stage of LIC models as an independent sub-task, mirroring the teacher model's task decomposition to the student, thereby simplifying knowledge transfer. We identify two crucial factors determining the effectiveness of knowledge distillation: student model construction and loss function design. Specifically, we first propose Teacher-Guided Student Model Construction, a pruning-like method ensuring architectural consistency between teacher and student models. Next, we introduce Implicit End-to-end Supervision, facilitating adaptive energy compaction and bitrate regularization. Based on these insights, we develop KDIC, a lightweight student model derived from the state-of-the-art S2CFormer model. Experimental results demonstrate that KDIC achieves top-tier RD performance with significantly reduced computational complexity. To our knowledge, this work is among the first successful applications of knowledge distillation to learned image compression.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Knowledge_Distillation_for_Learned_Image_Compression_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Knowledge_Distillation_for_Learned_Image_Compression_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 7
      },
      "raw_excerpt": "Knowledge Distillation for Learned Image Compression [PDF 11 ] [Copy] [Kimi 7 ] [REL] Authors : Yunuo Chen , Zezheng Lyu , Bing He , Ning Cao , Gang Chen , Guo Lu , Wenjun Zhang Recently, learned image compression (LIC) models have achieved remarkable rate-distortion (RD) performance, yet their high computational complexity severely limits practical deployment. To overcome this challenge, we propose a novel Stage-wise Modular Distillation framework, SMoDi, which efficiently compresses LIC models while preserving RD performance. This framework treats each stage of LIC models as an independent sub-task, mirroring the teacher model's task decomposition to the student, thereby simplifying knowledge transfer. We identify two crucial factors determining the effectiveness of knowledge distillation: student model construction and loss function design. Specifically, we first propose Teacher-Guided Student Model Construction, a pruning-like method ensuring architectural consistency between teacher and student models. Next, we introduce Implicit End-to-end Supervision, facilitating adaptive energy compaction and bitrate regularization. Based on these insights, we develop KDIC, a lightweight student model derived from the state-of-the-art S2CFormer model. Experimental results demonstrate that KDIC achieves top-tier RD performance with significantly reduced computational complexity. To our knowledge, this work is among the first successful applications of knowledge distillation to learned image compression. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF",
      "index": 34,
      "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
      "authors": [
        "Uranik Berisha",
        "Jens Mehnert",
        "Alexandru Paul Condurache"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "pruning",
        "compressing",
        "structured",
        "trained",
        "transfomers",
        "retraining",
        "epochs",
        "variance",
        "networks",
        "simultaneously"
      ],
      "summary": "Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are then used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44 times.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 3
      },
      "raw_excerpt": "Variance-Based Pruning for Accelerating and Compressing Trained Networks [PDF 12 ] [Copy] [Kimi 3 ] [REL] Authors : Uranik Berisha , Jens Mehnert , Alexandru Paul Condurache Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are then used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44 times. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF",
      "index": 35,
      "title": "RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model",
      "authors": [
        "Huiyang Hu",
        "Peijin Wang",
        "Hanbo Bi",
        "Boyuan Tong",
        "Zhaozhi Wang",
        "Wenhui Diao",
        "Hao Chang",
        "Yingchao Feng",
        "Ziqi Zhang",
        "Yaowei Wang",
        "Qixiang Ye",
        "Kun Fu",
        "Xian Sun"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "vheat",
        "remote",
        "sensing",
        "heat",
        "conduction",
        "foundation",
        "improves",
        "across",
        "tasks",
        "computational"
      ],
      "summary": "Remote sensing foundation models largely break away from the traditional paradigm of designing task-specific models, offering greater scalability across multiple tasks. However, they face challenges such as low computational efficiency and limited interpretability, especially when dealing with large-scale remote sensing images. To overcome these, we draw inspiration from heat conduction, a physical process modeling local heat diffusion. Building on this idea, we are the first to explore the potential of using the parallel computing model of heat conduction to simulate the local region correlations in high-resolution remote sensing images, and introduce RS-vHeat, an efficient multi-modal remote sensing foundation model. Specifically, RS-vHeat 1) applies the Heat Conduction Operator (HCO) with a complexity of O(N^ 1.5 ) and a global receptive field, reducing computational overhead while capturing remote sensing object structure information to guide heat diffusion; 2) learns the frequency distribution representations of various scenes through a self-supervised strategy based on frequency domain hierarchical masking and multi-domain reconstruction; 3) significantly improves efficiency and performance over state-of-the-art techniques across 4 tasks and 10 datasets. Compared to attention-based remote sensing foundation models, we reduce memory usage by 84%, FLOPs by 24% and improves throughput by 2.7 times. The code will be made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 2
      },
      "raw_excerpt": "RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model [PDF 11 ] [Copy] [Kimi 2 ] [REL] Authors : Huiyang Hu , Peijin Wang , Hanbo Bi , Boyuan Tong , Zhaozhi Wang , Wenhui Diao , Hao Chang , Yingchao Feng , Ziqi Zhang , Yaowei Wang , Qixiang Ye , Kun Fu , Xian Sun Remote sensing foundation models largely break away from the traditional paradigm of designing task-specific models, offering greater scalability across multiple tasks. However, they face challenges such as low computational efficiency and limited interpretability, especially when dealing with large-scale remote sensing images. To overcome these, we draw inspiration from heat conduction, a physical process modeling local heat diffusion. Building on this idea, we are the first to explore the potential of using the parallel computing model of heat conduction to simulate the local region correlations in high-resolution remote sensing images, and introduce RS-vHeat, an efficient multi-modal remote sensing foundation model. Specifically, RS-vHeat 1) applies the Heat Conduction Operator (HCO) with a complexity of O(N^ 1.5 ) and a global receptive field, reducing computational overhead while capturing remote sensing object structure information to guide heat diffusion; 2) learns the frequency distribution representations of various scenes through a self-supervised strategy based on frequency domain hierarchical masking and multi-domain reconstruction; 3) significantly improves efficiency and performance over state-of-the-art techniques across 4 tasks and 10 datasets. Compared to attention-based remote sensing foundation models, we reduce memory usage by 84%, FLOPs by 24% and improves throughput by 2.7 times. The code will be made publicly available. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF",
      "index": 36,
      "title": "Understanding Co-speech Gestures in-the-wild",
      "authors": [
        "Sindhu B Hegde",
        "K R Prajwal",
        "Taein Kwon",
        "Andrew Zisserman"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "gesture",
        "speech",
        "gestures",
        "wild",
        "tri",
        "gestured",
        "text",
        "understanding",
        "word",
        "modal"
      ],
      "summary": "Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-speech-text associations: (i) gesture based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal video-gesture-speech-text representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs). Further analysis reveals that speech and text modalities capture distinct gesture related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/ vgg/research/jegal.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Hegde_Understanding_Co-speech_Gestures_in-the-wild_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Hegde_Understanding_Co-speech_Gestures_in-the-wild_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Understanding Co-speech Gestures in-the-wild [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Sindhu B Hegde , K R Prajwal , Taein Kwon , Andrew Zisserman Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-speech-text associations: (i) gesture based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal video-gesture-speech-text representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs). Further analysis reveals that speech and text modalities capture distinct gesture related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/ vgg/research/jegal. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF",
      "index": 37,
      "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior",
      "authors": [
        "Junzhe Lu",
        "Jing Lin",
        "Hongkun Dou",
        "Ailing Zeng",
        "Yue Deng",
        "Xian Liu",
        "Zhongang Cai",
        "Lei Yang",
        "Yulun Zhang",
        "Haoqian Wang",
        "Ziwei Liu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "dposer",
        "pose",
        "body",
        "whole",
        "human",
        "prior",
        "diffusion",
        "poses",
        "modeling",
        "model"
      ],
      "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling.Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions.Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Junzhe Lu , Jing Lin , Hongkun Dou , Ailing Zeng , Yue Deng , Xian Liu , Zhongang Cai , Lei Yang , Yulun Zhang , Haoqian Wang , Ziwei Liu We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling.Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions.Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF",
      "index": 38,
      "title": "Teeth Reconstruction and Performance Capture Using a Phone Camera",
      "authors": [
        "Weixi Zheng",
        "Jingwang Ling",
        "Zhibo Wang",
        "Quan Wang",
        "Feng Xu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "teeth",
        "facial",
        "dental",
        "capture",
        "phone",
        "reconstruction",
        "camera",
        "rendering",
        "democratizes",
        "performance"
      ],
      "summary": "We present the first method for personalized dental shape reconstruction and teeth-inclusive facial performance capture using only a single phone camera. Our approach democratizes high-quality facial avatars through a non-invasive, low-cost setup by addressing the ill-posed monocular capture problem with an analysis-by-synthesis approach. We introduce a representation adaptation technique that maintains both mesh and SDF representations of teeth, enabling efficient differentiable rendering while preventing teeth-lip interpenetration. To overcome alignment challenges with similar-appearing dental components, we leverage foundation models for semantic teeth segmentation and design specialized optimization objectives. Our method addresses the challenging occlusions of teeth during facial performance through optimization strategies that leverage facial structural priors, while our semantic mask rendering loss with optimal transport-based matching ensures convergence despite significant variations in initial positioning.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Teeth Reconstruction and Performance Capture Using a Phone Camera [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Weixi Zheng , Jingwang Ling , Zhibo Wang , Quan Wang , Feng Xu We present the first method for personalized dental shape reconstruction and teeth-inclusive facial performance capture using only a single phone camera. Our approach democratizes high-quality facial avatars through a non-invasive, low-cost setup by addressing the ill-posed monocular capture problem with an analysis-by-synthesis approach. We introduce a representation adaptation technique that maintains both mesh and SDF representations of teeth, enabling efficient differentiable rendering while preventing teeth-lip interpenetration. To overcome alignment challenges with similar-appearing dental components, we leverage foundation models for semantic teeth segmentation and design specialized optimization objectives. Our method addresses the challenging occlusions of teeth during facial performance through optimization strategies that leverage facial structural priors, while our semantic mask rendering loss with optimal transport-based matching ensures convergence despite significant variations in initial positioning. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF",
      "index": 39,
      "title": "Certifiably Optimal Anisotropic Rotation Averaging",
      "authors": [
        "Carl Olsson",
        "Yaroslava Lochman",
        "Johan Malmport",
        "Christopher Zach"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "averaging",
        "certifiably",
        "anisotropic",
        "rotation",
        "incorporated",
        "isotropic",
        "uncertainties",
        "subproblem",
        "setting",
        "optimization"
      ],
      "summary": "Rotation averaging is a key subproblem in applications of computer vision and robotics. Many methods for solving this problem exist, and there are also several theoretical results analyzing difficulty and optimality. However, one aspect that most of these have in common is a focus on the isotropic setting, where the intrinsic uncertainties in the measurements are not fully incorporated into the resulting optimization task. Recent empirical results suggest that moving to an anisotropic framework, where these uncertainties are explicitly included, can result in an improvement of solution quality. However, global optimization for rotation averaging has remained a challenge in this scenario.In this paper we show how anisotropic costs can be incorporated in certifiably optimal rotation averaging. We also demonstrate how existing solvers, designed for isotropic situations, fail in the anisotropic setting. Finally, we propose a stronger relaxation and show empirically that it is able to recover global optima in all tested datasets and leads to a more accurate reconstruction in all but one of the scenes.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Certifiably Optimal Anisotropic Rotation Averaging [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Carl Olsson , Yaroslava Lochman , Johan Malmport , Christopher Zach Rotation averaging is a key subproblem in applications of computer vision and robotics. Many methods for solving this problem exist, and there are also several theoretical results analyzing difficulty and optimality. However, one aspect that most of these have in common is a focus on the isotropic setting, where the intrinsic uncertainties in the measurements are not fully incorporated into the resulting optimization task. Recent empirical results suggest that moving to an anisotropic framework, where these uncertainties are explicitly included, can result in an improvement of solution quality. However, global optimization for rotation averaging has remained a challenge in this scenario.In this paper we show how anisotropic costs can be incorporated in certifiably optimal rotation averaging. We also demonstrate how existing solvers, designed for isotropic situations, fail in the anisotropic setting. Finally, we propose a stronger relaxation and show empirically that it is able to recover global optima in all tested datasets and leads to a more accurate reconstruction in all but one of the scenes. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF",
      "index": 40,
      "title": "MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration",
      "authors": [
        "George Ciubotariu",
        "Zhuyun Zhou",
        "Zongwei Wu",
        "Radu Timofte"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "miore",
        "var",
        "restoration",
        "motion",
        "blur",
        "benchmarks",
        "push",
        "frame",
        "video",
        "truths"
      ],
      "summary": "We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address critical limitations in current motion restoration benchmarks. Designed with high-frame-rate (1000 FPS) acquisition and professional-grade optics, our datasets capture a broad spectrum of motion scenarios, which include complex ego-camera movements, dynamic multi-subject interactions, and depth-dependent blur effects. By adaptively averaging frames based on computed optical flow metrics, MIORe generates consistent motion blur, and preserves sharp inputs for video frame interpolation and optical flow estimation. VAR-MIORe further extends by spanning a variable range of motion magnitudes, from minimal to extreme, establishing the first benchmark to offer explicit control over motion amplitude. We provide high-resolution, scalable ground truths that challenge existing algorithms under both controlled and adverse conditions, paving the way for next-generation research of various image and video restoration tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : George Ciubotariu , Zhuyun Zhou , Zongwei Wu , Radu Timofte We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address critical limitations in current motion restoration benchmarks. Designed with high-frame-rate (1000 FPS) acquisition and professional-grade optics, our datasets capture a broad spectrum of motion scenarios, which include complex ego-camera movements, dynamic multi-subject interactions, and depth-dependent blur effects. By adaptively averaging frames based on computed optical flow metrics, MIORe generates consistent motion blur, and preserves sharp inputs for video frame interpolation and optical flow estimation. VAR-MIORe further extends by spanning a variable range of motion magnitudes, from minimal to extreme, establishing the first benchmark to offer explicit control over motion amplitude. We provide high-resolution, scalable ground truths that challenge existing algorithms under both controlled and adverse conditions, paving the way for next-generation research of various image and video restoration tasks. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF",
      "index": 41,
      "title": "MikuDance: Animating Character Art with Mixed Motion Dynamics",
      "authors": [
        "Jiaxu Zhang",
        "Xianfang Zeng",
        "Xin Chen",
        "Wei Zuo",
        "Gang Yu",
        "Zhigang Tu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "motion",
        "mikudance",
        "character",
        "mixed",
        "guidance",
        "art",
        "animation",
        "animating",
        "scene",
        "diffusion"
      ],
      "summary": "We propose MikuDance, a diffusion-based pipeline incorporating mixed motion dynamics to animate stylized character art. MikuDance consists of two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the challenges of high-dynamic motion and reference-guidance misalignment in character art animation. Specifically, a Scene Motion Tracking strategy is presented to explicitly model the dynamic camera in pixel-wise space, enabling unified character-scene motion modeling. Building on this, the Mixed-Control Diffusion implicitly aligns the scale and body shape of diverse characters with motion guidance, allowing flexible control of local character motion. Subsequently, a Motion-Adaptive Normalization module is incorporated to effectively inject global scene motion, paving the way for comprehensive character art animation. Through extensive experiments, we demonstrate the effectiveness and generalizability of MikuDance across various character art and motion guidance, consistently producing high-quality animations with remarkable motion dynamics.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 2
      },
      "raw_excerpt": "MikuDance: Animating Character Art with Mixed Motion Dynamics [PDF 10 ] [Copy] [Kimi 2 ] [REL] Authors : Jiaxu Zhang , Xianfang Zeng , Xin Chen , Wei Zuo , Gang Yu , Zhigang Tu We propose MikuDance, a diffusion-based pipeline incorporating mixed motion dynamics to animate stylized character art. MikuDance consists of two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the challenges of high-dynamic motion and reference-guidance misalignment in character art animation. Specifically, a Scene Motion Tracking strategy is presented to explicitly model the dynamic camera in pixel-wise space, enabling unified character-scene motion modeling. Building on this, the Mixed-Control Diffusion implicitly aligns the scale and body shape of diverse characters with motion guidance, allowing flexible control of local character motion. Subsequently, a Motion-Adaptive Normalization module is incorporated to effectively inject global scene motion, paving the way for comprehensive character art animation. Through extensive experiments, we demonstrate the effectiveness and generalizability of MikuDance across various character art and motion guidance, consistently producing high-quality animations with remarkable motion dynamics. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF",
      "index": 42,
      "title": "ROAR: Reducing Inversion Error in Generative Image Watermarking",
      "authors": [
        "Hanyi Wang",
        "Han Fang",
        "Shi-Lin Wang",
        "Ee-Chien Chang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "inversion",
        "roar",
        "errors",
        "watermarking",
        "latent",
        "watermark",
        "generative",
        "domain",
        "embedding",
        "distortions"
      ],
      "summary": "Generative image watermarking enables the proactive detection and traceability of generated images. Among existing methods, inversion-based frameworks achieve highly conceal ed watermark embedding by injecting watermarks into the latent representation before the diffusion process. The robustness of this approach hinges on both the embedding mechanism and inversion accuracy. However, prior works have predominantly focused on optimizing the embedding process while overlooking inversion errors, which significantly affect extraction fidelity. In this paper, we address the challenge of inversion errors and propose ROAR, a dual-domain optimization-based framework designed to mitigate errors arising from two key sources: 1) Latent-domain errors, which accumulate across inversion steps due to inherent approximation assumptions. 2) Pixel-domain errors, which result from channel distortions such as JPEG compression. To tackle these issues, we introduce two novel components: A Regeneration-based Optimization (RO) mechanism, which incorporates an optimizable starting latent to minimize latent-domain errors; A Mixture of Experts (MoE)-based distortion-adaptive restoration (AR) network, which effectively recovers watermarked distributions from pixel-level distortions.Extensive experiments demonstrate that ROAR significantly reduces inversion errors and enhances watermark extraction robustness, thereby improving the reliability of generative image watermarking.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 3
      },
      "raw_excerpt": "ROAR: Reducing Inversion Error in Generative Image Watermarking [PDF 9 ] [Copy] [Kimi 3 ] [REL] Authors : Hanyi Wang , Han Fang , Shi-Lin Wang , Ee-Chien Chang Generative image watermarking enables the proactive detection and traceability of generated images. Among existing methods, inversion-based frameworks achieve highly conceal ed watermark embedding by injecting watermarks into the latent representation before the diffusion process. The robustness of this approach hinges on both the embedding mechanism and inversion accuracy. However, prior works have predominantly focused on optimizing the embedding process while overlooking inversion errors, which significantly affect extraction fidelity. In this paper, we address the challenge of inversion errors and propose ROAR, a dual-domain optimization-based framework designed to mitigate errors arising from two key sources: 1) Latent-domain errors, which accumulate across inversion steps due to inherent approximation assumptions. 2) Pixel-domain errors, which result from channel distortions such as JPEG compression. To tackle these issues, we introduce two novel components: A Regeneration-based Optimization (RO) mechanism, which incorporates an optimizable starting latent to minimize latent-domain errors; A Mixture of Experts (MoE)-based distortion-adaptive restoration (AR) network, which effectively recovers watermarked distributions from pixel-level distortions.Extensive experiments demonstrate that ROAR significantly reduces inversion errors and enhances watermark extraction robustness, thereby improving the reliability of generative image watermarking. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF",
      "index": 43,
      "title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution",
      "authors": [
        "Peng Du",
        "Hui Li",
        "Han Xu",
        "Paul Barom Jeon",
        "Dongwook Lee",
        "Daehyun Ji",
        "Ran Yang",
        "Feng Zhu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "wavelet",
        "frequency",
        "dwt",
        "dtwsr",
        "interrelations",
        "transformer",
        "image",
        "bands",
        "mdwt",
        "diffusion"
      ],
      "summary": "Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image super-resolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multi-scale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multi-scale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform (MDWT) to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in low-frequency (LF) and high-frequency (HF) sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 6
      },
      "raw_excerpt": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution [PDF 20 ] [Copy] [Kimi 6 ] [REL] Authors : Peng Du , Hui Li , Han Xu , Paul Barom Jeon , Dongwook Lee , Daehyun Ji , Ran Yang , Feng Zhu Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image super-resolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multi-scale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multi-scale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform (MDWT) to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in low-frequency (LF) and high-frequency (HF) sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF",
      "index": 44,
      "title": "LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer",
      "authors": [
        "Yiren Song",
        "Danze Chen",
        "Mike Zheng Shou"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "layertracer",
        "svg",
        "svgs",
        "layered",
        "dit",
        "vectorization",
        "cognitive",
        "aligned",
        "rasterized",
        "blueprints"
      ],
      "summary": "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a DiT based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments show that LayerTracer surpasses optimization-based and neural baselines in generation quality and editability.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Yiren Song , Danze Chen , Mike Zheng Shou Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a DiT based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments show that LayerTracer surpasses optimization-based and neural baselines in generation quality and editability. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Qin_Spatially-Varying_Autofocus@ICCV2025@CVF",
      "index": 45,
      "title": "Spatially-Varying Autofocus",
      "authors": [
        "Yingsi Qin",
        "Aswin C. Sankaranarayanan",
        "Matthew O'Toole"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "depth",
        "lens",
        "focus",
        "defocus",
        "spatially",
        "autofocus",
        "scene",
        "lohmann",
        "precept",
        "swathe"
      ],
      "summary": "A lens brings a single plane into focus on a planar sensor; hence, parts of the scene that are outside this planar focus plane are resolved on the sensor under defocus. Can we break this precept by enabling a \"lens\" that can change its depth-of-field arbitrarily? This work investigates the design and implementation of such a computational lens with spatially-selective focusing. Our design uses an optical arrangement of a Lohmann lens and a phase-only spatial light modulator to allow each pixel to focus at a different depth. We extend classical techniques used in autofocusing to the spatially-varying scenario where the depth map is iteratively estimated using contrast and disparity cues, enabling the camera to progressively shape its depth-of-field to the scene's depth. By obtaining an optical all-in-focus image, our technique advances upon a broad swathe of prior work ranging from depth-from-focus/defocus to coded aperture techniques in two key aspects: the ability to bring an entire scene in focus simultaneously, and the ability to maintain the highest possible spatial resolution.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Spatially-Varying_Autofocus_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Qin_Spatially-Varying_Autofocus@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Spatially-Varying_Autofocus_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Spatially-Varying Autofocus [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Yingsi Qin , Aswin C. Sankaranarayanan , Matthew O'Toole A lens brings a single plane into focus on a planar sensor; hence, parts of the scene that are outside this planar focus plane are resolved on the sensor under defocus. Can we break this precept by enabling a \"lens\" that can change its depth-of-field arbitrarily? This work investigates the design and implementation of such a computational lens with spatially-selective focusing. Our design uses an optical arrangement of a Lohmann lens and a phase-only spatial light modulator to allow each pixel to focus at a different depth. We extend classical techniques used in autofocusing to the spatially-varying scenario where the depth map is iteratively estimated using contrast and disparity cues, enabling the camera to progressively shape its depth-of-field to the scene's depth. By obtaining an optical all-in-focus image, our technique advances upon a broad swathe of prior work ranging from depth-from-focus/defocus to coded aperture techniques in two key aspects: the ability to bring an entire scene in focus simultaneously, and the ability to maintain the highest possible spatial resolution. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF",
      "index": 46,
      "title": "Event-based Visual Vibrometry",
      "authors": [
        "Xinyu Zhou",
        "Peiqi Duan",
        "Yeliduosi Xiaokaiti",
        "Chao Xu",
        "Boxin Shi"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "vibrometry",
        "event",
        "visual",
        "vibration",
        "based",
        "camera",
        "lighting",
        "audio",
        "speed",
        "high"
      ],
      "summary": "Visual vibrometry has emerged as a powerful technique for remote acquisition of audio and the physical properties of materials. To capture high-frequency vibrations, frame-based approaches often require a high-speed video camera and bright lighting to compensate for the short exposure time. In this paper, we introduce event-based visual vibrometry, a new high-speed visual vibration sensing method using an event camera. By leveraging the high temporal resolution and low bandwidth characteristics of event cameras, event-based visual vibrometry enables high-speed vibration sensing under ambient lighting conditions with improved data efficiency. Specifically, we leverage a hybrid camera system and propose an event-based subtle motion estimation framework that integrates an optimization-based approach based on the event generation model and a motion refinement network. We demonstrate our method by capturing vibration caused by audio sources and estimating material properties for various objects.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Event-based_Visual_Vibrometry_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Event-based_Visual_Vibrometry_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 6
      },
      "raw_excerpt": "Event-based Visual Vibrometry [PDF 15 ] [Copy] [Kimi 6 ] [REL] Authors : Xinyu Zhou , Peiqi Duan , Yeliduosi Xiaokaiti , Chao Xu , Boxin Shi Visual vibrometry has emerged as a powerful technique for remote acquisition of audio and the physical properties of materials. To capture high-frequency vibrations, frame-based approaches often require a high-speed video camera and bright lighting to compensate for the short exposure time. In this paper, we introduce event-based visual vibrometry, a new high-speed visual vibration sensing method using an event camera. By leveraging the high temporal resolution and low bandwidth characteristics of event cameras, event-based visual vibrometry enables high-speed vibration sensing under ambient lighting conditions with improved data efficiency. Specifically, we leverage a hybrid camera system and propose an event-based subtle motion estimation framework that integrates an optimization-based approach based on the event generation model and a motion refinement network. We demonstrate our method by capturing vibration caused by audio sources and estimating material properties for various objects. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF",
      "index": 47,
      "title": "SuperDec: 3D Scene Decomposition with Superquadrics Primitives",
      "authors": [
        "Elisabetta Fedele",
        "Boyang Sun",
        "Leonidas Guibas",
        "Marc Pollefeys",
        "Francis Engelmann"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "superquadrics",
        "superdec",
        "primitives",
        "scene",
        "compact",
        "leverage",
        "scenes",
        "capabilities",
        "shapenet",
        "scannet"
      ],
      "summary": "We present SuperDec, an approach for compact 3D scene representations based on geometric primitives, namely superquadrics.While most recent works leverage geometric primitives to obtain photorealistic 3D scene representations, we propose to leverage them to obtain a compact yet expressive representation. We propose to solve the problem locally on individual objects and leverage the capabilities of instance segmentation methods to scale our solution to full 3D scenes. In doing that, we design a new architecture which efficiently decompose point clouds of arbitrary objects in a compact set of superquadrics. We train our architecture on ShapeNet and we prove its generalization capabilities on object instances extracted from the ScanNet++ dataset as well as on full Replica scenes. Finally, we show how a compact representation based on superquadrics can be useful for a diverse range of downstream applications, including robotic tasks and controllable visual content generation and editing.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "SuperDec: 3D Scene Decomposition with Superquadrics Primitives [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Elisabetta Fedele , Boyang Sun , Leonidas Guibas , Marc Pollefeys , Francis Engelmann We present SuperDec, an approach for compact 3D scene representations based on geometric primitives, namely superquadrics.While most recent works leverage geometric primitives to obtain photorealistic 3D scene representations, we propose to leverage them to obtain a compact yet expressive representation. We propose to solve the problem locally on individual objects and leverage the capabilities of instance segmentation methods to scale our solution to full 3D scenes. In doing that, we design a new architecture which efficiently decompose point clouds of arbitrary objects in a compact set of superquadrics. We train our architecture on ShapeNet and we prove its generalization capabilities on object instances extracted from the ScanNet++ dataset as well as on full Replica scenes. Finally, we show how a compact representation based on superquadrics can be useful for a diverse range of downstream applications, including robotic tasks and controllable visual content generation and editing. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF",
      "index": 48,
      "title": "Online Reasoning Video Segmentation with Just-in-Time Digital Twins",
      "authors": [
        "Yiqing Shen",
        "Bohan Liu",
        "Chenjia Li",
        "Lalithkumar Seenivasan",
        "Mathias Unberath"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "reasoning",
        "video",
        "llm",
        "twin",
        "digital",
        "queries",
        "perception",
        "specialist",
        "online",
        "segmentation"
      ],
      "summary": "Reasoning segmentation (RS) aims to identify and segment objects of interest based on implicit text queries. As such, RS is a catalyst for embodied AI agents, enabling them to interpret high-level commands without requiring explicit step-by-step guidance. However, current RS approaches rely heavily on the visual perception capabilities of multimodal large language models (LLMs), leading to several major limitations. First, they struggle with queries that require multiple steps of reasoning or those that involve complex spatial/temporal relationships. Second, they necessitate LLM fine-tuning, which may require frequent updates to maintain compatibility with contemporary LLMs and may increase risks of catastrophic forgetting during fine-tuning. Finally, being primarily designed for static images or offline video processing, they scale poorly to online video data. To address these limitations, we propose an agent framework that disentangles perception and reasoning for online video RS without LLM fine-tuning. Our innovation is the introduction of a just-in-time digital twin concept, where -- given an implicit query -- an LLM plans the construction of a low-level scene representation from high-level video using specialist vision models. We refer to this approach to creating a digital twin as \"just-in-time\" because the LLM planner will anticipate the need for specific information and only request this limited subset instead of always evaluating every specialist model. The LLM then performs reasoning on this digital twin representation to identify target objects. To evaluate our approach, we introduce a new comprehensive video reasoning segmentation benchmark comprising 200 videos with 895 implicit text queries. The benchmark spans three reasoning categories (semantic, spatial, and temporal) with three different reasoning chain complexity. Experimental results demonstrate that our method performs best across all reasoning categories, suggesting that our just-in-time digital twin can bridge the gap between high-level reasoning and low-level perception in embodied AI. Benchmark is available at https://github.com/yiqings/jitbench/.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins_ICCV_2025_paper.html",
          "/venue/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Oral",
          "/venue/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 7
      },
      "raw_excerpt": "Online Reasoning Video Segmentation with Just-in-Time Digital Twins [PDF 7 ] [Copy] [Kimi 7 ] [REL] Authors : Yiqing Shen , Bohan Liu , Chenjia Li , Lalithkumar Seenivasan , Mathias Unberath Reasoning segmentation (RS) aims to identify and segment objects of interest based on implicit text queries. As such, RS is a catalyst for embodied AI agents, enabling them to interpret high-level commands without requiring explicit step-by-step guidance. However, current RS approaches rely heavily on the visual perception capabilities of multimodal large language models (LLMs), leading to several major limitations. First, they struggle with queries that require multiple steps of reasoning or those that involve complex spatial/temporal relationships. Second, they necessitate LLM fine-tuning, which may require frequent updates to maintain compatibility with contemporary LLMs and may increase risks of catastrophic forgetting during fine-tuning. Finally, being primarily designed for static images or offline video processing, they scale poorly to online video data. To address these limitations, we propose an agent framework that disentangles perception and reasoning for online video RS without LLM fine-tuning. Our innovation is the introduction of a just-in-time digital twin concept, where -- given an implicit query -- an LLM plans the construction of a low-level scene representation from high-level video using specialist vision models. We refer to this approach to creating a digital twin as \"just-in-time\" because the LLM planner will anticipate the need for specific information and only request this limited subset instead of always evaluating every specialist model. The LLM then performs reasoning on this digital twin representation to identify target objects. To evaluate our approach, we introduce a new comprehensive video reasoning segmentation benchmark comprising 200 videos with 895 implicit text queries. The benchmark spans three reasoning categories (semantic, spatial, and temporal) with three different reasoning chain complexity. Experimental results demonstrate that our method performs best across all reasoning categories, suggesting that our just-in-time digital twin can bridge the gap between high-level reasoning and low-level perception in embodied AI. Benchmark is available at https://github.com/yiqings/jitbench/. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF",
      "index": 49,
      "title": "Towards Foundational Models for Single-Chip Radar",
      "authors": [
        "Tianshu Huang",
        "Akarsh Prabhakara",
        "Chuhan Chen",
        "Jay Karhade",
        "Deva Ramanan",
        "Matthew O'toole",
        "Anthony Rowe"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "radar",
        "foundational",
        "grt",
        "chip",
        "radars",
        "inexpensive",
        "mmwave",
        "raw",
        "hours",
        "single"
      ],
      "summary": "mmWave radars are compact, inexpensive, and durable sensors that are robust to occlusions and work regardless of environmental conditions, such as weather and darkness. However, this comes at the cost of poor angular resolution, especially for inexpensive single-chip radars, which are typically used in automotive and indoor sensing applications. Although many have proposed learning-based methods to mitigate this weakness, no standardized foundational models or large datasets for the mmWave radar have emerged, and practitioners have largely trained task-specific models from scratch using relatively small datasets.In this paper, we collect (to our knowledge) the largest available raw radar dataset with 1M samples (29 hours) and train a foundational model for 4D single-chip radar, which can predict 3D occupancy and semantic segmentation with quality that is typically only possible with much higher resolution sensors. We demonstrate that our Generalizable Radar Transformer (GRT) generalizes across diverse settings, can be fine-tuned for different tasks, and shows logarithmic data scaling of 20% per 10xdata. We also run extensive ablations on common design decisions, and find that using raw radar data significantly outperforms widely-used lossy representations, equivalent to a 10xincrease in training data. Finally, we estimate a total data requirement of ~100M samples (3000 hours) to fully exploit the potential of GRT.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Towards_Foundational_Models_for_Single-Chip_Radar_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Towards_Foundational_Models_for_Single-Chip_Radar_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "Towards Foundational Models for Single-Chip Radar [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Tianshu Huang , Akarsh Prabhakara , Chuhan Chen , Jay Karhade , Deva Ramanan , Matthew O'toole , Anthony Rowe mmWave radars are compact, inexpensive, and durable sensors that are robust to occlusions and work regardless of environmental conditions, such as weather and darkness. However, this comes at the cost of poor angular resolution, especially for inexpensive single-chip radars, which are typically used in automotive and indoor sensing applications. Although many have proposed learning-based methods to mitigate this weakness, no standardized foundational models or large datasets for the mmWave radar have emerged, and practitioners have largely trained task-specific models from scratch using relatively small datasets.In this paper, we collect (to our knowledge) the largest available raw radar dataset with 1M samples (29 hours) and train a foundational model for 4D single-chip radar, which can predict 3D occupancy and semantic segmentation with quality that is typically only possible with much higher resolution sensors. We demonstrate that our Generalizable Radar Transformer (GRT) generalizes across diverse settings, can be fine-tuned for different tasks, and shows logarithmic data scaling of 20% per 10xdata. We also run extensive ablations on common design decisions, and find that using raw radar data significantly outperforms widely-used lossy representations, equivalent to a 10xincrease in training data. Finally, we estimate a total data requirement of ~100M samples (3000 hours) to fully exploit the potential of GRT. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF",
      "index": 50,
      "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining",
      "authors": [
        "Yue Li",
        "Qi Ma",
        "Runyi Yang",
        "Huapeng Li",
        "Mengjiao Ma",
        "Bin Ren",
        "Nikola Popovic",
        "Nicu Sebe",
        "Ender Konukoglu",
        "Theo Gevers",
        "Luc Van Gool",
        "Martin R. Oswald",
        "Danda Pani Paudel"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "scenesplat",
        "3dgs",
        "scenes",
        "scene",
        "indoor",
        "splatting",
        "understanding",
        "6868",
        "vision",
        "gpu"
      ],
      "summary": "Recognizing arbitrary or previously unseen categories is essential for comprehensive real-world 3D scene understanding. Currently, all existing methods rely on 2D or textual modalities during training, or together at inference. This highlights a clear absence of a model capable of processing 3D data alone for learning semantics end-to-end, along with the necessary data to train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the de facto standard for 3D scene representation across various vision tasks. However, effectively integrating semantic reasoning into 3DGS in a generalizable fashion remains an open challenge.To address these limitations we introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene understanding approach that operates natively on 3DGS. Furthermore, we propose a self-supervised learning scheme that unlocks rich 3D feature learning from unlabeled scenes. In order to power the proposed methods, we introduce SceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising of 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D, etc. Generating SceneSplat-7K required computational resources equivalent to 119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning for indoor scenes.Our exhaustive experiments on SceneSplat-7K demonstrate the significant benefit of the proposed methods over the established baselines. Our code, model, and datasets will be released to facilitate further research.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 3
      },
      "raw_excerpt": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining [PDF 9 ] [Copy] [Kimi 3 ] [REL] Authors : Yue Li , Qi Ma , Runyi Yang , Huapeng Li , Mengjiao Ma , Bin Ren , Nikola Popovic , Nicu Sebe , Ender Konukoglu , Theo Gevers , Luc Van Gool , Martin R. Oswald , Danda Pani Paudel Recognizing arbitrary or previously unseen categories is essential for comprehensive real-world 3D scene understanding. Currently, all existing methods rely on 2D or textual modalities during training, or together at inference. This highlights a clear absence of a model capable of processing 3D data alone for learning semantics end-to-end, along with the necessary data to train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the de facto standard for 3D scene representation across various vision tasks. However, effectively integrating semantic reasoning into 3DGS in a generalizable fashion remains an open challenge.To address these limitations we introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene understanding approach that operates natively on 3DGS. Furthermore, we propose a self-supervised learning scheme that unlocks rich 3D feature learning from unlabeled scenes. In order to power the proposed methods, we introduce SceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising of 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D, etc. Generating SceneSplat-7K required computational resources equivalent to 119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning for indoor scenes.Our exhaustive experiments on SceneSplat-7K demonstrate the significant benefit of the proposed methods over the established baselines. Our code, model, and datasets will be released to facilitate further research. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF",
      "index": 51,
      "title": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds",
      "authors": [
        "Binbin Xiang",
        "Maciej Wielgosz",
        "Stefano Puliti",
        "Kamil Král",
        "Martin Krůček",
        "Azim Missarov",
        "Rasmus Astrup"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "forestformer3d",
        "forest",
        "segmentation",
        "end",
        "instancev2",
        "lidar",
        "tree",
        "lautx",
        "wytham",
        "unified"
      ],
      "summary": "The segmentation of forest LiDAR 3D point clouds, including both individual tree and semantic segmentation, is fundamental for advancing forest management and ecological research. However, current approaches often struggle with the complexity and variability of natural forest environments. We present ForestFormer3D, a new unified and end-to-end framework designed for precise individual tree and semantic segmentation. ForestFormer3D incorporates ISA-guided query point selection, a score-based block merging strategy during inference, and a one-to-many association mechanism for effective training. By combining these new components, our model achieves state-of-the-art performance for individual tree segmentation on the newly introduced FOR-instanceV2 dataset, which spans diverse forest types and regions. Additionally, ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx), showcasing its robustness across different forest conditions and sensor modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly available at https://bxiang233.github.io/FF3D/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Binbin Xiang , Maciej Wielgosz , Stefano Puliti , Kamil Král , Martin Krůček , Azim Missarov , Rasmus Astrup The segmentation of forest LiDAR 3D point clouds, including both individual tree and semantic segmentation, is fundamental for advancing forest management and ecological research. However, current approaches often struggle with the complexity and variability of natural forest environments. We present ForestFormer3D, a new unified and end-to-end framework designed for precise individual tree and semantic segmentation. ForestFormer3D incorporates ISA-guided query point selection, a score-based block merging strategy during inference, and a one-to-many association mechanism for effective training. By combining these new components, our model achieves state-of-the-art performance for individual tree segmentation on the newly introduced FOR-instanceV2 dataset, which spans diverse forest types and regions. Additionally, ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx), showcasing its robustness across different forest conditions and sensor modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly available at https://bxiang233.github.io/FF3D/. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF",
      "index": 52,
      "title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation",
      "authors": [
        "Andrea Simonelli",
        "Norman Müller",
        "Peter Kontschieder"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "scannet",
        "easy3d",
        "interactive",
        "segmentation",
        "unseen",
        "environments",
        "consistently",
        "s3dis",
        "unfamiliar",
        "across"
      ],
      "summary": "The increasing availability of digital 3D environments, whether through image reconstruction, generation, or scans obtained via lasers or robots, is driving innovation across various fields. Among the numerous applications, there is a significant demand for those that enable 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and consistently perform well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as Gaussian Splatting.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Andrea Simonelli , Norman Müller , Peter Kontschieder The increasing availability of digital 3D environments, whether through image reconstruction, generation, or scans obtained via lasers or robots, is driving innovation across various fields. Among the numerous applications, there is a significant demand for those that enable 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and consistently perform well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as Gaussian Splatting. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF",
      "index": 53,
      "title": "Diffusion Image Prior",
      "authors": [
        "Hamadi Chihaoui",
        "Paolo Favaro"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "diip",
        "degradation",
        "dip",
        "prior",
        "image",
        "diffusion",
        "restoration",
        "waterdrop",
        "pretrained",
        "blind"
      ],
      "summary": "Zero-shot image restoration (IR) methods based on pretrained diffusion models have recently achieved significant success. These methods typically require at least a parametric form of the degradation model. However, in real-world scenarios, the degradation may be too complex to define explicitly without relying on crude approximations. To handle this general case, we introduce the DIffusion Image Prior (DIIP). We take inspiration from the Deep Image Prior (DIP), since it can be used to remove artifacts without the need for an explicit degradation model. However, in contrast to DIP, we find that pretrained diffusion models offer a much stronger prior, despite being trained without knowledge from corrupted data. We show that, the optimization process in DIIP first reconstructs a clean version of the image before eventually overfitting to the degraded input, but it does so for a broader range of degradations than DIP. In light of this result, we propose a blind image restoration (IR) method based on early stopping, which does not require prior knowledge of the degradation model. We validate DIIP on various degradation-blind IR tasks, including JPEG artifact removal, waterdrop removal, denoising and super-resolution with state-of-the-art results.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chihaoui_Diffusion_Image_Prior_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chihaoui_Diffusion_Image_Prior_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 8
      },
      "raw_excerpt": "Diffusion Image Prior [PDF 14 ] [Copy] [Kimi 8 ] [REL] Authors : Hamadi Chihaoui , Paolo Favaro Zero-shot image restoration (IR) methods based on pretrained diffusion models have recently achieved significant success. These methods typically require at least a parametric form of the degradation model. However, in real-world scenarios, the degradation may be too complex to define explicitly without relying on crude approximations. To handle this general case, we introduce the DIffusion Image Prior (DIIP). We take inspiration from the Deep Image Prior (DIP), since it can be used to remove artifacts without the need for an explicit degradation model. However, in contrast to DIP, we find that pretrained diffusion models offer a much stronger prior, despite being trained without knowledge from corrupted data. We show that, the optimization process in DIIP first reconstructs a clean version of the image before eventually overfitting to the degraded input, but it does so for a broader range of degradations than DIP. In light of this result, we propose a blind image restoration (IR) method based on early stopping, which does not require prior knowledge of the degradation model. We validate DIIP on various degradation-blind IR tasks, including JPEG artifact removal, waterdrop removal, denoising and super-resolution with state-of-the-art results. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF",
      "index": 54,
      "title": "Soft Local Completeness: Rethinking Completeness in XAI",
      "authors": [
        "Ziv Weiss Haddad",
        "Oren Barkan",
        "Yehonatan Elisha",
        "Noam Koenigstein"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "completeness",
        "attributions",
        "subregions",
        "faithful",
        "attribution",
        "soft",
        "rethinking",
        "xai",
        "contend",
        "standalone"
      ],
      "summary": "Completeness is a widely discussed property in explainability research, requiring that the attributions sum to the model's response to the input. While completeness intuitively suggests that the model's prediction is \"completely explained\" by the attributions, its global formulation alone is insufficient to ensure faithful explanations. We contend that promoting completeness locally within attribution subregions, in a soft manner, can serve as a standalone guiding principle for producing faithful attributions. To this end, we introduce the concept of the completeness gap as a flexible measure of completeness and propose an optimization procedure that minimizes this gap across subregions within the attribution map. Extensive evaluations across various model architectures demonstrate that our method produces state-of-the-art results.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 2
      },
      "raw_excerpt": "Soft Local Completeness: Rethinking Completeness in XAI [PDF 14 ] [Copy] [Kimi 2 ] [REL] Authors : Ziv Weiss Haddad , Oren Barkan , Yehonatan Elisha , Noam Koenigstein Completeness is a widely discussed property in explainability research, requiring that the attributions sum to the model's response to the input. While completeness intuitively suggests that the model's prediction is \"completely explained\" by the attributions, its global formulation alone is insufficient to ensure faithful explanations. We contend that promoting completeness locally within attribution subregions, in a soft manner, can serve as a standalone guiding principle for producing faithful attributions. To this end, we introduce the concept of the completeness gap as a flexible measure of completeness and propose an optimization procedure that minimizes this gap across subregions within the attribution map. Extensive evaluations across various model architectures demonstrate that our method produces state-of-the-art results. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Dumery_Counting_Stacked_Objects@ICCV2025@CVF",
      "index": 55,
      "title": "Counting Stacked Objects",
      "authors": [
        "Corentin Dumery",
        "Noa Etté",
        "Aoxiang Fan",
        "Ren Li",
        "Jingyi Xu",
        "Hieu Le",
        "Pascal Fua"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "counting",
        "stacked",
        "objects",
        "biomedicine",
        "wildlife",
        "object",
        "task",
        "underpinning",
        "world",
        "underexplored"
      ],
      "summary": "Visual object counting is a fundamental computer vision task underpinning numerous real-world applications, from cell counting in biomedicine to traffic and wildlife monitoring. However, existing methods struggle to handle the challenge of stacked 3D objects in which most objects are hidden by those above them. To address this important yet underexplored problem, we propose a novel 3D counting approach that decomposes the task into two complementary subproblems - estimating the 3D geometry of the object stack and the occupancy ratio from multi-view images. By combining geometric reconstruction and deep learning-based depth analysis, our method can accurately count identical objects within containers, even when they are irregularly stacked. We validate our 3D Counting pipeline on large-scale synthetic and diverse real-world datasets with manually verified total counts.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Dumery_Counting_Stacked_Objects_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Dumery_Counting_Stacked_Objects@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Dumery_Counting_Stacked_Objects_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Counting Stacked Objects [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Corentin Dumery , Noa Etté , Aoxiang Fan , Ren Li , Jingyi Xu , Hieu Le , Pascal Fua Visual object counting is a fundamental computer vision task underpinning numerous real-world applications, from cell counting in biomedicine to traffic and wildlife monitoring. However, existing methods struggle to handle the challenge of stacked 3D objects in which most objects are hidden by those above them. To address this important yet underexplored problem, we propose a novel 3D counting approach that decomposes the task into two complementary subproblems - estimating the 3D geometry of the object stack and the occupancy ratio from multi-view images. By combining geometric reconstruction and deep learning-based depth analysis, our method can accurately count identical objects within containers, even when they are irregularly stacked. We validate our 3D Counting pipeline on large-scale synthetic and diverse real-world datasets with manually verified total counts. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF",
      "index": 56,
      "title": "Deterministic Object Pose Confidence Region Estimation",
      "authors": [
        "Jinghao Wang",
        "Zhang Li",
        "Zi Wang",
        "Banglei Guan",
        "Yang Shang",
        "Qifeng Yu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "confidence",
        "pose",
        "regions",
        "keypoint",
        "region",
        "estimation",
        "deterministic",
        "poses",
        "sampling",
        "regressed"
      ],
      "summary": "6D pose confidence region estimation has emerged as a critical direction, aiming to perform uncertainty quantification for assessing the reliability of estimated poses. However, current sampling-based approach suffers from critical limitations that severely impede their practical deployment: 1) the sampling speed significantly decreases as the number of samples increases. 2) the derived confidence regions are often excessively large. To address these challenges, we propose a deterministic and efficient method for estimating pose confidence regions. Our approach uses inductive conformal prediction to calibrate the deterministically regressed Gaussian keypoint distributions into 2D keypoint confidence regions. We then leverage the implicit function theorem to propagate these keypoint confidence regions directly into 6D pose confidence regions. This method avoids the inefficiency and inflated region sizes associated with sampling and ensembling. It provides compact confidence regions that cover the ground-truth poses with a user-defined confidence level. Experimental results on the LineMOD Occlusion and SPEED datasets show that our method achieves higher pose estimation accuracy with reduced computational time. For the same coverage rate, our method yields significantly smaller confidence region volumes, reducing them by up to 99.9% for rotations and 99.8% for translations. The code will be available soon.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Deterministic_Object_Pose_Confidence_Region_Estimation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Deterministic_Object_Pose_Confidence_Region_Estimation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "Deterministic Object Pose Confidence Region Estimation [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Jinghao Wang , Zhang Li , Zi Wang , Banglei Guan , Yang Shang , Qifeng Yu 6D pose confidence region estimation has emerged as a critical direction, aiming to perform uncertainty quantification for assessing the reliability of estimated poses. However, current sampling-based approach suffers from critical limitations that severely impede their practical deployment: 1) the sampling speed significantly decreases as the number of samples increases. 2) the derived confidence regions are often excessively large. To address these challenges, we propose a deterministic and efficient method for estimating pose confidence regions. Our approach uses inductive conformal prediction to calibrate the deterministically regressed Gaussian keypoint distributions into 2D keypoint confidence regions. We then leverage the implicit function theorem to propagate these keypoint confidence regions directly into 6D pose confidence regions. This method avoids the inefficiency and inflated region sizes associated with sampling and ensembling. It provides compact confidence regions that cover the ground-truth poses with a user-defined confidence level. Experimental results on the LineMOD Occlusion and SPEED datasets show that our method achieves higher pose estimation accuracy with reduced computational time. For the same coverage rate, our method yields significantly smaller confidence region volumes, reducing them by up to 99.9% for rotations and 99.8% for translations. The code will be available soon. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF",
      "index": 57,
      "title": "WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction",
      "authors": [
        "Richard Liu",
        "Daniel Fu",
        "Noah Tan",
        "Itai Lang",
        "Rana Hanocka"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "wir3d",
        "shape",
        "visually",
        "abstraction",
        "curves",
        "geometry",
        "shapes",
        "texture",
        "bezier",
        "deformation"
      ],
      "summary": "In this work we present WIR3D, a technique for abstracting 3D shapes through a sparse set of visually meaningful curves in 3D. We optimize the parameters of Bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. We leverage the intermediate activations of a pre-trained foundation model (CLIP) to guide our optimization process. We divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. Our second phase supervision is spatially guided by a novel localized keypoint loss. This spatial guidance enables user control over abstracted features. We ensure fidelity to the original surface through a neural SDF loss, which allows the curves to be used as intuitive deformation handles. We successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Richard Liu , Daniel Fu , Noah Tan , Itai Lang , Rana Hanocka In this work we present WIR3D, a technique for abstracting 3D shapes through a sparse set of visually meaningful curves in 3D. We optimize the parameters of Bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. We leverage the intermediate activations of a pre-trained foundation model (CLIP) to guide our optimization process. We divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. Our second phase supervision is spatially guided by a novel localized keypoint loss. This spatial guidance enables user control over abstracted features. We ensure fidelity to the original surface through a neural SDF loss, which allows the curves to be used as intuitive deformation handles. We successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF",
      "index": 58,
      "title": "Generating Physically Stable and Buildable Brick Structures from Text",
      "authors": [
        "Ava Pun",
        "Kangle Deng",
        "Ruixuan Liu",
        "Deva Ramanan",
        "Changliu Liu",
        "Jun-Yan Zhu"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "brick",
        "brickgpt",
        "physically",
        "buildable",
        "stable",
        "structures",
        "text",
        "designs",
        "captions",
        "generating"
      ],
      "summary": "We introduce BrickGPT, the first approach for generating physically stable interconnecting brick assembly models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of brick structures, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that BrickGPT produces stable, diverse, and aesthetically pleasing brick structures that align closely with the input text prompts. We also develop a text-based brick texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We release our new dataset, StableText2Brick, containing over 47,000 brick structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/BrickGPT/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "Generating Physically Stable and Buildable Brick Structures from Text [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Ava Pun , Kangle Deng , Ruixuan Liu , Deva Ramanan , Changliu Liu , Jun-Yan Zhu We introduce BrickGPT, the first approach for generating physically stable interconnecting brick assembly models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of brick structures, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that BrickGPT produces stable, diverse, and aesthetically pleasing brick structures that align closely with the input text prompts. We also develop a text-based brick texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We release our new dataset, StableText2Brick, containing over 47,000 brick structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/BrickGPT/. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF",
      "index": 59,
      "title": "GMMamba: Group Masking Mamba for Whole Slide Image Classification",
      "authors": [
        "Tingting Zheng",
        "Hongxun Yao",
        "Kui Jiang",
        "Yi Xiao",
        "Sicheng Zhao"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "mamba",
        "gmmamba",
        "group",
        "masking",
        "slide",
        "imm",
        "tcga",
        "wsis",
        "uninformative",
        "css"
      ],
      "summary": "Recent advances in selective state space models (Mamba) have shown great promise in whole slide image (WSI) classification. Despite this, WSIs contain explicit local redundancy (similar patches) and irrelevant regions (uninformative instances), posing significant challenges for Mamba-based multi-instance learning (MIL) methods in capturing global representations. Furthermore, bag-level approaches struggle to extract critical features from all instances, while group-level methods fail to adequately account for tumor dispersion and intrinsic correlations across groups, leading to suboptimal global representations. To address these issues, we propose group masking Mamba (GMMamba), a novel framework that combines two elaborate modules: (1) intra-group masking Mamba (IMM) for selective instance exploration within groups, and (2) cross-group super-feature sampling (CSS) to ameliorate long-range relation learning. Specifically, IMM adaptively predicts sparse masks to filter out features with low attention scores (i.e., uninformative patterns) during bidirectional Mamba modeling, facilitating the removal of instance redundancies for compact local representation. For improved bag prediction, the CSS module further aggregates sparse group representations into discriminative features, effectively grasping comprehensive dependencies among dispersed and sparse tumor regions inherent in large-scale WSIs. Extensive experiments on four datasets demonstrate that GMMamba outperforms the state-of-the-art ACMIL by 2.2% and 6.4% in accuracy on the TCGA-BRCA and TCGA-ESCA datasets, respectively.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 2
      },
      "raw_excerpt": "GMMamba: Group Masking Mamba for Whole Slide Image Classification [PDF 9 ] [Copy] [Kimi 2 ] [REL] Authors : Tingting Zheng , Hongxun Yao , Kui Jiang , Yi Xiao , Sicheng Zhao Recent advances in selective state space models (Mamba) have shown great promise in whole slide image (WSI) classification. Despite this, WSIs contain explicit local redundancy (similar patches) and irrelevant regions (uninformative instances), posing significant challenges for Mamba-based multi-instance learning (MIL) methods in capturing global representations. Furthermore, bag-level approaches struggle to extract critical features from all instances, while group-level methods fail to adequately account for tumor dispersion and intrinsic correlations across groups, leading to suboptimal global representations. To address these issues, we propose group masking Mamba (GMMamba), a novel framework that combines two elaborate modules: (1) intra-group masking Mamba (IMM) for selective instance exploration within groups, and (2) cross-group super-feature sampling (CSS) to ameliorate long-range relation learning. Specifically, IMM adaptively predicts sparse masks to filter out features with low attention scores (i.e., uninformative patterns) during bidirectional Mamba modeling, facilitating the removal of instance redundancies for compact local representation. For improved bag prediction, the CSS module further aggregates sparse group representations into discriminative features, effectively grasping comprehensive dependencies among dispersed and sparse tumor regions inherent in large-scale WSIs. Extensive experiments on four datasets demonstrate that GMMamba outperforms the state-of-the-art ACMIL by 2.2% and 6.4% in accuracy on the TCGA-BRCA and TCGA-ESCA datasets, respectively. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF",
      "index": 60,
      "title": "Learning Streaming Video Representation via Multitask Training",
      "authors": [
        "Yibin Yan",
        "Jilan Xu",
        "Shangzhe Di",
        "Yikun Liu",
        "Yudi Shi",
        "Qirui Chen",
        "Zeqian Li",
        "Yifei Huang",
        "Weidi Xie"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "video",
        "streamformer",
        "streaming",
        "multitask",
        "understanding",
        "streams",
        "temporal",
        "representation",
        "frame",
        "maintaining"
      ],
      "summary": "Understanding continuous video streams plays a fundamental role in real-time applications, including embodied AI and autonomous driving. Unlike offline video processing, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions. To address these challenges, our main contributions are three-fold. (i) we develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability. (ii) to train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) we conduct extensive experiments for online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive performance while maintaining efficiency, demonstrating its potential for real-time applications.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training_ICCV_2025_paper.html",
          "/venue/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Oral",
          "/venue/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 2
      },
      "raw_excerpt": "Learning Streaming Video Representation via Multitask Training [PDF 8 ] [Copy] [Kimi 2 ] [REL] Authors : Yibin Yan , Jilan Xu , Shangzhe Di , Yikun Liu , Yudi Shi , Qirui Chen , Zeqian Li , Yifei Huang , Weidi Xie Understanding continuous video streams plays a fundamental role in real-time applications, including embodied AI and autonomous driving. Unlike offline video processing, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions. To address these challenges, our main contributions are three-fold. (i) we develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability. (ii) to train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) we conduct extensive experiments for online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive performance while maintaining efficiency, demonstrating its potential for real-time applications. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF",
      "index": 61,
      "title": "Heavy Labels Out! Dataset Distillation with Label Space Lightening",
      "authors": [
        "Ruonan Yu",
        "Songhua Liu",
        "Zigeng Chen",
        "Jingwen Ye",
        "Xinchao Wang"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "label",
        "lightening",
        "distilled",
        "labels",
        "hello",
        "distillation",
        "dataset",
        "original",
        "projectors",
        "synthetic"
      ],
      "summary": "Dataset distillation or condensation aims to condense a large-scale training dataset into a much smaller synthetic one such that the training performance of distilled and original sets on neural networks are similar. Although the number of training samples can be reduced substantially, current state-of-the-art methods heavily rely on enormous soft labels to achieve satisfactory performance. As a result, the required storage can be comparable even to original datasets, especially for large-scale ones. To solve this problem, instead of storing these heavy labels, we propose a novel label-lightening framework termed HeLlO aiming at effective image-to-label projectors, with which synthetic labels can be directly generated online from synthetic images. Specifically, to construct such projectors, we leverage prior knowledge in open-source foundation models, e.g., CLIP, and introduce a LoRA-like fine-tuning strategy to mitigate the gap between pre-trained and target distributions, so that original models for soft-label generation can be distilled into a group of low-rank matrices. Moreover, an effective image optimization method is proposed to further mitigate the potential error between the original and distilled label generators. Extensive experiments show that our method significantly reduces the storage cost to merely 0.001% compared to full soft-label storage methods while achieving comparable performance to state-of-the-art dataset distillation methods on large-scale datasets. Our codes are available at https://github.com/Lexie-YU/HeLlO.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 4
      },
      "raw_excerpt": "Heavy Labels Out! Dataset Distillation with Label Space Lightening [PDF 10 ] [Copy] [Kimi 4 ] [REL] Authors : Ruonan Yu , Songhua Liu , Zigeng Chen , Jingwen Ye , Xinchao Wang Dataset distillation or condensation aims to condense a large-scale training dataset into a much smaller synthetic one such that the training performance of distilled and original sets on neural networks are similar. Although the number of training samples can be reduced substantially, current state-of-the-art methods heavily rely on enormous soft labels to achieve satisfactory performance. As a result, the required storage can be comparable even to original datasets, especially for large-scale ones. To solve this problem, instead of storing these heavy labels, we propose a novel label-lightening framework termed HeLlO aiming at effective image-to-label projectors, with which synthetic labels can be directly generated online from synthetic images. Specifically, to construct such projectors, we leverage prior knowledge in open-source foundation models, e.g., CLIP, and introduce a LoRA-like fine-tuning strategy to mitigate the gap between pre-trained and target distributions, so that original models for soft-label generation can be distilled into a group of low-rank matrices. Moreover, an effective image optimization method is proposed to further mitigate the potential error between the original and distilled label generators. Extensive experiments show that our method significantly reduces the storage cost to merely 0.001% compared to full soft-label storage methods while achieving comparable performance to state-of-the-art dataset distillation methods on large-scale datasets. Our codes are available at https://github.com/Lexie-YU/HeLlO. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF",
      "index": 62,
      "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
      "authors": [
        "Shengyuan Zhang",
        "An Zhao",
        "Ling Yang",
        "Zejian Li",
        "Chenye Meng",
        "Haoran Xu",
        "Tianrun Chen",
        "AnYang Wei",
        "Perry Pengyun Gu",
        "Lingyun Sun"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "completion",
        "scene",
        "lidar",
        "scorelidar",
        "distilled",
        "diffusion",
        "distilling",
        "quality",
        "distillation",
        "constraining"
      ],
      "summary": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. Score- LiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5x) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our model and code are publicly available on https://github.com/happyw1nd/ScoreLiDAR.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 4
      },
      "raw_excerpt": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion [PDF 9 ] [Copy] [Kimi 4 ] [REL] Authors : Shengyuan Zhang , An Zhao , Ling Yang , Zejian Li , Chenye Meng , Haoran Xu , Tianrun Chen , AnYang Wei , Perry Pengyun Gu , Lingyun Sun Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. Score- LiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5x) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our model and code are publicly available on https://github.com/happyw1nd/ScoreLiDAR. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF",
      "index": 63,
      "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image",
      "authors": [
        "Jerred Chen",
        "Ronald Clark"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "motion",
        "camera",
        "blur",
        "imu",
        "blurred",
        "mast3r",
        "image",
        "colmap",
        "scannet",
        "fast"
      ],
      "summary": "In many robotics and VR/AR applications, fast camera motions lead to a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 6
      },
      "raw_excerpt": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image [PDF 8 ] [Copy] [Kimi 6 ] [REL] Authors : Jerred Chen , Ronald Clark In many robotics and VR/AR applications, fast camera motions lead to a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP. Subject : ICCV.2025 - Oral"
    },
    {
      "paper_id": "Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF",
      "index": 64,
      "title": "Scaling Laws for Native Multimodal Models",
      "authors": [
        "Mustafa Shukor",
        "Enrico Fini",
        "Victor Guilherme Turrisi da Costa",
        "Matthieu Cord",
        "Joshua Susskind",
        "Alaaeldin El-Nouby"
      ],
      "subjects": [
        "ICCV.2025 - Oral"
      ],
      "keywords": [
        "multimodal",
        "fusion",
        "architectures",
        "native",
        "early",
        "encoders",
        "nmms",
        "laws",
        "trained",
        "late"
      ],
      "summary": "Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders or tokenizers. On the contrary, early fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows models to learn modality-specific weights, significantly benefiting performance.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Shukor_Scaling_Laws_for_Native_Multimodal_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Shukor_Scaling_Laws_for_Native_Multimodal_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 10
      },
      "raw_excerpt": "Scaling Laws for Native Multimodal Models [PDF 15 ] [Copy] [Kimi 10 ] [REL] Authors : Mustafa Shukor , Enrico Fini , Victor Guilherme Turrisi da Costa , Matthieu Cord , Joshua Susskind , Alaaeldin El-Nouby Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders or tokenizers. On the contrary, early fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows models to learn modality-specific weights, significantly benefiting performance. Subject : ICCV.2025 - Oral"
    }
  ]
}