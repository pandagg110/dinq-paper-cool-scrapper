<html><head>
    <title>ICML.2025 - Spotlight | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for ICML.2025 - Spotlight, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.9') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Typewriter; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf?V=2.7.9') format('opentype')}
</style></head>
<body id="venue"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">ICML.2025 - Spotlight</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 225
    </p>
    <div class="papers">
        <div id="Hp53p5AU7X@OpenReview" class="panel paper" keywords="nal,loss,unbiased,variance,nash,optimization,approximating,convex,function,nfgs">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Hp53p5AU7X" target="_blank" title="1/225"><span class="index notranslate">#1</span></a>
                <a id="title-Hp53p5AU7X@OpenReview" class="title-link" href="/venue/Hp53p5AU7X@OpenReview" target="_blank">Reducing Variance of Stochastic Optimization for Approximating Nash Equilibria in Normal-Form Games</a>
                <a id="pdf-Hp53p5AU7X@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Hp53p5AU7X@OpenReview', this)" data="https://openreview.net/pdf?id=Hp53p5AU7X">[PDF<sup id="pdf-stars-Hp53p5AU7X@OpenReview">31</sup>]</a>
                <a id="copy-Hp53p5AU7X@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Hp53p5AU7X@OpenReview')">[Copy]</a>
                <a id="kimi-Hp53p5AU7X@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Hp53p5AU7X@OpenReview', this)">[Kimi<sup id="kimi-stars-Hp53p5AU7X@OpenReview">27</sup>]</a>
                <a id="rel-Hp53p5AU7X@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Hp53p5AU7X@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Hp53p5AU7X@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linjian Meng" target="_blank">Linjian Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wubing Chen" target="_blank">Wubing Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbin Li" target="_blank">Wenbin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianpei Yang" target="_blank">Tianpei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youzhi Zhang" target="_blank">Youzhi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Gao" target="_blank">Yang Gao</a>
            </p>
            <p id="summary-Hp53p5AU7X@OpenReview" class="summary">Nash equilibrium (NE) plays an important role in game theory. How to efficiently compute an NE in NFGs is challenging due to its complexity and non-convex optimization property. Machine Learning (ML), the cornerstone of modern artificial intelligence, has demonstrated remarkable empirical performance across various applications including non-convex optimization. To leverage non-convex stochastic optimization techniques from ML for approximating an NE, various loss functions have been proposed. Among these, only one loss function is unbiased, allowing for unbiased estimation under the sampled play. Unfortunately, this loss function suffers from high variance, which degrades the convergence rate. To improve the convergence rate by mitigating the high variance associated with the existing unbiased loss function, we propose a novel surrogate loss function named Nash Advantage Loss (NAL). NAL is theoretically proved unbiased and exhibits significantly lower variance than the existing unbiased loss function. Experimental results demonstrate that the algorithm minimizing NAL achieves a significantly faster empirical convergence rates compared to other algorithms, while also reducing the variance of estimated loss value by several orders of magnitude.</p>
            <p id="subjects-Hp53p5AU7X@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Hp53p5AU7X@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hp53p5AU7X@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hp53p5AU7X@OpenReview" onclick="foldPdfKimi('Hp53p5AU7X@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="B9DOjtj9xK@OpenReview" class="panel paper" keywords="shapes,soft,shape,classification,shapelets,shapelet,softshape,sparsified,subsequence,sparsification">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=B9DOjtj9xK" target="_blank" title="2/225"><span class="index notranslate">#2</span></a>
                <a id="title-B9DOjtj9xK@OpenReview" class="title-link" href="/venue/B9DOjtj9xK@OpenReview" target="_blank">Learning Soft Sparse Shapes for Efficient Time-Series Classification</a>
                <a id="pdf-B9DOjtj9xK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('B9DOjtj9xK@OpenReview', this)" data="https://openreview.net/pdf?id=B9DOjtj9xK">[PDF<sup id="pdf-stars-B9DOjtj9xK@OpenReview">20</sup>]</a>
                <a id="copy-B9DOjtj9xK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('B9DOjtj9xK@OpenReview')">[Copy]</a>
                <a id="kimi-B9DOjtj9xK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('B9DOjtj9xK@OpenReview', this)">[Kimi<sup id="kimi-stars-B9DOjtj9xK@OpenReview">20</sup>]</a>
                <a id="rel-B9DOjtj9xK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('B9DOjtj9xK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-B9DOjtj9xK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Liu" target="_blank">Zhen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yicheng Luo" target="_blank">Yicheng Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyuan Li" target="_blank">Boyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emadeldeen Eldele" target="_blank">Emadeldeen Eldele</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Wu" target="_blank">Min Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianli Ma" target="_blank">Qianli Ma</a>
            </p>
            <p id="summary-B9DOjtj9xK@OpenReview" class="summary">Shapelets are discriminative subsequences (or shapes) with high interpretability in time series classification. Due to the time-intensive nature of shapelet discovery, existing shapelet-based methods mainly focus on selecting discriminative shapes while discarding others to achieve candidate subsequence sparsification. However, this approach may exclude beneficial shapes and overlook the varying contributions of shapelets to classification performance. To this end, we propose a Soft sparse Shapes (SoftShape) model for efficient time series classification. Our approach mainly introduces soft shape sparsification and soft shape learning blocks. The former transforms shapes into soft representations based on classification contribution scores, merging lower-scored ones into a single shape to retain and differentiate all subsequence information. The latter facilitates intra- and inter-shape temporal pattern learning, improving model efficiency by using sparsified soft shapes as inputs. Specifically, we employ a learnable router to activate a subset of class-specific expert networks for intra-shape pattern learning. Meanwhile, a shared expert network learns inter-shape patterns by converting sparsified shapes into sequences. Extensive experiments show that SoftShape outperforms state-of-the-art methods and produces interpretable results.</p>
            <p id="subjects-B9DOjtj9xK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-B9DOjtj9xK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-B9DOjtj9xK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-B9DOjtj9xK@OpenReview" onclick="foldPdfKimi('B9DOjtj9xK@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="XEyGcrhxB8@OpenReview" class="panel paper" keywords="corruption,offline,ltc,privacy,ctl,alignment,rlhf,dpo,labels,corrupted">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XEyGcrhxB8" target="_blank" title="3/225"><span class="index notranslate">#3</span></a>
                <a id="title-XEyGcrhxB8@OpenReview" class="title-link" href="/venue/XEyGcrhxB8@OpenReview" target="_blank">A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO</a>
                <a id="pdf-XEyGcrhxB8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XEyGcrhxB8@OpenReview', this)" data="https://openreview.net/pdf?id=XEyGcrhxB8">[PDF<sup id="pdf-stars-XEyGcrhxB8@OpenReview">21</sup>]</a>
                <a id="copy-XEyGcrhxB8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XEyGcrhxB8@OpenReview')">[Copy]</a>
                <a id="kimi-XEyGcrhxB8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XEyGcrhxB8@OpenReview', this)">[Kimi<sup id="kimi-stars-XEyGcrhxB8@OpenReview">18</sup>]</a>
                <a id="rel-XEyGcrhxB8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XEyGcrhxB8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XEyGcrhxB8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyu Zhou" target="_blank">Xingyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulian Wu" target="_blank">Yulian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francesco Orabona" target="_blank">Francesco Orabona</a>
            </p>
            <p id="summary-XEyGcrhxB8@OpenReview" class="summary">In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios.</p>
            <p id="subjects-XEyGcrhxB8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-XEyGcrhxB8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XEyGcrhxB8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XEyGcrhxB8@OpenReview" onclick="foldPdfKimi('XEyGcrhxB8@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="il3KRr4H9u@OpenReview" class="panel paper" keywords="baxbench,llms,secure,backends,security,generate,functionality,correct,backend,code">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=il3KRr4H9u" target="_blank" title="4/225"><span class="index notranslate">#4</span></a>
                <a id="title-il3KRr4H9u@OpenReview" class="title-link" href="/venue/il3KRr4H9u@OpenReview" target="_blank">BaxBench: Can LLMs Generate Correct and Secure Backends?</a>
                <a id="pdf-il3KRr4H9u@OpenReview" class="title-pdf notranslate" onclick="togglePdf('il3KRr4H9u@OpenReview', this)" data="https://openreview.net/pdf?id=il3KRr4H9u">[PDF<sup id="pdf-stars-il3KRr4H9u@OpenReview">11</sup>]</a>
                <a id="copy-il3KRr4H9u@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('il3KRr4H9u@OpenReview')">[Copy]</a>
                <a id="kimi-il3KRr4H9u@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('il3KRr4H9u@OpenReview', this)">[Kimi<sup id="kimi-stars-il3KRr4H9u@OpenReview">8</sup>]</a>
                <a id="rel-il3KRr4H9u@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('il3KRr4H9u@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-il3KRr4H9u@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Vero" target="_blank">Mark Vero</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niels Mündler" target="_blank">Niels Mündler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viktor Chibotaru" target="_blank">Viktor Chibotaru</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Veselin Raychev" target="_blank">Veselin Raychev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maximilian Baader" target="_blank">Maximilian Baader</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikola Jovanović" target="_blank">Nikola Jovanović</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingxuan He" target="_blank">Jingxuan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Vechev" target="_blank">Martin Vechev</a>
            </p>
            <p id="summary-il3KRr4H9u@OpenReview" class="summary">Automatic program generation has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could successfully execute security exploits on around half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.</p>
            <p id="subjects-il3KRr4H9u@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-il3KRr4H9u@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-il3KRr4H9u@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-il3KRr4H9u@OpenReview" onclick="foldPdfKimi('il3KRr4H9u@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Vhc0KrcqWu@OpenReview" class="panel paper" keywords="correctors,feynman,kac,guidance,pretrained,annealing,principled,inference,distributions,sampling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Vhc0KrcqWu" target="_blank" title="5/225"><span class="index notranslate">#5</span></a>
                <a id="title-Vhc0KrcqWu@OpenReview" class="title-link" href="/venue/Vhc0KrcqWu@OpenReview" target="_blank">Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</a>
                <a id="pdf-Vhc0KrcqWu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Vhc0KrcqWu@OpenReview', this)" data="https://openreview.net/pdf?id=Vhc0KrcqWu">[PDF<sup id="pdf-stars-Vhc0KrcqWu@OpenReview">13</sup>]</a>
                <a id="copy-Vhc0KrcqWu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Vhc0KrcqWu@OpenReview')">[Copy]</a>
                <a id="kimi-Vhc0KrcqWu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Vhc0KrcqWu@OpenReview', this)">[Kimi<sup id="kimi-stars-Vhc0KrcqWu@OpenReview">16</sup>]</a>
                <a id="rel-Vhc0KrcqWu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Vhc0KrcqWu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Vhc0KrcqWu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marta Skreta" target="_blank">Marta Skreta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tara Akhound-Sadegh" target="_blank">Tara Akhound-Sadegh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viktor Ohanesian" target="_blank">Viktor Ohanesian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roberto Bondesan" target="_blank">Roberto Bondesan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Aspuru-Guzik" target="_blank">Alan Aspuru-Guzik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arnaud Doucet" target="_blank">Arnaud Doucet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rob Brekelmans" target="_blank">Rob Brekelmans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Tong" target="_blank">Alexander Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kirill Neklyudov" target="_blank">Kirill Neklyudov</a>
            </p>
            <p id="summary-Vhc0KrcqWu@OpenReview" class="summary">While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional `corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation.</p>
            <p id="subjects-Vhc0KrcqWu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Vhc0KrcqWu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Vhc0KrcqWu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Vhc0KrcqWu@OpenReview" onclick="foldPdfKimi('Vhc0KrcqWu@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="gKdjHLrHDS@OpenReview" class="panel paper" keywords="lazy,dichotomy,rich,feature,representational,learning,task,relevant,geometry,insights">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gKdjHLrHDS" target="_blank" title="6/225"><span class="index notranslate">#6</span></a>
                <a id="title-gKdjHLrHDS@OpenReview" class="title-link" href="/venue/gKdjHLrHDS@OpenReview" target="_blank">Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry</a>
                <a id="pdf-gKdjHLrHDS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gKdjHLrHDS@OpenReview', this)" data="https://openreview.net/pdf?id=gKdjHLrHDS">[PDF<sup id="pdf-stars-gKdjHLrHDS@OpenReview">13</sup>]</a>
                <a id="copy-gKdjHLrHDS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gKdjHLrHDS@OpenReview')">[Copy]</a>
                <a id="kimi-gKdjHLrHDS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gKdjHLrHDS@OpenReview', this)">[Kimi<sup id="kimi-stars-gKdjHLrHDS@OpenReview">11</sup>]</a>
                <a id="rel-gKdjHLrHDS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gKdjHLrHDS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gKdjHLrHDS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chi-Ning Chou" target="_blank">Chi-Ning Chou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Le" target="_blank">Hang Le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yichen Wang" target="_blank">Yichen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=SueYeon Chung" target="_blank">SueYeon Chung</a>
            </p>
            <p id="summary-gKdjHLrHDS@OpenReview" class="summary">Integrating task-relevant information into neural representations is a fundamental ability of both biological and artificial intelligence systems. Recent theories have categorized learning into two regimes: the rich regime, where neural networks actively learn task-relevant features, and the lazy regime, where networks behave like random feature models. Yet this simple lazy–rich dichotomy overlooks a diverse underlying taxonomy of feature learning, shaped by differences in learning algorithms, network architectures, and data properties. To address this gap, we introduce an analysis framework to study feature learning via the geometry of neural representations. Rather than inspecting individual learned features, we characterize how task-relevant representational manifolds evolve throughout the learning process. We show, in both theoretical and empirical settings, that as networks learn features, task-relevant manifolds untangle, with changes in manifold geometry revealing distinct learning stages and strategies beyond the lazy–rich dichotomy. This framework provides novel insights into feature learning across neuroscience and machine learning, shedding light on structural inductive biases in neural circuits and the mechanisms underlying out-of-distribution generalization.</p>
            <p id="subjects-gKdjHLrHDS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gKdjHLrHDS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gKdjHLrHDS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gKdjHLrHDS@OpenReview" onclick="foldPdfKimi('gKdjHLrHDS@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="m3zrHhiCCj@OpenReview" class="panel paper" keywords="fisher,accumulator,squared,gradient,recycling,squisher,fishers,squ,isher,ared">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=m3zrHhiCCj" target="_blank" title="7/225"><span class="index notranslate">#7</span></a>
                <a id="title-m3zrHhiCCj@OpenReview" class="title-link" href="/venue/m3zrHhiCCj@OpenReview" target="_blank">Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator</a>
                <a id="pdf-m3zrHhiCCj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('m3zrHhiCCj@OpenReview', this)" data="https://openreview.net/pdf?id=m3zrHhiCCj">[PDF<sup id="pdf-stars-m3zrHhiCCj@OpenReview">16</sup>]</a>
                <a id="copy-m3zrHhiCCj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('m3zrHhiCCj@OpenReview')">[Copy]</a>
                <a id="kimi-m3zrHhiCCj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('m3zrHhiCCj@OpenReview', this)">[Kimi<sup id="kimi-stars-m3zrHhiCCj@OpenReview">13</sup>]</a>
                <a id="rel-m3zrHhiCCj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('m3zrHhiCCj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-m3zrHhiCCj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=YuXin Li" target="_blank">YuXin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Dangel" target="_blank">Felix Dangel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Derek Tam" target="_blank">Derek Tam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Colin Raffel" target="_blank">Colin Raffel</a>
            </p>
            <p id="summary-m3zrHhiCCj@OpenReview" class="summary">The diagonal of a model's Fisher Information Matrix (the "Fisher") has frequently been used as a way to measure parameter sensitivity.Typically, the Fisher is estimated by computing the squared gradient of the model's outputs with respect to its parameters, averaged over a few hundred or thousand examples — a process which incurs nontrivial computational costs.At the same time, adaptive gradient methods like the ubiquitous Adam optimizer compute a moving average of the squared gradient over the course of training.This paper therefore explores whether an approximation of the Fisher can be obtained "for free" by recycling the squared gradient accumulator that has already been computed over the course of training.Through a comprehensive set of experiments covering five applications of the Fisher, we demonstrate that the "Squisher" (**Squ**ared gradient accumulator as an approximation of the F**isher**) consistently performs similarly to the Fisher while outperforming baseline methods.Additionally, we clarify the exact differences between the Squisher and the Fisher and provide empirical quantification of their respective impact.</p>
            <p id="subjects-m3zrHhiCCj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-m3zrHhiCCj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-m3zrHhiCCj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-m3zrHhiCCj@OpenReview" onclick="foldPdfKimi('m3zrHhiCCj@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="dzwUOiBlQW@OpenReview" class="panel paper" keywords="latent,maetok,diffusion,autoencoders,generation,512,tokenizers,gfid,space,discriminative">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dzwUOiBlQW" target="_blank" title="8/225"><span class="index notranslate">#8</span></a>
                <a id="title-dzwUOiBlQW@OpenReview" class="title-link" href="/venue/dzwUOiBlQW@OpenReview" target="_blank">Masked Autoencoders Are Effective Tokenizers for Diffusion Models</a>
                <a id="pdf-dzwUOiBlQW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dzwUOiBlQW@OpenReview', this)" data="https://openreview.net/pdf?id=dzwUOiBlQW">[PDF<sup id="pdf-stars-dzwUOiBlQW@OpenReview">22</sup>]</a>
                <a id="copy-dzwUOiBlQW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dzwUOiBlQW@OpenReview')">[Copy]</a>
                <a id="kimi-dzwUOiBlQW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dzwUOiBlQW@OpenReview', this)">[Kimi<sup id="kimi-stars-dzwUOiBlQW@OpenReview">23</sup>]</a>
                <a id="rel-dzwUOiBlQW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dzwUOiBlQW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dzwUOiBlQW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chen" target="_blank">Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujin Han" target="_blank">Yujin Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangyi Chen" target="_blank">Fangyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Li" target="_blank">Xiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yidong Wang" target="_blank">Yidong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jindong Wang" target="_blank">Jindong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ze Wang" target="_blank">Ze Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zicheng Liu" target="_blank">Zicheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Difan Zou" target="_blank">Difan Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bhiksha Raj" target="_blank">Bhiksha Raj</a>
            </p>
            <p id="summary-dzwUOiBlQW@OpenReview" class="summary">Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76× faster training and 31× higher inference throughput for 512×512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models will be released.</p>
            <p id="subjects-dzwUOiBlQW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dzwUOiBlQW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dzwUOiBlQW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dzwUOiBlQW@OpenReview" onclick="foldPdfKimi('dzwUOiBlQW@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Ossg1IbHDT@OpenReview" class="panel paper" keywords="slide,histology,transcriptomics,whole,gene,cell,stflow,expression,stimage,1k4m">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Ossg1IbHDT" target="_blank" title="9/225"><span class="index notranslate">#9</span></a>
                <a id="title-Ossg1IbHDT@OpenReview" class="title-link" href="/venue/Ossg1IbHDT@OpenReview" target="_blank">Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching</a>
                <a id="pdf-Ossg1IbHDT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Ossg1IbHDT@OpenReview', this)" data="https://openreview.net/pdf?id=Ossg1IbHDT">[PDF<sup id="pdf-stars-Ossg1IbHDT@OpenReview">12</sup>]</a>
                <a id="copy-Ossg1IbHDT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Ossg1IbHDT@OpenReview')">[Copy]</a>
                <a id="kimi-Ossg1IbHDT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Ossg1IbHDT@OpenReview', this)">[Kimi<sup id="kimi-stars-Ossg1IbHDT@OpenReview">10</sup>]</a>
                <a id="rel-Ossg1IbHDT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Ossg1IbHDT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Ossg1IbHDT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tinglin Huang" target="_blank">Tinglin Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Liu" target="_blank">Tianyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mehrtash Babadi" target="_blank">Mehrtash Babadi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wengong Jin" target="_blank">Wengong Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=ZHITAO YING" target="_blank">ZHITAO YING</a>
            </p>
            <p id="summary-Ossg1IbHDT@OpenReview" class="summary">Spatial transcriptomics (ST) has emerged as a powerful technology for bridging histology imaging with gene expression profiling. However, its application has been limited by low throughput and the need for specialized experimental facilities. Prior works sought to predict ST from whole-slide histology images to accelerate this process, but they suffer from two major limitations. First, they do not explicitly model cell-cell interaction as they factorize the joint distribution of whole-slide ST data and predict the gene expression of each spot independently. Second, their encoders struggle with memory constraints due to the large number of spots (often exceeding 10,000) in typical ST datasets. Herein, we propose STFlow, a flow matching generative model that considers cell-cell interaction by modeling the joint distribution of gene expression of an entire slide. It also employs an efficient slide-level encoder with local spatial attention, enabling whole-slide processing without excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M benchmarks, STFlow substantially outperforms state-of-the-art baselines and achieves over 18% relative improvements over the pathology foundation models.</p>
            <p id="subjects-Ossg1IbHDT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Ossg1IbHDT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ossg1IbHDT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ossg1IbHDT@OpenReview" onclick="foldPdfKimi('Ossg1IbHDT@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Vf9f7eNX6T@OpenReview" class="panel paper" keywords="modality,collapse,multimodal,fusion,predictive,abhrac,modalities,features,head,freeing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Vf9f7eNX6T" target="_blank" title="10/225"><span class="index notranslate">#10</span></a>
                <a id="title-Vf9f7eNX6T@OpenReview" class="title-link" href="/venue/Vf9f7eNX6T@OpenReview" target="_blank">A Closer Look at Multimodal Representation Collapse</a>
                <a id="pdf-Vf9f7eNX6T@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Vf9f7eNX6T@OpenReview', this)" data="https://openreview.net/pdf?id=Vf9f7eNX6T">[PDF<sup id="pdf-stars-Vf9f7eNX6T@OpenReview">23</sup>]</a>
                <a id="copy-Vf9f7eNX6T@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Vf9f7eNX6T@OpenReview')">[Copy]</a>
                <a id="kimi-Vf9f7eNX6T@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Vf9f7eNX6T@OpenReview', this)">[Kimi<sup id="kimi-stars-Vf9f7eNX6T@OpenReview">19</sup>]</a>
                <a id="rel-Vf9f7eNX6T@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Vf9f7eNX6T@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Vf9f7eNX6T@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Abhra Chaudhuri" target="_blank">Abhra Chaudhuri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anjan Dutta" target="_blank">Anjan Dutta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tu Bui" target="_blank">Tu Bui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Serban Georgescu" target="_blank">Serban Georgescu</a>
            </p>
            <p id="summary-Vf9f7eNX6T@OpenReview" class="summary">We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: https://abhrac.github.io/mmcollapse/.</p>
            <p id="subjects-Vf9f7eNX6T@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Vf9f7eNX6T@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Vf9f7eNX6T@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Vf9f7eNX6T@OpenReview" onclick="foldPdfKimi('Vf9f7eNX6T@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="TyArXyYnvz@OpenReview" class="panel paper" keywords="lowpopart,catoni,2024,minimax,generalized,instance,wise,estimator,rank,trace">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TyArXyYnvz" target="_blank" title="11/225"><span class="index notranslate">#11</span></a>
                <a id="title-TyArXyYnvz@OpenReview" class="title-link" href="/venue/TyArXyYnvz@OpenReview" target="_blank">GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for Generalized Low-Rank Trace Regression</a>
                <a id="pdf-TyArXyYnvz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TyArXyYnvz@OpenReview', this)" data="https://openreview.net/pdf?id=TyArXyYnvz">[PDF<sup id="pdf-stars-TyArXyYnvz@OpenReview">5</sup>]</a>
                <a id="copy-TyArXyYnvz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TyArXyYnvz@OpenReview')">[Copy]</a>
                <a id="kimi-TyArXyYnvz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TyArXyYnvz@OpenReview', this)">[Kimi<sup id="kimi-stars-TyArXyYnvz@OpenReview">4</sup>]</a>
                <a id="rel-TyArXyYnvz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TyArXyYnvz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TyArXyYnvz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junghyun Lee" target="_blank">Junghyun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyoungseok Jang" target="_blank">Kyoungseok Jang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwang-Sung Jun" target="_blank">Kwang-Sung Jun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Milan Vojnovic" target="_blank">Milan Vojnovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Se-Young Yun" target="_blank">Se-Young Yun</a>
            </p>
            <p id="summary-TyArXyYnvz@OpenReview" class="summary">We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it employs a two-stage approach: nuclear norm regularization followed by matrix Catoni estimation. We establish state-of-the-art estimation error bounds, surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and reveal a novel experimental design objective, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;G&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03C0;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 3.336em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.763em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.66em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="texatom" id="MathJax-Span-3"><span class="mrow" id="MathJax-Span-4"><span class="mi" id="MathJax-Span-5" style="font-family: MathJax_Main;">G</span><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Main;">L</span></span></span><span class="mo" id="MathJax-Span-7" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-8" style="font-family: MathJax_Math-italic;">π<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-9" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">G</mi><mi mathvariant="normal">L</mi></mrow><mo stretchy="false">(</mo><mi>π</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">\mathrm{GL}(\pi)</script>. The key technical challenge is controlling bias from the nonlinear inverse link function, which we address by our two-stage approach. We prove a *local* minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise optimality up to the condition number of the ground-truth Hessian. Applications include generalized linear matrix completion, where `GL-LowPopArt` achieves a state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a novel setting inspired by general preference learning (Zhang et al., 2024). Our analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new, potentially interesting problem-dependent quantity, along with improved Borda regret bound than vectorization (Wu et al., 2024).</p>
            <p id="subjects-TyArXyYnvz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TyArXyYnvz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TyArXyYnvz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TyArXyYnvz@OpenReview" onclick="foldPdfKimi('TyArXyYnvz@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="zk5k2NQcEA@OpenReview" class="panel paper" keywords="smt,score,mixture,training,smd,generative,distillation,step,distributions,jensen">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zk5k2NQcEA" target="_blank" title="12/225"><span class="index notranslate">#12</span></a>
                <a id="title-zk5k2NQcEA@OpenReview" class="title-link" href="/venue/zk5k2NQcEA@OpenReview" target="_blank">Score-of-Mixture Training: One-Step Generative Model Training Made Simple via Score Estimation of Mixture Distributions</a>
                <a id="pdf-zk5k2NQcEA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zk5k2NQcEA@OpenReview', this)" data="https://openreview.net/pdf?id=zk5k2NQcEA">[PDF<sup id="pdf-stars-zk5k2NQcEA@OpenReview">9</sup>]</a>
                <a id="copy-zk5k2NQcEA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zk5k2NQcEA@OpenReview')">[Copy]</a>
                <a id="kimi-zk5k2NQcEA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zk5k2NQcEA@OpenReview', this)">[Kimi<sup id="kimi-stars-zk5k2NQcEA@OpenReview">11</sup>]</a>
                <a id="rel-zk5k2NQcEA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zk5k2NQcEA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zk5k2NQcEA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tejas Jayashankar" target="_blank">Tejas Jayashankar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jongha (Jon) Ryu" target="_blank">Jongha (Jon) Ryu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gregory Wornell" target="_blank">Gregory Wornell</a>
            </p>
            <p id="summary-zk5k2NQcEA@OpenReview" class="summary">We propose *Score-of-Mixture Training* (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-10" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="mi" id="MathJax-Span-12" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">\alpha</script>-skew Jensen–Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels.Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call *Score-of-Mixture Distillation* (SMD).It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64×64 show that SMT/SMD are competitive with and can even outperform existing methods.</p>
            <p id="subjects-zk5k2NQcEA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-zk5k2NQcEA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zk5k2NQcEA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zk5k2NQcEA@OpenReview" onclick="foldPdfKimi('zk5k2NQcEA@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="VpBBw1bL47@OpenReview" class="panel paper" keywords="sam,infosam,peft,anything,fine,tuning,information,pre,trained,segment">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VpBBw1bL47" target="_blank" title="13/225"><span class="index notranslate">#13</span></a>
                <a id="title-VpBBw1bL47@OpenReview" class="title-link" href="/venue/VpBBw1bL47@OpenReview" target="_blank">InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective</a>
                <a id="pdf-VpBBw1bL47@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VpBBw1bL47@OpenReview', this)" data="https://openreview.net/pdf?id=VpBBw1bL47">[PDF<sup id="pdf-stars-VpBBw1bL47@OpenReview">14</sup>]</a>
                <a id="copy-VpBBw1bL47@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VpBBw1bL47@OpenReview')">[Copy]</a>
                <a id="kimi-VpBBw1bL47@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VpBBw1bL47@OpenReview', this)">[Kimi<sup id="kimi-stars-VpBBw1bL47@OpenReview">6</sup>]</a>
                <a id="rel-VpBBw1bL47@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VpBBw1bL47@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VpBBw1bL47@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanhong Zhang" target="_blank">Yuanhong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muyao Yuan" target="_blank">Muyao Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weizhan Zhang" target="_blank">Weizhan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tieliang Gong" target="_blank">Tieliang Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen Wen" target="_blank">Wen Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangyong Ying" target="_blank">Jiangyong Ying</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijie Shi" target="_blank">Weijie Shi</a>
            </p>
            <p id="summary-VpBBw1bL47@OpenReview" class="summary">The Segment Anything Model (SAM), a vision foundation model, exhibits impressive zero-shot capabilities in general tasks but struggles in specialized domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to unleash the potential of SAM in novel scenarios. However, existing PEFT methods for SAM neglect the domain-invariant relations encoded in the pre-trained model. To bridge this gap, we propose InfoSAM, an information-theoretic approach that enhances SAM fine-tuning by distilling and preserving its pre-trained segmentation knowledge. Specifically, we formulate the knowledge transfer process as two novel mutual information-based objectives: (i) to compress the domain-invariant relation extracted from pre-trained SAM, excluding pseudo-invariant information as possible, and (ii) to maximize mutual information between the relational knowledge learned by the teacher (pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM establishes a robust distillation framework for PEFT of SAM. Extensive experiments across diverse benchmarks validate InfoSAM's effectiveness in improving SAM family's performance on real-world tasks, demonstrating its adaptability and superiority in handling specialized scenarios. The code and models are available at https://muyaoyuan.github.io/InfoSAM_Page.</p>
            <p id="subjects-VpBBw1bL47@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-VpBBw1bL47@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VpBBw1bL47@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VpBBw1bL47@OpenReview" onclick="foldPdfKimi('VpBBw1bL47@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="79O2XccGXZ@OpenReview" class="panel paper" keywords="generation,molecule,representation,geometric,molecular,quality,georcg,stage,semlaflow,drug">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=79O2XccGXZ" target="_blank" title="14/225"><span class="index notranslate">#14</span></a>
                <a id="title-79O2XccGXZ@OpenReview" class="title-link" href="/venue/79O2XccGXZ@OpenReview" target="_blank">Geometric Representation Condition Improves Equivariant Molecule Generation</a>
                <a id="pdf-79O2XccGXZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('79O2XccGXZ@OpenReview', this)" data="https://openreview.net/pdf?id=79O2XccGXZ">[PDF<sup id="pdf-stars-79O2XccGXZ@OpenReview">10</sup>]</a>
                <a id="copy-79O2XccGXZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('79O2XccGXZ@OpenReview')">[Copy]</a>
                <a id="kimi-79O2XccGXZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('79O2XccGXZ@OpenReview', this)">[Kimi<sup id="kimi-stars-79O2XccGXZ@OpenReview">7</sup>]</a>
                <a id="rel-79O2XccGXZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('79O2XccGXZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-79O2XccGXZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zian Li" target="_blank">Zian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cai Zhou" target="_blank">Cai Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiyuan Wang" target="_blank">Xiyuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingang Peng" target="_blank">Xingang Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhan Zhang" target="_blank">Muhan Zhang</a>
            </p>
            <p id="summary-79O2XccGXZ@OpenReview" class="summary">Recent advances in molecular generative models have demonstrated great promise for accelerating scientific discovery, particularly in drug design. However, these models often struggle to generate high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to improve molecular generative models by integrating geometric representation conditions with provable theoretical guarantees. We decompose the generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared with single-stage generation, the easy-to-generate representation in the first stage guides the second stage generation toward a high-quality molecule in a goal-oriented way. Leveraging EDM and SemlaFlow as base generators, we observe significant quality improvements in unconditional molecule generation on the widely used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 50\% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations. Furthermore, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while largely preserving the generation quality achieved with 1,000 steps, thereby significantly reducing the generation iterations needed.</p>
            <p id="subjects-79O2XccGXZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-79O2XccGXZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-79O2XccGXZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-79O2XccGXZ@OpenReview" onclick="foldPdfKimi('79O2XccGXZ@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="jNCTdUsQaC@OpenReview" class="panel paper" keywords="weights,gmms,uniform,pancakes,bound,complexity,task,min,posits,gaussians">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jNCTdUsQaC" target="_blank" title="15/225"><span class="index notranslate">#15</span></a>
                <a id="title-jNCTdUsQaC@OpenReview" class="title-link" href="/venue/jNCTdUsQaC@OpenReview" target="_blank">On Learning Parallel Pancakes with Mostly Uniform Weights</a>
                <a id="pdf-jNCTdUsQaC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jNCTdUsQaC@OpenReview', this)" data="https://openreview.net/pdf?id=jNCTdUsQaC">[PDF<sup id="pdf-stars-jNCTdUsQaC@OpenReview">3</sup>]</a>
                <a id="copy-jNCTdUsQaC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jNCTdUsQaC@OpenReview')">[Copy]</a>
                <a id="kimi-jNCTdUsQaC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jNCTdUsQaC@OpenReview', this)">[Kimi<sup id="kimi-stars-jNCTdUsQaC@OpenReview">2</sup>]</a>
                <a id="rel-jNCTdUsQaC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jNCTdUsQaC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jNCTdUsQaC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ilias Diakonikolas" target="_blank">Ilias Diakonikolas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Kane" target="_blank">Daniel Kane</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sushrut Karmalkar" target="_blank">Sushrut Karmalkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jasper Lee" target="_blank">Jasper Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thanasis Pittas" target="_blank">Thanasis Pittas</a>
            </p>
            <p id="summary-jNCTdUsQaC@OpenReview" class="summary">We study the complexity of learning <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-13" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">k</script>-mixtures of Gaussians (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mi" id="MathJax-Span-18" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-4">k</script>-GMMs) on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="msubsup" id="MathJax-Span-21"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.607em, -999.997em); top: -2.445em; left: 0em;"><span class="texatom" id="MathJax-Span-22"><span class="mrow" id="MathJax-Span-23"><span class="mi" id="MathJax-Span-24" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.732em;"><span class="mi" id="MathJax-Span-25" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-5">\mathbb R^d</script>. This task is known to have complexity <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-26" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1002.03em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-27"><span class="msubsup" id="MathJax-Span-28"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-29" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-30"><span class="mrow" id="MathJax-Span-31"><span class="mi" id="MathJax-Span-32" style="font-size: 70.7%; font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-33" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-34" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-35" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-6">d^{\Omega(k)}</script> in full generality. To circumvent this exponential lower bound on the number of components, research has focused on learning families of GMMs satisfying additional structural properties. A natural assumption posits that the component weights are not exponentially small and that the components have the same unknown covariance. Recent work gave a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-36" style="width: 6.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1005.26em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-37"><span class="msubsup" id="MathJax-Span-38"><span style="display: inline-block; position: relative; width: 5.263em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-39" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-40"><span class="mrow" id="MathJax-Span-41"><span class="mi" id="MathJax-Span-42" style="font-size: 70.7%; font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-43" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-44" style="font-size: 70.7%; font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-45" style="font-size: 70.7%;"></span><span class="mo" id="MathJax-Span-46" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-47" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-48"><span class="mrow" id="MathJax-Span-49"><span class="mo" id="MathJax-Span-50" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="msubsup" id="MathJax-Span-51"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-52" style="font-size: 70.7%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.029em; left: 0.523em;"><span class="texatom" id="MathJax-Span-53"><span class="mrow" id="MathJax-Span-54"><span class="mo" id="MathJax-Span-55" style="font-size: 50%; font-family: MathJax_Main;">min</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-56" style="font-size: 70.7%; font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-57" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mi>O</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo movablelimits="true" form="prefix">min</mo></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-7">d^{O(\log(1/w_{\min}))}</script>-time algorithm for this class of GMMs, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-58" style="width: 2.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1001.98em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-59"><span class="msubsup" id="MathJax-Span-60"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-61" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="texatom" id="MathJax-Span-62"><span class="mrow" id="MathJax-Span-63"><span class="mo" id="MathJax-Span-64" style="font-size: 70.7%; font-family: MathJax_Main;">min</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo movablelimits="true" form="prefix">min</mo></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-8">w_{\min}</script> is the minimum weight. Our first main result is a Statistical Query (SQ) lower bound showing that this quasi-polynomial upper bound is essentially best possible, even for the special case of uniform weights. Specifically, we show that it is SQ-hard to distinguish between such a mixture and the standard Gaussian. We further explore how the distribution of weights affects the complexity of this task. Our second main result is a quasi-polynomial upper bound for the aforementioned testing task when most of the weights are uniform while a small fraction of the weights are potentially arbitrary.</p>
            <p id="subjects-jNCTdUsQaC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jNCTdUsQaC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jNCTdUsQaC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jNCTdUsQaC@OpenReview" onclick="foldPdfKimi('jNCTdUsQaC@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="GFsMJKt9Kp@OpenReview" class="panel paper" keywords="benign,samples,tuning,fine,outlier,llms,severely,safety,inf,attack">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GFsMJKt9Kp" target="_blank" title="16/225"><span class="index notranslate">#16</span></a>
                <a id="title-GFsMJKt9Kp@OpenReview" class="title-link" href="/venue/GFsMJKt9Kp@OpenReview" target="_blank">Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety</a>
                <a id="pdf-GFsMJKt9Kp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GFsMJKt9Kp@OpenReview', this)" data="https://openreview.net/pdf?id=GFsMJKt9Kp">[PDF<sup id="pdf-stars-GFsMJKt9Kp@OpenReview">11</sup>]</a>
                <a id="copy-GFsMJKt9Kp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GFsMJKt9Kp@OpenReview')">[Copy]</a>
                <a id="kimi-GFsMJKt9Kp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GFsMJKt9Kp@OpenReview', this)">[Kimi<sup id="kimi-stars-GFsMJKt9Kp@OpenReview">6</sup>]</a>
                <a id="rel-GFsMJKt9Kp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GFsMJKt9Kp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GFsMJKt9Kp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zihan Guan" target="_blank">Zihan Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengxuan Hu" target="_blank">Mengxuan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronghang Zhu" target="_blank">Ronghang Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Li" target="_blank">Sheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anil Vullikanti" target="_blank">Anil Vullikanti</a>
            </p>
            <p id="summary-GFsMJKt9Kp@OpenReview" class="summary">Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter.</p>
            <p id="subjects-GFsMJKt9Kp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GFsMJKt9Kp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GFsMJKt9Kp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GFsMJKt9Kp@OpenReview" onclick="foldPdfKimi('GFsMJKt9Kp@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="J5MmGPWKfb@OpenReview" class="panel paper" keywords="overlap,train,test,language,developers,publish,public,statistics,report,models">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=J5MmGPWKfb" target="_blank" title="17/225"><span class="index notranslate">#17</span></a>
                <a id="title-J5MmGPWKfb@OpenReview" class="title-link" href="/venue/J5MmGPWKfb@OpenReview" target="_blank">Position: Language model developers should report train-test overlap</a>
                <a id="pdf-J5MmGPWKfb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('J5MmGPWKfb@OpenReview', this)" data="https://openreview.net/pdf?id=J5MmGPWKfb">[PDF<sup id="pdf-stars-J5MmGPWKfb@OpenReview">5</sup>]</a>
                <a id="copy-J5MmGPWKfb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('J5MmGPWKfb@OpenReview')">[Copy]</a>
                <a id="kimi-J5MmGPWKfb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('J5MmGPWKfb@OpenReview', this)">[Kimi<sup id="kimi-stars-J5MmGPWKfb@OpenReview">10</sup>]</a>
                <a id="rel-J5MmGPWKfb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('J5MmGPWKfb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-J5MmGPWKfb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andy Zhang" target="_blank">Andy Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Klyman" target="_blank">Kevin Klyman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Mai" target="_blank">Yifan Mai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yoav Levine" target="_blank">Yoav Levine</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yian Zhang" target="_blank">Yian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishi Bommasani" target="_blank">Rishi Bommasani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Percy Liang" target="_blank">Percy Liang</a>
            </p>
            <p id="summary-J5MmGPWKfb@OpenReview" class="summary">Language models are extensively evaluated, but correctly interpreting evaluation results requires knowledge of train-test overlap, which refers to the extent to which the language model is trained on the very data it is being tested on. The public currently lacks adequate information about train-test overlap: most models have no public train-test overlap statistics, and third parties cannot directly measure train-test overlap since they do not have access to the training data. To make this clear, we document the practices of 30 models, finding that just 9 models report train-test overlap: 4 models release training data under open-source licenses, enabling the community to directly measure train-test overlap, and 5 models publish their train-test overlap methodology and statistics. By engaging with language model developers, we provide novel information about train-test overlap for three additional models. Overall, this position paper argues that language model developers should publish train-test overlap statistics and/or training data whenever they report evaluation results on public test sets. We hope our work increases transparency into train-test overlap to increase the community-wide trust in model evaluations.</p>
            <p id="subjects-J5MmGPWKfb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-J5MmGPWKfb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-J5MmGPWKfb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-J5MmGPWKfb@OpenReview" onclick="foldPdfKimi('J5MmGPWKfb@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="0ysC6VS0y3@OpenReview" class="panel paper" keywords="icl,task,encoding,transformers,decoding,vectors,representations,pretraining,finetuning,olmo">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0ysC6VS0y3" target="_blank" title="18/225"><span class="index notranslate">#18</span></a>
                <a id="title-0ysC6VS0y3@OpenReview" class="title-link" href="/venue/0ysC6VS0y3@OpenReview" target="_blank">Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective</a>
                <a id="pdf-0ysC6VS0y3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0ysC6VS0y3@OpenReview', this)" data="https://openreview.net/pdf?id=0ysC6VS0y3">[PDF<sup id="pdf-stars-0ysC6VS0y3@OpenReview">9</sup>]</a>
                <a id="copy-0ysC6VS0y3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0ysC6VS0y3@OpenReview')">[Copy]</a>
                <a id="kimi-0ysC6VS0y3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0ysC6VS0y3@OpenReview', this)">[Kimi<sup id="kimi-stars-0ysC6VS0y3@OpenReview">10</sup>]</a>
                <a id="rel-0ysC6VS0y3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0ysC6VS0y3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0ysC6VS0y3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seungwook Han" target="_blank">Seungwook Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinyeop Song" target="_blank">Jinyeop Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeff Gore" target="_blank">Jeff Gore</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pulkit Agrawal" target="_blank">Pulkit Agrawal</a>
            </p>
            <p id="summary-0ysC6VS0y3@OpenReview" class="summary">Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., "Finding the first noun in a sentence.") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.</p>
            <p id="subjects-0ysC6VS0y3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-0ysC6VS0y3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0ysC6VS0y3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0ysC6VS0y3@OpenReview" onclick="foldPdfKimi('0ysC6VS0y3@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="10l1pGeOcK@OpenReview" class="panel paper" keywords="safe,pruning,sparse,finding,flat,minima,sparsifying,performance,subnetworks,optimization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=10l1pGeOcK" target="_blank" title="19/225"><span class="index notranslate">#19</span></a>
                <a id="title-10l1pGeOcK@OpenReview" class="title-link" href="/venue/10l1pGeOcK@OpenReview" target="_blank">SAFE: Finding Sparse and Flat Minima to Improve Pruning</a>
                <a id="pdf-10l1pGeOcK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('10l1pGeOcK@OpenReview', this)" data="https://openreview.net/pdf?id=10l1pGeOcK">[PDF<sup id="pdf-stars-10l1pGeOcK@OpenReview">10</sup>]</a>
                <a id="copy-10l1pGeOcK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('10l1pGeOcK@OpenReview')">[Copy]</a>
                <a id="kimi-10l1pGeOcK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('10l1pGeOcK@OpenReview', this)">[Kimi<sup id="kimi-stars-10l1pGeOcK@OpenReview">5</sup>]</a>
                <a id="rel-10l1pGeOcK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('10l1pGeOcK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-10l1pGeOcK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongyeop Lee" target="_blank">Dongyeop Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwanhee Lee" target="_blank">Kwanhee Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinseok Chung" target="_blank">Jinseok Chung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Namhoon Lee" target="_blank">Namhoon Lee</a>
            </p>
            <p id="summary-10l1pGeOcK@OpenReview" class="summary">Sparsifying neural networks often suffers from seemingly inevitable performance degradation, and it remains challenging to restore the original performance despite much recent progress.Motivated by recent studies in robust optimization, we aim to tackle this problem by finding subnetworks that are both sparse and flat at the same time.Specifically, we formulate pruning as a sparsity-constrained optimization problem where flatness is encouraged as an objective.We solve it explicitly via an augmented Lagrange dual approach and extend it further by proposing a generalized projection operation, resulting in novel pruning methods called SAFE and its extension, SAFE<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-65" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-66"><span class="msubsup" id="MathJax-Span-67"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-68"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="mo" id="MathJax-Span-69" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mo>+</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-9">^+</script>.Extensive evaluations on standard image classification and language modeling tasks reveal that SAFE consistently yields sparse networks with improved generalization performance, which compares competitively to well-established baselines.In addition, SAFE demonstrates resilience to noisy data, making it well-suited for real-world conditions.</p>
            <p id="subjects-10l1pGeOcK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-10l1pGeOcK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-10l1pGeOcK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-10l1pGeOcK@OpenReview" onclick="foldPdfKimi('10l1pGeOcK@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="1w0Zp99dnX@OpenReview" class="panel paper" keywords="grfs,forests,generalized,random,fixed,computationally,alternative,grf,dimensions,gradient">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1w0Zp99dnX" target="_blank" title="20/225"><span class="index notranslate">#20</span></a>
                <a id="title-1w0Zp99dnX@OpenReview" class="title-link" href="/venue/1w0Zp99dnX@OpenReview" target="_blank">Generalized Random Forests Using Fixed-Point Trees</a>
                <a id="pdf-1w0Zp99dnX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1w0Zp99dnX@OpenReview', this)" data="https://openreview.net/pdf?id=1w0Zp99dnX">[PDF<sup id="pdf-stars-1w0Zp99dnX@OpenReview">3</sup>]</a>
                <a id="copy-1w0Zp99dnX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1w0Zp99dnX@OpenReview')">[Copy]</a>
                <a id="kimi-1w0Zp99dnX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1w0Zp99dnX@OpenReview', this)">[Kimi<sup id="kimi-stars-1w0Zp99dnX@OpenReview">3</sup>]</a>
                <a id="rel-1w0Zp99dnX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1w0Zp99dnX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1w0Zp99dnX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=David Fleischer" target="_blank">David Fleischer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David A Stephens" target="_blank">David A Stephens</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Archer Yang" target="_blank">Archer Yang</a>
            </p>
            <p id="summary-1w0Zp99dnX@OpenReview" class="summary">We propose a computationally efficient alternative to generalized random forests (GRFs) for estimating heterogeneous effects in large dimensions. While GRFs rely on a gradient-based splitting criterion, which in large dimensions is computationally expensive and unstable, our method introduces a fixed-point approximation that eliminates the need for Jacobian estimation. This gradient-free approach preserves GRF’s theoretical guarantees of consistency and asymptotic normality while significantly improving computational efficiency. We demonstrate that our method achieves a speedup of multiple times over standard GRFs without compromising statistical accuracy. Experiments on both simulated and real-world data validate our approach. Our findings suggest that the proposed method is a scalable alternative for localized effect estimation in machine learning and causal inference applications.</p>
            <p id="subjects-1w0Zp99dnX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-1w0Zp99dnX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1w0Zp99dnX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1w0Zp99dnX@OpenReview" onclick="foldPdfKimi('1w0Zp99dnX@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="2Oqm2IzTy9@OpenReview" class="panel paper" keywords="lmo,lmos,norm,nanogpt,optimization,speedups,adam,training,deep,constrained">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2Oqm2IzTy9" target="_blank" title="21/225"><span class="index notranslate">#21</span></a>
                <a id="title-2Oqm2IzTy9@OpenReview" class="title-link" href="/venue/2Oqm2IzTy9@OpenReview" target="_blank">Training Deep Learning Models with Norm-Constrained LMOs</a>
                <a id="pdf-2Oqm2IzTy9@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2Oqm2IzTy9@OpenReview', this)" data="https://openreview.net/pdf?id=2Oqm2IzTy9">[PDF<sup id="pdf-stars-2Oqm2IzTy9@OpenReview">5</sup>]</a>
                <a id="copy-2Oqm2IzTy9@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2Oqm2IzTy9@OpenReview')">[Copy]</a>
                <a id="kimi-2Oqm2IzTy9@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2Oqm2IzTy9@OpenReview', this)">[Kimi<sup id="kimi-stars-2Oqm2IzTy9@OpenReview">5</sup>]</a>
                <a id="rel-2Oqm2IzTy9@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2Oqm2IzTy9@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2Oqm2IzTy9@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Pethick" target="_blank">Thomas Pethick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanyun Xie" target="_blank">Wanyun Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kimon Antonakopoulos" target="_blank">Kimon Antonakopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu Zhu" target="_blank">Zhenyu Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antonio Silveti-Falls" target="_blank">Antonio Silveti-Falls</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Volkan Cevher" target="_blank">Volkan Cevher</a>
            </p>
            <p id="summary-2Oqm2IzTy9@OpenReview" class="summary">In this work, we study optimization methods that leverage the linear minimization oracle (LMO) over a norm-ball. We propose a new stochastic family of algorithms that uses the LMO to adapt to the geometry of the problem and, perhaps surprisingly, show that they can be applied to unconstrained problems. The resulting update rule unifies several existing optimization methods under a single framework. Furthermore, we propose an explicit choice of norm for deep architectures, which, as a side benefit, leads to the transferability of hyperparameters across model sizes. Experimentally, we demonstrate significant speedups on nanoGPT training without any reliance on Adam. The proposed method is memory-efficient, requiring only one set of model weights and one set of gradients, which can be stored in half-precision.</p>
            <p id="subjects-2Oqm2IzTy9@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2Oqm2IzTy9@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2Oqm2IzTy9@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2Oqm2IzTy9@OpenReview" onclick="foldPdfKimi('2Oqm2IzTy9@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="2PWn1LtCwP@OpenReview" class="panel paper" keywords="survival,censored,conformalized,censoring,right,imputes,robust,conformal,doubly,data">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2PWn1LtCwP" target="_blank" title="22/225"><span class="index notranslate">#22</span></a>
                <a id="title-2PWn1LtCwP@OpenReview" class="title-link" href="/venue/2PWn1LtCwP@OpenReview" target="_blank">Doubly Robust Conformalized Survival Analysis with Right-Censored Data</a>
                <a id="pdf-2PWn1LtCwP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2PWn1LtCwP@OpenReview', this)" data="https://openreview.net/pdf?id=2PWn1LtCwP">[PDF<sup id="pdf-stars-2PWn1LtCwP@OpenReview">4</sup>]</a>
                <a id="copy-2PWn1LtCwP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2PWn1LtCwP@OpenReview')">[Copy]</a>
                <a id="kimi-2PWn1LtCwP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2PWn1LtCwP@OpenReview', this)">[Kimi<sup id="kimi-stars-2PWn1LtCwP@OpenReview">5</sup>]</a>
                <a id="rel-2PWn1LtCwP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2PWn1LtCwP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2PWn1LtCwP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Sesia" target="_blank">Matteo Sesia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=vladimir svetnik" target="_blank">vladimir svetnik</a>
            </p>
            <p id="summary-2PWn1LtCwP@OpenReview" class="summary">We present a conformal inference method for constructing lower prediction bounds for survival times from right-censored data, extending recent approaches designed for more restrictive type-I censoring scenarios. The proposed method imputes unobserved censoring times using a machine learning model, and then analyzes the imputed data using a survival model calibrated via weighted conformal inference. This approach is theoretically supported by an asymptotic double robustness property. Empirical studies on simulated and real data demonstrate that our method leads to relatively informative predictive inferences and is especially robust in challenging settings where the survival model may be inaccurate.</p>
            <p id="subjects-2PWn1LtCwP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2PWn1LtCwP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2PWn1LtCwP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2PWn1LtCwP@OpenReview" onclick="foldPdfKimi('2PWn1LtCwP@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="2dz6psiiA0@OpenReview" class="panel paper" keywords="tom,bayesian,multimodal,mental,mind,planner,complexity,lms,reasoning,scalable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2dz6psiiA0" target="_blank" title="23/225"><span class="index notranslate">#23</span></a>
                <a id="title-2dz6psiiA0@OpenReview" class="title-link" href="/venue/2dz6psiiA0@OpenReview" target="_blank">Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner</a>
                <a id="pdf-2dz6psiiA0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2dz6psiiA0@OpenReview', this)" data="https://openreview.net/pdf?id=2dz6psiiA0">[PDF<sup id="pdf-stars-2dz6psiiA0@OpenReview">9</sup>]</a>
                <a id="copy-2dz6psiiA0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2dz6psiiA0@OpenReview')">[Copy]</a>
                <a id="kimi-2dz6psiiA0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2dz6psiiA0@OpenReview', this)">[Kimi<sup id="kimi-stars-2dz6psiiA0@OpenReview">8</sup>]</a>
                <a id="rel-2dz6psiiA0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2dz6psiiA0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2dz6psiiA0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chunhui Zhang" target="_blank">Chunhui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongyu Ouyang" target="_blank">Zhongyu Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwonjoon Lee" target="_blank">Kwonjoon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nakul Agarwal" target="_blank">Nakul Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sean Houlihan" target="_blank">Sean Houlihan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soroush Vosoughi" target="_blank">Soroush Vosoughi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shao-Yuan Lo" target="_blank">Shao-Yuan Lo</a>
            </p>
            <p id="summary-2dz6psiiA0@OpenReview" class="summary">Theory-of-mind (ToM) enables humans to infer mental states—such as beliefs, desires, and intentions—forming the foundation of social cognition. Existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning but struggle with scalability in multimodal environments. They remain trapped within the gravitational pull of multi-step planning complexity, failing to generalize as task demands increase. To overcome these limitations, we propose a scalable Bayesian ToM planner. It breaks down ToM complexity into stepwise Bayesian updates. Meanwhile, weak-to-strong control specializes smaller LMs to refine ToM-specific likelihood estimation, transferring their ToM reasoning behavior to larger LMs (7B to 405B) for social and world knowledge integration. This synergistic approach enables scalability, aligning large-model inference with human mental states with Bayesian principles. Extensive experiments demonstrate a 4.6% improvement in accuracy over state-of-the-art methods on multimodal ToM benchmarks, including unseen scenarios, establishing a new standard for modeling human mental states in complex environments.</p>
            <p id="subjects-2dz6psiiA0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-2dz6psiiA0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2dz6psiiA0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2dz6psiiA0@OpenReview" onclick="foldPdfKimi('2dz6psiiA0@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="3Z827FtMNe@OpenReview" class="panel paper" keywords="oversight,capa,mistakes,judge,capabilities,similarity,model,harder,undermines,alike">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3Z827FtMNe" target="_blank" title="24/225"><span class="index notranslate">#24</span></a>
                <a id="title-3Z827FtMNe@OpenReview" class="title-link" href="/venue/3Z827FtMNe@OpenReview" target="_blank">Great Models Think Alike and this Undermines AI Oversight</a>
                <a id="pdf-3Z827FtMNe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3Z827FtMNe@OpenReview', this)" data="https://openreview.net/pdf?id=3Z827FtMNe">[PDF<sup id="pdf-stars-3Z827FtMNe@OpenReview">5</sup>]</a>
                <a id="copy-3Z827FtMNe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3Z827FtMNe@OpenReview')">[Copy]</a>
                <a id="kimi-3Z827FtMNe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3Z827FtMNe@OpenReview', this)">[Kimi<sup id="kimi-stars-3Z827FtMNe@OpenReview">7</sup>]</a>
                <a id="rel-3Z827FtMNe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3Z827FtMNe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3Z827FtMNe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shashwat Goel" target="_blank">Shashwat Goel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joschka Strüber" target="_blank">Joschka Strüber</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ilze Amanda Auzina" target="_blank">Ilze Amanda Auzina</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karuna Chandra" target="_blank">Karuna Chandra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ponnurangam Kumaraguru" target="_blank">Ponnurangam Kumaraguru</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Douwe Kiela" target="_blank">Douwe Kiela</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ameya Pandurang Prabhu" target="_blank">Ameya Pandurang Prabhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthias Bethge" target="_blank">Matthias Bethge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Geiping" target="_blank">Jonas Geiping</a>
            </p>
            <p id="summary-3Z827FtMNe@OpenReview" class="summary">As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as *AI Oversight*. We study how model similarity affects both aspects of AI oversight by proposing *Chance Adjusted Probabilistic Agreement (CAPA)*--a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that *LLM-as-a-judge* scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from *weak-to-strong generalization*. As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend--model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.</p>
            <p id="subjects-3Z827FtMNe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3Z827FtMNe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3Z827FtMNe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3Z827FtMNe@OpenReview" onclick="foldPdfKimi('3Z827FtMNe@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="3rB0bVU6z6@OpenReview" class="panel paper" keywords="experts,frontier,bench,human,attempts,hours,agents,budgets,score,agent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3rB0bVU6z6" target="_blank" title="25/225"><span class="index notranslate">#25</span></a>
                <a id="title-3rB0bVU6z6@OpenReview" class="title-link" href="/venue/3rB0bVU6z6@OpenReview" target="_blank">RE-Bench: Evaluating Frontier AI R&amp;D Capabilities of Language Model Agents against Human Experts</a>
                <a id="pdf-3rB0bVU6z6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3rB0bVU6z6@OpenReview', this)" data="https://openreview.net/pdf?id=3rB0bVU6z6">[PDF<sup id="pdf-stars-3rB0bVU6z6@OpenReview">3</sup>]</a>
                <a id="copy-3rB0bVU6z6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3rB0bVU6z6@OpenReview')">[Copy]</a>
                <a id="kimi-3rB0bVU6z6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3rB0bVU6z6@OpenReview', this)">[Kimi<sup id="kimi-stars-3rB0bVU6z6@OpenReview">2</sup>]</a>
                <a id="rel-3rB0bVU6z6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3rB0bVU6z6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3rB0bVU6z6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hjalmar Wijk" target="_blank">Hjalmar Wijk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Lin" target="_blank">Tao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joel Becker" target="_blank">Joel Becker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sami Jawhar" target="_blank">Sami Jawhar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neev Parikh" target="_blank">Neev Parikh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Broadley" target="_blank">Thomas Broadley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lawrence Chan" target="_blank">Lawrence Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Chen" target="_blank">Michael Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua Clymer" target="_blank">Joshua Clymer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jai Dhyani" target="_blank">Jai Dhyani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elena Ericheva" target="_blank">Elena Ericheva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katharyn Garcia" target="_blank">Katharyn Garcia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Goodrich" target="_blank">Brian Goodrich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikola Jurkovic" target="_blank">Nikola Jurkovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Megan Kinniment" target="_blank">Megan Kinniment</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aron Lajko" target="_blank">Aron Lajko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seraphina Nix" target="_blank">Seraphina Nix</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lucas Jun Koba Sato" target="_blank">Lucas Jun Koba Sato</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=William Saunders" target="_blank">William Saunders</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maksym Taran" target="_blank">Maksym Taran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben West" target="_blank">Ben West</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elizabeth Barnes" target="_blank">Elizabeth Barnes</a>
            </p>
            <p id="summary-3rB0bVU6z6@OpenReview" class="summary">Frontier AI safety policies highlight automation of AI research and development (R&amp;D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&amp;D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, V1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-70" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-71"><span class="mi" id="MathJax-Span-72" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-10">k</script> with varying time budgets and agent designs, and find that the best AI agents achieve a score 4× higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2× the score of the top AI agent when both are given 32 total hours (across different attempts).</p>
            <p id="subjects-3rB0bVU6z6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3rB0bVU6z6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3rB0bVU6z6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3rB0bVU6z6@OpenReview" onclick="foldPdfKimi('3rB0bVU6z6@OpenReview', this)" class="hr hr-fold">
        </div>
    <div id="4gWE7CMOlH@OpenReview" class="panel paper" keywords="reasoning,embedding,exploration,soft,controlled,navigating,search,language,optimises,verifier">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4gWE7CMOlH" target="_blank" title="26/225"><span class="index notranslate">#26</span></a>
                <a id="title-4gWE7CMOlH@OpenReview" class="title-link" href="/venue/4gWE7CMOlH@OpenReview" target="_blank">Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration</a>
                <a id="pdf-4gWE7CMOlH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4gWE7CMOlH@OpenReview', this)" data="https://openreview.net/pdf?id=4gWE7CMOlH">[PDF<sup id="pdf-stars-4gWE7CMOlH@OpenReview">20</sup>]</a>
                <a id="copy-4gWE7CMOlH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4gWE7CMOlH@OpenReview')">[Copy]</a>
                <a id="kimi-4gWE7CMOlH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4gWE7CMOlH@OpenReview', this)">[Kimi<sup id="kimi-stars-4gWE7CMOlH@OpenReview">16</sup>]</a>
                <a id="rel-4gWE7CMOlH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4gWE7CMOlH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4gWE7CMOlH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qinglin Zhu" target="_blank">Qinglin Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runcong Zhao" target="_blank">Runcong Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanqi Yan" target="_blank">Hanqi Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulan He" target="_blank">Yulan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yudong Chen" target="_blank">Yudong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Gui" target="_blank">Lin Gui</a>
            </p>
            <p id="summary-4gWE7CMOlH@OpenReview" class="summary">Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution.</p>
            <p id="subjects-4gWE7CMOlH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4gWE7CMOlH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4gWE7CMOlH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4gWE7CMOlH@OpenReview" onclick="foldPdfKimi('4gWE7CMOlH@OpenReview', this)" class="hr hr-fold">
        </div><div id="4qIP1sXcR1@OpenReview" class="panel paper" keywords="resq,ptq,quantization,bit,precision,subspace,mixed,anonymous,outliers,spinquant">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4qIP1sXcR1" target="_blank" title="27/225"><span class="index notranslate">#27</span></a>
                <a id="title-4qIP1sXcR1@OpenReview" class="title-link" href="/venue/4qIP1sXcR1@OpenReview" target="_blank">ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals</a>
                <a id="pdf-4qIP1sXcR1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4qIP1sXcR1@OpenReview', this)" data="https://openreview.net/pdf?id=4qIP1sXcR1">[PDF<sup id="pdf-stars-4qIP1sXcR1@OpenReview">16</sup>]</a>
                <a id="copy-4qIP1sXcR1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4qIP1sXcR1@OpenReview')">[Copy]</a>
                <a id="kimi-4qIP1sXcR1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4qIP1sXcR1@OpenReview', this)">[Kimi<sup id="kimi-stars-4qIP1sXcR1@OpenReview">5</sup>]</a>
                <a id="rel-4qIP1sXcR1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4qIP1sXcR1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4qIP1sXcR1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Utkarsh Saxena" target="_blank">Utkarsh Saxena</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sayeh Sharify" target="_blank">Sayeh Sharify</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaushik Roy" target="_blank">Kaushik Roy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Wang" target="_blank">Xin Wang</a>
            </p>
            <p id="summary-4qIP1sXcR1@OpenReview" class="summary">Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g.~8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama and Qwen2.5 families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33\% lower perplexity on Wikitext than the next best method SpinQuant, and upto 3X speedup over 16-bit baseline. Anonymous code repository available at https://anonymous.4open.science/r/project-resq-2142.</p>
            <p id="subjects-4qIP1sXcR1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4qIP1sXcR1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4qIP1sXcR1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4qIP1sXcR1@OpenReview" onclick="foldPdfKimi('4qIP1sXcR1@OpenReview', this)" class="hr hr-fold">
        </div><div id="4vAa0A98xI@OpenReview" class="panel paper" keywords="copinn,cognitive,informed,pinn,physical,easy,hard,physics,upp,regions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4vAa0A98xI" target="_blank" title="28/225"><span class="index notranslate">#28</span></a>
                <a id="title-4vAa0A98xI@OpenReview" class="title-link" href="/venue/4vAa0A98xI@OpenReview" target="_blank">CoPINN: Cognitive Physics-Informed Neural Networks</a>
                <a id="pdf-4vAa0A98xI@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4vAa0A98xI@OpenReview', this)" data="https://openreview.net/pdf?id=4vAa0A98xI">[PDF<sup id="pdf-stars-4vAa0A98xI@OpenReview">17</sup>]</a>
                <a id="copy-4vAa0A98xI@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4vAa0A98xI@OpenReview')">[Copy]</a>
                <a id="kimi-4vAa0A98xI@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4vAa0A98xI@OpenReview', this)">[Kimi<sup id="kimi-stars-4vAa0A98xI@OpenReview">4</sup>]</a>
                <a id="rel-4vAa0A98xI@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4vAa0A98xI@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4vAa0A98xI@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Duan" target="_blank">Siyuan Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenyuan Wu" target="_blank">Wenyuan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Hu" target="_blank">Peng Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenwen Ren" target="_blank">Zhenwen Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dezhong Peng" target="_blank">Dezhong Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Sun" target="_blank">Yuan Sun</a>
            </p>
            <p id="summary-4vAa0A98xI@OpenReview" class="summary">Physics-informed neural networks (PINNs) aim to constrain the outputs and gradients of deep learning models to satisfy specified governing physics equations, which have demonstrated significant potential for solving partial differential equations (PDEs). Although existing PINN methods have achieved pleasing performance, they always treat both easy and hard sample points indiscriminately, especially ones in the physical boundaries. This easily causes the PINN model to fall into undesirable local minima and unstable learning, thereby resulting in an Unbalanced Prediction Problem (UPP). To deal with this daunting problem, we propose a novel framework named Cognitive Physical Informed Neural Network (CoPINN) that imitates the human cognitive learning manner from easy to hard. Specifically, we first employ separable subnetworks to encode independent one-dimensional coordinates and apply an aggregation scheme to generate multi-dimensional predicted physical variables. Then, during the training phase, we dynamically evaluate the difficulty of each sample according to the gradient of the PDE residuals. Finally, we propose a cognitive training scheduler to progressively optimize the entire sampling regions from easy to hard, thereby embracing robustness and generalization against predicting physical boundary regions. Extensive experiments demonstrate that our CoPINN achieves state-of-the-art performance, particularly significantly reducing prediction errors in stubborn regions.</p>
            <p id="subjects-4vAa0A98xI@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4vAa0A98xI@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4vAa0A98xI@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4vAa0A98xI@OpenReview" onclick="foldPdfKimi('4vAa0A98xI@OpenReview', this)" class="hr hr-fold">
        </div><div id="4yHWV3B6g4@OpenReview" class="panel paper" keywords="raptor,volumes,medical,pretrained,foundation,volumetric,voco,train,embeddings,tokens">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4yHWV3B6g4" target="_blank" title="29/225"><span class="index notranslate">#29</span></a>
                <a id="title-4yHWV3B6g4@OpenReview" class="title-link" href="/venue/4yHWV3B6g4@OpenReview" target="_blank">Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models</a>
                <a id="pdf-4yHWV3B6g4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4yHWV3B6g4@OpenReview', this)" data="https://openreview.net/pdf?id=4yHWV3B6g4">[PDF<sup id="pdf-stars-4yHWV3B6g4@OpenReview">5</sup>]</a>
                <a id="copy-4yHWV3B6g4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4yHWV3B6g4@OpenReview')">[Copy]</a>
                <a id="kimi-4yHWV3B6g4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4yHWV3B6g4@OpenReview', this)">[Kimi<sup id="kimi-stars-4yHWV3B6g4@OpenReview">3</sup>]</a>
                <a id="rel-4yHWV3B6g4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4yHWV3B6g4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4yHWV3B6g4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ulzee An" target="_blank">Ulzee An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moonseong Jeong" target="_blank">Moonseong Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Lee" target="_blank">Simon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditya Gorla" target="_blank">Aditya Gorla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhe Yang" target="_blank">Yuzhe Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sriram Sankararaman" target="_blank">Sriram Sankararaman</a>
            </p>
            <p id="summary-4yHWV3B6g4@OpenReview" class="summary">Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes.To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining rich semantic information. Extensive experiments on 10 diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3 SuPreM, +6 MISFM, +10 Merlin, +13 VoCo, and +14 SLIViT), while entirely bypassing the need for costly training. Our results highlight Raptor's effectiveness and versatility as a foundation for advancing deep learning-based methods for medical volumes (code: github.com/sriramlab/raptor).</p>
            <p id="subjects-4yHWV3B6g4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4yHWV3B6g4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4yHWV3B6g4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4yHWV3B6g4@OpenReview" onclick="foldPdfKimi('4yHWV3B6g4@OpenReview', this)" class="hr hr-fold">
        </div><div id="5IpVe9PH14@OpenReview" class="panel paper" keywords="variance,rewards,catoni,reward,contextual,regret,bandits,tailed,bound,range">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5IpVe9PH14" target="_blank" title="30/225"><span class="index notranslate">#30</span></a>
                <a id="title-5IpVe9PH14@OpenReview" class="title-link" href="/venue/5IpVe9PH14@OpenReview" target="_blank">Catoni Contextual Bandits are Robust to Heavy-tailed Rewards</a>
                <a id="pdf-5IpVe9PH14@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5IpVe9PH14@OpenReview', this)" data="https://openreview.net/pdf?id=5IpVe9PH14">[PDF<sup id="pdf-stars-5IpVe9PH14@OpenReview">5</sup>]</a>
                <a id="copy-5IpVe9PH14@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5IpVe9PH14@OpenReview')">[Copy]</a>
                <a id="kimi-5IpVe9PH14@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5IpVe9PH14@OpenReview', this)">[Kimi<sup id="kimi-stars-5IpVe9PH14@OpenReview">5</sup>]</a>
                <a id="rel-5IpVe9PH14@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5IpVe9PH14@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5IpVe9PH14@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenlu Ye" target="_blank">Chenlu Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujia Jin" target="_blank">Yujia Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alekh Agarwal" target="_blank">Alekh Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Zhang" target="_blank">Tong Zhang</a>
            </p>
            <p id="summary-5IpVe9PH14@OpenReview" class="summary">Typical contextual bandit algorithms assume that the rewards at each round lie in some fixed range <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-73" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.14em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-74"><span class="mo" id="MathJax-Span-75" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-76" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-77" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-78" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">R</span><span class="mo" id="MathJax-Span-79" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>R</mi><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-11">[0, R]</script>, and their regret scales polynomially with this reward range <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-80" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-81"><span class="mi" id="MathJax-Span-82" style="font-family: MathJax_Math-italic;">R</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi></math></span></span><script type="math/tex" id="MathJax-Element-12">R</script>. However, many practical scenarios naturally involve heavy-tailed rewards or rewards where the worst-case range can be substantially larger than the variance. In this paper, we develop an algorithmic approach building on Catoni's estimator from robust statistics, and apply it to contextual bandits with general function approximation. When the variance of the reward at each round is known, we use a variance-weighted regression approach and establish a regret bound that depends only on the cumulative reward variance and logarithmically on the reward range <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-83" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-84"><span class="mi" id="MathJax-Span-85" style="font-family: MathJax_Math-italic;">R</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi></math></span></span><script type="math/tex" id="MathJax-Element-13">R</script> as well as the number of rounds <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-86" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-87"><span class="mi" id="MathJax-Span-88" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-14">T</script>. For the unknown-variance case, we further propose a careful peeling-based algorithm and remove the need for cumbersome variance estimation. With additional dependence on the fourth moment, our algorithm also enjoys a variance-based bound with logarithmic reward-range dependence. Moreover, we demonstrate the optimality of the leading-order term in our regret bound through a matching lower bound.</p>
            <p id="subjects-5IpVe9PH14@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5IpVe9PH14@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5IpVe9PH14@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5IpVe9PH14@OpenReview" onclick="foldPdfKimi('5IpVe9PH14@OpenReview', this)" class="hr hr-fold">
        </div><div id="5hZCK4Wbex@OpenReview" class="panel paper" keywords="llms,icl,superposition,task,phenomenon,context,tasks,everything,everywhere,learn">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5hZCK4Wbex" target="_blank" title="31/225"><span class="index notranslate">#31</span></a>
                <a id="title-5hZCK4Wbex@OpenReview" class="title-link" href="/venue/5hZCK4Wbex@OpenReview" target="_blank">Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition</a>
                <a id="pdf-5hZCK4Wbex@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5hZCK4Wbex@OpenReview', this)" data="https://openreview.net/pdf?id=5hZCK4Wbex">[PDF<sup id="pdf-stars-5hZCK4Wbex@OpenReview">9</sup>]</a>
                <a id="copy-5hZCK4Wbex@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5hZCK4Wbex@OpenReview')">[Copy]</a>
                <a id="kimi-5hZCK4Wbex@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5hZCK4Wbex@OpenReview', this)">[Kimi<sup id="kimi-stars-5hZCK4Wbex@OpenReview">11</sup>]</a>
                <a id="rel-5hZCK4Wbex@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5hZCK4Wbex@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5hZCK4Wbex@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheyang Xiong" target="_blank">Zheyang Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jack Cai" target="_blank">Jack Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Cooper" target="_blank">John Cooper</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albert Ge" target="_blank">Albert Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vasilis Papageorgiou" target="_blank">Vasilis Papageorgiou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zack Sifakis" target="_blank">Zack Sifakis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angeliki Giannou" target="_blank">Angeliki Giannou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqian Lin" target="_blank">Ziqian Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liu Yang" target="_blank">Liu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saurabh Agarwal" target="_blank">Saurabh Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Grigorios Chrysos" target="_blank">Grigorios Chrysos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samet Oymak" target="_blank">Samet Oymak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kangwook Lee" target="_blank">Kangwook Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dimitris Papailiopoulos" target="_blank">Dimitris Papailiopoulos</a>
            </p>
            <p id="summary-5hZCK4Wbex@OpenReview" class="summary">Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term task superposition". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of "LLMs as superposition of simulators", and raise questions about the mechanisms enabling simultaneous task execution.</p>
            <p id="subjects-5hZCK4Wbex@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5hZCK4Wbex@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5hZCK4Wbex@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5hZCK4Wbex@OpenReview" onclick="foldPdfKimi('5hZCK4Wbex@OpenReview', this)" class="hr hr-fold">
        </div><div id="5hyfZ2jYfI@OpenReview" class="panel paper" keywords="teduo,language,offline,conditioned,textit,llms,generalizable,policies,unlocks,synergy">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5hyfZ2jYfI" target="_blank" title="32/225"><span class="index notranslate">#32</span></a>
                <a id="title-5hyfZ2jYfI@OpenReview" class="title-link" href="/venue/5hyfZ2jYfI@OpenReview" target="_blank">The Synergy of LLMs &amp; RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data</a>
                <a id="pdf-5hyfZ2jYfI@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5hyfZ2jYfI@OpenReview', this)" data="https://openreview.net/pdf?id=5hyfZ2jYfI">[PDF<sup id="pdf-stars-5hyfZ2jYfI@OpenReview">10</sup>]</a>
                <a id="copy-5hyfZ2jYfI@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5hyfZ2jYfI@OpenReview')">[Copy]</a>
                <a id="kimi-5hyfZ2jYfI@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5hyfZ2jYfI@OpenReview', this)">[Kimi<sup id="kimi-stars-5hyfZ2jYfI@OpenReview">4</sup>]</a>
                <a id="rel-5hyfZ2jYfI@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5hyfZ2jYfI@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5hyfZ2jYfI@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Pouplin" target="_blank">Thomas Pouplin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katarzyna Kobalczyk" target="_blank">Katarzyna Kobalczyk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Sun" target="_blank">Hao Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=M van der Schaar" target="_blank">M van der Schaar</a>
            </p>
            <p id="summary-5hyfZ2jYfI@OpenReview" class="summary">Developing autonomous agents capable of performing complex, multi-step decision-making tasks specified in natural language remains a significant challenge, particularly in realistic settings where labeled data is scarce and real-time experimentation is impractical. Existing reinforcement learning (RL) approaches often struggle to generalize to unseen goals and states, limiting their applicability. In this paper, we introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;TEDUO&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-89" style="width: 4.378em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-90"><span class="texatom" id="MathJax-Span-91"><span class="mrow" id="MathJax-Span-92"><span class="mtext" id="MathJax-Span-93" style="font-family: MathJax_Main-italic;">TEDUO</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">TEDUO</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-15">\textit{TEDUO}</script>, a novel training pipeline for offline language-conditioned policy learning in symbolic environments. Unlike conventional methods, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;TEDUO&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-94" style="width: 4.378em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-95"><span class="texatom" id="MathJax-Span-96"><span class="mrow" id="MathJax-Span-97"><span class="mtext" id="MathJax-Span-98" style="font-family: MathJax_Main-italic;">TEDUO</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">TEDUO</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-16">\textit{TEDUO}</script> operates on readily available, unlabeled datasets and addresses the challenge of generalization to previously unseen goals and states. Our approach harnesses large language models (LLMs) in a dual capacity: first, as automatization tools augmenting offline datasets with richer annotations, and second, as generalizable instruction-following agents. Empirical results demonstrate that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;TEDUO&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-99" style="width: 4.378em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-100"><span class="texatom" id="MathJax-Span-101"><span class="mrow" id="MathJax-Span-102"><span class="mtext" id="MathJax-Span-103" style="font-family: MathJax_Main-italic;">TEDUO</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">TEDUO</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-17">\textit{TEDUO}</script> achieves data-efficient learning of robust language-conditioned policies, accomplishing tasks beyond the reach of conventional RL frameworks or out-of-the-box LLMs alone.</p>
            <p id="subjects-5hyfZ2jYfI@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5hyfZ2jYfI@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5hyfZ2jYfI@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5hyfZ2jYfI@OpenReview" onclick="foldPdfKimi('5hyfZ2jYfI@OpenReview', this)" class="hr hr-fold">
        </div><div id="6gX4rP6QJW@OpenReview" class="panel paper" keywords="masked,graph,edges,curriculum,difficulty,self,autoencoder,node,supervised,pretext">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6gX4rP6QJW" target="_blank" title="33/225"><span class="index notranslate">#33</span></a>
                <a id="title-6gX4rP6QJW@OpenReview" class="title-link" href="/venue/6gX4rP6QJW@OpenReview" target="_blank">Self-supervised Masked Graph Autoencoder via Structure-aware Curriculum</a>
                <a id="pdf-6gX4rP6QJW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6gX4rP6QJW@OpenReview', this)" data="https://openreview.net/pdf?id=6gX4rP6QJW">[PDF<sup id="pdf-stars-6gX4rP6QJW@OpenReview">10</sup>]</a>
                <a id="copy-6gX4rP6QJW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6gX4rP6QJW@OpenReview')">[Copy]</a>
                <a id="kimi-6gX4rP6QJW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6gX4rP6QJW@OpenReview', this)">[Kimi<sup id="kimi-stars-6gX4rP6QJW@OpenReview">4</sup>]</a>
                <a id="rel-6gX4rP6QJW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6gX4rP6QJW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6gX4rP6QJW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyang Li" target="_blank">Haoyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Wang" target="_blank">Xin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyang Zhang" target="_blank">Zeyang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongyuan Wu" target="_blank">Zongyuan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linxin Xiao" target="_blank">Linxin Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenwu Zhu" target="_blank">Wenwu Zhu</a>
            </p>
            <p id="summary-6gX4rP6QJW@OpenReview" class="summary">Self-supervised learning (SSL) on graph-structured data has attracted considerable attention recently. Masked graph autoencoder, as one promising generative graph SSL approach that aims to recover masked parts of the input graph data, has shown great success in various downstream graph tasks. However, existing masked graph autoencoders fail to consider the degree of difficulty of recovering the masked edges that often have different impacts on the model performance, resulting in suboptimal node representations. To tackle this challenge, in this paper, we propose a novel curriculum based self-supervised masked graph autoencoder that is able to capture and leverage the underlying degree of difficulty of data dependencies hidden in edges, and design better mask-reconstruction pretext tasks for learning informative node representations. Specifically, we first design a difficulty measurer to identify the underlying structural degree of difficulty of edges during the masking step. Then, we adopt a self-paced scheduler to determine the order of masking edges, which encourages the graph encoder to learn from easy to difficult parts. Finally, the masked edges are gradually incorporated into the reconstruction pretext task, leading to high-quality node representations. Experiments on several real-world node classification and link prediction datasets demonstrate the superiority of our proposed method over state-of-the-art graph self-supervised learning baselines. This work is the first study of curriculum strategy for masked graph autoencoders, to the best of our knowledge.</p>
            <p id="subjects-6gX4rP6QJW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-6gX4rP6QJW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6gX4rP6QJW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6gX4rP6QJW@OpenReview" onclick="foldPdfKimi('6gX4rP6QJW@OpenReview', this)" class="hr hr-fold">
        </div><div id="71Mm8GDGYd@OpenReview" class="panel paper" keywords="forecasting,vae,ptsf,series,probabilistic,term,excell,kalmannet,koopman,kalman">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=71Mm8GDGYd" target="_blank" title="34/225"><span class="index notranslate">#34</span></a>
                <a id="title-71Mm8GDGYd@OpenReview" class="title-link" href="/venue/71Mm8GDGYd@OpenReview" target="_blank"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-104" style="width: 1.634em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.356em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.217em, 1001.36em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-105"><span class="msubsup" id="MathJax-Span-106"><span style="display: inline-block; position: relative; width: 1.356em; height: 0px;"><span style="position: absolute; clip: rect(1.356em, 1000.87em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mi" id="MathJax-Span-107" style="font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.036em;"></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span><span style="position: absolute; top: -2.498em; left: 0.939em;"><span class="mn" id="MathJax-Span-108" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.04em; border-left: 0px solid; width: 0px; height: 1.085em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>K</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-18">K^2</script>VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting</a>
                <a id="pdf-71Mm8GDGYd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('71Mm8GDGYd@OpenReview', this)" data="https://openreview.net/pdf?id=71Mm8GDGYd">[PDF<sup id="pdf-stars-71Mm8GDGYd@OpenReview">12</sup>]</a>
                <a id="copy-71Mm8GDGYd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('71Mm8GDGYd@OpenReview')">[Copy]</a>
                <a id="kimi-71Mm8GDGYd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('71Mm8GDGYd@OpenReview', this)">[Kimi<sup id="kimi-stars-71Mm8GDGYd@OpenReview">4</sup>]</a>
                <a id="rel-71Mm8GDGYd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('71Mm8GDGYd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-71Mm8GDGYd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingjian Wu" target="_blank">Xingjian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangfei Qiu" target="_blank">Xiangfei Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongfan Gao" target="_blank">Hongfan Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jilin Hu" target="_blank">Jilin Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Yang" target="_blank">Bin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenjuan Guo" target="_blank">Chenjuan Guo</a>
            </p>
            <p id="summary-71Mm8GDGYd@OpenReview" class="summary">Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-109" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-110"><span class="msubsup" id="MathJax-Span-111"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-112" style="font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.94em;"><span class="mn" id="MathJax-Span-113" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>K</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-19">K^2</script>VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-114" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-115"><span class="msubsup" id="MathJax-Span-116"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-117" style="font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.94em;"><span class="mn" id="MathJax-Span-118" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>K</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-20">K^2</script>VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.</p>
            <p id="subjects-71Mm8GDGYd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-71Mm8GDGYd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-71Mm8GDGYd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-71Mm8GDGYd@OpenReview" onclick="foldPdfKimi('71Mm8GDGYd@OpenReview', this)" class="hr hr-fold">
        </div><div id="73mDARqOtQ@OpenReview" class="panel paper" keywords="rag,context,speculative,drafters,rapid,retrieval,inference,long,decoding,llms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=73mDARqOtQ" target="_blank" title="35/225"><span class="index notranslate">#35</span></a>
                <a id="title-73mDARqOtQ@OpenReview" class="title-link" href="/venue/73mDARqOtQ@OpenReview" target="_blank">RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding</a>
                <a id="pdf-73mDARqOtQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('73mDARqOtQ@OpenReview', this)" data="https://openreview.net/pdf?id=73mDARqOtQ">[PDF<sup id="pdf-stars-73mDARqOtQ@OpenReview">16</sup>]</a>
                <a id="copy-73mDARqOtQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('73mDARqOtQ@OpenReview')">[Copy]</a>
                <a id="kimi-73mDARqOtQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('73mDARqOtQ@OpenReview', this)">[Kimi<sup id="kimi-stars-73mDARqOtQ@OpenReview">20</sup>]</a>
                <a id="rel-73mDARqOtQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('73mDARqOtQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-73mDARqOtQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guanzheng Chen" target="_blank">Guanzheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qilong Feng" target="_blank">Qilong Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinjie Ni" target="_blank">Jinjie Ni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Li" target="_blank">Xin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Shieh" target="_blank">Michael Shieh</a>
            </p>
            <p id="summary-73mDARqOtQ@OpenReview" class="summary">The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter—a draft LLM operating on shortened retrieval contexts—to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-119" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-120"><span class="mo" id="MathJax-Span-121" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-21">\times</script> speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.</p>
            <p id="subjects-73mDARqOtQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-73mDARqOtQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-73mDARqOtQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-73mDARqOtQ@OpenReview" onclick="foldPdfKimi('73mDARqOtQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="8JGwoZceQs@OpenReview" class="panel paper" keywords="pooling,outputs,snr,transformer,signal,avgpool,noise,adaptive,maxpool,attenuation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8JGwoZceQs" target="_blank" title="36/225"><span class="index notranslate">#36</span></a>
                <a id="title-8JGwoZceQs@OpenReview" class="title-link" href="/venue/8JGwoZceQs@OpenReview" target="_blank">Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs</a>
                <a id="pdf-8JGwoZceQs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8JGwoZceQs@OpenReview', this)" data="https://openreview.net/pdf?id=8JGwoZceQs">[PDF<sup id="pdf-stars-8JGwoZceQs@OpenReview">15</sup>]</a>
                <a id="copy-8JGwoZceQs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8JGwoZceQs@OpenReview')">[Copy]</a>
                <a id="kimi-8JGwoZceQs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8JGwoZceQs@OpenReview', this)">[Kimi<sup id="kimi-stars-8JGwoZceQs@OpenReview">7</sup>]</a>
                <a id="rel-8JGwoZceQs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8JGwoZceQs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8JGwoZceQs@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Greyson Brothers" target="_blank">Greyson Brothers</a>
            </p>
            <p id="summary-8JGwoZceQs@OpenReview" class="summary">We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based *adaptive pooling* method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks.</p>
            <p id="subjects-8JGwoZceQs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-8JGwoZceQs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8JGwoZceQs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8JGwoZceQs@OpenReview" onclick="foldPdfKimi('8JGwoZceQs@OpenReview', this)" class="hr hr-fold">
        </div><div id="9hFQvmCl7P@OpenReview" class="panel paper" keywords="cfl,rehearsal,fedssi,synaptic,regularization,continual,intelligence,federated,data,synergistic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9hFQvmCl7P" target="_blank" title="37/225"><span class="index notranslate">#37</span></a>
                <a id="title-9hFQvmCl7P@OpenReview" class="title-link" href="/venue/9hFQvmCl7P@OpenReview" target="_blank">FedSSI: Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence</a>
                <a id="pdf-9hFQvmCl7P@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9hFQvmCl7P@OpenReview', this)" data="https://openreview.net/pdf?id=9hFQvmCl7P">[PDF<sup id="pdf-stars-9hFQvmCl7P@OpenReview">4</sup>]</a>
                <a id="copy-9hFQvmCl7P@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9hFQvmCl7P@OpenReview')">[Copy]</a>
                <a id="kimi-9hFQvmCl7P@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9hFQvmCl7P@OpenReview', this)">[Kimi<sup id="kimi-stars-9hFQvmCl7P@OpenReview">4</sup>]</a>
                <a id="rel-9hFQvmCl7P@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9hFQvmCl7P@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9hFQvmCl7P@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yichen Li" target="_blank">Yichen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuying Wang" target="_blank">Yuying Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haozhao Wang" target="_blank">Haozhao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yining Qi" target="_blank">Yining Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianzhe Xiao" target="_blank">Tianzhe Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruixuan Li" target="_blank">Ruixuan Li</a>
            </p>
            <p id="summary-9hFQvmCl7P@OpenReview" class="summary">Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding \textit{knowledge forgetting} of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named \textbf{FedSSI}, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.</p>
            <p id="subjects-9hFQvmCl7P@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9hFQvmCl7P@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9hFQvmCl7P@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9hFQvmCl7P@OpenReview" onclick="foldPdfKimi('9hFQvmCl7P@OpenReview', this)" class="hr hr-fold">
        </div><div id="9hd5WA6QCn@OpenReview" class="panel paper" keywords="moda,attention,duplex,multimodal,cognition,emotion,modality,perception,modal,modular">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9hd5WA6QCn" target="_blank" title="38/225"><span class="index notranslate">#38</span></a>
                <a id="title-9hd5WA6QCn@OpenReview" class="title-link" href="/venue/9hd5WA6QCn@OpenReview" target="_blank">MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding</a>
                <a id="pdf-9hd5WA6QCn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9hd5WA6QCn@OpenReview', this)" data="https://openreview.net/pdf?id=9hd5WA6QCn">[PDF<sup id="pdf-stars-9hd5WA6QCn@OpenReview">11</sup>]</a>
                <a id="copy-9hd5WA6QCn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9hd5WA6QCn@OpenReview')">[Copy]</a>
                <a id="kimi-9hd5WA6QCn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9hd5WA6QCn@OpenReview', this)">[Kimi<sup id="kimi-stars-9hd5WA6QCn@OpenReview">9</sup>]</a>
                <a id="rel-9hd5WA6QCn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9hd5WA6QCn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9hd5WA6QCn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhicheng Zhang" target="_blank">Zhicheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wuyou Xia" target="_blank">Wuyou Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxi Zhao" target="_blank">Chenxi Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhou Yan" target="_blank">Zhou Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoqiang Liu" target="_blank">Xiaoqiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongjie Zhu" target="_blank">Yongjie Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenyu Qin" target="_blank">Wenyu Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Wan" target="_blank">Pengfei Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di ZHANG" target="_blank">Di ZHANG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jufeng Yang" target="_blank">Jufeng Yang</a>
            </p>
            <p id="summary-9hd5WA6QCn@OpenReview" class="summary">Multimodal large language models (MLLMs) recently showed strong capacity in integrating data among multiple modalities, empowered by generalizable attention architecture. Advanced methods predominantly focus on language-centric tuning while less exploring multimodal tokens mixed through attention, posing challenges in high-level tasks that require fine-grained cognition and emotion understanding. In this work, we identify the attention deficit disorder problem in multimodal learning, caused by inconsistent cross-modal attention and layer-by-layer decayed attention activation. To address this, we propose a novel attention mechanism, termed MOdular Duplex Attention (MODA), simultaneously conducting the inner-modal refinement and inter-modal interaction. MODA employs a correct-after-align strategy to effectively decouple modality alignment from cross-layer token mixing. In the alignment phase, tokens are mapped to duplex modality spaces based on the basis vectors, enabling the interaction between visual and language modality. Further, the correctness of attention scores is ensured through adaptive masked attention, which enhances the model's flexibility by allowing customizable masking patterns for different modalities. Extensive experiments on 21 benchmark datasets verify the effectiveness of MODA in perception, cognition, and emotion tasks.</p>
            <p id="subjects-9hd5WA6QCn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9hd5WA6QCn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9hd5WA6QCn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9hd5WA6QCn@OpenReview" onclick="foldPdfKimi('9hd5WA6QCn@OpenReview', this)" class="hr hr-fold">
        </div><div id="9vYGZX4OVN@OpenReview" class="panel paper" keywords="big,size,continual,adjust,enough,dataset,adjusting,gaussian,processes,performance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9vYGZX4OVN" target="_blank" title="39/225"><span class="index notranslate">#39</span></a>
                <a id="title-9vYGZX4OVN@OpenReview" class="title-link" href="/venue/9vYGZX4OVN@OpenReview" target="_blank">Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?</a>
                <a id="pdf-9vYGZX4OVN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9vYGZX4OVN@OpenReview', this)" data="https://openreview.net/pdf?id=9vYGZX4OVN">[PDF<sup id="pdf-stars-9vYGZX4OVN@OpenReview">3</sup>]</a>
                <a id="copy-9vYGZX4OVN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9vYGZX4OVN@OpenReview')">[Copy]</a>
                <a id="kimi-9vYGZX4OVN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9vYGZX4OVN@OpenReview', this)">[Kimi<sup id="kimi-stars-9vYGZX4OVN@OpenReview">2</sup>]</a>
                <a id="rel-9vYGZX4OVN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9vYGZX4OVN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9vYGZX4OVN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guiomar Pescador-Barrios" target="_blank">Guiomar Pescador-Barrios</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sarah Filippi" target="_blank">Sarah Filippi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark van der Wilk" target="_blank">Mark van der Wilk</a>
            </p>
            <p id="summary-9vYGZX4OVN@OpenReview" class="summary">Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question "How big is big enough?" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.</p>
            <p id="subjects-9vYGZX4OVN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9vYGZX4OVN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9vYGZX4OVN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9vYGZX4OVN@OpenReview" onclick="foldPdfKimi('9vYGZX4OVN@OpenReview', this)" class="hr hr-fold">
        </div><div id="AiaVCVDuxF@OpenReview" class="panel paper" keywords="auditor,auditing,audit,manipulation,prior,platform,knowledge,answers,robust,public">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=AiaVCVDuxF" target="_blank" title="40/225"><span class="index notranslate">#40</span></a>
                <a id="title-AiaVCVDuxF@OpenReview" class="title-link" href="/venue/AiaVCVDuxF@OpenReview" target="_blank">Robust ML Auditing using Prior Knowledge</a>
                <a id="pdf-AiaVCVDuxF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('AiaVCVDuxF@OpenReview', this)" data="https://openreview.net/pdf?id=AiaVCVDuxF">[PDF<sup id="pdf-stars-AiaVCVDuxF@OpenReview">1</sup>]</a>
                <a id="copy-AiaVCVDuxF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('AiaVCVDuxF@OpenReview')">[Copy]</a>
                <a id="kimi-AiaVCVDuxF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('AiaVCVDuxF@OpenReview', this)">[Kimi<sup id="kimi-stars-AiaVCVDuxF@OpenReview">1</sup>]</a>
                <a id="rel-AiaVCVDuxF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('AiaVCVDuxF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-AiaVCVDuxF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jade Garcia Bourrée" target="_blank">Jade Garcia Bourrée</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Augustin Godinot" target="_blank">Augustin Godinot</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sayan Biswas" target="_blank">Sayan Biswas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anne-Marie Kermarrec" target="_blank">Anne-Marie Kermarrec</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erwan Le Merrer" target="_blank">Erwan Le Merrer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gilles Tredan" target="_blank">Gilles Tredan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martijn de Vos" target="_blank">Martijn de Vos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Milos Vujasinovic" target="_blank">Milos Vujasinovic</a>
            </p>
            <p id="summary-AiaVCVDuxF@OpenReview" class="summary">Among the many technical challenges to enforcing AI regulations, one crucial yet underexplored problem is the risk of audit manipulation.This manipulation occurs when a platform deliberately alters its answers to a regulator to pass an audit without modifying its answers to other users.In this paper, we introduce a novel approach to manipulation-proof auditing by taking into account the auditor's prior knowledge of the task solved by the platform. We first demonstrate that regulators must not rely on public priors (e.g. a public dataset), as platforms could easily fool the auditor in such cases. We then formally establish the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth. Finally, our experiments with two standard datasets illustrate the maximum level of unfairness a platform can hide before being detected as malicious.Our formalization and generalization of manipulation-proof auditing with a prior opens up new research directions for more robust fairness audits.</p>
            <p id="subjects-AiaVCVDuxF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-AiaVCVDuxF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-AiaVCVDuxF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-AiaVCVDuxF@OpenReview" onclick="foldPdfKimi('AiaVCVDuxF@OpenReview', this)" class="hr hr-fold">
        </div><div id="AnoIgkc6WS@OpenReview" class="panel paper" keywords="identifiability,scms,counterfactual,sim,exogenous,isomorphism,mathcal,causal,pch,assumptions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=AnoIgkc6WS" target="_blank" title="41/225"><span class="index notranslate">#41</span></a>
                <a id="title-AnoIgkc6WS@OpenReview" class="title-link" href="/venue/AnoIgkc6WS@OpenReview" target="_blank">Exogenous Isomorphism for Counterfactual Identifiability</a>
                <a id="pdf-AnoIgkc6WS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('AnoIgkc6WS@OpenReview', this)" data="https://openreview.net/pdf?id=AnoIgkc6WS">[PDF<sup id="pdf-stars-AnoIgkc6WS@OpenReview">5</sup>]</a>
                <a id="copy-AnoIgkc6WS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('AnoIgkc6WS@OpenReview')">[Copy]</a>
                <a id="kimi-AnoIgkc6WS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('AnoIgkc6WS@OpenReview', this)">[Kimi<sup id="kimi-stars-AnoIgkc6WS@OpenReview">1</sup>]</a>
                <a id="rel-AnoIgkc6WS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('AnoIgkc6WS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-AnoIgkc6WS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yikang Chen" target="_blank">Yikang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dehui du" target="_blank">Dehui du</a>
            </p>
            <p id="summary-AnoIgkc6WS@OpenReview" class="summary">This paper investigates <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x005F;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-122" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1002.03em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-123"><span class="msubsup" id="MathJax-Span-124"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-125" style="font-family: MathJax_Main;">∼</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.784em;"><span class="texatom" id="MathJax-Span-126"><span class="mrow" id="MathJax-Span-127"><span class="texatom" id="MathJax-Span-128"><span class="mrow" id="MathJax-Span-129"><span class="mi" id="MathJax-Span-130" style="font-size: 70.7%; font-family: MathJax_Caligraphic;">L</span></span></span><span class="mi" id="MathJax-Span-131" style="font-size: 70.7%; font-family: MathJax_Main;">_</span><span class="mn" id="MathJax-Span-132" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mo>∼</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">L</mi></mrow><mi mathvariant="normal">_</mi><mn>3</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-22">\sim_{\mathcal{L}\_3}</script>-identifiability, a form of complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH) framework, ensuring that all Structural Causal Models (SCMs) satisfying the given assumptions provide consistent answers to all causal questions. To simplify this problem, we introduce exogenous isomorphism and propose <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;E&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;I&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-133" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1001.57em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-134"><span class="msubsup" id="MathJax-Span-135"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-136" style="font-family: MathJax_Main;">∼</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.784em;"><span class="texatom" id="MathJax-Span-137"><span class="mrow" id="MathJax-Span-138"><span class="texatom" id="MathJax-Span-139"><span class="mrow" id="MathJax-Span-140"><span class="mi" id="MathJax-Span-141" style="font-size: 70.7%; font-family: MathJax_Main;">E</span><span class="mi" id="MathJax-Span-142" style="font-size: 70.7%; font-family: MathJax_Main;">I</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mo>∼</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">E</mi><mi mathvariant="normal">I</mi></mrow></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-23">\sim_{\mathrm{EI}}</script>-identifiability, reflecting the strength of model identifiability required for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x005F;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-143" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1002.03em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-144"><span class="msubsup" id="MathJax-Span-145"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-146" style="font-family: MathJax_Main;">∼</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.784em;"><span class="texatom" id="MathJax-Span-147"><span class="mrow" id="MathJax-Span-148"><span class="texatom" id="MathJax-Span-149"><span class="mrow" id="MathJax-Span-150"><span class="mi" id="MathJax-Span-151" style="font-size: 70.7%; font-family: MathJax_Caligraphic;">L</span></span></span><span class="mi" id="MathJax-Span-152" style="font-size: 70.7%; font-family: MathJax_Main;">_</span><span class="mn" id="MathJax-Span-153" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mo>∼</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">L</mi></mrow><mi mathvariant="normal">_</mi><mn>3</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-24">\sim_{\mathcal{L}\_3}</script>-identifiability. We explore sufficient assumptions for achieving <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;E&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;I&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-154" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1001.57em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-155"><span class="msubsup" id="MathJax-Span-156"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-157" style="font-family: MathJax_Main;">∼</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.784em;"><span class="texatom" id="MathJax-Span-158"><span class="mrow" id="MathJax-Span-159"><span class="texatom" id="MathJax-Span-160"><span class="mrow" id="MathJax-Span-161"><span class="mi" id="MathJax-Span-162" style="font-size: 70.7%; font-family: MathJax_Main;">E</span><span class="mi" id="MathJax-Span-163" style="font-size: 70.7%; font-family: MathJax_Main;">I</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mo>∼</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">E</mi><mi mathvariant="normal">I</mi></mrow></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-25">\sim_{\mathrm{EI}}</script>-identifiability in two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual transport, and Triangular Monotonic SCMs (TM-SCMs), which extend <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x005F;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-164" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1002.03em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-165"><span class="msubsup" id="MathJax-Span-166"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-167" style="font-family: MathJax_Main;">∼</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.784em;"><span class="texatom" id="MathJax-Span-168"><span class="mrow" id="MathJax-Span-169"><span class="texatom" id="MathJax-Span-170"><span class="mrow" id="MathJax-Span-171"><span class="mi" id="MathJax-Span-172" style="font-size: 70.7%; font-family: MathJax_Caligraphic;">L</span></span></span><span class="mi" id="MathJax-Span-173" style="font-size: 70.7%; font-family: MathJax_Main;">_</span><span class="mn" id="MathJax-Span-174" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mo>∼</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">L</mi></mrow><mi mathvariant="normal">_</mi><mn>2</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-26">\sim_{\mathcal{L}\_2}</script>-identifiability. Our results unify and generalize existing theories, providing theoretical guarantees for practical applications. Finally, we leverage neural TM-SCMs to address the consistency problem in counterfactual reasoning, with experiments validating both the effectiveness of our method and the correctness of the theory.</p>
            <p id="subjects-AnoIgkc6WS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-AnoIgkc6WS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-AnoIgkc6WS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-AnoIgkc6WS@OpenReview" onclick="foldPdfKimi('AnoIgkc6WS@OpenReview', this)" class="hr hr-fold">
        </div><div id="AsODat0dkE@OpenReview" class="panel paper" keywords="watermarking,attacks,watermarks,content,robustness,adaptive,evading,watermark,nilslukas,paraphrasers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=AsODat0dkE" target="_blank" title="42/225"><span class="index notranslate">#42</span></a>
                <a id="title-AsODat0dkE@OpenReview" class="title-link" href="/venue/AsODat0dkE@OpenReview" target="_blank">Optimizing Adaptive Attacks against Watermarks for Language Models</a>
                <a id="pdf-AsODat0dkE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('AsODat0dkE@OpenReview', this)" data="https://openreview.net/pdf?id=AsODat0dkE">[PDF<sup id="pdf-stars-AsODat0dkE@OpenReview">6</sup>]</a>
                <a id="copy-AsODat0dkE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('AsODat0dkE@OpenReview')">[Copy]</a>
                <a id="kimi-AsODat0dkE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('AsODat0dkE@OpenReview', this)">[Kimi<sup id="kimi-stars-AsODat0dkE@OpenReview">4</sup>]</a>
                <a id="rel-AsODat0dkE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('AsODat0dkE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-AsODat0dkE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Abdulrahman Diaa" target="_blank">Abdulrahman Diaa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Toluwani Aremu" target="_blank">Toluwani Aremu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nils Lukas" target="_blank">Nils Lukas</a>
            </p>
            <p id="summary-AsODat0dkE@OpenReview" class="summary">Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret *watermarking key*. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against *non-adaptive* attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune *adaptive* attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against *any* watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively tuned paraphrasers at &lt;https://github.com/nilslukas/ada-wm-evasion&gt;.</p>
            <p id="subjects-AsODat0dkE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-AsODat0dkE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-AsODat0dkE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-AsODat0dkE@OpenReview" onclick="foldPdfKimi('AsODat0dkE@OpenReview', this)" class="hr hr-fold">
        </div><div id="W2Fe1hT7Ks@OpenReview" class="panel paper" keywords="randomness,mathcal,stability,textit,yehudayoff,2024,replicability,chase,moran,complexity">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=W2Fe1hT7Ks" target="_blank" title="43/225"><span class="index notranslate">#43</span></a>
                <a id="title-W2Fe1hT7Ks@OpenReview" class="title-link" href="/venue/W2Fe1hT7Ks@OpenReview" target="_blank">The Role of Randomness in Stability</a>
                <a id="pdf-W2Fe1hT7Ks@OpenReview" class="title-pdf notranslate" onclick="togglePdf('W2Fe1hT7Ks@OpenReview', this)" data="https://openreview.net/pdf?id=W2Fe1hT7Ks">[PDF<sup id="pdf-stars-W2Fe1hT7Ks@OpenReview">3</sup>]</a>
                <a id="copy-W2Fe1hT7Ks@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('W2Fe1hT7Ks@OpenReview')">[Copy]</a>
                <a id="kimi-W2Fe1hT7Ks@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('W2Fe1hT7Ks@OpenReview', this)">[Kimi<sup id="kimi-stars-W2Fe1hT7Ks@OpenReview">1</sup>]</a>
                <a id="rel-W2Fe1hT7Ks@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('W2Fe1hT7Ks@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-W2Fe1hT7Ks@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Max Hopkins" target="_blank">Max Hopkins</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shay Moran" target="_blank">Shay Moran</a>
            </p>
            <p id="summary-W2Fe1hT7Ks@OpenReview" class="summary">Stability is a central property in learning and statistics promising the output of an algorithm <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-175" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-176"><span class="texatom" id="MathJax-Span-177"><span class="mrow" id="MathJax-Span-178"><span class="mi" id="MathJax-Span-179" style="font-family: MathJax_Caligraphic;">A<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-27">\mathcal{A}</script> does not change substantially when applied to similar datasets <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-180" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-181"><span class="mi" id="MathJax-Span-182" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi></math></span></span><script type="math/tex" id="MathJax-Element-28">S</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo&gt;&amp;#x2032;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-183" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.201em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-184"><span class="msup" id="MathJax-Span-185"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-186" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.732em;"><span class="mo" id="MathJax-Span-187" style="font-size: 70.7%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>S</mi><mo>′</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-29">S'</script>. It is an elementary fact that any sufficiently stable algorithm (e.g.\ one returning the same result with high probability, satisfying privacy guarantees, etc.) must be randomized. This raises a natural question: can we quantify \textit{how much} randomness is needed for algorithmic stability? We study the randomness complexity of two influential notions of stability in learning: \textit{replicability} (which promises <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-188" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-189"><span class="texatom" id="MathJax-Span-190"><span class="mrow" id="MathJax-Span-191"><span class="mi" id="MathJax-Span-192" style="font-family: MathJax_Caligraphic;">A<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-30">\mathcal{A}</script> usually outputs the same result when run over samples from the same distribution), and \textit{differential privacy} (which promises the output distribution of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-31-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-193" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-194"><span class="texatom" id="MathJax-Span-195"><span class="mrow" id="MathJax-Span-196"><span class="mi" id="MathJax-Span-197" style="font-family: MathJax_Caligraphic;">A<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-31">\mathcal{A}</script> remains similar under neighboring datasets). In particular, building on the ideas of (Dixon, Pavan, Vander Woude, and Vinodchandran ICML 2024) and (Cannone, Su, and Vadhan ITCS 2024), we prove a "weak-to-strong" boosting theorem for stability in these settings: the randomness complexity of a task <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-32-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-198" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.15em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-199"><span class="texatom" id="MathJax-Span-200"><span class="mrow" id="MathJax-Span-201"><span class="mi" id="MathJax-Span-202" style="font-family: MathJax_Caligraphic;">M</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-32">\mathcal{M}</script> is tightly controlled by the best replication probability of any \textit{deterministic} algorithm solving <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-33-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-203" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.15em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-204"><span class="texatom" id="MathJax-Span-205"><span class="mrow" id="MathJax-Span-206"><span class="mi" id="MathJax-Span-207" style="font-family: MathJax_Caligraphic;">M</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-33">\mathcal{M}</script>, a parameter known as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-34-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-208" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.15em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-209"><span class="texatom" id="MathJax-Span-210"><span class="mrow" id="MathJax-Span-211"><span class="mi" id="MathJax-Span-212" style="font-family: MathJax_Caligraphic;">M</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-34">\mathcal{M}</script>'s "global stability" (Chase, Moran, Yehudayoff FOCS 2023). Finally, we use this connection to characterize the randomness complexity of PAC Learning: a class has bounded randomness complexity iff it has finite Littlestone dimension, and moreover scales at worst logarithmically in the excess error of the learner. As a corollary, we resolve a question of (Chase, Chornomaz, Moran, and Yehudayoff STOC 2024) about the error-dependent list-replicability of agnostic learning.</p>
            <p id="subjects-W2Fe1hT7Ks@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-W2Fe1hT7Ks@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-W2Fe1hT7Ks@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-W2Fe1hT7Ks@OpenReview" onclick="foldPdfKimi('W2Fe1hT7Ks@OpenReview', this)" class="hr hr-fold">
        </div><div id="BkdAnSKNoX@OpenReview" class="panel paper" keywords="worker,tllc,label,completion,instances,crowdsourced,modeling,transfer,domain,annotated">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BkdAnSKNoX" target="_blank" title="44/225"><span class="index notranslate">#44</span></a>
                <a id="title-BkdAnSKNoX@OpenReview" class="title-link" href="/venue/BkdAnSKNoX@OpenReview" target="_blank">TLLC: Transfer Learning-based Label Completion for Crowdsourcing</a>
                <a id="pdf-BkdAnSKNoX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BkdAnSKNoX@OpenReview', this)" data="https://openreview.net/pdf?id=BkdAnSKNoX">[PDF<sup id="pdf-stars-BkdAnSKNoX@OpenReview">3</sup>]</a>
                <a id="copy-BkdAnSKNoX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BkdAnSKNoX@OpenReview')">[Copy]</a>
                <a id="kimi-BkdAnSKNoX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BkdAnSKNoX@OpenReview', this)">[Kimi<sup id="kimi-stars-BkdAnSKNoX@OpenReview">1</sup>]</a>
                <a id="rel-BkdAnSKNoX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BkdAnSKNoX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BkdAnSKNoX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjun Zhang" target="_blank">Wenjun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangxiao Jiang" target="_blank">Liangxiao Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaoqun Li" target="_blank">Chaoqun Li</a>
            </p>
            <p id="summary-BkdAnSKNoX@OpenReview" class="summary">Label completion serves as a preprocessing approach to handling the sparse crowdsourced label matrix problem, significantly boosting the effectiveness of the downstream label aggregation. In recent advances, worker modeling has been proved to be a powerful strategy to further improve the performance of label completion. However, in real-world scenarios, workers typically annotate only a few instances, leading to insufficient worker modeling and thus limiting the improvement of label completion. To address this issue, we propose a novel transfer learning-based label completion (TLLC) method. Specifically, we first identify all high-confidence instances from the whole crowdsourced data as a source domain and use it to pretrain a Siamese network. The abundant annotated instances in the source domain provide essential knowledge for worker modeling. Then, we transfer the pretrained network to the target domain with the instances annotated by each worker separately, ensuring worker modeling captures unique characteristics of each worker. Finally, we leverage the new embeddings learned by the transferred network to complete each worker’s missing labels. Extensive experiments on several widely used real-world datasets demonstrate the effectiveness of TLLC. Our codes and datasets are available at https://github.com/jiangliangxiao/TLLC.</p>
            <p id="subjects-BkdAnSKNoX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-BkdAnSKNoX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BkdAnSKNoX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BkdAnSKNoX@OpenReview" onclick="foldPdfKimi('BkdAnSKNoX@OpenReview', this)" class="hr hr-fold">
        </div><div id="BkrIQPREkn@OpenReview" class="panel paper" keywords="unlearning,forget,amun,adversarial,examples,samples,model,sota,wrong,bad">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BkrIQPREkn" target="_blank" title="45/225"><span class="index notranslate">#45</span></a>
                <a id="title-BkrIQPREkn@OpenReview" class="title-link" href="/venue/BkrIQPREkn@OpenReview" target="_blank">Not All Wrong is Bad: Using Adversarial Examples for Unlearning</a>
                <a id="pdf-BkrIQPREkn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BkrIQPREkn@OpenReview', this)" data="https://openreview.net/pdf?id=BkrIQPREkn">[PDF<sup id="pdf-stars-BkrIQPREkn@OpenReview">6</sup>]</a>
                <a id="copy-BkrIQPREkn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BkrIQPREkn@OpenReview')">[Copy]</a>
                <a id="kimi-BkrIQPREkn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BkrIQPREkn@OpenReview', this)">[Kimi<sup id="kimi-stars-BkrIQPREkn@OpenReview">5</sup>]</a>
                <a id="rel-BkrIQPREkn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BkrIQPREkn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BkrIQPREkn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Ebrahimpour-Boroojeny" target="_blank">Ali Ebrahimpour-Boroojeny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hari Sundaram" target="_blank">Hari Sundaram</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Varun Chandrasekaran" target="_blank">Varun Chandrasekaran</a>
            </p>
            <p id="summary-BkrIQPREkn@OpenReview" class="summary">Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on "exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, "approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random 10% of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing.</p>
            <p id="subjects-BkrIQPREkn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-BkrIQPREkn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BkrIQPREkn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BkrIQPREkn@OpenReview" onclick="foldPdfKimi('BkrIQPREkn@OpenReview', this)" class="hr hr-fold">
        </div><div id="Bm706VlAtU@OpenReview" class="panel paper" keywords="adversarial,purification,damage,frequency,spectrum,image,perturbations,amplitude,domain,perspective">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Bm706VlAtU" target="_blank" title="46/225"><span class="index notranslate">#46</span></a>
                <a id="title-Bm706VlAtU@OpenReview" class="title-link" href="/venue/Bm706VlAtU@OpenReview" target="_blank">Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</a>
                <a id="pdf-Bm706VlAtU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Bm706VlAtU@OpenReview', this)" data="https://openreview.net/pdf?id=Bm706VlAtU">[PDF<sup id="pdf-stars-Bm706VlAtU@OpenReview">4</sup>]</a>
                <a id="copy-Bm706VlAtU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Bm706VlAtU@OpenReview')">[Copy]</a>
                <a id="kimi-Bm706VlAtU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Bm706VlAtU@OpenReview', this)">[Kimi<sup id="kimi-stars-Bm706VlAtU@OpenReview">3</sup>]</a>
                <a id="rel-Bm706VlAtU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Bm706VlAtU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Bm706VlAtU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gaozheng Pei" target="_blank">Gaozheng Pei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ke Ma" target="_blank">Ke Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingfei Sun" target="_blank">Yingfei Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianqian Xu" target="_blank">Qianqian Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingming Huang" target="_blank">Qingming Huang</a>
            </p>
            <p id="summary-Bm706VlAtU@OpenReview" class="summary">The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image.For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.</p>
            <p id="subjects-Bm706VlAtU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Bm706VlAtU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bm706VlAtU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bm706VlAtU@OpenReview" onclick="foldPdfKimi('Bm706VlAtU@OpenReview', this)" class="hr hr-fold">
        </div><div id="DDIGCk25BO@OpenReview" class="panel paper" keywords="amc,modulation,classification,confusable,fuzzy,ambiguity,automatic,regularization,pillar,dynamic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DDIGCk25BO" target="_blank" title="47/225"><span class="index notranslate">#47</span></a>
                <a id="title-DDIGCk25BO@OpenReview" class="title-link" href="/venue/DDIGCk25BO@OpenReview" target="_blank">Robust Automatic Modulation Classification with Fuzzy Regularization</a>
                <a id="pdf-DDIGCk25BO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DDIGCk25BO@OpenReview', this)" data="https://openreview.net/pdf?id=DDIGCk25BO">[PDF<sup id="pdf-stars-DDIGCk25BO@OpenReview">3</sup>]</a>
                <a id="copy-DDIGCk25BO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DDIGCk25BO@OpenReview')">[Copy]</a>
                <a id="kimi-DDIGCk25BO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DDIGCk25BO@OpenReview', this)">[Kimi<sup id="kimi-stars-DDIGCk25BO@OpenReview">2</sup>]</a>
                <a id="rel-DDIGCk25BO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DDIGCk25BO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DDIGCk25BO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyan Liang" target="_blank">Xinyan Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruijie Sang" target="_blank">Ruijie Sang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhua Qian" target="_blank">Yuhua Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Guo" target="_blank">Qian Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feijiang Li" target="_blank">Feijiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Du" target="_blank">Liang Du</a>
            </p>
            <p id="summary-DDIGCk25BO@OpenReview" class="summary">Automatic Modulation Classification (AMC) serves as a foundational pillar for cognitive radio systems, enabling critical functionalities including dynamic spectrum allocation, non-cooperative signal surveillance, and adaptive waveform optimization. However, practical deployment of AMC faces a fundamental challenge: prediction ambiguity arising from intrinsic similarity among modulation schemes and exacerbated under low signal-to-noise ratio (SNR) conditions. This phenomenon manifests as near-identical probability distributions across confusable modulation types, significantly degrading classification reliability. To address this, we propose Fuzzy Regularization-enhanced AMC (FR-AMC), a novel framework that integrates uncertainty quantification into the classification pipeline. The proposed FR has three features: (1) Explicitly model prediction ambiguity during backpropagation, (2) dynamic sample reweighting through adaptive loss scaling, (3) encourage margin maximization between confusable modulation clusters. Experimental results on benchmark datasets demonstrate that the FR achieves superior classification accuracy and robustness compared to compared methods, making it a promising solution for real-world spectrum management and communication applications.</p>
            <p id="subjects-DDIGCk25BO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-DDIGCk25BO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DDIGCk25BO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DDIGCk25BO@OpenReview" onclick="foldPdfKimi('DDIGCk25BO@OpenReview', this)" class="hr hr-fold">
        </div><div id="DJcEoC9JpQ@OpenReview" class="panel paper" keywords="lvlms,reasoning,rag,ranking,knowledge,tree,vqa,base,context,search">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DJcEoC9JpQ" target="_blank" title="48/225"><span class="index notranslate">#48</span></a>
                <a id="title-DJcEoC9JpQ@OpenReview" class="title-link" href="/venue/DJcEoC9JpQ@OpenReview" target="_blank">Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger</a>
                <a id="pdf-DJcEoC9JpQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DJcEoC9JpQ@OpenReview', this)" data="https://openreview.net/pdf?id=DJcEoC9JpQ">[PDF<sup id="pdf-stars-DJcEoC9JpQ@OpenReview">12</sup>]</a>
                <a id="copy-DJcEoC9JpQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DJcEoC9JpQ@OpenReview')">[Copy]</a>
                <a id="kimi-DJcEoC9JpQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DJcEoC9JpQ@OpenReview', this)">[Kimi<sup id="kimi-stars-DJcEoC9JpQ@OpenReview">10</sup>]</a>
                <a id="rel-DJcEoC9JpQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DJcEoC9JpQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DJcEoC9JpQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Yang" target="_blank">Qi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenghao Zhang" target="_blank">Chenghao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lubin Fan" target="_blank">Lubin Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Ding" target="_blank">Kun Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jieping Ye" target="_blank">Jieping Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiming Xiang" target="_blank">Shiming Xiang</a>
            </p>
            <p id="summary-DJcEoC9JpQ@OpenReview" class="summary">Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs.</p>
            <p id="subjects-DJcEoC9JpQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-DJcEoC9JpQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DJcEoC9JpQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DJcEoC9JpQ@OpenReview" onclick="foldPdfKimi('DJcEoC9JpQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="DUGFTH9W8B@OpenReview" class="panel paper" keywords="backup,wasserstein,mcts,barycenter,tree,value,operator,nodes,monte,carlo">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DUGFTH9W8B" target="_blank" title="49/225"><span class="index notranslate">#49</span></a>
                <a id="title-DUGFTH9W8B@OpenReview" class="title-link" href="/venue/DUGFTH9W8B@OpenReview" target="_blank">Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport</a>
                <a id="pdf-DUGFTH9W8B@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DUGFTH9W8B@OpenReview', this)" data="https://openreview.net/pdf?id=DUGFTH9W8B">[PDF<sup id="pdf-stars-DUGFTH9W8B@OpenReview">4</sup>]</a>
                <a id="copy-DUGFTH9W8B@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DUGFTH9W8B@OpenReview')">[Copy]</a>
                <a id="kimi-DUGFTH9W8B@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DUGFTH9W8B@OpenReview', this)">[Kimi<sup id="kimi-stars-DUGFTH9W8B@OpenReview">5</sup>]</a>
                <a id="rel-DUGFTH9W8B@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DUGFTH9W8B@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DUGFTH9W8B@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tuan Dam" target="_blank">Tuan Dam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Stenger" target="_blank">Pascal Stenger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lukas Schneider" target="_blank">Lukas Schneider</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joni Pajarinen" target="_blank">Joni Pajarinen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carlo D&amp;#x27;Eramo" target="_blank">Carlo D&amp;#x27;Eramo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Odalric-Ambrym Maillard" target="_blank">Odalric-Ambrym Maillard</a>
            </p>
            <p id="summary-DUGFTH9W8B@OpenReview" class="summary">This paper introduces a novel backup strategy for Monte-Carlo Tree Search (MCTS) tailored for highly stochastic and partially observable Markov decision processes. We adopt a probabilistic approach, modeling both value and action-value nodes as Gaussian distributions, to introduce a novel backup operator that computes value nodes as the Wasserstein barycenter of their action-value children nodes; thus, propagating the uncertainty of the estimate across the tree to the root node. We study our novel backup operator when using a novel combination of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-35-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-213" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-214"><span class="msubsup" id="MathJax-Span-215"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-216" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.68em;"><span class="mn" id="MathJax-Span-217" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mn>1</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-35">L^1</script>-Wasserstein barycenter with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-36-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-218" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-219"><span class="mi" id="MathJax-Span-220" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-36">\alpha</script>-divergence, by drawing a crucial connection to the generalized mean backup operator. We complement our probabilistic backup operator with two sampling strategies, based on optimistic selection and Thompson sampling, obtaining our Wasserstein MCTS algorithm. We provide theoretical guarantees of asymptotic convergence of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-37-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-221" style="width: 4.638em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.75em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-222"><span class="texatom" id="MathJax-Span-223"><span class="mrow" id="MathJax-Span-224"><span class="mi" id="MathJax-Span-225" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-226" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-227"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-228" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-229"><span class="mrow" id="MathJax-Span-230"><span class="mo" id="MathJax-Span-231" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-232" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-233"><span class="mrow" id="MathJax-Span-234"><span class="mo" id="MathJax-Span-235" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-236" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-237" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-37">\mathcal{O}(n^{-1/2})</script>, with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-38-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-238" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-239"><span class="mi" id="MathJax-Span-240" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-38">n</script> as the number of visited trajectories, to the optimal policy and an empirical evaluation on several stochastic and partially observable environments, where our approach outperforms well-known related baselines.</p>
            <p id="subjects-DUGFTH9W8B@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-DUGFTH9W8B@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DUGFTH9W8B@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DUGFTH9W8B@OpenReview" onclick="foldPdfKimi('DUGFTH9W8B@OpenReview', this)" class="hr hr-fold">
        </div><div id="DoDXFkF10S@OpenReview" class="panel paper" keywords="latent,interpolations,euclidean,cell,manifold,geometry,flatvi,single,interpolation,rna">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DoDXFkF10S" target="_blank" title="50/225"><span class="index notranslate">#50</span></a>
                <a id="title-DoDXFkF10S@OpenReview" class="title-link" href="/venue/DoDXFkF10S@OpenReview" target="_blank">Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation</a>
                <a id="pdf-DoDXFkF10S@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DoDXFkF10S@OpenReview', this)" data="https://openreview.net/pdf?id=DoDXFkF10S">[PDF<sup id="pdf-stars-DoDXFkF10S@OpenReview">3</sup>]</a>
                <a id="copy-DoDXFkF10S@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DoDXFkF10S@OpenReview')">[Copy]</a>
                <a id="kimi-DoDXFkF10S@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DoDXFkF10S@OpenReview', this)">[Kimi<sup id="kimi-stars-DoDXFkF10S@OpenReview">3</sup>]</a>
                <a id="rel-DoDXFkF10S@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DoDXFkF10S@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DoDXFkF10S@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alessandro Palma" target="_blank">Alessandro Palma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergei Rybakov" target="_blank">Sergei Rybakov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leon Hetzel" target="_blank">Leon Hetzel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Günnemann" target="_blank">Stephan Günnemann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Theis" target="_blank">Fabian Theis</a>
            </p>
            <p id="summary-DoDXFkF10S@OpenReview" class="summary">Latent space interpolations are a powerful tool for navigating deep generative models in applied settings. An example is single-cell RNA sequencing, where existing methods model cellular state transitions as latent space interpolations with variational autoencoders, often assuming linear shifts and Euclidean geometry. However, unless explicitly enforced, linear interpolations in the latent space may not correspond to geodesic paths on the data manifold, limiting methods that assume Euclidean geometry in the data representations. We introduce FlatVI, a novel training framework that regularises the latent manifold of discrete-likelihood variational autoencoders towards Euclidean geometry, specifically tailored for modelling single-cell count data. By encouraging straight lines in the latent space to approximate geodesic interpolations on the decoded single-cell manifold, FlatVI enhances compatibility with downstream approaches that assume Euclidean latent geometry. Experiments on synthetic data support the theoretical soundness of our approach, while applications to time-resolved single-cell RNA sequencing data demonstrate improved trajectory reconstruction and manifold interpolation.</p>
            <p id="subjects-DoDXFkF10S@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-DoDXFkF10S@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DoDXFkF10S@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DoDXFkF10S@OpenReview" onclick="foldPdfKimi('DoDXFkF10S@OpenReview', this)" class="hr hr-fold">
        </div><div id="DzLP43CbiX@OpenReview" class="panel paper" keywords="flops,flopping,networksthat,equivariance,equivariant,mirror,irreps,computational,efficiency,invariance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DzLP43CbiX" target="_blank" title="51/225"><span class="index notranslate">#51</span></a>
                <a id="title-DzLP43CbiX@OpenReview" class="title-link" href="/venue/DzLP43CbiX@OpenReview" target="_blank">Flopping for FLOPs: Leveraging Equivariance for Computational Efficiency</a>
                <a id="pdf-DzLP43CbiX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DzLP43CbiX@OpenReview', this)" data="https://openreview.net/pdf?id=DzLP43CbiX">[PDF<sup id="pdf-stars-DzLP43CbiX@OpenReview">1</sup>]</a>
                <a id="copy-DzLP43CbiX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DzLP43CbiX@OpenReview')">[Copy]</a>
                <a id="kimi-DzLP43CbiX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DzLP43CbiX@OpenReview', this)">[Kimi<sup id="kimi-stars-DzLP43CbiX@OpenReview">1</sup>]</a>
                <a id="rel-DzLP43CbiX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DzLP43CbiX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DzLP43CbiX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Georg Bökman" target="_blank">Georg Bökman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Nordström" target="_blank">David Nordström</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fredrik Kahl" target="_blank">Fredrik Kahl</a>
            </p>
            <p id="summary-DzLP43CbiX@OpenReview" class="summary">Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs.This paper introduces new equivariant neural networksthat preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks.The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group.This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs.Our approach reduces both FLOPs and wall-clock time,providing a practical solution for efficient, scalable symmetry-aware architectures.</p>
            <p id="subjects-DzLP43CbiX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-DzLP43CbiX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DzLP43CbiX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DzLP43CbiX@OpenReview" onclick="foldPdfKimi('DzLP43CbiX@OpenReview', this)" class="hr hr-fold">
        </div><div id="EW2JR5aVLm@OpenReview" class="panel paper" keywords="memorization,sharpness,metric,landscapes,probability,mitigating,diffusion,initial,generation,generative">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EW2JR5aVLm" target="_blank" title="52/225"><span class="index notranslate">#52</span></a>
                <a id="title-EW2JR5aVLm@OpenReview" class="title-link" href="/venue/EW2JR5aVLm@OpenReview" target="_blank">Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes</a>
                <a id="pdf-EW2JR5aVLm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EW2JR5aVLm@OpenReview', this)" data="https://openreview.net/pdf?id=EW2JR5aVLm">[PDF<sup id="pdf-stars-EW2JR5aVLm@OpenReview">1</sup>]</a>
                <a id="copy-EW2JR5aVLm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EW2JR5aVLm@OpenReview')">[Copy]</a>
                <a id="kimi-EW2JR5aVLm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EW2JR5aVLm@OpenReview', this)">[Kimi<sup id="kimi-stars-EW2JR5aVLm@OpenReview">1</sup>]</a>
                <a id="rel-EW2JR5aVLm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EW2JR5aVLm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EW2JR5aVLm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongjae Jeon" target="_blank">Dongjae Jeon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dueun Kim" target="_blank">Dueun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albert No" target="_blank">Albert No</a>
            </p>
            <p id="summary-EW2JR5aVLm@OpenReview" class="summary">In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term.</p>
            <p id="subjects-EW2JR5aVLm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-EW2JR5aVLm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EW2JR5aVLm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EW2JR5aVLm@OpenReview" onclick="foldPdfKimi('EW2JR5aVLm@OpenReview', this)" class="hr hr-fold">
        </div><div id="F1ff8zcjPp@OpenReview" class="panel paper" keywords="ppo,vlms,encoder,harmful,alignment,early,across,llava,exits,wise">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=F1ff8zcjPp" target="_blank" title="53/225"><span class="index notranslate">#53</span></a>
                <a id="title-F1ff8zcjPp@OpenReview" class="title-link" href="/venue/F1ff8zcjPp@OpenReview" target="_blank">Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models</a>
                <a id="pdf-F1ff8zcjPp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('F1ff8zcjPp@OpenReview', this)" data="https://openreview.net/pdf?id=F1ff8zcjPp">[PDF<sup id="pdf-stars-F1ff8zcjPp@OpenReview">5</sup>]</a>
                <a id="copy-F1ff8zcjPp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('F1ff8zcjPp@OpenReview')">[Copy]</a>
                <a id="kimi-F1ff8zcjPp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('F1ff8zcjPp@OpenReview', this)">[Kimi<sup id="kimi-stars-F1ff8zcjPp@OpenReview">6</sup>]</a>
                <a id="rel-F1ff8zcjPp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('F1ff8zcjPp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-F1ff8zcjPp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Saketh Bachu" target="_blank">Saketh Bachu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erfan Shayegani" target="_blank">Erfan Shayegani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohit Lal" target="_blank">Rohit Lal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Trishna Chakraborty" target="_blank">Trishna Chakraborty</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arindam Dutta" target="_blank">Arindam Dutta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengyu Song" target="_blank">Chengyu Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Dong" target="_blank">Yue Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nael Abu-Ghazaleh" target="_blank">Nael Abu-Ghazaleh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Roy-Chowdhury" target="_blank">Amit Roy-Chowdhury</a>
            </p>
            <p id="summary-F1ff8zcjPp@OpenReview" class="summary">Vision-language models (VLMs) have improved significantly in their capabilities, but their complex architecture makes their safety alignment challenging. In this paper, we reveal an uneven distribution of harmful information across the intermediate layers of the image encoder and show that skipping a certain set of layers and exiting early can increase the chance of the VLM generating harmful responses. We call it as “Image enCoder Early-exiT” based vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5, LLaVA-NeXT, and Llama 3.2 show that performing early exits from the image encoder significantly increases the likelihood of generating harmful outputs. To tackle this, we propose a simple yet effective modification of the Clipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing layer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO). We evaluate our L-PPO algorithm across three multi-modal datasets and show that it consistently reduces the harmfulness caused by early exits.</p>
            <p id="subjects-F1ff8zcjPp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-F1ff8zcjPp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-F1ff8zcjPp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-F1ff8zcjPp@OpenReview" onclick="foldPdfKimi('F1ff8zcjPp@OpenReview', this)" class="hr hr-fold">
        </div><div id="F8NTPAz5HH@OpenReview" class="panel paper" keywords="cqa,benchmarks,queries,query,kgs,complex,answering,decreses,really,link">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=F8NTPAz5HH" target="_blank" title="54/225"><span class="index notranslate">#54</span></a>
                <a id="title-F8NTPAz5HH@OpenReview" class="title-link" href="/venue/F8NTPAz5HH@OpenReview" target="_blank">Is Complex Query Answering Really Complex?</a>
                <a id="pdf-F8NTPAz5HH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('F8NTPAz5HH@OpenReview', this)" data="https://openreview.net/pdf?id=F8NTPAz5HH">[PDF<sup id="pdf-stars-F8NTPAz5HH@OpenReview">4</sup>]</a>
                <a id="copy-F8NTPAz5HH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('F8NTPAz5HH@OpenReview')">[Copy]</a>
                <a id="kimi-F8NTPAz5HH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('F8NTPAz5HH@OpenReview', this)">[Kimi<sup id="kimi-stars-F8NTPAz5HH@OpenReview">3</sup>]</a>
                <a id="rel-F8NTPAz5HH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('F8NTPAz5HH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-F8NTPAz5HH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cosimo Gregucci" target="_blank">Cosimo Gregucci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Xiong" target="_blank">Bo Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Hernández" target="_blank">Daniel Hernández</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Loconte" target="_blank">Lorenzo Loconte</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pasquale Minervini" target="_blank">Pasquale Minervini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Steffen Staab" target="_blank">Steffen Staab</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antonio Vergari" target="_blank">Antonio Vergari</a>
            </p>
            <p id="summary-F8NTPAz5HH@OpenReview" class="summary">Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum as a challenging reasoning task.In this paper, we show that the current benchmarks for CQA might not be as *complex* as we think, as the way they are built distorts our perception of progress in this field.For example, we find that in these benchmarks most queries (up to 98% for some query types) can be reduced to simpler problems, e.g., link prediction, where only one link needs to be predicted.The performance of state-of-the-art CQA models decreses significantly when such models are evaluated on queries that cannot be reduced to easier types.Thus, we propose a set of more challenging benchmarks composed of queries that *require* models to reason over multiple hops and better reflect the construction of real-world KGs.In a systematic empirical investigation, the new benchmarks show that current methods leave much to be desired from current CQA methods.</p>
            <p id="subjects-F8NTPAz5HH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-F8NTPAz5HH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-F8NTPAz5HH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-F8NTPAz5HH@OpenReview" onclick="foldPdfKimi('F8NTPAz5HH@OpenReview', this)" class="hr hr-fold">
        </div><div id="FKi6yjXwCN@OpenReview" class="panel paper" keywords="locate,jepa,pointcloud,referential,locate3d,self,supervised,grounding,featurized,dataset">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FKi6yjXwCN" target="_blank" title="55/225"><span class="index notranslate">#55</span></a>
                <a id="title-FKi6yjXwCN@OpenReview" class="title-link" href="/venue/FKi6yjXwCN@OpenReview" target="_blank">LOCATE 3D: Real-World Object Localization via Self-Supervised Learning in 3D</a>
                <a id="pdf-FKi6yjXwCN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FKi6yjXwCN@OpenReview', this)" data="https://openreview.net/pdf?id=FKi6yjXwCN">[PDF<sup id="pdf-stars-FKi6yjXwCN@OpenReview">3</sup>]</a>
                <a id="copy-FKi6yjXwCN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FKi6yjXwCN@OpenReview')">[Copy]</a>
                <a id="kimi-FKi6yjXwCN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FKi6yjXwCN@OpenReview', this)">[Kimi<sup id="kimi-stars-FKi6yjXwCN@OpenReview">2</sup>]</a>
                <a id="rel-FKi6yjXwCN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FKi6yjXwCN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FKi6yjXwCN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Paul McVay" target="_blank">Paul McVay</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergio Arnaud" target="_blank">Sergio Arnaud</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ada Martin" target="_blank">Ada Martin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arjun Majumdar" target="_blank">Arjun Majumdar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Krishna Murthy Jatavallabhula" target="_blank">Krishna Murthy Jatavallabhula</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Phillip Thomas" target="_blank">Phillip Thomas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruslan Partsey" target="_blank">Ruslan Partsey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Dugas" target="_blank">Daniel Dugas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abha Gejji" target="_blank">Abha Gejji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Sax" target="_blank">Alexander Sax</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent-Pierre Berges" target="_blank">Vincent-Pierre Berges</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mikael Henaff" target="_blank">Mikael Henaff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayush Jain" target="_blank">Ayush Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ang Cao" target="_blank">Ang Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ishita Prasad" target="_blank">Ishita Prasad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mrinal Kalakrishnan" target="_blank">Mrinal Kalakrishnan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Rabbat" target="_blank">Michael Rabbat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Ballas" target="_blank">Nicolas Ballas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahmoud Assran" target="_blank">Mahmoud Assran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oleksandr Maksymets" target="_blank">Oleksandr Maksymets</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aravind Rajeswaran" target="_blank">Aravind Rajeswaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Franziska Meier" target="_blank">Franziska Meier</a>
            </p>
            <p id="summary-FKi6yjXwCN@OpenReview" class="summary">We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like "the small coffee table between the sofa and the lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model. Code, models and dataset can be found at the project website: locate3d.atmeta.com</p>
            <p id="subjects-FKi6yjXwCN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FKi6yjXwCN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FKi6yjXwCN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FKi6yjXwCN@OpenReview" onclick="foldPdfKimi('FKi6yjXwCN@OpenReview', this)" class="hr hr-fold">
        </div><div id="FSVdEzR4To@OpenReview" class="panel paper" keywords="pars,offline,infeasible,reward,extrapolation,penalizing,actions,reinforcement,antmaze,scaling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FSVdEzR4To" target="_blank" title="56/225"><span class="index notranslate">#56</span></a>
                <a id="title-FSVdEzR4To@OpenReview" class="title-link" href="/venue/FSVdEzR4To@OpenReview" target="_blank">Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data</a>
                <a id="pdf-FSVdEzR4To@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FSVdEzR4To@OpenReview', this)" data="https://openreview.net/pdf?id=FSVdEzR4To">[PDF<sup id="pdf-stars-FSVdEzR4To@OpenReview">4</sup>]</a>
                <a id="copy-FSVdEzR4To@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FSVdEzR4To@OpenReview')">[Copy]</a>
                <a id="kimi-FSVdEzR4To@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FSVdEzR4To@OpenReview', this)">[Kimi<sup id="kimi-stars-FSVdEzR4To@OpenReview">6</sup>]</a>
                <a id="rel-FSVdEzR4To@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FSVdEzR4To@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FSVdEzR4To@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jeonghye Kim" target="_blank">Jeonghye Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongjae Shin" target="_blank">Yongjae Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Whiyoung Jung" target="_blank">Whiyoung Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sunghoon Hong" target="_blank">Sunghoon Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deunsol Yoon" target="_blank">Deunsol Yoon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youngchul Sung" target="_blank">Youngchul Sung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kanghoon Lee" target="_blank">Kanghoon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Woohyung Lim" target="_blank">Woohyung Lim</a>
            </p>
            <p id="summary-FSVdEzR4To@OpenReview" class="summary">Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task.</p>
            <p id="subjects-FSVdEzR4To@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FSVdEzR4To@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FSVdEzR4To@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FSVdEzR4To@OpenReview" onclick="foldPdfKimi('FSVdEzR4To@OpenReview', this)" class="hr hr-fold">
        </div><div id="Ffpc7vx6qq@OpenReview" class="panel paper" keywords="safety,polytope,sap,unsafe,facets,llms,constraints,outputs,geometric,representation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Ffpc7vx6qq" target="_blank" title="57/225"><span class="index notranslate">#57</span></a>
                <a id="title-Ffpc7vx6qq@OpenReview" class="title-link" href="/venue/Ffpc7vx6qq@OpenReview" target="_blank">Learning Safety Constraints for Large Language Models</a>
                <a id="pdf-Ffpc7vx6qq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Ffpc7vx6qq@OpenReview', this)" data="https://openreview.net/pdf?id=Ffpc7vx6qq">[PDF<sup id="pdf-stars-Ffpc7vx6qq@OpenReview">4</sup>]</a>
                <a id="copy-Ffpc7vx6qq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Ffpc7vx6qq@OpenReview')">[Copy]</a>
                <a id="kimi-Ffpc7vx6qq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Ffpc7vx6qq@OpenReview', this)">[Kimi<sup id="kimi-stars-Ffpc7vx6qq@OpenReview">7</sup>]</a>
                <a id="rel-Ffpc7vx6qq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Ffpc7vx6qq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Ffpc7vx6qq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Chen" target="_blank">Xin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yarden As" target="_blank">Yarden As</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Krause" target="_blank">Andreas Krause</a>
            </p>
            <p id="summary-Ffpc7vx6qq@OpenReview" class="summary">Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP–short for Safety Polytope–a geometric approach to LLM safety, that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.</p>
            <p id="subjects-Ffpc7vx6qq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Ffpc7vx6qq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ffpc7vx6qq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ffpc7vx6qq@OpenReview" onclick="foldPdfKimi('Ffpc7vx6qq@OpenReview', this)" class="hr hr-fold">
        </div><div id="FuGps5Zyia@OpenReview" class="panel paper" keywords="human,ah2ac2,flairox,coordination,hanabi,evaluation,hoc,agents,challenge,proxy">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FuGps5Zyia" target="_blank" title="58/225"><span class="index notranslate">#58</span></a>
                <a id="title-FuGps5Zyia@OpenReview" class="title-link" href="/venue/FuGps5Zyia@OpenReview" target="_blank">Ad-Hoc Human-AI Coordination Challenge</a>
                <a id="pdf-FuGps5Zyia@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FuGps5Zyia@OpenReview', this)" data="https://openreview.net/pdf?id=FuGps5Zyia">[PDF<sup id="pdf-stars-FuGps5Zyia@OpenReview">3</sup>]</a>
                <a id="copy-FuGps5Zyia@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FuGps5Zyia@OpenReview')">[Copy]</a>
                <a id="kimi-FuGps5Zyia@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FuGps5Zyia@OpenReview', this)">[Kimi<sup id="kimi-stars-FuGps5Zyia@OpenReview">2</sup>]</a>
                <a id="rel-FuGps5Zyia@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FuGps5Zyia@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FuGps5Zyia@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tin Dizdarevic" target="_blank">Tin Dizdarevic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ravi Hammond" target="_blank">Ravi Hammond</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tobias Gessler" target="_blank">Tobias Gessler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anisoara Calinescu" target="_blank">Anisoara Calinescu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Cook" target="_blank">Jonathan Cook</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Gallici" target="_blank">Matteo Gallici</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrei Lupu" target="_blank">Andrei Lupu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Foerster" target="_blank">Jakob Foerster</a>
            </p>
            <p id="summary-FuGps5Zyia@OpenReview" class="summary">Achieving seamless coordination between AI agents and humans is crucial for real-world applications, yet it remains a significant open challenge. Hanabi is a cooperative card game featuring imperfect information, constrained communication, theory of mind requirements, and coordinated action -- making it an ideal testbed for human-AI coordination. However, its use for human-AI interaction has been limited by the challenges of human evaluation. In this work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to overcome the constraints of costly and difficult-to-reproduce human evaluations. We develop \textit{human proxy agents} on a large-scale human dataset that serve as robust, cheap, and reproducible human-like evaluation partners in AH2AC2. To encourage the development of data-efficient methods, we open-source a dataset of 3,079 games, deliberately limiting the amount of available human gameplay data. We present baseline results for both two- and three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy agents through a controlled evaluation system rather than releasing them publicly. The code is available at \href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.</p>
            <p id="subjects-FuGps5Zyia@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-FuGps5Zyia@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FuGps5Zyia@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FuGps5Zyia@OpenReview" onclick="foldPdfKimi('FuGps5Zyia@OpenReview', this)" class="hr hr-fold">
        </div><div id="GazlTYxZss@OpenReview" class="panel paper" keywords="failure,attribution,agent,llm,failures,automated,systems,agents,task,multi">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GazlTYxZss" target="_blank" title="59/225"><span class="index notranslate">#59</span></a>
                <a id="title-GazlTYxZss@OpenReview" class="title-link" href="/venue/GazlTYxZss@OpenReview" target="_blank">Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</a>
                <a id="pdf-GazlTYxZss@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GazlTYxZss@OpenReview', this)" data="https://openreview.net/pdf?id=GazlTYxZss">[PDF<sup id="pdf-stars-GazlTYxZss@OpenReview">9</sup>]</a>
                <a id="copy-GazlTYxZss@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GazlTYxZss@OpenReview')">[Copy]</a>
                <a id="kimi-GazlTYxZss@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GazlTYxZss@OpenReview', this)">[Kimi<sup id="kimi-stars-GazlTYxZss@OpenReview">2</sup>]</a>
                <a id="rel-GazlTYxZss@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GazlTYxZss@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GazlTYxZss@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaokun Zhang" target="_blank">Shaokun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Yin" target="_blank">Ming Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jieyu Zhang" target="_blank">Jieyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiale Liu" target="_blank">Jiale Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiguang Han" target="_blank">Zhiguang Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyang Zhang" target="_blank">Jingyang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beibin Li" target="_blank">Beibin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chi Wang" target="_blank">Chi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huazheng Wang" target="_blank">Huazheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiran Chen" target="_blank">Yiran Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingyun Wu" target="_blank">Qingyun Wu</a>
            </p>
            <p id="summary-GazlTYxZss@OpenReview" class="summary">Failure attribution in LLM multi-agent systems—identifying the agent and step responsible for task failures—provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems.To support this initiative, we introduce the Who\&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps.Using the Who\&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5\% accuracy in identifying failure-responsible agents but only 14.2\% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available in https://github.com/mingyin1/Agents_Failure_Attribution.</p>
            <p id="subjects-GazlTYxZss@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GazlTYxZss@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GazlTYxZss@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GazlTYxZss@OpenReview" onclick="foldPdfKimi('GazlTYxZss@OpenReview', this)" class="hr hr-fold">
        </div><div id="GhTdNOMfOD@OpenReview" class="panel paper" keywords="timebase,forecasting,minimalism,ltsf,temporal,hqh0728,term,series,secures,long">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GhTdNOMfOD" target="_blank" title="60/225"><span class="index notranslate">#60</span></a>
                <a id="title-GhTdNOMfOD@OpenReview" class="title-link" href="/venue/GhTdNOMfOD@OpenReview" target="_blank">TimeBase: The Power of Minimalism in Efficient Long-term Time Series Forecasting</a>
                <a id="pdf-GhTdNOMfOD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GhTdNOMfOD@OpenReview', this)" data="https://openreview.net/pdf?id=GhTdNOMfOD">[PDF<sup id="pdf-stars-GhTdNOMfOD@OpenReview">22</sup>]</a>
                <a id="copy-GhTdNOMfOD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GhTdNOMfOD@OpenReview')">[Copy]</a>
                <a id="kimi-GhTdNOMfOD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GhTdNOMfOD@OpenReview', this)">[Kimi<sup id="kimi-stars-GhTdNOMfOD@OpenReview">3</sup>]</a>
                <a id="rel-GhTdNOMfOD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GhTdNOMfOD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GhTdNOMfOD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qihe Huang" target="_blank">Qihe Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyang Zhou" target="_blank">Zhengyang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuo Yang" target="_blank">Kuo Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongchao Yi" target="_blank">Zhongchao Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Wang" target="_blank">Xu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Wang" target="_blank">Yang Wang</a>
            </p>
            <p id="summary-GhTdNOMfOD@OpenReview" class="summary">Long-term time series forecasting (LTSF) has traditionally relied on large parameters to capture extended temporal dependencies, resulting in substantial computational costs and inefficiencies in both memory usage and processing time. However, time series data, unlike high-dimensional images or text, often exhibit temporal pattern similarity and low-rank structures, especially in long-term horizons. By leveraging this structure, models can be guided to focus on more essential, concise temporal data, improving both accuracy and computational efficiency. In this paper, we introduce TimeBase, an ultra-lightweight network to harness the power of minimalism in LTSF. TimeBase 1) extracts core basis temporal components and 2) transforms traditional point-level forecasting into efficient segment-level forecasting, achieving optimal utilization of both data and parameters. Extensive experiments on diverse real-world datasets show that TimeBase achieves remarkable efficiency and secures competitive forecasting performance. Additionally, TimeBase can also serve as a very effective plug-and-play complexity reducer for any patch-based forecasting models. Code is available at \url{https://github.com/hqh0728/TimeBase}.</p>
            <p id="subjects-GhTdNOMfOD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GhTdNOMfOD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GhTdNOMfOD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GhTdNOMfOD@OpenReview" onclick="foldPdfKimi('GhTdNOMfOD@OpenReview', this)" class="hr hr-fold">
        </div><div id="Gn6L4QRKf7@OpenReview" class="panel paper" keywords="context,enhanced,learning,icl,llms,gradient,appears,copyright,regressive,setting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Gn6L4QRKf7" target="_blank" title="61/225"><span class="index notranslate">#61</span></a>
                <a id="title-Gn6L4QRKf7@OpenReview" class="title-link" href="/venue/Gn6L4QRKf7@OpenReview" target="_blank">On the Power of Context-Enhanced Learning in LLMs</a>
                <a id="pdf-Gn6L4QRKf7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Gn6L4QRKf7@OpenReview', this)" data="https://openreview.net/pdf?id=Gn6L4QRKf7">[PDF<sup id="pdf-stars-Gn6L4QRKf7@OpenReview">7</sup>]</a>
                <a id="copy-Gn6L4QRKf7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Gn6L4QRKf7@OpenReview')">[Copy]</a>
                <a id="kimi-Gn6L4QRKf7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Gn6L4QRKf7@OpenReview', this)">[Kimi<sup id="kimi-stars-Gn6L4QRKf7@OpenReview">4</sup>]</a>
                <a id="rel-Gn6L4QRKf7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Gn6L4QRKf7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Gn6L4QRKf7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyu Zhu" target="_blank">Xingyu Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhishek Panigrahi" target="_blank">Abhishek Panigrahi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjeev Arora" target="_blank">Sanjeev Arora</a>
            </p>
            <p id="summary-Gn6L4QRKf7@OpenReview" class="summary">We formalize a new concept for LLMs, **context-enhanced learning**. It involves standard gradient-based learning on text except that the context is enhanced with additional data on which no auto-regressive gradients are computed. This setting is a gradient-based analog of usual in-context learning (ICL) and appears in some recent works.Using a multi-step reasoning task, we prove in a simplified setting that context-enhanced learning can be **exponentially more sample-efficient** than standard learning when the model is capable of ICL. At a mechanistic level, we find that the benefit of context-enhancement arises from a more accurate gradient learning signal.We also experimentally demonstrate that it appears hard to detect or recover learning materials that were used in the context during training. This may have implications for data security as well as copyright.</p>
            <p id="subjects-Gn6L4QRKf7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Gn6L4QRKf7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gn6L4QRKf7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gn6L4QRKf7@OpenReview" onclick="foldPdfKimi('Gn6L4QRKf7@OpenReview', this)" class="hr hr-fold">
        </div><div id="Go0DdhjATH@OpenReview" class="panel paper" keywords="asor,policy,ifo,accessible,imitation,expert,dynamics,globally,inaccessible,states">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Go0DdhjATH" target="_blank" title="62/225"><span class="index notranslate">#62</span></a>
                <a id="title-Go0DdhjATH@OpenReview" class="title-link" href="/venue/Go0DdhjATH@OpenReview" target="_blank">Policy Regularization on Globally Accessible States in Cross-Dynamics Reinforcement Learning</a>
                <a id="pdf-Go0DdhjATH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Go0DdhjATH@OpenReview', this)" data="https://openreview.net/pdf?id=Go0DdhjATH">[PDF<sup id="pdf-stars-Go0DdhjATH@OpenReview">4</sup>]</a>
                <a id="copy-Go0DdhjATH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Go0DdhjATH@OpenReview')">[Copy]</a>
                <a id="kimi-Go0DdhjATH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Go0DdhjATH@OpenReview', this)">[Kimi<sup id="kimi-stars-Go0DdhjATH@OpenReview">3</sup>]</a>
                <a id="rel-Go0DdhjATH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Go0DdhjATH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Go0DdhjATH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenghai Xue" target="_blank">Zhenghai Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lang Feng" target="_blank">Lang Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacheng Xu" target="_blank">Jiacheng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kang Kang" target="_blank">Kang Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=xiang wen" target="_blank">xiang wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo An" target="_blank">Bo An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuicheng YAN" target="_blank">Shuicheng YAN</a>
            </p>
            <p id="summary-Go0DdhjATH@OpenReview" class="summary">To learn from data collected in diverse dynamics, Imitation from Observation (IfO) methods leverage expert state trajectories based on thepremise that recovering expert state distributions in other dynamics facilitates policy learning in the current one. However, Imitation Learning inherently imposes a performance upper bound of learned policies. Additionally, as the environment dynamics change, certain expert states may become inaccessible, rendering their distributions less valuable for imitation. To address this, we propose a novel framework that integrates reward maximization with IfO, employing F-distance regularized policy optimization. This framework enforces constraints on globally accessible states—those with nonzero visitation frequency across all considered dynamics—mitigating the challenge posed by inaccessible states. By instantiating F-distance in different ways, we derive two theoretical analysis and develop a practical algorithm called Accessible State Oriented Policy Regularization (ASOR). ASOR serves as a general-purpose module that can be incorporated into various RL approaches, including offline RL and off-policy RL. Extensive experiments across multiple benchmarks demonstrate ASOR’s effectiveness in enhancing state-of-the-art cross-domain policy transfer algorithms, significantly improving their performance.</p>
            <p id="subjects-Go0DdhjATH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Go0DdhjATH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Go0DdhjATH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Go0DdhjATH@OpenReview" onclick="foldPdfKimi('Go0DdhjATH@OpenReview', this)" class="hr hr-fold">
        </div><div id="GoGuB1yFko@OpenReview" class="panel paper" keywords="ood,prompts,amcn,shot,detection,adaptive,prompt,samples,contrastive,learnable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GoGuB1yFko" target="_blank" title="63/225"><span class="index notranslate">#63</span></a>
                <a id="title-GoGuB1yFko@OpenReview" class="title-link" href="/venue/GoGuB1yFko@OpenReview" target="_blank">Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection</a>
                <a id="pdf-GoGuB1yFko@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GoGuB1yFko@OpenReview', this)" data="https://openreview.net/pdf?id=GoGuB1yFko">[PDF<sup id="pdf-stars-GoGuB1yFko@OpenReview">5</sup>]</a>
                <a id="copy-GoGuB1yFko@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GoGuB1yFko@OpenReview')">[Copy]</a>
                <a id="kimi-GoGuB1yFko@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GoGuB1yFko@OpenReview', this)">[Kimi<sup id="kimi-stars-GoGuB1yFko@OpenReview">1</sup>]</a>
                <a id="rel-GoGuB1yFko@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GoGuB1yFko@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GoGuB1yFko@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Fang" target="_blank">Xiang Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arvind Easwaran" target="_blank">Arvind Easwaran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Blaise Genest" target="_blank">Blaise Genest</a>
            </p>
            <p id="summary-GoGuB1yFko@OpenReview" class="summary">Out-of-distribution (OOD) detection attempts to distinguish outlier samples to prevent models trained on the in-distribution (ID) dataset from producing unavailable outputs. Most OOD detection methods require many ID samples for training, which seriously limits their real-world applications. To this end, we target a challenging setting: few-shot OOD detection, where only a few labeled ID samples are available. Therefore, few-shot OOD detection is much more challenging than the traditional OOD detection setting. Previous few-shot OOD detection works ignore the distinct diversity between different classes. In this paper, we propose a novel network: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. To compensate for the absence of OOD and scarcity of ID image samples, we leverage CLIP, connecting text with images, engineering learnable ID and OOD textual prompts. Specifically, we first generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts, and label-adaptive OOD prompts). Then, we generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, we propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts. Experimental results show that AMCN outperforms other state-of-the-art works.</p>
            <p id="subjects-GoGuB1yFko@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GoGuB1yFko@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GoGuB1yFko@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GoGuB1yFko@OpenReview" onclick="foldPdfKimi('GoGuB1yFko@OpenReview', this)" class="hr hr-fold">
        </div><div id="Gp7NfP7Erm@OpenReview" class="panel paper" keywords="algorithm,explainability,dag,selection,robustness,features,distribution,feature,rejected,automatic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Gp7NfP7Erm" target="_blank" title="64/225"><span class="index notranslate">#64</span></a>
                <a id="title-Gp7NfP7Erm@OpenReview" class="title-link" href="/venue/Gp7NfP7Erm@OpenReview" target="_blank">Towards Robustness and Explainability of Automatic Algorithm Selection</a>
                <a id="pdf-Gp7NfP7Erm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Gp7NfP7Erm@OpenReview', this)" data="https://openreview.net/pdf?id=Gp7NfP7Erm">[PDF<sup id="pdf-stars-Gp7NfP7Erm@OpenReview">4</sup>]</a>
                <a id="copy-Gp7NfP7Erm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Gp7NfP7Erm@OpenReview')">[Copy]</a>
                <a id="kimi-Gp7NfP7Erm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Gp7NfP7Erm@OpenReview', this)">[Kimi<sup id="kimi-stars-Gp7NfP7Erm@OpenReview"></sup>]</a>
                <a id="rel-Gp7NfP7Erm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Gp7NfP7Erm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Gp7NfP7Erm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyu Wu" target="_blank">Xingyu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jibin Wu" target="_blank">Jibin Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Zhou" target="_blank">Yu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Feng" target="_blank">Liang Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=KC Tan" target="_blank">KC Tan</a>
            </p>
            <p id="summary-Gp7NfP7Erm@OpenReview" class="summary">Algorithm selection aims to identify the optimal performing algorithm before execution. Existing techniques typically focus on the observed correlations between algorithm performance and meta-features. However, little research has explored the underlying mechanisms of algorithm selection, specifically what characteristics an algorithm must possess to effectively tackle problems with certain feature values. This gap not only limits the explainability but also makes existing models vulnerable to data bias and distribution shift. This paper introduces directed acyclic graph (DAG) to describe this mechanism, proposing a novel modeling paradigm that aligns more closely with the fundamental logic of algorithm selection. By leveraging DAG to characterize the algorithm feature distribution conditioned on problem features, our approach enhances robustness against marginal distribution changes and allows for finer-grained predictions through the reconstruction of optimal algorithm features, with the final decision relying on differences between reconstructed and rejected algorithm features. Furthermore, we demonstrate that, the learned DAG and the proposed counterfactual calculations offer our approach with both model-level and instance-level explainability.</p>
            <p id="subjects-Gp7NfP7Erm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Gp7NfP7Erm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gp7NfP7Erm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gp7NfP7Erm@OpenReview" onclick="foldPdfKimi('Gp7NfP7Erm@OpenReview', this)" class="hr hr-fold">
        </div><div id="HxCuvx2uUi@OpenReview" class="panel paper" keywords="lse,estimator,policy,propensity,epsilon,evaluation,logged,feedback,variance,bandit">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HxCuvx2uUi" target="_blank" title="65/225"><span class="index notranslate">#65</span></a>
                <a id="title-HxCuvx2uUi@OpenReview" class="title-link" href="/venue/HxCuvx2uUi@OpenReview" target="_blank">Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning</a>
                <a id="pdf-HxCuvx2uUi@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HxCuvx2uUi@OpenReview', this)" data="https://openreview.net/pdf?id=HxCuvx2uUi">[PDF<sup id="pdf-stars-HxCuvx2uUi@OpenReview">3</sup>]</a>
                <a id="copy-HxCuvx2uUi@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HxCuvx2uUi@OpenReview')">[Copy]</a>
                <a id="kimi-HxCuvx2uUi@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HxCuvx2uUi@OpenReview', this)">[Kimi<sup id="kimi-stars-HxCuvx2uUi@OpenReview">2</sup>]</a>
                <a id="rel-HxCuvx2uUi@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HxCuvx2uUi@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HxCuvx2uUi@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Armin Behnamnia" target="_blank">Armin Behnamnia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gholamali Aminian" target="_blank">Gholamali Aminian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alireza Aghaei" target="_blank">Alireza Aghaei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengchun Shi" target="_blank">Chengchun Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent Tan" target="_blank">Vincent Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hamid R Rabiee" target="_blank">Hamid R Rabiee</a>
            </p>
            <p id="summary-HxCuvx2uUi@OpenReview" class="summary">Off-policy learning and evaluation leverage logged bandit feedback datasets, which contain context, action, propensity score, and feedback for each data point. These scenarios face significant challenges due to high variance and poor performance with low-quality propensity scores and heavy-tailed reward distributions. We address these issues by introducing a novel estimator based on the log-sum-exponential (LSE) operator, which outperforms traditional inverse propensity score estimators. Our LSE estimator demonstrates variance reduction and robustness under heavy-tailed conditions. For off-policy evaluation, we derive upper bounds on the estimator's bias and variance. In the off-policy learning scenario, we establish bounds on the regret—the performance gap between our LSE estimator and the optimal policy—assuming bounded <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-39-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-241" style="width: 3.44em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.867em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.76em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-242"><span class="mo" id="MathJax-Span-243" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-244" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-245" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-246" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">ϵ</span><span class="mo" id="MathJax-Span-247" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-39">(1+\epsilon)</script>-th moment of weighted reward. Notably, we achieve a convergence rate of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-40-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-248" style="width: 6.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1005.05em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-249"><span class="mi" id="MathJax-Span-250" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-251" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-252"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-253" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-254"><span class="mrow" id="MathJax-Span-255"><span class="mo" id="MathJax-Span-256" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-257" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span><span class="texatom" id="MathJax-Span-258"><span class="mrow" id="MathJax-Span-259"><span class="mo" id="MathJax-Span-260" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mo" id="MathJax-Span-261" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-262" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-263" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-264" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span><span class="mo" id="MathJax-Span-265" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-266" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mi>ϵ</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-40">O(n^{-\epsilon/(1+\epsilon)})</script> for the regret bounds, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-41-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-267" style="width: 4.326em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.49em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-268"><span class="mi" id="MathJax-Span-269" style="font-family: MathJax_Math-italic;">ϵ</span><span class="mo" id="MathJax-Span-270" style="font-family: MathJax_Main; padding-left: 0.263em;">∈</span><span class="mo" id="MathJax-Span-271" style="font-family: MathJax_Main; padding-left: 0.263em;">[</span><span class="mn" id="MathJax-Span-272" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-273" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-274" style="font-family: MathJax_Main; padding-left: 0.159em;">1</span><span class="mo" id="MathJax-Span-275" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-41">\epsilon\in[0,1]</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-42-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-276" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-277"><span class="mi" id="MathJax-Span-278" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-42">n</script> is the size of logged bandit feedback dataset. Theoretical analysis is complemented by comprehensive empirical evaluations in both off-policy learning and evaluation scenarios, confirming the practical advantages of our approach. The code for our estimator is available at the following link: https://github.com/armin-behnamnia/lse-offpolicy-learning .</p>
            <p id="subjects-HxCuvx2uUi@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-HxCuvx2uUi@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HxCuvx2uUi@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HxCuvx2uUi@OpenReview" onclick="foldPdfKimi('HxCuvx2uUi@OpenReview', this)" class="hr hr-fold">
        </div><div id="I4jNAbqHnM@OpenReview" class="panel paper" keywords="gumdps,trials,discounted,infinite,visitation,policy,number,horizon,mdps,gumdp">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=I4jNAbqHnM" target="_blank" title="66/225"><span class="index notranslate">#66</span></a>
                <a id="title-I4jNAbqHnM@OpenReview" class="title-link" href="/venue/I4jNAbqHnM@OpenReview" target="_blank">The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes</a>
                <a id="pdf-I4jNAbqHnM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('I4jNAbqHnM@OpenReview', this)" data="https://openreview.net/pdf?id=I4jNAbqHnM">[PDF<sup id="pdf-stars-I4jNAbqHnM@OpenReview">1</sup>]</a>
                <a id="copy-I4jNAbqHnM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('I4jNAbqHnM@OpenReview')">[Copy]</a>
                <a id="kimi-I4jNAbqHnM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('I4jNAbqHnM@OpenReview', this)">[Kimi<sup id="kimi-stars-I4jNAbqHnM@OpenReview"></sup>]</a>
                <a id="rel-I4jNAbqHnM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('I4jNAbqHnM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-I4jNAbqHnM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pedro Santos" target="_blank">Pedro Santos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alberto Sardinha" target="_blank">Alberto Sardinha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francisco S. Melo" target="_blank">Francisco S. Melo</a>
            </p>
            <p id="summary-I4jNAbqHnM@OpenReview" class="summary">The general-utility Markov decision processes (GUMDPs) framework generalizes the MDPs framework by considering objective functions that depend on the frequency of visitation of state-action pairs induced by a given policy. In this work, we contribute with the first analysis on the impact of the number of trials, i.e., the number of randomly sampled trajectories, in infinite-horizon GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a key-role in infinite-horizon GUMDPs and the expected performance of a given policy depends, in general, on the number of trials. We consider both discounted and average GUMDPs, where the objective function depends, respectively, on discounted and average frequencies of visitation of state-action pairs. First, we study policy evaluation under discounted GUMDPs, proving lower and upper bounds on the mismatch between the finite and infinite trials formulations for GUMDPs. Second, we address average GUMDPs, studying how different classes of GUMDPs impact the mismatch between the finite and infinite trials formulations. Third, we provide a set of empirical results to support our claims, highlighting how the number of trajectories and the structure of the underlying GUMDP influence policy evaluation.</p>
            <p id="subjects-I4jNAbqHnM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-I4jNAbqHnM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-I4jNAbqHnM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-I4jNAbqHnM@OpenReview" onclick="foldPdfKimi('I4jNAbqHnM@OpenReview', this)" class="hr hr-fold">
        </div><div id="IKCfxWtTsu@OpenReview" class="panel paper" keywords="private,pcevolve,apis,api,synthetic,shot,contrastive,evolution,generative,data">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IKCfxWtTsu" target="_blank" title="67/225"><span class="index notranslate">#67</span></a>
                <a id="title-IKCfxWtTsu@OpenReview" class="title-link" href="/venue/IKCfxWtTsu@OpenReview" target="_blank">PCEvolve: Private Contrastive Evolution for Synthetic Dataset Generation via Few-Shot Private Data and Generative APIs</a>
                <a id="pdf-IKCfxWtTsu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IKCfxWtTsu@OpenReview', this)" data="https://openreview.net/pdf?id=IKCfxWtTsu">[PDF<sup id="pdf-stars-IKCfxWtTsu@OpenReview">5</sup>]</a>
                <a id="copy-IKCfxWtTsu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IKCfxWtTsu@OpenReview')">[Copy]</a>
                <a id="kimi-IKCfxWtTsu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IKCfxWtTsu@OpenReview', this)">[Kimi<sup id="kimi-stars-IKCfxWtTsu@OpenReview">1</sup>]</a>
                <a id="rel-IKCfxWtTsu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IKCfxWtTsu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IKCfxWtTsu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianqing Zhang" target="_blank">Jianqing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Liu" target="_blank">Yang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Fu" target="_blank">Jie Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Hua" target="_blank">Yang Hua</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyuan Zou" target="_blank">Tianyuan Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Cao" target="_blank">Jian Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Yang" target="_blank">Qiang Yang</a>
            </p>
            <p id="summary-IKCfxWtTsu@OpenReview" class="summary">The rise of generative APIs has fueled interest in privacy-preserving synthetic data generation. While the Private Evolution (PE) algorithm generates Differential Privacy (DP) synthetic images using diffusion model APIs, it struggles with few-shot private data due to the limitations of its DP-protected similarity voting approach. In practice, the few-shot private data challenge is particularly prevalent in specialized domains like healthcare and industry. To address this challenge, we propose a novel API-assisted algorithm, Private Contrastive Evolution (PCEvolve), which iteratively mines inherent inter-class contrastive relationships in few-shot private data beyond individual data points and seamlessly integrates them into an adapted Exponential Mechanism (EM) to optimize DP’s utility in an evolution loop. We conduct extensive experiments on four specialized datasets, demonstrating that PCEvolve outperforms PE and other API-assisted baselines. These results highlight the potential of leveraging API access with private data for quality evaluation, enabling the generation of high-quality DP synthetic images and paving the way for more accessible and effective privacy-preserving generative API applications. Our code is available at https://github.com/TsingZ0/PCEvolve.</p>
            <p id="subjects-IKCfxWtTsu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IKCfxWtTsu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IKCfxWtTsu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IKCfxWtTsu@OpenReview" onclick="foldPdfKimi('IKCfxWtTsu@OpenReview', this)" class="hr hr-fold">
        </div><div id="JxnOZwFNcU@OpenReview" class="panel paper" keywords="altt,ltt,mht,testing,hyperparameter,learn,test,selection,rounds,adaptive">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=JxnOZwFNcU" target="_blank" title="68/225"><span class="index notranslate">#68</span></a>
                <a id="title-JxnOZwFNcU@OpenReview" class="title-link" href="/venue/JxnOZwFNcU@OpenReview" target="_blank">Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection</a>
                <a id="pdf-JxnOZwFNcU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('JxnOZwFNcU@OpenReview', this)" data="https://openreview.net/pdf?id=JxnOZwFNcU">[PDF<sup id="pdf-stars-JxnOZwFNcU@OpenReview">3</sup>]</a>
                <a id="copy-JxnOZwFNcU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('JxnOZwFNcU@OpenReview')">[Copy]</a>
                <a id="kimi-JxnOZwFNcU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('JxnOZwFNcU@OpenReview', this)">[Kimi<sup id="kimi-stars-JxnOZwFNcU@OpenReview">2</sup>]</a>
                <a id="rel-JxnOZwFNcU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('JxnOZwFNcU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-JxnOZwFNcU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Zecchin" target="_blank">Matteo Zecchin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangwoo Park" target="_blank">Sangwoo Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Osvaldo Simeone" target="_blank">Osvaldo Simeone</a>
            </p>
            <p id="summary-JxnOZwFNcU@OpenReview" class="summary">We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and prompt engineering, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds.</p>
            <p id="subjects-JxnOZwFNcU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-JxnOZwFNcU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-JxnOZwFNcU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-JxnOZwFNcU@OpenReview" onclick="foldPdfKimi('JxnOZwFNcU@OpenReview', this)" class="hr hr-fold">
        </div><div id="K2CckZjNy0@OpenReview" class="panel paper" keywords="axbench,steering,finetuning,reft,saes,prompting,autoencoders,representation,sparse,interpretability">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=K2CckZjNy0" target="_blank" title="69/225"><span class="index notranslate">#69</span></a>
                <a id="title-K2CckZjNy0@OpenReview" class="title-link" href="/venue/K2CckZjNy0@OpenReview" target="_blank">AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders</a>
                <a id="pdf-K2CckZjNy0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('K2CckZjNy0@OpenReview', this)" data="https://openreview.net/pdf?id=K2CckZjNy0">[PDF<sup id="pdf-stars-K2CckZjNy0@OpenReview">1</sup>]</a>
                <a id="copy-K2CckZjNy0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('K2CckZjNy0@OpenReview')">[Copy]</a>
                <a id="kimi-K2CckZjNy0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('K2CckZjNy0@OpenReview', this)">[Kimi<sup id="kimi-stars-K2CckZjNy0@OpenReview"></sup>]</a>
                <a id="rel-K2CckZjNy0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('K2CckZjNy0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-K2CckZjNy0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengxuan Wu" target="_blank">Zhengxuan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aryaman Arora" target="_blank">Aryaman Arora</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Atticus Geiger" target="_blank">Atticus Geiger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Wang" target="_blank">Zheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Huang" target="_blank">Jing Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Jurafsky" target="_blank">Dan Jurafsky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Manning" target="_blank">Christopher Manning</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Potts" target="_blank">Christopher Potts</a>
            </p>
            <p id="summary-K2CckZjNy0@OpenReview" class="summary">Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.</p>
            <p id="subjects-K2CckZjNy0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-K2CckZjNy0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-K2CckZjNy0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-K2CckZjNy0@OpenReview" onclick="foldPdfKimi('K2CckZjNy0@OpenReview', this)" class="hr hr-fold">
        </div><div id="KRosBwvhDx@OpenReview" class="panel paper" keywords="bqn,brain,message,passing,lywjun,network,demo,gnns,graph,transformers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KRosBwvhDx" target="_blank" title="70/225"><span class="index notranslate">#70</span></a>
                <a id="title-KRosBwvhDx@OpenReview" class="title-link" href="/venue/KRosBwvhDx@OpenReview" target="_blank">Do We Really Need Message Passing in Brain Network Modeling?</a>
                <a id="pdf-KRosBwvhDx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KRosBwvhDx@OpenReview', this)" data="https://openreview.net/pdf?id=KRosBwvhDx">[PDF<sup id="pdf-stars-KRosBwvhDx@OpenReview">5</sup>]</a>
                <a id="copy-KRosBwvhDx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KRosBwvhDx@OpenReview')">[Copy]</a>
                <a id="kimi-KRosBwvhDx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KRosBwvhDx@OpenReview', this)">[Kimi<sup id="kimi-stars-KRosBwvhDx@OpenReview"></sup>]</a>
                <a id="rel-KRosBwvhDx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KRosBwvhDx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KRosBwvhDx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Yang" target="_blank">Liang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuwei Liu" target="_blank">Yuwei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaming Zhuo" target="_blank">Jiaming Zhuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Jin" target="_blank">Di Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuan Wang" target="_blank">Chuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Wang" target="_blank">Zhen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaochun Cao" target="_blank">Xiaochun Cao</a>
            </p>
            <p id="summary-KRosBwvhDx@OpenReview" class="summary">Brain network analysis plays a critical role in brain disease prediction and diagnosis. Graph mining tools have made remarkable progress. Graph neural networks (GNNs) and Transformers, which rely on the message-passing scheme, recently dominated this field due to their powerful expressive ability on graph data. Unfortunately, by considering brain network construction using pairwise Pearson’s coefficients between any pairs of ROIs, model analysis and experimental verification reveal that *the message-passing under both GNNs and Transformers can NOT be fully explored and exploited*. Surprisingly, this paper observes the significant performance and efficiency enhancements of the Hadamard product compared to the matrix product, which is the matrix form of message passing, in processing the brain network. Inspired by this finding, a novel Brain Quadratic Network (BQN) is proposed by incorporating quadratic networks, which possess better universal approximation properties. Moreover, theoretical analysis demonstrates that BQN implicitly performs community detection along with representation learning. Extensive evaluations verify the superiority of the proposed BQN compared to the message-passing-based brain network modeling. Source code is available at [https://github.com/LYWJUN/BQN-demo](https://github.com/LYWJUN/BQN-demo).</p>
            <p id="subjects-KRosBwvhDx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-KRosBwvhDx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KRosBwvhDx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KRosBwvhDx@OpenReview" onclick="foldPdfKimi('KRosBwvhDx@OpenReview', this)" class="hr hr-fold">
        </div><div id="KZo2XhcSg6@OpenReview" class="panel paper" keywords="lipsnet,filter,policy,controller,layer,control,action,fluctuation,causes,noise">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KZo2XhcSg6" target="_blank" title="71/225"><span class="index notranslate">#71</span></a>
                <a id="title-KZo2XhcSg6@OpenReview" class="title-link" href="/venue/KZo2XhcSg6@OpenReview" target="_blank">LipsNet++: Unifying Filter and Controller into a Policy Network</a>
                <a id="pdf-KZo2XhcSg6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KZo2XhcSg6@OpenReview', this)" data="https://openreview.net/pdf?id=KZo2XhcSg6">[PDF<sup id="pdf-stars-KZo2XhcSg6@OpenReview">4</sup>]</a>
                <a id="copy-KZo2XhcSg6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KZo2XhcSg6@OpenReview')">[Copy]</a>
                <a id="kimi-KZo2XhcSg6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KZo2XhcSg6@OpenReview', this)">[Kimi<sup id="kimi-stars-KZo2XhcSg6@OpenReview"></sup>]</a>
                <a id="rel-KZo2XhcSg6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KZo2XhcSg6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KZo2XhcSg6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xujie Song" target="_blank">Xujie Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangfa Chen" target="_blank">Liangfa Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Liu" target="_blank">Tong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxuan Wang" target="_blank">Wenxuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinuo Wang" target="_blank">Yinuo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shentao Qin" target="_blank">Shentao Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinsong Ma" target="_blank">Yinsong Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingliang Duan" target="_blank">Jingliang Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengbo Li" target="_blank">Shengbo Li</a>
            </p>
            <p id="summary-KZo2XhcSg6@OpenReview" class="summary">Deep reinforcement learning (RL) is effective for decision-making and control tasks like autonomous driving and embodied AI. However, RL policies often suffer from the action fluctuation problem in real-world applications, resulting in severe actuator wear, safety risk, and performance degradation. This paper identifies the two fundamental causes of action fluctuation: observation noise and policy non-smoothness. We propose LipsNet++, a novel policy network with Fourier filter layer and Lipschitz controller layer to separately address both causes. The filter layer incorporates a trainable filter matrix that automatically extracts important frequencies while suppressing noise frequencies in the observations. The controller layer introduces a Jacobian regularization technique to achieve a low Lipschitz constant, ensuring smooth fitting of a policy function. These two layers function analogously to the filter and controller in classical control theory, suggesting that filtering and control capabilities can be seamlessly integrated into a single policy network. Both simulated and real-world experiments demonstrate that LipsNet++ achieves the state-of-the-art noise robustness and action smoothness. The code and videos are publicly available at https://xjsong99.github.io/LipsNet_v2.</p>
            <p id="subjects-KZo2XhcSg6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-KZo2XhcSg6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KZo2XhcSg6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KZo2XhcSg6@OpenReview" onclick="foldPdfKimi('KZo2XhcSg6@OpenReview', this)" class="hr hr-fold">
        </div><div id="IYLNdCII48@OpenReview" class="panel paper" keywords="missingness,cacti,contextual,imputation,copy,masking,tabular,leveraging,patterns,random">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IYLNdCII48" target="_blank" title="72/225"><span class="index notranslate">#72</span></a>
                <a id="title-IYLNdCII48@OpenReview" class="title-link" href="/venue/IYLNdCII48@OpenReview" target="_blank">CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation</a>
                <a id="pdf-IYLNdCII48@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IYLNdCII48@OpenReview', this)" data="https://openreview.net/pdf?id=IYLNdCII48">[PDF<sup id="pdf-stars-IYLNdCII48@OpenReview">2</sup>]</a>
                <a id="copy-IYLNdCII48@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IYLNdCII48@OpenReview')">[Copy]</a>
                <a id="kimi-IYLNdCII48@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IYLNdCII48@OpenReview', this)">[Kimi<sup id="kimi-stars-IYLNdCII48@OpenReview"></sup>]</a>
                <a id="rel-IYLNdCII48@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IYLNdCII48@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IYLNdCII48@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aditya Gorla" target="_blank">Aditya Gorla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Wang" target="_blank">Ryan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengtong Liu" target="_blank">Zhengtong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ulzee An" target="_blank">Ulzee An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sriram Sankararaman" target="_blank">Sriram Sankararaman</a>
            </p>
            <p id="summary-IYLNdCII48@OpenReview" class="summary">We present CACTI, a masked autoencoding approach for imputing tabular data that leverages the structure in missingness patterns and contextual information. Our approach employs a novel median truncated copy masking training strategy that encourages the model to learn from empirical patterns of missingness while incorporating semantic relationships between features — captured by column names and text descriptions — to better represent feature dependence. These dual sources of inductive bias enable CACTIto outperform state-of-the-art methods — an average <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-43-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-279" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-280"><span class="msubsup" id="MathJax-Span-281"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-282" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mn" id="MathJax-Span-283" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-43">R^2</script> gain of 7.8\% over the next best method (13.4%, 6.1%, and 5.3% under missing not at random, at random and completely at random, respectively) — across a diverse range of datasets and missingness conditions. Our results highlight the value of leveraging dataset-specific contextual information and missingness patterns to enhance imputation performance.</p>
            <p id="subjects-IYLNdCII48@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IYLNdCII48@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IYLNdCII48@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IYLNdCII48@OpenReview" onclick="foldPdfKimi('IYLNdCII48@OpenReview', this)" class="hr hr-fold">
        </div><div id="Lie2rOCgkh@OpenReview" class="panel paper" keywords="posterior,estimands,causal,outcome,attribution,continuous,binarizing,ignorability,hypertension,outcomes">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Lie2rOCgkh" target="_blank" title="73/225"><span class="index notranslate">#73</span></a>
                <a id="title-Lie2rOCgkh@OpenReview" class="title-link" href="/venue/Lie2rOCgkh@OpenReview" target="_blank">Causal Attribution Analysis for Continuous Outcomes</a>
                <a id="pdf-Lie2rOCgkh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Lie2rOCgkh@OpenReview', this)" data="https://openreview.net/pdf?id=Lie2rOCgkh">[PDF<sup id="pdf-stars-Lie2rOCgkh@OpenReview">4</sup>]</a>
                <a id="copy-Lie2rOCgkh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Lie2rOCgkh@OpenReview')">[Copy]</a>
                <a id="kimi-Lie2rOCgkh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Lie2rOCgkh@OpenReview', this)">[Kimi<sup id="kimi-stars-Lie2rOCgkh@OpenReview">2</sup>]</a>
                <a id="rel-Lie2rOCgkh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Lie2rOCgkh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Lie2rOCgkh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shanshan Luo" target="_blank">Shanshan Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu yixuan" target="_blank">Yu yixuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunchen LIU" target="_blank">Chunchen LIU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Xie" target="_blank">Feng Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=zhi geng" target="_blank">zhi geng</a>
            </p>
            <p id="summary-Lie2rOCgkh@OpenReview" class="summary">Previous studies have extensively addressed the attribution problem for binary outcome variables. However, in many practical scenarios, the outcome variable is continuous, and simply binarizing it may result in information loss or biased conclusions. To address this issue, we propose a series of posterior causal estimands for retrospectively evaluating multiple correlated causes from a continuous outcome. These estimands include posterior intervention effects, posterior total causal effects, and posterior natural direct effects. Under assumptions of sequential ignorability, monotonicity, and perfect positive rank, we show that the posterior causal estimands of interest are identifiable and present the corresponding identification equations. We also provide a simple but effective estimation procedure and establish asymptotic properties of the proposed estimators. An artificial hypertension example and a real developmental toxicity dataset are employed to illustrate our method.</p>
            <p id="subjects-Lie2rOCgkh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Lie2rOCgkh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lie2rOCgkh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lie2rOCgkh@OpenReview" onclick="foldPdfKimi('Lie2rOCgkh@OpenReview', this)" class="hr hr-fold">
        </div><div id="Lktwi30g63@OpenReview" class="panel paper" keywords="generalization,domain,clip,compositional,diversity,unseen,questions,generalize,training,unanswered">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Lktwi30g63" target="_blank" title="74/225"><span class="index notranslate">#74</span></a>
                <a id="title-Lktwi30g63@OpenReview" class="title-link" href="/venue/Lktwi30g63@OpenReview" target="_blank">When and How Does CLIP Enable Domain and Compositional Generalization?</a>
                <a id="pdf-Lktwi30g63@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Lktwi30g63@OpenReview', this)" data="https://openreview.net/pdf?id=Lktwi30g63">[PDF<sup id="pdf-stars-Lktwi30g63@OpenReview">5</sup>]</a>
                <a id="copy-Lktwi30g63@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Lktwi30g63@OpenReview')">[Copy]</a>
                <a id="kimi-Lktwi30g63@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Lktwi30g63@OpenReview', this)">[Kimi<sup id="kimi-stars-Lktwi30g63@OpenReview"></sup>]</a>
                <a id="rel-Lktwi30g63@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Lktwi30g63@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Lktwi30g63@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Elias Kempf" target="_blank">Elias Kempf</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Schrodi" target="_blank">Simon Schrodi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Argus" target="_blank">Max Argus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Brox" target="_blank">Thomas Brox</a>
            </p>
            <p id="summary-Lktwi30g63@OpenReview" class="summary">The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric *and* mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits.</p>
            <p id="subjects-Lktwi30g63@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Lktwi30g63@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lktwi30g63@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lktwi30g63@OpenReview" onclick="foldPdfKimi('Lktwi30g63@OpenReview', this)" class="hr hr-fold">
        </div><div id="GKqoqGCHTq@OpenReview" class="panel paper" keywords="consistency,thibautissenhuth,distillation,discrepancy,generator,training,transports,imitate,velocity,corresponding">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GKqoqGCHTq" target="_blank" title="75/225"><span class="index notranslate">#75</span></a>
                <a id="title-GKqoqGCHTq@OpenReview" class="title-link" href="/venue/GKqoqGCHTq@OpenReview" target="_blank">Improving Consistency Models with Generator-Augmented Flows</a>
                <a id="pdf-GKqoqGCHTq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GKqoqGCHTq@OpenReview', this)" data="https://openreview.net/pdf?id=GKqoqGCHTq">[PDF<sup id="pdf-stars-GKqoqGCHTq@OpenReview">7</sup>]</a>
                <a id="copy-GKqoqGCHTq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GKqoqGCHTq@OpenReview')">[Copy]</a>
                <a id="kimi-GKqoqGCHTq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GKqoqGCHTq@OpenReview', this)">[Kimi<sup id="kimi-stars-GKqoqGCHTq@OpenReview">1</sup>]</a>
                <a id="rel-GKqoqGCHTq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GKqoqGCHTq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GKqoqGCHTq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thibaut Issenhuth" target="_blank">Thibaut Issenhuth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangchul Lee" target="_blank">Sangchul Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ludovic Dos Santos" target="_blank">Ludovic Dos Santos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jean-Yves Franceschi" target="_blank">Jean-Yves Franceschi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chansoo Kim" target="_blank">Chansoo Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=alain rakotomamonjy" target="_blank">alain rakotomamonjy</a>
            </p>
            <p id="summary-GKqoqGCHTq@OpenReview" class="summary">Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network.They can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network.In contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field.The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit.To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from a consistency model.We prove that this flow reduces the previously identified discrepancy and the noise-data transport cost.Consequently, our method not only accelerates consistency training convergence but also enhances its overall performance. The code is available at https://github.com/thibautissenhuth/consistency_GC.</p>
            <p id="subjects-GKqoqGCHTq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GKqoqGCHTq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GKqoqGCHTq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GKqoqGCHTq@OpenReview" onclick="foldPdfKimi('GKqoqGCHTq@OpenReview', this)" class="hr hr-fold">
        </div><div id="M7mVzCV6uU@OpenReview" class="panel paper" keywords="fedgvi,federated,misspecification,robust,variational,probabilistic,inference,likelihood,ashman,robustness">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=M7mVzCV6uU" target="_blank" title="76/225"><span class="index notranslate">#76</span></a>
                <a id="title-M7mVzCV6uU@OpenReview" class="title-link" href="/venue/M7mVzCV6uU@OpenReview" target="_blank">Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework</a>
                <a id="pdf-M7mVzCV6uU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('M7mVzCV6uU@OpenReview', this)" data="https://openreview.net/pdf?id=M7mVzCV6uU">[PDF<sup id="pdf-stars-M7mVzCV6uU@OpenReview">4</sup>]</a>
                <a id="copy-M7mVzCV6uU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('M7mVzCV6uU@OpenReview')">[Copy]</a>
                <a id="kimi-M7mVzCV6uU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('M7mVzCV6uU@OpenReview', this)">[Kimi<sup id="kimi-stars-M7mVzCV6uU@OpenReview"></sup>]</a>
                <a id="rel-M7mVzCV6uU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('M7mVzCV6uU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-M7mVzCV6uU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Terje Mildner" target="_blank">Terje Mildner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Hamelijnck" target="_blank">Oliver Hamelijnck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paris Giampouras" target="_blank">Paris Giampouras</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Theodoros Damoulas" target="_blank">Theodoros Damoulas</a>
            </p>
            <p id="summary-M7mVzCV6uU@OpenReview" class="summary">We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.</p>
            <p id="subjects-M7mVzCV6uU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-M7mVzCV6uU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-M7mVzCV6uU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-M7mVzCV6uU@OpenReview" onclick="foldPdfKimi('M7mVzCV6uU@OpenReview', this)" class="hr hr-fold">
        </div><div id="MHaSq1LlTe@OpenReview" class="panel paper" keywords="clustering,signed,constrained,setminus,laplacians,subset,problem,minimises,streamlining,cheeger">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MHaSq1LlTe" target="_blank" title="77/225"><span class="index notranslate">#77</span></a>
                <a id="title-MHaSq1LlTe@OpenReview" class="title-link" href="/venue/MHaSq1LlTe@OpenReview" target="_blank">Signed Laplacians for Constrained Graph Clustering</a>
                <a id="pdf-MHaSq1LlTe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MHaSq1LlTe@OpenReview', this)" data="https://openreview.net/pdf?id=MHaSq1LlTe">[PDF<sup id="pdf-stars-MHaSq1LlTe@OpenReview">3</sup>]</a>
                <a id="copy-MHaSq1LlTe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MHaSq1LlTe@OpenReview')">[Copy]</a>
                <a id="kimi-MHaSq1LlTe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MHaSq1LlTe@OpenReview', this)">[Kimi<sup id="kimi-stars-MHaSq1LlTe@OpenReview">1</sup>]</a>
                <a id="rel-MHaSq1LlTe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MHaSq1LlTe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MHaSq1LlTe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=John Stewart Fabila-Carrasco" target="_blank">John Stewart Fabila-Carrasco</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Sun" target="_blank">He Sun</a>
            </p>
            <p id="summary-MHaSq1LlTe@OpenReview" class="summary">Given two weighted graphs <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-44-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-284" style="width: 7.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.617em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1006.51em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-285"><span class="mi" id="MathJax-Span-286" style="font-family: MathJax_Math-italic;">G</span><span class="mo" id="MathJax-Span-287" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mo" id="MathJax-Span-288" style="font-family: MathJax_Main; padding-left: 0.263em;">(</span><span class="mi" id="MathJax-Span-289" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span><span class="mo" id="MathJax-Span-290" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-291" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-292" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-293" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-294" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="mi" id="MathJax-Span-295" style="font-size: 70.7%; font-family: MathJax_Math-italic;">G</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-296" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi><mo>=</mo><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>E</mi><mo>,</mo><msub><mi>w</mi><mi>G</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-44">G = (V, E, w_G)</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-45-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-297" style="width: 8.232em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1006.72em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-298"><span class="mi" id="MathJax-Span-299" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-300" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mo" id="MathJax-Span-301" style="font-family: MathJax_Main; padding-left: 0.263em;">(</span><span class="mi" id="MathJax-Span-302" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span><span class="mo" id="MathJax-Span-303" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-304" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">F<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-305" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-306" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-307" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="mi" id="MathJax-Span-308" style="font-size: 70.7%; font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-309" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi><mo>=</mo><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>F</mi><mo>,</mo><msub><mi>w</mi><mi>H</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-45">H = (V, F, w_H)</script> defined on the same vertex set, the constrained clustering problem seeks to find a subset <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-46-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo&gt;&amp;#x2282;&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-310" style="width: 3.336em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.763em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.76em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-311"><span class="mi" id="MathJax-Span-312" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-313" style="font-family: MathJax_Main; padding-left: 0.263em;">⊂</span><span class="mi" id="MathJax-Span-314" style="font-family: MathJax_Math-italic; padding-left: 0.263em;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mo>⊂</mo><mi>V</mi></math></span></span><script type="math/tex" id="MathJax-Element-46">S \subset V</script> that minimises the cut ratio between <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-47-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo class=&quot;MJX-variant&quot;&gt;&amp;#x2216;&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-315" style="width: 6.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.52em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-316"><span class="msubsup" id="MathJax-Span-317"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-318" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="mi" id="MathJax-Span-319" style="font-size: 70.7%; font-family: MathJax_Math-italic;">G</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-320" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-321" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-322" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-323" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span><span class="mo" id="MathJax-Span-324" style="font-family: MathJax_Main; padding-left: 0.211em;">∖</span><span class="mi" id="MathJax-Span-325" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-326" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mi>G</mi></msub><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo class="MJX-variant">∖</mo><mi>S</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-47">w_G(S, V \setminus S)</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-48-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo class=&quot;MJX-variant&quot;&gt;&amp;#x2216;&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-327" style="width: 6.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.58em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-328"><span class="msubsup" id="MathJax-Span-329"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-330" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.732em;"><span class="mi" id="MathJax-Span-331" style="font-size: 70.7%; font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-332" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-333" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-334" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-335" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span><span class="mo" id="MathJax-Span-336" style="font-family: MathJax_Main; padding-left: 0.211em;">∖</span><span class="mi" id="MathJax-Span-337" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mo" id="MathJax-Span-338" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mi>H</mi></msub><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo class="MJX-variant">∖</mo><mi>S</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-48">w_H(S, V \setminus S)</script>. In this work, we establish a Cheeger-type inequality that relates the solution of the constrained clustering problem to the spectral properties of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-49-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-339" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-340"><span class="mi" id="MathJax-Span-341" style="font-family: MathJax_Math-italic;">G</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-49"> G</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-50-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-342" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-343"><span class="mi" id="MathJax-Span-344" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math></span></span><script type="math/tex" id="MathJax-Element-50">H</script>. To reduce computational complexity, we utilise the signed Laplacian of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-51-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-345" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-346"><span class="mi" id="MathJax-Span-347" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math></span></span><script type="math/tex" id="MathJax-Element-51">H</script>, streamlining calculations while maintaining accuracy. By solving a generalised eigenvalue problem, our proposed algorithm achieves notable performance improvements, particularly in challenging scenarios where traditional spectral clustering methods struggle. We demonstrate its practical effectiveness through experiments on both synthetic and real-world datasets.</p>
            <p id="subjects-MHaSq1LlTe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-MHaSq1LlTe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MHaSq1LlTe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MHaSq1LlTe@OpenReview" onclick="foldPdfKimi('MHaSq1LlTe@OpenReview', this)" class="hr hr-fold">
        </div><div id="MkCnPNOLMk@OpenReview" class="panel paper" keywords="constrained,clustering,clusters,approximation,correlation,zuylen,cluster,mandating,better,edges">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=MkCnPNOLMk" target="_blank" title="78/225"><span class="index notranslate">#78</span></a>
                <a id="title-MkCnPNOLMk@OpenReview" class="title-link" href="/venue/MkCnPNOLMk@OpenReview" target="_blank">Towards Better-than-2 Approximation for Constrained Correlation Clustering</a>
                <a id="pdf-MkCnPNOLMk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('MkCnPNOLMk@OpenReview', this)" data="https://openreview.net/pdf?id=MkCnPNOLMk">[PDF<sup id="pdf-stars-MkCnPNOLMk@OpenReview">3</sup>]</a>
                <a id="copy-MkCnPNOLMk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('MkCnPNOLMk@OpenReview')">[Copy]</a>
                <a id="kimi-MkCnPNOLMk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('MkCnPNOLMk@OpenReview', this)">[Kimi<sup id="kimi-stars-MkCnPNOLMk@OpenReview"></sup>]</a>
                <a id="rel-MkCnPNOLMk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('MkCnPNOLMk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-MkCnPNOLMk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Kalavas" target="_blank">Andreas Kalavas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Evangelos Kipouridis" target="_blank">Evangelos Kipouridis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nithin Varma" target="_blank">Nithin Varma</a>
            </p>
            <p id="summary-MkCnPNOLMk@OpenReview" class="summary">In the Correlation Clustering problem, we are given an undirected graph and are tasked with computing a clustering (partition of the nodes) that minimizes the sum of the number of edges across different clusters and the number of non-edges within clusters. In the constrained version of this problem, the goal is to compute a clustering that satisfies additional hard constraints mandating certain pairs to be in the same cluster and certain pairs to be in different clusters. Constrained Correlation Clustering is APX-Hard, and the best known approximation factor is 3 (van Zuylen et al. [SODA '07]). In this work, we show that in order to obtain a better-than-2 approximation, solving the (exponentially large) Constrained Cluster LP would be sufficient.[The peer-reviewed version of this article claimed an efficient algorithm for solving the Constrained Cluster LP. An error in the proof, that the authors discovered after the review process, led them to revise the results to be conditional on the existence of a valid LP solution.]</p>
            <p id="subjects-MkCnPNOLMk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-MkCnPNOLMk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-MkCnPNOLMk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-MkCnPNOLMk@OpenReview" onclick="foldPdfKimi('MkCnPNOLMk@OpenReview', this)" class="hr hr-fold">
        </div><div id="O0oe7hPtbl@OpenReview" class="panel paper" keywords="gridded,tnps,spatio,processes,temporal,data,transformer,neural,pseudo,scale">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=O0oe7hPtbl" target="_blank" title="79/225"><span class="index notranslate">#79</span></a>
                <a id="title-O0oe7hPtbl@OpenReview" class="title-link" href="/venue/O0oe7hPtbl@OpenReview" target="_blank">Gridded Transformer Neural Processes for Spatio-Temporal Data</a>
                <a id="pdf-O0oe7hPtbl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('O0oe7hPtbl@OpenReview', this)" data="https://openreview.net/pdf?id=O0oe7hPtbl">[PDF<sup id="pdf-stars-O0oe7hPtbl@OpenReview">4</sup>]</a>
                <a id="copy-O0oe7hPtbl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('O0oe7hPtbl@OpenReview')">[Copy]</a>
                <a id="kimi-O0oe7hPtbl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('O0oe7hPtbl@OpenReview', this)">[Kimi<sup id="kimi-stars-O0oe7hPtbl@OpenReview">2</sup>]</a>
                <a id="rel-O0oe7hPtbl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('O0oe7hPtbl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-O0oe7hPtbl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Ashman" target="_blank">Matthew Ashman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cristiana Diaconu" target="_blank">Cristiana Diaconu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Langezaal" target="_blank">Eric Langezaal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adrian Weller" target="_blank">Adrian Weller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard E Turner" target="_blank">Richard E Turner</a>
            </p>
            <p id="summary-O0oe7hPtbl@OpenReview" class="summary">Effective modelling of large-scale spatio-temporal datasets is essential for many domains, yet existing approaches often impose rigid constraints on the input data, such as requiring them to lie on fixed-resolution grids. With the rise of foundation models, the ability to process diverse, heterogeneous data structures is becoming increasingly important. Neural processes (NPs), particularly transformer neural processes (TNPs), offer a promising framework for such tasks, but struggle to scale to large spatio-temporal datasets due to the lack of an efficient attention mechanism. To address this, we introduce gridded pseudo-token TNPs which employ specialised encoders and decoders to handle unstructured data and utilise a processor comprising gridded pseudo-tokens with efficient attention mechanisms. Furthermore, we develop equivariant gridded TNPs for applications where exact or approximate translation equivariance is a useful inductive bias, improving accuracy and training efficiency. Our method consistently outperforms a range of strong baselines in various synthetic and real-world regression tasks involving large-scale data, while maintaining competitive computational efficiency. Experiments with weather data highlight the potential of gridded TNPs and serve as just one example of a domain where they can have a significant impact.</p>
            <p id="subjects-O0oe7hPtbl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-O0oe7hPtbl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-O0oe7hPtbl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-O0oe7hPtbl@OpenReview" onclick="foldPdfKimi('O0oe7hPtbl@OpenReview', this)" class="hr hr-fold">
        </div><div id="O6q2BHK1BL@OpenReview" class="panel paper" keywords="causal,identifying,relationships,variables,latent,pag,local,observational,ancestral,algorithms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=O6q2BHK1BL" target="_blank" title="80/225"><span class="index notranslate">#80</span></a>
                <a id="title-O6q2BHK1BL@OpenReview" class="title-link" href="/venue/O6q2BHK1BL@OpenReview" target="_blank">Local Identifying Causal Relations in the Presence of Latent Variables</a>
                <a id="pdf-O6q2BHK1BL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('O6q2BHK1BL@OpenReview', this)" data="https://openreview.net/pdf?id=O6q2BHK1BL">[PDF<sup id="pdf-stars-O6q2BHK1BL@OpenReview">3</sup>]</a>
                <a id="copy-O6q2BHK1BL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('O6q2BHK1BL@OpenReview')">[Copy]</a>
                <a id="kimi-O6q2BHK1BL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('O6q2BHK1BL@OpenReview', this)">[Kimi<sup id="kimi-stars-O6q2BHK1BL@OpenReview"></sup>]</a>
                <a id="rel-O6q2BHK1BL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('O6q2BHK1BL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-O6q2BHK1BL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Li" target="_blank">Zheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Liu" target="_blank">Zeyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Xie" target="_blank">Feng Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhang" target="_blank">Hao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunchen LIU" target="_blank">Chunchen LIU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=zhi geng" target="_blank">zhi geng</a>
            </p>
            <p id="summary-O6q2BHK1BL@OpenReview" class="summary">We tackle the problem of identifying whether a variable is the cause of a specified target using observational data. State-of-the-art causal learning algorithms that handle latent variables typically rely on identifying the global causal structure, often represented as a partial ancestral graph (PAG), to infer causal relationships. Although effective, these approaches are often redundant and computationally expensive when the focus is limited to a specific causal relationship. In this work, we introduce novel local characterizations that are necessary and sufficient for various types of causal relationships between two variables, enabling us to bypass the need for global structure learning. Leveraging these local insights, we develop efficient and fully localized algorithms that accurately identify causal relationships from observational data. We theoretically demonstrate the soundness and completeness of our approach. Extensive experiments on benchmark networks and real-world datasets further validate the effectiveness and efficiency of our method.</p>
            <p id="subjects-O6q2BHK1BL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-O6q2BHK1BL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-O6q2BHK1BL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-O6q2BHK1BL@OpenReview" onclick="foldPdfKimi('O6q2BHK1BL@OpenReview', this)" class="hr hr-fold">
        </div><div id="OJ6WE7F8tK@OpenReview" class="panel paper" keywords="ddo,likelihood,discriminator,direct,imagenet,generative,mle,512,256,discriminative">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OJ6WE7F8tK" target="_blank" title="81/225"><span class="index notranslate">#81</span></a>
                <a id="title-OJ6WE7F8tK@OpenReview" class="title-link" href="/venue/OJ6WE7F8tK@OpenReview" target="_blank">Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator</a>
                <a id="pdf-OJ6WE7F8tK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OJ6WE7F8tK@OpenReview', this)" data="https://openreview.net/pdf?id=OJ6WE7F8tK">[PDF<sup id="pdf-stars-OJ6WE7F8tK@OpenReview">4</sup>]</a>
                <a id="copy-OJ6WE7F8tK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OJ6WE7F8tK@OpenReview')">[Copy]</a>
                <a id="kimi-OJ6WE7F8tK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OJ6WE7F8tK@OpenReview', this)">[Kimi<sup id="kimi-stars-OJ6WE7F8tK@OpenReview">2</sup>]</a>
                <a id="rel-OJ6WE7F8tK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OJ6WE7F8tK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OJ6WE7F8tK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiwen Zheng" target="_blank">Kaiwen Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxin Chen" target="_blank">Yongxin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huayu Chen" target="_blank">Huayu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guande He" target="_blank">Guande He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Yu Liu" target="_blank">Ming-Yu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Zhu" target="_blank">Jun Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qinsheng Zhang" target="_blank">Qinsheng Zhang</a>
            </p>
            <p id="summary-OJ6WE7F8tK@OpenReview" class="summary">While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1\% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-52-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-348" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-349"><span class="mo" id="MathJax-Span-350" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-52">\times</script>512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-53-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-351" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-352"><span class="mo" id="MathJax-Span-353" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-53">\times</script>256.</p>
            <p id="subjects-OJ6WE7F8tK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-OJ6WE7F8tK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OJ6WE7F8tK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OJ6WE7F8tK@OpenReview" onclick="foldPdfKimi('OJ6WE7F8tK@OpenReview', this)" class="hr hr-fold">
        </div><div id="Obet2x6GNl@OpenReview" class="panel paper" keywords="calibrated,advice,ski,rental,algorithms,predictions,machine,job,scheduling,online">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Obet2x6GNl" target="_blank" title="82/225"><span class="index notranslate">#82</span></a>
                <a id="title-Obet2x6GNl@OpenReview" class="title-link" href="/venue/Obet2x6GNl@OpenReview" target="_blank">Algorithms with Calibrated Machine Learning Predictions</a>
                <a id="pdf-Obet2x6GNl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Obet2x6GNl@OpenReview', this)" data="https://openreview.net/pdf?id=Obet2x6GNl">[PDF<sup id="pdf-stars-Obet2x6GNl@OpenReview">2</sup>]</a>
                <a id="copy-Obet2x6GNl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Obet2x6GNl@OpenReview')">[Copy]</a>
                <a id="kimi-Obet2x6GNl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Obet2x6GNl@OpenReview', this)">[Kimi<sup id="kimi-stars-Obet2x6GNl@OpenReview"></sup>]</a>
                <a id="rel-Obet2x6GNl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Obet2x6GNl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Obet2x6GNl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Judy Hanwen Shen" target="_blank">Judy Hanwen Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ellen Vitercik" target="_blank">Ellen Vitercik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anders Wikum" target="_blank">Anders Wikum</a>
            </p>
            <p id="summary-Obet2x6GNl@OpenReview" class="summary">The field of *algorithms with predictions* incorporates machine learning advice in the design of online algorithms to improve real-world performance. A central consideration is the extent to which predictions can be trusted—while existing approaches often require users to specify an aggregate trust level, modern machine learning models can provide estimates of prediction-level uncertainty. In this paper, we propose *calibration* as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the *ski rental* and *online job scheduling* problems. For ski rental, we design an algorithm that achieves near-optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions.</p>
            <p id="subjects-Obet2x6GNl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Obet2x6GNl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Obet2x6GNl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Obet2x6GNl@OpenReview" onclick="foldPdfKimi('Obet2x6GNl@OpenReview', this)" class="hr hr-fold">
        </div><div id="OpineZj5bj@OpenReview" class="panel paper" keywords="continuity,domain,vit,tokens,cdfsl,gaps,smaller,image,shot,domains">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OpineZj5bj" target="_blank" title="83/225"><span class="index notranslate">#83</span></a>
                <a id="title-OpineZj5bj@OpenReview" class="title-link" href="/venue/OpineZj5bj@OpenReview" target="_blank">Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning</a>
                <a id="pdf-OpineZj5bj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OpineZj5bj@OpenReview', this)" data="https://openreview.net/pdf?id=OpineZj5bj">[PDF<sup id="pdf-stars-OpineZj5bj@OpenReview">2</sup>]</a>
                <a id="copy-OpineZj5bj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OpineZj5bj@OpenReview')">[Copy]</a>
                <a id="kimi-OpineZj5bj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OpineZj5bj@OpenReview', this)">[Kimi<sup id="kimi-stars-OpineZj5bj@OpenReview">1</sup>]</a>
                <a id="rel-OpineZj5bj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OpineZj5bj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OpineZj5bj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Yi" target="_blank">Shuai Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixiong Zou" target="_blank">Yixiong Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhua Li" target="_blank">Yuhua Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruixuan Li" target="_blank">Ruixuan Li</a>
            </p>
            <p id="summary-OpineZj5bj@OpenReview" class="summary">Vision Transformer (ViT) has achieved remarkable success due to its large-scale pretraining on general domains, but it still faces challenges when applying it to downstream distant domains that have only scarce training data, which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired by Self-Attention's insensitivity to token orders, we find an interesting phenomenon neglected in current works: disrupting the continuity of image tokens (i.e., making pixels not smoothly transited across patches) in ViT leads to a noticeable performance decline in the general (source) domain but only a marginal decrease in downstream target domains. This questions the role of image tokens' continuity in ViT's generalization under large domain gaps. In this paper, we delve into this phenomenon for an interpretation. We find continuity aids ViT in learning larger spatial patterns, which are harder to transfer than smaller ones, enlarging domain distances. Meanwhile, it implies that only smaller patterns within each patch could be transferred under extreme domain gaps. Based on this interpretation, we further propose a simple yet effective method for CDFSL that better disrupts the continuity of image tokens, encouraging the model to rely less on large patterns and more on smaller ones. Extensive experiments show the effectiveness of our method in reducing domain gaps and outperforming state-of-the-art works. Codes and models are available at https://github.com/shuaiyi308/ReCIT.</p>
            <p id="subjects-OpineZj5bj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-OpineZj5bj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OpineZj5bj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OpineZj5bj@OpenReview" onclick="foldPdfKimi('OpineZj5bj@OpenReview', this)" class="hr hr-fold">
        </div><div id="PUzNwYmb3l@OpenReview" class="panel paper" keywords="objective,preference,problem,pareto,multi,bilevel,semivectorial,optimization,merit,reformulation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PUzNwYmb3l" target="_blank" title="84/225"><span class="index notranslate">#84</span></a>
                <a id="title-PUzNwYmb3l@OpenReview" class="title-link" href="/venue/PUzNwYmb3l@OpenReview" target="_blank">Efficient First-Order Optimization on the Pareto Set for Multi-Objective Learning under Preference Guidance</a>
                <a id="pdf-PUzNwYmb3l@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PUzNwYmb3l@OpenReview', this)" data="https://openreview.net/pdf?id=PUzNwYmb3l">[PDF<sup id="pdf-stars-PUzNwYmb3l@OpenReview">3</sup>]</a>
                <a id="copy-PUzNwYmb3l@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PUzNwYmb3l@OpenReview')">[Copy]</a>
                <a id="kimi-PUzNwYmb3l@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PUzNwYmb3l@OpenReview', this)">[Kimi<sup id="kimi-stars-PUzNwYmb3l@OpenReview"></sup>]</a>
                <a id="rel-PUzNwYmb3l@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PUzNwYmb3l@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PUzNwYmb3l@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lisha Chen" target="_blank">Lisha Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Xiao" target="_blank">Quan Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ellen Fukuda" target="_blank">Ellen Fukuda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyi Chen" target="_blank">Xinyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Yuan" target="_blank">Kun Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Chen" target="_blank">Tianyi Chen</a>
            </p>
            <p id="summary-PUzNwYmb3l@OpenReview" class="summary">Multi-objective learning under user-specified preference is common in real-world problems such as multi-lingual speech recognition under fairness. In this work, we frame such a problem as a semivectorial bilevel optimization problem, whose goal is to optimize a pre-defined preference function, subject to the constraint that the model parameters are weakly Pareto optimal. To solve this problem, we convert the multi-objective constraints to a single-objective constraint through a merit function with an easy-to-evaluate gradient, and then, we use a penalty-based reformulation of the bilevel optimization problem. We theoretically establish the properties of the merit function, and the relations of solutions for the penalty reformulation and the constrained formulation. Then we propose algorithms to solve the reformulated single-level problem, and establish its convergence guarantees. We test the method on various synthetic and real-world problems. The results demonstrate the effectiveness of the proposed method in finding preference-guided optimal solutions to the multi-objective problem.</p>
            <p id="subjects-PUzNwYmb3l@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PUzNwYmb3l@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PUzNwYmb3l@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PUzNwYmb3l@OpenReview" onclick="foldPdfKimi('PUzNwYmb3l@OpenReview', this)" class="hr hr-fold">
        </div><div id="Ppcf30NGL0@OpenReview" class="panel paper" keywords="variational,sparse,bound,gaussian,tractable,posterior,processes,assumption,extra,distribution">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Ppcf30NGL0" target="_blank" title="85/225"><span class="index notranslate">#85</span></a>
                <a id="title-Ppcf30NGL0@OpenReview" class="title-link" href="/venue/Ppcf30NGL0@OpenReview" target="_blank">New Bounds for Sparse Variational Gaussian Processes</a>
                <a id="pdf-Ppcf30NGL0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Ppcf30NGL0@OpenReview', this)" data="https://openreview.net/pdf?id=Ppcf30NGL0">[PDF<sup id="pdf-stars-Ppcf30NGL0@OpenReview">3</sup>]</a>
                <a id="copy-Ppcf30NGL0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Ppcf30NGL0@OpenReview')">[Copy]</a>
                <a id="kimi-Ppcf30NGL0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Ppcf30NGL0@OpenReview', this)">[Kimi<sup id="kimi-stars-Ppcf30NGL0@OpenReview"></sup>]</a>
                <a id="rel-Ppcf30NGL0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Ppcf30NGL0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Ppcf30NGL0@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michalis Titsias" target="_blank">Michalis Titsias</a>
            </p>
            <p id="summary-Ppcf30NGL0@OpenReview" class="summary">Sparse variational Gaussian processes (GPs) construct tractable posterior approximations to GP models. At the core of these methods is the assumption that the true posterior distribution over training function values <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-54-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-354" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-355"><span class="texatom" id="MathJax-Span-356"><span class="mrow" id="MathJax-Span-357"><span class="mi" id="MathJax-Span-358" style="font-family: MathJax_Main-bold;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">f</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-54">{\bf f}</script> and inducing variables <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-55-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;u&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-359" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-360"><span class="texatom" id="MathJax-Span-361"><span class="mrow" id="MathJax-Span-362"><span class="mi" id="MathJax-Span-363" style="font-family: MathJax_Main-bold;">u</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">u</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-55">{\bf u}</script> is approximated by a variational distribution that incorporates the conditional GP prior <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-56-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;u&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-364" style="width: 3.232em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.55em, 2.607em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-365"><span class="mi" id="MathJax-Span-366" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-367" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-368"><span class="mrow" id="MathJax-Span-369"><span class="mi" id="MathJax-Span-370" style="font-family: MathJax_Main-bold;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span></span><span class="texatom" id="MathJax-Span-371"><span class="mrow" id="MathJax-Span-372"><span class="mo" id="MathJax-Span-373" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-374"><span class="mrow" id="MathJax-Span-375"><span class="mi" id="MathJax-Span-376" style="font-family: MathJax_Main-bold;">u</span></span></span><span class="mo" id="MathJax-Span-377" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">f</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">u</mi></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-56">p({\bf f} | {\bf u})</script> in its factorization. While this assumption is considered as fundamental, we show that for model training we can relax it through the use of a more general variational distribution <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-57-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;u&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-378" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.5em, 2.607em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-379"><span class="mi" id="MathJax-Span-380" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-381" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-382"><span class="mrow" id="MathJax-Span-383"><span class="mi" id="MathJax-Span-384" style="font-family: MathJax_Main-bold;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span></span><span class="texatom" id="MathJax-Span-385"><span class="mrow" id="MathJax-Span-386"><span class="mo" id="MathJax-Span-387" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-388"><span class="mrow" id="MathJax-Span-389"><span class="mi" id="MathJax-Span-390" style="font-family: MathJax_Main-bold;">u</span></span></span><span class="mo" id="MathJax-Span-391" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>q</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">f</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">u</mi></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-57">q({\bf f} | {\bf u} )</script> that depends on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-58-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-392" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-393"><span class="mi" id="MathJax-Span-394" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-58">N</script> extra parameters, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-59-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-395" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-396"><span class="mi" id="MathJax-Span-397" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-59">N</script> is the number of training examples. In GP regression, we can analytically optimize the evidence lower bound over the extra parameters and express a tractable collapsed bound that is tighter than the previous bound. The new bound is also amenable to stochastic optimization and its implementation requires minor modifications to existing sparse GP code. Further, we also describe extensions to non-Gaussian likelihoods. On several datasets we demonstrate that our method can reduce bias when learning the hyperparameters and can lead to better predictive performance.</p>
            <p id="subjects-Ppcf30NGL0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Ppcf30NGL0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ppcf30NGL0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ppcf30NGL0@OpenReview" onclick="foldPdfKimi('Ppcf30NGL0@OpenReview', this)" class="hr hr-fold">
        </div><div id="PzSG5nKe1q@OpenReview" class="panel paper" keywords="llms,feedback,rlef,execution,code,reinforcement,grounding,steps,leverage,70b">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PzSG5nKe1q" target="_blank" title="86/225"><span class="index notranslate">#86</span></a>
                <a id="title-PzSG5nKe1q@OpenReview" class="title-link" href="/venue/PzSG5nKe1q@OpenReview" target="_blank">RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning</a>
                <a id="pdf-PzSG5nKe1q@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PzSG5nKe1q@OpenReview', this)" data="https://openreview.net/pdf?id=PzSG5nKe1q">[PDF<sup id="pdf-stars-PzSG5nKe1q@OpenReview">9</sup>]</a>
                <a id="copy-PzSG5nKe1q@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PzSG5nKe1q@OpenReview')">[Copy]</a>
                <a id="kimi-PzSG5nKe1q@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PzSG5nKe1q@OpenReview', this)">[Kimi<sup id="kimi-stars-PzSG5nKe1q@OpenReview">8</sup>]</a>
                <a id="rel-PzSG5nKe1q@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PzSG5nKe1q@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PzSG5nKe1q@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Gehring" target="_blank">Jonas Gehring</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kunhao Zheng" target="_blank">Kunhao Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jade Copet" target="_blank">Jade Copet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vegard Mella" target="_blank">Vegard Mella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taco Cohen" target="_blank">Taco Cohen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Synnaeve" target="_blank">Gabriel Synnaeve</a>
            </p>
            <p id="summary-PzSG5nKe1q@OpenReview" class="summary">Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve the desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks and achieve large performance gains with both small (8B parameters) and large (70B) models, outperforming previous work while reducing the number of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.</p>
            <p id="subjects-PzSG5nKe1q@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-PzSG5nKe1q@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PzSG5nKe1q@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PzSG5nKe1q@OpenReview" onclick="foldPdfKimi('PzSG5nKe1q@OpenReview', this)" class="hr hr-fold">
        </div><div id="Q3rGQUGgWo@OpenReview" class="panel paper" keywords="synevo,spatiotemporal,rodger,evolutional,cross,domain,lau,container,learners,neuroai">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Q3rGQUGgWo" target="_blank" title="87/225"><span class="index notranslate">#87</span></a>
                <a id="title-Q3rGQUGgWo@OpenReview" class="title-link" href="/venue/Q3rGQUGgWo@OpenReview" target="_blank">SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</a>
                <a id="pdf-Q3rGQUGgWo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Q3rGQUGgWo@OpenReview', this)" data="https://openreview.net/pdf?id=Q3rGQUGgWo">[PDF<sup id="pdf-stars-Q3rGQUGgWo@OpenReview">6</sup>]</a>
                <a id="copy-Q3rGQUGgWo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Q3rGQUGgWo@OpenReview')">[Copy]</a>
                <a id="kimi-Q3rGQUGgWo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Q3rGQUGgWo@OpenReview', this)">[Kimi<sup id="kimi-stars-Q3rGQUGgWo@OpenReview">2</sup>]</a>
                <a id="rel-Q3rGQUGgWo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Q3rGQUGgWo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Q3rGQUGgWo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=jiayue Liu" target="_blank">jiayue Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongchao Yi" target="_blank">Zhongchao Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyang Zhou" target="_blank">Zhengyang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qihe Huang" target="_blank">Qihe Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuo Yang" target="_blank">Kuo Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Wang" target="_blank">Xu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Wang" target="_blank">Yang Wang</a>
            </p>
            <p id="summary-Q3rGQUGgWo@OpenReview" class="summary">Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42\% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.Code available at [https://github.com/Rodger-Lau/SynEVO](https://github.com/Rodger-Lau/SynEVO).</p>
            <p id="subjects-Q3rGQUGgWo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Q3rGQUGgWo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Q3rGQUGgWo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Q3rGQUGgWo@OpenReview" onclick="foldPdfKimi('Q3rGQUGgWo@OpenReview', this)" class="hr hr-fold">
        </div><div id="QC4dfobOLQ@OpenReview" class="panel paper" keywords="drrho,clip,steering,reference,model,insights,dro,generalization,improves,optimization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QC4dfobOLQ" target="_blank" title="88/225"><span class="index notranslate">#88</span></a>
                <a id="title-QC4dfobOLQ@OpenReview" class="title-link" href="/venue/QC4dfobOLQ@OpenReview" target="_blank">Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</a>
                <a id="pdf-QC4dfobOLQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QC4dfobOLQ@OpenReview', this)" data="https://openreview.net/pdf?id=QC4dfobOLQ">[PDF<sup id="pdf-stars-QC4dfobOLQ@OpenReview">6</sup>]</a>
                <a id="copy-QC4dfobOLQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QC4dfobOLQ@OpenReview')">[Copy]</a>
                <a id="kimi-QC4dfobOLQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QC4dfobOLQ@OpenReview', this)">[Kimi<sup id="kimi-stars-QC4dfobOLQ@OpenReview">3</sup>]</a>
                <a id="rel-QC4dfobOLQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QC4dfobOLQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QC4dfobOLQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiyuan Wei" target="_blank">Xiyuan Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Lin" target="_blank">Ming Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fanjiang Ye" target="_blank">Fanjiang Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengguang Song" target="_blank">Fengguang Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangliang Cao" target="_blank">Liangliang Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=My T. Thai" target="_blank">My T. Thai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianbao Yang" target="_blank">Tianbao Yang</a>
            </p>
            <p id="summary-QC4dfobOLQ@OpenReview" class="summary">This paper formalizes an emerging learning paradigm that uses a trained model as a reference to guide and enhance the training of a target model through strategic data selection or weighting, named **model steering**. While ad-hoc methods have been used in various contexts, including the training of large foundation models, its underlying principles remain insufficiently understood, leading to sub-optimal performance. In this work, we propose a theory-driven framework for model steering called **DRRho risk minimization**, which is rooted in Distributionally Robust Optimization (DRO). Through a generalization analysis, we provide theoretical insights into why this approach improves generalization and data efficiency compared to training without a reference model. To the best of our knowledge, this is the first time such theoretical insights are provided for the new learning paradigm, which significantly enhance our understanding and practice of model steering. Building on these insights and the connection between contrastive learning and DRO, we introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments validate the theoretical insights, reveal a superior scaling law compared to CLIP without a reference model, and demonstrate its strength over existing heuristic approaches. Code is released at [github.com/Optimization-AI/DRRho-CLIP](https://github.com/Optimization-AI/DRRho-CLIP)</p>
            <p id="subjects-QC4dfobOLQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-QC4dfobOLQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QC4dfobOLQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QC4dfobOLQ@OpenReview" onclick="foldPdfKimi('QC4dfobOLQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="S2K5MyRjrL@OpenReview" class="panel paper" keywords="certified,lipschitz,bro,robustness,reflector,bronet,orthogonal,annealing,loss,block">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=S2K5MyRjrL" target="_blank" title="89/225"><span class="index notranslate">#89</span></a>
                <a id="title-S2K5MyRjrL@OpenReview" class="title-link" href="/venue/S2K5MyRjrL@OpenReview" target="_blank">Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss</a>
                <a id="pdf-S2K5MyRjrL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('S2K5MyRjrL@OpenReview', this)" data="https://openreview.net/pdf?id=S2K5MyRjrL">[PDF<sup id="pdf-stars-S2K5MyRjrL@OpenReview">3</sup>]</a>
                <a id="copy-S2K5MyRjrL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('S2K5MyRjrL@OpenReview')">[Copy]</a>
                <a id="kimi-S2K5MyRjrL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('S2K5MyRjrL@OpenReview', this)">[Kimi<sup id="kimi-stars-S2K5MyRjrL@OpenReview">2</sup>]</a>
                <a id="rel-S2K5MyRjrL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('S2K5MyRjrL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-S2K5MyRjrL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bo-Han Lai" target="_blank">Bo-Han Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pin-Han Huang" target="_blank">Pin-Han Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo-Han Kung" target="_blank">Bo-Han Kung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shang-Tse Chen" target="_blank">Shang-Tse Chen</a>
            </p>
            <p id="summary-S2K5MyRjrL@OpenReview" class="summary">Lipschitz neural networks are well-known for providing certified robustness in deep learning. In this paper, we present a novel, efficient Block Reflector Orthogonal (BRO) layer that enhances the capability of orthogonal layers on constructing more expressive Lipschitz neural architectures. In addition, by theoretically analyzing the nature of Lipschitz neural networks, we introduce a new loss function that employs an annealing mechanism to increase margin for most data points. This enables Lipschitz models to provide better certified robustness. By employing our BRO layer and loss function, we design BRONet — a simple yet effective Lipschitz neural network that achieves state-of-the-art certified robustness. Extensive experiments and empirical analysis on CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms existing baselines. The implementation is available at [GitHub Link](https://github.com/ntuaislab/BRONet).</p>
            <p id="subjects-S2K5MyRjrL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-S2K5MyRjrL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-S2K5MyRjrL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-S2K5MyRjrL@OpenReview" onclick="foldPdfKimi('S2K5MyRjrL@OpenReview', this)" class="hr hr-fold">
        </div><div id="SnZ7SKykHh@OpenReview" class="panel paper" keywords="pokéchamp,pokémon,minimax,llm,bot,agent,elo,powered,player,battle">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SnZ7SKykHh" target="_blank" title="90/225"><span class="index notranslate">#90</span></a>
                <a id="title-SnZ7SKykHh@OpenReview" class="title-link" href="/venue/SnZ7SKykHh@OpenReview" target="_blank">PokéChamp: an Expert-level Minimax Language Agent</a>
                <a id="pdf-SnZ7SKykHh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SnZ7SKykHh@OpenReview', this)" data="https://openreview.net/pdf?id=SnZ7SKykHh">[PDF<sup id="pdf-stars-SnZ7SKykHh@OpenReview">3</sup>]</a>
                <a id="copy-SnZ7SKykHh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SnZ7SKykHh@OpenReview')">[Copy]</a>
                <a id="kimi-SnZ7SKykHh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SnZ7SKykHh@OpenReview', this)">[Kimi<sup id="kimi-stars-SnZ7SKykHh@OpenReview">4</sup>]</a>
                <a id="rel-SnZ7SKykHh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SnZ7SKykHh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SnZ7SKykHh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seth Karten" target="_blank">Seth Karten</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andy Nguyen" target="_blank">Andy Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chi Jin" target="_blank">Chi Jin</a>
            </p>
            <p id="summary-SnZ7SKykHh@OpenReview" class="summary">We introduce PokéChamp, a minimax agent powered by Large Language Models (LLMs) for Pokémon battles. Built on a general framework for two-player competitive games, PokéChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate PokéChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76\% against the best existing LLM-based bot and 84\% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, PokéChamp consistently outperforms the previous best LLM-based bot, Pokéllmon powered by GPT-4o, with a 64\% win rate. PokéChamp attains a projected Elo of 1300-1500 on the Pokémon Showdown online ladder, placing it among the top 30\%-10\% of human players. In addition, this work compiles the largest real-player Pokémon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. This work establishes Pokémon as a benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multi-agent problems. Videos, code, and dataset are available online.</p>
            <p id="subjects-SnZ7SKykHh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-SnZ7SKykHh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SnZ7SKykHh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SnZ7SKykHh@OpenReview" onclick="foldPdfKimi('SnZ7SKykHh@OpenReview', this)" class="hr hr-fold">
        </div><div id="T5IZ32ImAB@OpenReview" class="panel paper" keywords="coordination,agent,mcgd,multi,diffusion,robustness,environments,graph,attributes,policy">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=T5IZ32ImAB" target="_blank" title="91/225"><span class="index notranslate">#91</span></a>
                <a id="title-T5IZ32ImAB@OpenReview" class="title-link" href="/venue/T5IZ32ImAB@OpenReview" target="_blank">Graph Diffusion for Robust Multi-Agent Coordination</a>
                <a id="pdf-T5IZ32ImAB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('T5IZ32ImAB@OpenReview', this)" data="https://openreview.net/pdf?id=T5IZ32ImAB">[PDF<sup id="pdf-stars-T5IZ32ImAB@OpenReview">11</sup>]</a>
                <a id="copy-T5IZ32ImAB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('T5IZ32ImAB@OpenReview')">[Copy]</a>
                <a id="kimi-T5IZ32ImAB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('T5IZ32ImAB@OpenReview', this)">[Kimi<sup id="kimi-stars-T5IZ32ImAB@OpenReview">6</sup>]</a>
                <a id="rel-T5IZ32ImAB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('T5IZ32ImAB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-T5IZ32ImAB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xianghua Zeng" target="_blank">Xianghua Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Su" target="_blank">Hang Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyi Wang" target="_blank">Zhengyi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan LIN" target="_blank">Zhiyuan LIN</a>
            </p>
            <p id="summary-T5IZ32ImAB@OpenReview" class="summary">Offline multi-agent reinforcement learning (MARL) struggles to estimate out-of-distribution states and actions due to the absence of real-time environmental feedback. While diffusion models show promise in addressing these challenges, their application primarily focuses on independently diffusing the historical trajectories of individual agents, neglecting crucial multi-agent coordination dynamics and reducing policy robustness in dynamic environments. In this paper, we propose MCGD, a novel Multi-agent Coordination framework based on Graph Diffusion models to improve the effectiveness and robustness of collaborative policies. Specifically, we begin by constructing a sparse coordination graph that includes continuous node attributes and discrete edge attributes to effectively identify the underlying dynamics of multi-agent interactions. Next, we derive transition probabilities between edge categories and present adaptive categorical diffusion to capture the structure diversity of multi-agent coordination. Leveraging this coordination structure, we define neighbor-dependent forward noise and develop anisotropic diffusion to enhance the action diversity of each agent. Extensive experiments across various multi-agent environments demonstrate that MCGD significantly outperforms existing state-of-the-art baselines in coordination performance and policy robustness in dynamic environments.</p>
            <p id="subjects-T5IZ32ImAB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-T5IZ32ImAB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-T5IZ32ImAB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-T5IZ32ImAB@OpenReview" onclick="foldPdfKimi('T5IZ32ImAB@OpenReview', this)" class="hr hr-fold">
        </div><div id="TmJvacopmV@OpenReview" class="panel paper" keywords="bansal,larsen,widetilde,nnz,discrepancy,coloring,mathrm,algorithm,soda,sparsity">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TmJvacopmV" target="_blank" title="92/225"><span class="index notranslate">#92</span></a>
                <a id="title-TmJvacopmV@OpenReview" class="title-link" href="/venue/TmJvacopmV@OpenReview" target="_blank">Discrepancy Minimization in Input-Sparsity Time</a>
                <a id="pdf-TmJvacopmV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TmJvacopmV@OpenReview', this)" data="https://openreview.net/pdf?id=TmJvacopmV">[PDF<sup id="pdf-stars-TmJvacopmV@OpenReview">2</sup>]</a>
                <a id="copy-TmJvacopmV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TmJvacopmV@OpenReview')">[Copy]</a>
                <a id="kimi-TmJvacopmV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TmJvacopmV@OpenReview', this)">[Kimi<sup id="kimi-stars-TmJvacopmV@OpenReview">1</sup>]</a>
                <a id="rel-TmJvacopmV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TmJvacopmV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TmJvacopmV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yichuan Deng" target="_blank">Yichuan Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Li" target="_blank">Xiaoyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhao Song" target="_blank">Zhao Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=OMRI WEINSTEIN" target="_blank">OMRI WEINSTEIN</a>
            </p>
            <p id="summary-TmJvacopmV@OpenReview" class="summary">A recent work by [Larsen, SODA 2023] introduced a faster combinatorial alternative to Bansal's SDP algorithm for finding a coloring <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-60-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;{&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;}&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-398" style="width: 6.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.47em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-399"><span class="mi" id="MathJax-Span-400" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-401" style="font-family: MathJax_Main; padding-left: 0.263em;">∈</span><span class="mo" id="MathJax-Span-402" style="font-family: MathJax_Main; padding-left: 0.263em;">{</span><span class="mo" id="MathJax-Span-403" style="font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-404" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-405" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-406" style="font-family: MathJax_Main; padding-left: 0.159em;">1</span><span class="msubsup" id="MathJax-Span-407"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.47em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-408" style="font-family: MathJax_Main;">}</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.523em;"><span class="mi" id="MathJax-Span-409" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>∈</mo><mo fence="false" stretchy="false">{</mo><mo>−</mo><mn>1</mn><mo>,</mo><mn>1</mn><msup><mo fence="false" stretchy="false">}</mo><mi>n</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-60">x \in \{-1, 1\}^n</script> that approximately minimizes the discrepancy <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-61-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;:=&lt;/mo&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;msub&gt;&lt;mo fence=&quot;false&quot; stretchy=&quot;false&quot;&gt;&amp;#x2016;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-410" style="width: 10.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.909em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1008.91em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-411"><span class="texatom" id="MathJax-Span-412"><span class="mrow" id="MathJax-Span-413"><span class="mi" id="MathJax-Span-414" style="font-family: MathJax_Main;">d</span><span class="mi" id="MathJax-Span-415" style="font-family: MathJax_Main;">i</span><span class="mi" id="MathJax-Span-416" style="font-family: MathJax_Main;">s</span><span class="mi" id="MathJax-Span-417" style="font-family: MathJax_Main;">c</span></span></span><span class="mo" id="MathJax-Span-418" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-419" style="font-family: MathJax_Math-italic;">A</span><span class="mo" id="MathJax-Span-420" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-421" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">x</span><span class="mo" id="MathJax-Span-422" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-423" style="font-family: MathJax_Main; padding-left: 0.263em;">:<span style="font-family: MathJax_Main;">=</span></span><span class="mo" id="MathJax-Span-424" style="font-family: MathJax_Main; padding-left: 0.263em;">∥</span><span class="mi" id="MathJax-Span-425" style="font-family: MathJax_Math-italic;">A</span><span class="mi" id="MathJax-Span-426" style="font-family: MathJax_Math-italic;">x</span><span class="msubsup" id="MathJax-Span-427"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.37em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-428" style="font-family: MathJax_Main;">∥</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.523em;"><span class="texatom" id="MathJax-Span-429"><span class="mrow" id="MathJax-Span-430"><span class="mi" id="MathJax-Span-431" style="font-size: 70.7%; font-family: MathJax_Main;">∞</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">c</mi></mrow><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:=</mo><mo fence="false" stretchy="false">‖</mo><mi>A</mi><mi>x</mi><msub><mo fence="false" stretchy="false">‖</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">∞</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-61">\mathrm{disc}(A, x) := \| A x \|_{\infty}</script> of a real-valued <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-62-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-432" style="width: 3.232em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1002.66em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-433"><span class="mi" id="MathJax-Span-434" style="font-family: MathJax_Math-italic;">m</span><span class="mo" id="MathJax-Span-435" style="font-family: MathJax_Main; padding-left: 0.211em;">×</span><span class="mi" id="MathJax-Span-436" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mo>×</mo><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-62">m \times n</script> matrix <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-63-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-437" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-438"><span class="mi" id="MathJax-Span-439" style="font-family: MathJax_Math-italic;">A</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></span></span><script type="math/tex" id="MathJax-Element-63">A</script>. Larsen's algorithm runs in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-64-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-440" style="width: 4.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.492em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.39em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-441"><span class="texatom" id="MathJax-Span-442"><span class="mrow" id="MathJax-Span-443"><span class="munderover" id="MathJax-Span-444"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-445" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-446" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-447" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-448" style="font-family: MathJax_Math-italic;">m</span><span class="msubsup" id="MathJax-Span-449"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-450" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-451" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-452" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mi>m</mi><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-64">\widetilde{O}(mn^2)</script> time compared to Bansal's <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-65-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;4.5&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-453" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.91em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-454"><span class="texatom" id="MathJax-Span-455"><span class="mrow" id="MathJax-Span-456"><span class="munderover" id="MathJax-Span-457"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-458" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-459" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-460" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-461" style="font-family: MathJax_Math-italic;">m</span><span class="msubsup" id="MathJax-Span-462"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-463" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-464"><span class="mrow" id="MathJax-Span-465"><span class="mn" id="MathJax-Span-466" style="font-size: 70.7%; font-family: MathJax_Main;">4.5</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-467" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mi>m</mi><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mn>4.5</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-65">\widetilde{O}(mn^{4.5})</script>-time algorithm, with a slightly weaker logarithmic approximation ratio in terms of the hereditary discrepancy of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-66-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-468" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-469"><span class="mi" id="MathJax-Span-470" style="font-family: MathJax_Math-italic;">A</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></span></span><script type="math/tex" id="MathJax-Element-66">A</script> [Bansal, FOCS 2010]. We present a combinatorial <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-67-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-471" style="width: 8.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.878em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1006.77em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-472"><span class="texatom" id="MathJax-Span-473"><span class="mrow" id="MathJax-Span-474"><span class="munderover" id="MathJax-Span-475"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-476" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-477" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-478" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-479"><span class="mrow" id="MathJax-Span-480"><span class="mi" id="MathJax-Span-481" style="font-family: MathJax_Main;">n</span><span class="mi" id="MathJax-Span-482" style="font-family: MathJax_Main;">n</span><span class="mi" id="MathJax-Span-483" style="font-family: MathJax_Main;">z</span></span></span><span class="mo" id="MathJax-Span-484" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-485" style="font-family: MathJax_Math-italic;">A</span><span class="mo" id="MathJax-Span-486" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-487" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="msubsup" id="MathJax-Span-488" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-489" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-490" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-491" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">n</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">z</mi></mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>+</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-67">\widetilde{O}(\mathrm{nnz}(A) + n^3)</script>-time algorithm with the same approximation guarantee as Larsen's, optimal for tall matrices where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-68-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-492" style="width: 6.513em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.32em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-493"><span class="mi" id="MathJax-Span-494" style="font-family: MathJax_Math-italic;">m</span><span class="mo" id="MathJax-Span-495" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="texatom" id="MathJax-Span-496" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-497"><span class="mi" id="MathJax-Span-498" style="font-family: MathJax_Main;">p</span><span class="mi" id="MathJax-Span-499" style="font-family: MathJax_Main;">o</span><span class="mi" id="MathJax-Span-500" style="font-family: MathJax_Main;">l</span><span class="mi" id="MathJax-Span-501" style="font-family: MathJax_Main;">y</span></span></span><span class="mo" id="MathJax-Span-502" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-503" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-504" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">p</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">y</mi></mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-68">m = \mathrm{poly}(n)</script>. Using a more intricate analysis and fast matrix multiplication, we further achieve a runtime of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-69-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2.53&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-505" style="width: 9.326em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1007.66em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-506"><span class="texatom" id="MathJax-Span-507"><span class="mrow" id="MathJax-Span-508"><span class="munderover" id="MathJax-Span-509"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-510" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-511" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-512" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-513"><span class="mrow" id="MathJax-Span-514"><span class="mi" id="MathJax-Span-515" style="font-family: MathJax_Main;">n</span><span class="mi" id="MathJax-Span-516" style="font-family: MathJax_Main;">n</span><span class="mi" id="MathJax-Span-517" style="font-family: MathJax_Main;">z</span></span></span><span class="mo" id="MathJax-Span-518" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-519" style="font-family: MathJax_Math-italic;">A</span><span class="mo" id="MathJax-Span-520" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-521" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="msubsup" id="MathJax-Span-522" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-523" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-524"><span class="mrow" id="MathJax-Span-525"><span class="mn" id="MathJax-Span-526" style="font-size: 70.7%; font-family: MathJax_Main;">2.53</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-527" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">n</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">z</mi></mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>+</mo><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mn>2.53</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-69">\widetilde{O}(\mathrm{nnz}(A) + n^{2.53})</script>, breaking the cubic barrier for square matrices and surpassing the limitations of linear-programming approaches [Eldan and Singh, RS\&amp;A 2018]. Our algorithm relies on two key ideas: (i) a new sketching technique for finding a projection matrix with a short <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-70-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-528" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-529"><span class="msubsup" id="MathJax-Span-530"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-531" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mn" id="MathJax-Span-532" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-70">\ell_2</script>-basis using implicit leverage-score sampling, and (ii) a data structure for efficiently implementing the iterative Edge-Walk partial-coloring algorithm [Lovett and Meka, SICOMP 2015], and using an alternative analysis to enable ``lazy'' batch updates with low-rank corrections. Our results nearly close the computational gap between real-valued and binary matrices, for which input-sparsity time coloring was recently obtained by [Jain, Sah and Sawhney, SODA 2023].</p>
            <p id="subjects-TmJvacopmV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-TmJvacopmV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TmJvacopmV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TmJvacopmV@OpenReview" onclick="foldPdfKimi('TmJvacopmV@OpenReview', this)" class="hr hr-fold">
        </div><div id="U354tbTjav@OpenReview" class="panel paper" keywords="optimisation,cowboys,latent,bayesian,vae,decoupled,space,structured,vaes,surrogate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=U354tbTjav" target="_blank" title="93/225"><span class="index notranslate">#93</span></a>
                <a id="title-U354tbTjav@OpenReview" class="title-link" href="/venue/U354tbTjav@OpenReview" target="_blank">Return of the Latent Space COWBOYS: Re-thinking the use of VAEs for Bayesian Optimisation of Structured Spaces</a>
                <a id="pdf-U354tbTjav@OpenReview" class="title-pdf notranslate" onclick="togglePdf('U354tbTjav@OpenReview', this)" data="https://openreview.net/pdf?id=U354tbTjav">[PDF<sup id="pdf-stars-U354tbTjav@OpenReview">6</sup>]</a>
                <a id="copy-U354tbTjav@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('U354tbTjav@OpenReview')">[Copy]</a>
                <a id="kimi-U354tbTjav@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('U354tbTjav@OpenReview', this)">[Kimi<sup id="kimi-stars-U354tbTjav@OpenReview"></sup>]</a>
                <a id="rel-U354tbTjav@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('U354tbTjav@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-U354tbTjav@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Henry Moss" target="_blank">Henry Moss</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Ober" target="_blank">Sebastian Ober</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Diethe" target="_blank">Tom Diethe</a>
            </p>
            <p id="summary-U354tbTjav@OpenReview" class="summary">Bayesian optimisation in the latent space of a VAE is a powerful framework for optimisation tasks over complex structured domains, such as the space of valid molecules. However, existing approaches tightly couple the surrogate and generative models, which can lead to suboptimal performance when the latent space is not tailored to specific tasks, which in turn has led to the proposal of increasingly sophisticated algorithms. In this work, we explore a new direction, instead proposing a decoupled approach that trains a generative model and a GP surrogate separately, then combines them via a simple yet principled Bayesian update rule. This separation allows each component to focus on its strengths— structure generation from the VAE and predictive modelling by the GP. We show that our decoupled approach improves our ability to identify high-potential candidates in molecular optimisation problems under constrained evaluation budgets.</p>
            <p id="subjects-U354tbTjav@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-U354tbTjav@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-U354tbTjav@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-U354tbTjav@OpenReview" onclick="foldPdfKimi('U354tbTjav@OpenReview', this)" class="hr hr-fold">
        </div><div id="U64wEbM7NB@OpenReview" class="panel paper" keywords="tmcek,expert,mvc,view,classification,trusted,knowledge,multi,interpretability,safety">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=U64wEbM7NB" target="_blank" title="94/225"><span class="index notranslate">#94</span></a>
                <a id="title-U64wEbM7NB@OpenReview" class="title-link" href="/venue/U64wEbM7NB@OpenReview" target="_blank">Trusted Multi-View Classification with Expert Knowledge Constraints</a>
                <a id="pdf-U64wEbM7NB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('U64wEbM7NB@OpenReview', this)" data="https://openreview.net/pdf?id=U64wEbM7NB">[PDF<sup id="pdf-stars-U64wEbM7NB@OpenReview">1</sup>]</a>
                <a id="copy-U64wEbM7NB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('U64wEbM7NB@OpenReview')">[Copy]</a>
                <a id="kimi-U64wEbM7NB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('U64wEbM7NB@OpenReview', this)">[Kimi<sup id="kimi-stars-U64wEbM7NB@OpenReview">1</sup>]</a>
                <a id="rel-U64wEbM7NB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('U64wEbM7NB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-U64wEbM7NB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyan Liang" target="_blank">Xinyan Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijie Wang" target="_blank">Shijie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhua Qian" target="_blank">Yuhua Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Guo" target="_blank">Qian Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Du" target="_blank">Liang Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingbing Jiang" target="_blank">Bingbing Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tingjin Luo" target="_blank">Tingjin Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feijiang Li" target="_blank">Feijiang Li</a>
            </p>
            <p id="summary-U64wEbM7NB@OpenReview" class="summary">Multi-view classification (MVC) based on the Dempster-Shafer theory has gained significant recognition for its reliability in safety-critical applications. However, existing methods predominantly focus on providing confidence levels for decision outcomes without explaining the reasoning behind these decisions. Moreover, the reliance on first-order statistical magnitudes of belief masses often inadequately capture the intrinsic uncertainty within the evidence. To address these limitations, we propose a novel framework termed Trusted Multi-view Classification Constrained with Expert Knowledge (TMCEK). TMCEK integrates expert knowledge to enhance feature-level interpretability and introduces a distribution-aware subjective opinion mechanism to derive more reliable and realistic confidence estimates. The theoretical superiority of the proposed uncertainty measure over conventional approaches is rigorously established. Extensive experiments conducted on three multi-view datasets for sleep stage classification demonstrate that TMCEK achieves state-of-the-art performance while offering interpretability at both the feature and decision levels. These results position TMCEK as a robust and interpretable solution for MVC in safety-critical domains. The code is available at https://github.com/jie019/TMCEK_ICML2025.</p>
            <p id="subjects-U64wEbM7NB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-U64wEbM7NB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-U64wEbM7NB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-U64wEbM7NB@OpenReview" onclick="foldPdfKimi('U64wEbM7NB@OpenReview', this)" class="hr hr-fold">
        </div><div id="UFlyLkvyAE@OpenReview" class="panel paper" keywords="grama,arma,graph,ssms,equivariance,autoregressive,moving,permutation,selective,adaptive">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UFlyLkvyAE" target="_blank" title="95/225"><span class="index notranslate">#95</span></a>
                <a id="title-UFlyLkvyAE@OpenReview" class="title-link" href="/venue/UFlyLkvyAE@OpenReview" target="_blank">Graph Adaptive Autoregressive Moving Average Models</a>
                <a id="pdf-UFlyLkvyAE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UFlyLkvyAE@OpenReview', this)" data="https://openreview.net/pdf?id=UFlyLkvyAE">[PDF<sup id="pdf-stars-UFlyLkvyAE@OpenReview">5</sup>]</a>
                <a id="copy-UFlyLkvyAE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UFlyLkvyAE@OpenReview')">[Copy]</a>
                <a id="kimi-UFlyLkvyAE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UFlyLkvyAE@OpenReview', this)">[Kimi<sup id="kimi-stars-UFlyLkvyAE@OpenReview"></sup>]</a>
                <a id="rel-UFlyLkvyAE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UFlyLkvyAE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UFlyLkvyAE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Moshe Eliasof" target="_blank">Moshe Eliasof</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessio Gravina" target="_blank">Alessio Gravina</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Ceni" target="_blank">Andrea Ceni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Claudio Gallicchio" target="_blank">Claudio Gallicchio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Davide Bacciu" target="_blank">Davide Bacciu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carola-Bibiane Schönlieb" target="_blank">Carola-Bibiane Schönlieb</a>
            </p>
            <p id="summary-UFlyLkvyAE@OpenReview" class="summary">Graph State Space Models (SSMs) have recently been introduced to enhance Graph Neural Networks (GNNs) in modeling long-range interactions. Despite their success, existing methods either compromise on permutation equivariance or limit their focus to pairwise interactions rather than sequences. Building on the connection between Autoregressive Moving Average (ARMA) and SSM, in this paper, we introduce GRAMA, a Graph Adaptive method based on a learnable ARMA framework that addresses these limitations. By transforming from static to sequential graph data, GRAMA leverages the strengths of the ARMA framework, while preserving permutation equivariance. Moreover, GRAMA incorporates a selective attention mechanism for dynamic learning of ARMA coefficients, enabling efficient and flexible long-range information propagation. We also establish theoretical connections between GRAMA and Selective SSMs, providing insights into its ability to capture long-range dependencies. Experiments on 26 synthetic and real-world datasets demonstrate that GRAMA consistently outperforms backbone models and performs competitively with state-of-the-art methods.</p>
            <p id="subjects-UFlyLkvyAE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-UFlyLkvyAE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UFlyLkvyAE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UFlyLkvyAE@OpenReview" onclick="foldPdfKimi('UFlyLkvyAE@OpenReview', this)" class="hr hr-fold">
        </div><div id="F0sinjQMnv@OpenReview" class="panel paper" keywords="codelengths,causal,complexity,direction,fitness,variational,bayesian,approaches,codelength,evaluable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=F0sinjQMnv" target="_blank" title="96/225"><span class="index notranslate">#96</span></a>
                <a id="title-F0sinjQMnv@OpenReview" class="title-link" href="/venue/F0sinjQMnv@OpenReview" target="_blank">Identifying Causal Direction via Variational Bayesian Compression</a>
                <a id="pdf-F0sinjQMnv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('F0sinjQMnv@OpenReview', this)" data="https://openreview.net/pdf?id=F0sinjQMnv">[PDF<sup id="pdf-stars-F0sinjQMnv@OpenReview">3</sup>]</a>
                <a id="copy-F0sinjQMnv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('F0sinjQMnv@OpenReview')">[Copy]</a>
                <a id="kimi-F0sinjQMnv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('F0sinjQMnv@OpenReview', this)">[Kimi<sup id="kimi-stars-F0sinjQMnv@OpenReview">1</sup>]</a>
                <a id="rel-F0sinjQMnv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('F0sinjQMnv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-F0sinjQMnv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Quang-Duy Tran" target="_blank">Quang-Duy Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bao Duong" target="_blank">Bao Duong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Phuoc Nguyen" target="_blank">Phuoc Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thin Nguyen" target="_blank">Thin Nguyen</a>
            </p>
            <p id="summary-F0sinjQMnv@OpenReview" class="summary">Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches.</p>
            <p id="subjects-F0sinjQMnv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-F0sinjQMnv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-F0sinjQMnv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-F0sinjQMnv@OpenReview" onclick="foldPdfKimi('F0sinjQMnv@OpenReview', this)" class="hr hr-fold">
        </div><div id="EUH4VUCXay@OpenReview" class="panel paper" keywords="elo,arena,rating,evaluation,mle,annotator,stable,abilities,framework,ranking">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EUH4VUCXay" target="_blank" title="97/225"><span class="index notranslate">#97</span></a>
                <a id="title-EUH4VUCXay@OpenReview" class="title-link" href="/venue/EUH4VUCXay@OpenReview" target="_blank">am-ELO: A Stable Framework for Arena-based LLM Evaluation</a>
                <a id="pdf-EUH4VUCXay@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EUH4VUCXay@OpenReview', this)" data="https://openreview.net/pdf?id=EUH4VUCXay">[PDF<sup id="pdf-stars-EUH4VUCXay@OpenReview">2</sup>]</a>
                <a id="copy-EUH4VUCXay@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EUH4VUCXay@OpenReview')">[Copy]</a>
                <a id="kimi-EUH4VUCXay@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EUH4VUCXay@OpenReview', this)">[Kimi<sup id="kimi-stars-EUH4VUCXay@OpenReview"></sup>]</a>
                <a id="rel-EUH4VUCXay@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EUH4VUCXay@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EUH4VUCXay@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zirui Liu" target="_blank">Zirui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiatong Li" target="_blank">Jiatong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Zhuang" target="_blank">Yan Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Liu" target="_blank">Qi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuanghong Shen" target="_blank">Shuanghong Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Ouyang" target="_blank">Jie Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyue Cheng" target="_blank">Mingyue Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijin Wang" target="_blank">Shijin Wang</a>
            </p>
            <p id="summary-EUH4VUCXay@OpenReview" class="summary">Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating’s probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.</p>
            <p id="subjects-EUH4VUCXay@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-EUH4VUCXay@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EUH4VUCXay@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EUH4VUCXay@OpenReview" onclick="foldPdfKimi('EUH4VUCXay@OpenReview', this)" class="hr hr-fold">
        </div><div id="BUONdewsBa@OpenReview" class="panel paper" keywords="dmoe,medical,segmentation,fairness,clinical,image,control,imbalanced,demographic,aware">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BUONdewsBa" target="_blank" title="98/225"><span class="index notranslate">#98</span></a>
                <a id="title-BUONdewsBa@OpenReview" class="title-link" href="/venue/BUONdewsBa@OpenReview" target="_blank">Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective</a>
                <a id="pdf-BUONdewsBa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BUONdewsBa@OpenReview', this)" data="https://openreview.net/pdf?id=BUONdewsBa">[PDF<sup id="pdf-stars-BUONdewsBa@OpenReview">3</sup>]</a>
                <a id="copy-BUONdewsBa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BUONdewsBa@OpenReview')">[Copy]</a>
                <a id="kimi-BUONdewsBa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BUONdewsBa@OpenReview', this)">[Kimi<sup id="kimi-stars-BUONdewsBa@OpenReview"></sup>]</a>
                <a id="rel-BUONdewsBa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BUONdewsBa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BUONdewsBa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yujin Oh" target="_blank">Yujin Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Jin" target="_blank">Pengfei Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangjoon Park" target="_blank">Sangjoon Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sekeun Kim" target="_blank">Sekeun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyeop yoon" target="_blank">Siyeop yoon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Kim" target="_blank">Jin Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyungsang Kim" target="_blank">Kyungsang Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Li" target="_blank">Xiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanzheng Li" target="_blank">Quanzheng Li</a>
            </p>
            <p id="summary-BUONdewsBa@OpenReview" class="summary">Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoE's role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code is available at https://github.com/tvseg/dMoE.</p>
            <p id="subjects-BUONdewsBa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-BUONdewsBa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BUONdewsBa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BUONdewsBa@OpenReview" onclick="foldPdfKimi('BUONdewsBa@OpenReview', this)" class="hr hr-fold">
        </div><div id="5liHhkgvAn@OpenReview" class="panel paper" keywords="sdp,crown,tightness,neuron,bound,verifiers,propagation,verification,semidefinite,inter">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5liHhkgvAn" target="_blank" title="99/225"><span class="index notranslate">#99</span></a>
                <a id="title-5liHhkgvAn@OpenReview" class="title-link" href="/venue/5liHhkgvAn@OpenReview" target="_blank">SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming</a>
                <a id="pdf-5liHhkgvAn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5liHhkgvAn@OpenReview', this)" data="https://openreview.net/pdf?id=5liHhkgvAn">[PDF<sup id="pdf-stars-5liHhkgvAn@OpenReview">2</sup>]</a>
                <a id="copy-5liHhkgvAn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5liHhkgvAn@OpenReview')">[Copy]</a>
                <a id="kimi-5liHhkgvAn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5liHhkgvAn@OpenReview', this)">[Kimi<sup id="kimi-stars-5liHhkgvAn@OpenReview">1</sup>]</a>
                <a id="rel-5liHhkgvAn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5liHhkgvAn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5liHhkgvAn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hong-Ming Chiu" target="_blank">Hong-Ming Chiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chen" target="_blank">Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Zhang" target="_blank">Huan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Zhang" target="_blank">Richard Zhang</a>
            </p>
            <p id="summary-5liHhkgvAn@OpenReview" class="summary">Neural network verifiers based on linear bound propagation scale impressively to massive models but can be surprisingly loose when neuron coupling is crucial. Conversely, semidefinite programming (SDP) verifiers capture inter-neuron coupling naturally, but their cubic complexity restricts them to only small models. In this paper, we propose SDP-CROWN, a novel hybrid verification framework that combines the tightness of SDP relaxations with the scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new linear bound---derived via SDP principles---that explicitly captures <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-71-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-533" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-534"><span class="msubsup" id="MathJax-Span-535"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-536" style="font-family: MathJax_Main;">ℓ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="texatom" id="MathJax-Span-537"><span class="mrow" id="MathJax-Span-538"><span class="mn" id="MathJax-Span-539" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-71">\ell_{2}</script>-norm-based inter-neuron coupling while adding only one extra parameter per layer. This bound can be integrated seamlessly into any linear bound-propagation pipeline, preserving the inherent scalability of such methods yet significantly improving tightness. In theory, we prove that our inter-neuron bound can be up to a factor of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-72-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-540" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.46em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-541"><span class="msqrt" id="MathJax-Span-542"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-543"><span class="mi" id="MathJax-Span-544" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi></msqrt></math></span></span><script type="math/tex" id="MathJax-Element-72">\sqrt{n}</script> tighter than traditional per-neuron bounds. In practice, when incorporated into the state-of-the-art <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-73-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-545" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-546"><span class="mi" id="MathJax-Span-547" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-73">\alpha</script>-CROWN verifier, we observe markedly improved verification performance on large models with up to 65 thousand neurons and 2.47 million parameters, achieving tightness that approaches that of costly SDP-based methods.</p>
            <p id="subjects-5liHhkgvAn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5liHhkgvAn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5liHhkgvAn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5liHhkgvAn@OpenReview" onclick="foldPdfKimi('5liHhkgvAn@OpenReview', this)" class="hr hr-fold">
        </div><div id="0cEZyhHEks@OpenReview" class="panel paper" keywords="juice,heads,memory,contextual,knowledge,conflict,conflicts,parametric,attention,superposition">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0cEZyhHEks" target="_blank" title="100/225"><span class="index notranslate">#100</span></a>
                <a id="title-0cEZyhHEks@OpenReview" class="title-link" href="/venue/0cEZyhHEks@OpenReview" target="_blank">Taming Knowledge Conflicts in Language Models</a>
                <a id="pdf-0cEZyhHEks@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0cEZyhHEks@OpenReview', this)" data="https://openreview.net/pdf?id=0cEZyhHEks">[PDF<sup id="pdf-stars-0cEZyhHEks@OpenReview">7</sup>]</a>
                <a id="copy-0cEZyhHEks@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0cEZyhHEks@OpenReview')">[Copy]</a>
                <a id="kimi-0cEZyhHEks@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0cEZyhHEks@OpenReview', this)">[Kimi<sup id="kimi-stars-0cEZyhHEks@OpenReview">4</sup>]</a>
                <a id="rel-0cEZyhHEks@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0cEZyhHEks@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0cEZyhHEks@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gaotang Li" target="_blank">Gaotang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhong Chen" target="_blank">Yuzhong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanghang Tong" target="_blank">Hanghang Tong</a>
            </p>
            <p id="summary-0cEZyhHEks@OpenReview" class="summary">Language Models (LMs) often encounter knowledge conflicts when parametric memory contradicts contextual knowledge. Previous works attribute this conflict to the interplay between "memory heads" and "context heads", attention heads assumed to promote either memory or context exclusively. In this study, we go beyond this fundamental assumption by uncovering a critical phenomenon we term the *superposition of contextual information and parametric memory*, where highly influential attention heads simultaneously contribute to both memory and context. Building upon this insight, we propose Just Run Twice (JuICE), a test-time attention intervention method that steers LMs toward either parametric beliefs or contextual knowledge without requiring fine-tuning. JuICE identifies a set of reliable attention heads and leverages a dual-run approach to mitigate the superposition effects. Extensive experiments across 11 datasets and 6 model architectures demonstrate that JuICE sets the new state-of-the-art performance and robust generalization, achieving significant and consistent improvement across different domains under various conflict types. Finally, we theoretically analyze knowledge conflict and the superposition of contextual information and parametric memory in attention heads, which further elucidates the effectiveness of JuICE in these settings. Our code is available at https://github.com/GaotangLi/JUICE.</p>
            <p id="subjects-0cEZyhHEks@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-0cEZyhHEks@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0cEZyhHEks@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0cEZyhHEks@OpenReview" onclick="foldPdfKimi('0cEZyhHEks@OpenReview', this)" class="hr hr-fold">
        </div><div id="Pokj70ZAxJ@OpenReview" class="panel paper" keywords="domain,dfn,decoupler,adapter,shot,fss,serves,finetuning,svn,naturally">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Pokj70ZAxJ" target="_blank" title="101/225"><span class="index notranslate">#101</span></a>
                <a id="title-Pokj70ZAxJ@OpenReview" class="title-link" href="/venue/Pokj70ZAxJ@OpenReview" target="_blank">Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation</a>
                <a id="pdf-Pokj70ZAxJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Pokj70ZAxJ@OpenReview', this)" data="https://openreview.net/pdf?id=Pokj70ZAxJ">[PDF<sup id="pdf-stars-Pokj70ZAxJ@OpenReview">5</sup>]</a>
                <a id="copy-Pokj70ZAxJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Pokj70ZAxJ@OpenReview')">[Copy]</a>
                <a id="kimi-Pokj70ZAxJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Pokj70ZAxJ@OpenReview', this)">[Kimi<sup id="kimi-stars-Pokj70ZAxJ@OpenReview">1</sup>]</a>
                <a id="rel-Pokj70ZAxJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Pokj70ZAxJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Pokj70ZAxJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jintao Tong" target="_blank">Jintao Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Ma" target="_blank">Ran Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixiong Zou" target="_blank">Yixiong Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangyao Chen" target="_blank">Guangyao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhua Li" target="_blank">Yuhua Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruixuan Li" target="_blank">Ruixuan Li</a>
            </p>
            <p id="summary-Pokj70ZAxJ@OpenReview" class="summary">Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few training samples are available for efficient finetuning. There are majorly two challenges in this task: (1) the domain gap and (2) finetuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and we find the model's inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the model's attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn knowledge specific to target domains. Extensive experiments demonstrate that our method surpasses the state-of-the-art method in CD-FSS significantly by 2.69% and 4.68% average MIoU in 1-shot and 5-shot scenarios, respectively.</p>
            <p id="subjects-Pokj70ZAxJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Pokj70ZAxJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pokj70ZAxJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pokj70ZAxJ@OpenReview" onclick="foldPdfKimi('Pokj70ZAxJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="LpE54NUnmO@OpenReview" class="panel paper" keywords="designer,agent,task,architecting,communication,topologies,textbf,humaneval,agents,performing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LpE54NUnmO" target="_blank" title="102/225"><span class="index notranslate">#102</span></a>
                <a id="title-LpE54NUnmO@OpenReview" class="title-link" href="/venue/LpE54NUnmO@OpenReview" target="_blank">G-Designer: Architecting Multi-agent Communication Topologies via Graph Neural Networks</a>
                <a id="pdf-LpE54NUnmO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LpE54NUnmO@OpenReview', this)" data="https://openreview.net/pdf?id=LpE54NUnmO">[PDF<sup id="pdf-stars-LpE54NUnmO@OpenReview">6</sup>]</a>
                <a id="copy-LpE54NUnmO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LpE54NUnmO@OpenReview')">[Copy]</a>
                <a id="kimi-LpE54NUnmO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LpE54NUnmO@OpenReview', this)">[Kimi<sup id="kimi-stars-LpE54NUnmO@OpenReview">4</sup>]</a>
                <a id="rel-LpE54NUnmO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LpE54NUnmO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LpE54NUnmO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guibin Zhang" target="_blank">Guibin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Yue" target="_blank">Yanwei Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangguo Sun" target="_blank">Xiangguo Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guancheng Wan" target="_blank">Guancheng Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miao Yu" target="_blank">Miao Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junfeng Fang" target="_blank">Junfeng Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Wang" target="_blank">Kun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianlong Chen" target="_blank">Tianlong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawei Cheng" target="_blank">Dawei Cheng</a>
            </p>
            <p id="summary-LpE54NUnmO@OpenReview" class="summary">Recent advancements in large language model (LLM)-based agents have demonstrated that collective intelligence can significantly surpass the capabilities of individual agents, primarily due to well-crafted inter-agent communication topologies. Despite the diverse and high-performing designs available, practitioners often face confusion when selecting the most effective pipeline for their specific task: \textit{Which topology is the best choice for my task, avoiding unnecessary communication token overhead while ensuring high-quality solution?} In response to this dilemma, we introduce G-Designer, an adaptive, efficient, and robust solution for multi-agent deployment, which dynamically designs task-aware, customized communication topologies. Specifically, G-Designer models the multi-agent system as a multi-agent network, leveraging a variational graph auto-encoder to encode both the nodes (agents) and a task-specific virtual node, and decodes a task-adaptive and high-performing communication topology. Extensive experiments on six benchmarks showcase that G-Designer is: \textbf{(1) high-performing}, achieving superior results on MMLU with accuracy at <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-74-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;84.50&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-548" style="width: 3.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.08em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-549"><span class="mn" id="MathJax-Span-550" style="font-family: MathJax_Main;">84.50</span><span class="mi" id="MathJax-Span-551" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>84.50</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-74">84.50\%</script> and on HumanEval with pass@1 at <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-75-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;89.90&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-552" style="width: 3.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.08em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-553"><span class="mn" id="MathJax-Span-554" style="font-family: MathJax_Main;">89.90</span><span class="mi" id="MathJax-Span-555" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>89.90</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-75">89.90\%</script>; \textbf{(2) task-adaptive}, architecting communication protocols tailored to task difficulty, reducing token consumption by up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-76-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;95.33&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-556" style="width: 3.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.08em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-557"><span class="mn" id="MathJax-Span-558" style="font-family: MathJax_Main;">95.33</span><span class="mi" id="MathJax-Span-559" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>95.33</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-76">95.33\%</script> on HumanEval; and \textbf{(3) adversarially robust}, defending against agent adversarial attacks with merely <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-77-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;0.3&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-560" style="width: 2.607em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.09em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-561"><span class="mn" id="MathJax-Span-562" style="font-family: MathJax_Main;">0.3</span><span class="mi" id="MathJax-Span-563" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.3</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-77">0.3\%</script> accuracy drop.</p>
            <p id="subjects-LpE54NUnmO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-LpE54NUnmO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LpE54NUnmO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LpE54NUnmO@OpenReview" onclick="foldPdfKimi('LpE54NUnmO@OpenReview', this)" class="hr hr-fold">
        </div><div id="KKwBo3u3IW@OpenReview" class="panel paper" keywords="search,board,chess,games,external,planning,chess960,game,language,mastering">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KKwBo3u3IW" target="_blank" title="103/225"><span class="index notranslate">#103</span></a>
                <a id="title-KKwBo3u3IW@OpenReview" class="title-link" href="/venue/KKwBo3u3IW@OpenReview" target="_blank">Mastering Board Games by External and Internal Planning with Language Models</a>
                <a id="pdf-KKwBo3u3IW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KKwBo3u3IW@OpenReview', this)" data="https://openreview.net/pdf?id=KKwBo3u3IW">[PDF<sup id="pdf-stars-KKwBo3u3IW@OpenReview">4</sup>]</a>
                <a id="copy-KKwBo3u3IW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KKwBo3u3IW@OpenReview')">[Copy]</a>
                <a id="kimi-KKwBo3u3IW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KKwBo3u3IW@OpenReview', this)">[Kimi<sup id="kimi-stars-KKwBo3u3IW@OpenReview">2</sup>]</a>
                <a id="rel-KKwBo3u3IW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KKwBo3u3IW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KKwBo3u3IW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=John Schultz" target="_blank">John Schultz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakub Adamek" target="_blank">Jakub Adamek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matej Jusup" target="_blank">Matej Jusup</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Lanctot" target="_blank">Marc Lanctot</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Kaisers" target="_blank">Michael Kaisers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sarah Perrin" target="_blank">Sarah Perrin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Hennes" target="_blank">Daniel Hennes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeremy Shar" target="_blank">Jeremy Shar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cannada Lewis" target="_blank">Cannada Lewis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anian Ruoss" target="_blank">Anian Ruoss</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Zahavy" target="_blank">Tom Zahavy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Petar Veličković" target="_blank">Petar Veličković</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laurel Prince" target="_blank">Laurel Prince</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Satinder Singh" target="_blank">Satinder Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Malmi" target="_blank">Eric Malmi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nenad Tomasev" target="_blank">Nenad Tomasev</a>
            </p>
            <p id="summary-KKwBo3u3IW@OpenReview" class="summary">Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In *external search*, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in *internal search*, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.</p>
            <p id="subjects-KKwBo3u3IW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-KKwBo3u3IW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KKwBo3u3IW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KKwBo3u3IW@OpenReview" onclick="foldPdfKimi('KKwBo3u3IW@OpenReview', this)" class="hr hr-fold">
        </div><div id="3pk0p4NGmQ@OpenReview" class="panel paper" keywords="vulnerabilities,cve,bench,agents,exploit,world,benchmark,llm,web,real">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3pk0p4NGmQ" target="_blank" title="104/225"><span class="index notranslate">#104</span></a>
                <a id="title-3pk0p4NGmQ@OpenReview" class="title-link" href="/venue/3pk0p4NGmQ@OpenReview" target="_blank">CVE-Bench: A Benchmark for AI Agents’ Ability to Exploit Real-World Web Application Vulnerabilities</a>
                <a id="pdf-3pk0p4NGmQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3pk0p4NGmQ@OpenReview', this)" data="https://openreview.net/pdf?id=3pk0p4NGmQ">[PDF<sup id="pdf-stars-3pk0p4NGmQ@OpenReview">1</sup>]</a>
                <a id="copy-3pk0p4NGmQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3pk0p4NGmQ@OpenReview')">[Copy]</a>
                <a id="kimi-3pk0p4NGmQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3pk0p4NGmQ@OpenReview', this)">[Kimi<sup id="kimi-stars-3pk0p4NGmQ@OpenReview">2</sup>]</a>
                <a id="rel-3pk0p4NGmQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3pk0p4NGmQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3pk0p4NGmQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Zhu" target="_blank">Yuxuan Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antony Kellermann" target="_blank">Antony Kellermann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dylan Bowman" target="_blank">Dylan Bowman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Li" target="_blank">Philip Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akul Gupta" target="_blank">Akul Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adarsh Danda" target="_blank">Adarsh Danda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Fang" target="_blank">Richard Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Conner Jensen" target="_blank">Conner Jensen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Ihli" target="_blank">Eric Ihli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Benn" target="_blank">Jason Benn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jet Geronimo" target="_blank">Jet Geronimo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Avi Dhir" target="_blank">Avi Dhir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sudhit Rao" target="_blank">Sudhit Rao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaicheng Yu" target="_blank">Kaicheng Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Twm Stone" target="_blank">Twm Stone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Kang" target="_blank">Daniel Kang</a>
            </p>
            <p id="summary-3pk0p4NGmQ@OpenReview" class="summary">Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture-the-Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized exper-tise to reproduce exploits and a systematic approach to evaluating unpredictable attacks. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our experiments show that the state-of-the-art agent framework can exploit up to 13% of the vulnerabilities.</p>
            <p id="subjects-3pk0p4NGmQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-3pk0p4NGmQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3pk0p4NGmQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3pk0p4NGmQ@OpenReview" onclick="foldPdfKimi('3pk0p4NGmQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="13HPTmZKbM@OpenReview" class="panel paper" keywords="tuning,fine,forgetting,pre,trained,upweighting,metamathqa,easy,upweight,mitigates">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=13HPTmZKbM" target="_blank" title="105/225"><span class="index notranslate">#105</span></a>
                <a id="title-13HPTmZKbM@OpenReview" class="title-link" href="/venue/13HPTmZKbM@OpenReview" target="_blank">Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting</a>
                <a id="pdf-13HPTmZKbM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('13HPTmZKbM@OpenReview', this)" data="https://openreview.net/pdf?id=13HPTmZKbM">[PDF<sup id="pdf-stars-13HPTmZKbM@OpenReview">3</sup>]</a>
                <a id="copy-13HPTmZKbM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('13HPTmZKbM@OpenReview')">[Copy]</a>
                <a id="kimi-13HPTmZKbM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('13HPTmZKbM@OpenReview', this)">[Kimi<sup id="kimi-stars-13HPTmZKbM@OpenReview">3</sup>]</a>
                <a id="rel-13HPTmZKbM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('13HPTmZKbM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-13HPTmZKbM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sunny Sanyal" target="_blank">Sunny Sanyal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hayden Prairie" target="_blank">Hayden Prairie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rudrajit Das" target="_blank">Rudrajit Das</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Kavis" target="_blank">Ali Kavis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sujay Sanghavi" target="_blank">Sujay Sanghavi</a>
            </p>
            <p id="summary-13HPTmZKbM@OpenReview" class="summary">Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as "catastrophic forgetting". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a *sample weighting scheme for the fine-tuning data* solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace, which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-78-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;0.8&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-564" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.25em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-565"><span class="mn" id="MathJax-Span-566" style="font-family: MathJax_Main;">0.8</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.8</mn></math></span></span><script type="math/tex" id="MathJax-Element-78">0.8</script>% drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-79-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;5.4&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-567" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.25em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-568"><span class="mn" id="MathJax-Span-569" style="font-family: MathJax_Main;">5.4</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5.4</mn></math></span></span><script type="math/tex" id="MathJax-Element-79">5.4</script>% more accuracy on the pre-training datasets.</p>
            <p id="subjects-13HPTmZKbM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-13HPTmZKbM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-13HPTmZKbM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-13HPTmZKbM@OpenReview" onclick="foldPdfKimi('13HPTmZKbM@OpenReview', this)" class="hr hr-fold">
        </div><div id="9u5hPIcr6j@OpenReview" class="panel paper" keywords="lotterycodec,compression,image,hypothesis,initialized,randomly,mask,rewind,overfits,vtm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9u5hPIcr6j" target="_blank" title="106/225"><span class="index notranslate">#106</span></a>
                <a id="title-9u5hPIcr6j@OpenReview" class="title-link" href="/venue/9u5hPIcr6j@OpenReview" target="_blank">LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression</a>
                <a id="pdf-9u5hPIcr6j@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9u5hPIcr6j@OpenReview', this)" data="https://openreview.net/pdf?id=9u5hPIcr6j">[PDF<sup id="pdf-stars-9u5hPIcr6j@OpenReview">4</sup>]</a>
                <a id="copy-9u5hPIcr6j@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9u5hPIcr6j@OpenReview')">[Copy]</a>
                <a id="kimi-9u5hPIcr6j@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9u5hPIcr6j@OpenReview', this)">[Kimi<sup id="kimi-stars-9u5hPIcr6j@OpenReview">4</sup>]</a>
                <a id="rel-9u5hPIcr6j@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9u5hPIcr6j@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9u5hPIcr6j@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haotian Wu" target="_blank">Haotian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gongpu Chen" target="_blank">Gongpu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pier Luigi Dragotti" target="_blank">Pier Luigi Dragotti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deniz Gunduz" target="_blank">Deniz Gunduz</a>
            </p>
            <p id="summary-9u5hPIcr6j@OpenReview" class="summary">We introduce and validate the lottery codec hypothesis, which states that untrained subnetworks within randomly initialized networks can serve as synthesis networks for overfitted image compression, achieving rate-distortion (RD) performance comparable to trained networks. This hypothesis leads to a new paradigm for image compression by encoding image statistics into the network substructure. Building on this hypothesis, we propose LotteryCodec, which overfits a binary mask to an individual image, leveraging an over-parameterized and randomly initialized network shared by the encoder and the decoder. To address over-parameterization challenges and streamline subnetwork search, we develop a rewind modulation mechanism that improves the RD performance. LotteryCodec outperforms VTM and sets a new state-of-the-art in single-image compression. LotteryCodec also enables adaptive decoding complexity through adjustable mask ratios, offering flexible compression solutions for diverse device constraints and application requirements.</p>
            <p id="subjects-9u5hPIcr6j@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-9u5hPIcr6j@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9u5hPIcr6j@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9u5hPIcr6j@OpenReview" onclick="foldPdfKimi('9u5hPIcr6j@OpenReview', this)" class="hr hr-fold">
        </div><div id="V61nluxFlR@OpenReview" class="panel paper" keywords="consistency,logical,preference,llms,decision,logic,improving,making,llm,aligning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=V61nluxFlR" target="_blank" title="107/225"><span class="index notranslate">#107</span></a>
                <a id="title-V61nluxFlR@OpenReview" class="title-link" href="/venue/V61nluxFlR@OpenReview" target="_blank">Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models</a>
                <a id="pdf-V61nluxFlR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('V61nluxFlR@OpenReview', this)" data="https://openreview.net/pdf?id=V61nluxFlR">[PDF<sup id="pdf-stars-V61nluxFlR@OpenReview">2</sup>]</a>
                <a id="copy-V61nluxFlR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('V61nluxFlR@OpenReview')">[Copy]</a>
                <a id="kimi-V61nluxFlR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('V61nluxFlR@OpenReview', this)">[Kimi<sup id="kimi-stars-V61nluxFlR@OpenReview">2</sup>]</a>
                <a id="rel-V61nluxFlR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('V61nluxFlR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-V61nluxFlR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yinhong Liu" target="_blank">Yinhong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijiang Guo" target="_blank">Zhijiang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianya Liang" target="_blank">Tianya Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ehsan Shareghi" target="_blank">Ehsan Shareghi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivan Vulić" target="_blank">Ivan Vulić</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nigel Collier" target="_blank">Nigel Collier</a>
            </p>
            <p id="summary-V61nluxFlR@OpenReview" class="summary">Large Language Models (LLMs) are expected to be predictable and trustworthy to support reliable decision-making systems. Yet current LLMs often show inconsistencies in their judgments. In this work, we examine \textit{logical preference consistency} as a foundational requirement for building more dependable LLM systems, ensuring stable and coherent decision-making while minimizing erratic or contradictory outputs.To quantify the logical preference consistency, we propose a universal evaluation framework based on three fundamental properties: *transitivity*, *commutativity* and *negation invariance*.Through extensive experimentation across diverse LLMs, we demonstrate that these properties serve as strong indicators of judgment robustness.Furthermore, we introduce a data refinement and augmentation technique, REPAIR, that enhances logical consistency while maintaining alignment with human preferences. Finally, we show that improving consistency leads to better performance in LLM-driven logic-based algorithms, reinforcing stability and coherence in decision-making systems.</p>
            <p id="subjects-V61nluxFlR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-V61nluxFlR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-V61nluxFlR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-V61nluxFlR@OpenReview" onclick="foldPdfKimi('V61nluxFlR@OpenReview', this)" class="hr hr-fold">
        </div><div id="RL6d53a5jj@OpenReview" class="panel paper" keywords="treatments,factorial,experimenter,dosage,design,combinatorial,interventions,intervention,probabilistic,optimal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=RL6d53a5jj" target="_blank" title="108/225"><span class="index notranslate">#108</span></a>
                <a id="title-RL6d53a5jj@OpenReview" class="title-link" href="/venue/RL6d53a5jj@OpenReview" target="_blank">Probabilistic Factorial Experimental Design for Combinatorial Interventions</a>
                <a id="pdf-RL6d53a5jj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('RL6d53a5jj@OpenReview', this)" data="https://openreview.net/pdf?id=RL6d53a5jj">[PDF<sup id="pdf-stars-RL6d53a5jj@OpenReview">2</sup>]</a>
                <a id="copy-RL6d53a5jj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('RL6d53a5jj@OpenReview')">[Copy]</a>
                <a id="kimi-RL6d53a5jj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('RL6d53a5jj@OpenReview', this)">[Kimi<sup id="kimi-stars-RL6d53a5jj@OpenReview">1</sup>]</a>
                <a id="rel-RL6d53a5jj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('RL6d53a5jj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-RL6d53a5jj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Divya Shyamal" target="_blank">Divya Shyamal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Zhang" target="_blank">Jiaqi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caroline Uhler" target="_blank">Caroline Uhler</a>
            </p>
            <p id="summary-RL6d53a5jj@OpenReview" class="summary">A _combinatorial intervention_, consisting of multiple treatments applied to a single unit with potential interactive effects, has substantial applications in fields such as biomedicine, engineering, and beyond. Given <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-80-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-570" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-571"><span class="mi" id="MathJax-Span-572" style="font-family: MathJax_Math-italic;">p</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></span></span><script type="math/tex" id="MathJax-Element-80">p</script> possible treatments, conducting all possible <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-81-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-573" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-574"><span class="msubsup" id="MathJax-Span-575"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-576" style="font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 0.523em;"><span class="mi" id="MathJax-Span-577" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>2</mn><mi>p</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-81">2^p</script> combinatorial interventions can be laborious and quickly becomes infeasible as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-82-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-578" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-579"><span class="mi" id="MathJax-Span-580" style="font-family: MathJax_Math-italic;">p</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></span></span><script type="math/tex" id="MathJax-Element-82">p</script> increases. Here we introduce the _probabilistic factorial experimental design_, formalized from how scientists perform lab experiments. In this framework, the experimenter selects a dosage for each possible treatment and applies it to a group of units. Each unit independently receives a random combination of treatments, sampled from a product Bernoulli distribution determined by the dosages. Additionally, the experimenter can carry out such experiments over multiple rounds, adapting the design in an active manner. We address the optimal experimental design problem within a novel intervention model that imposes bounded-degree interactions between treatments. In the passive setting, we provide a closed-form solution for the near-optimal design. Our results prove that a dosage of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-83-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-581" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1000.68em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-582"><span class="mfrac" id="MathJax-Span-583"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-584" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-585" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mn>1</mn><mn>2</mn></mfrac></math></span></span><script type="math/tex" id="MathJax-Element-83">\frac{1}{2}</script> for each treatment is optimal up to a factor of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-84-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-586" style="width: 6.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.107em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.888em, 1005em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-587"><span class="mn" id="MathJax-Span-588" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-589" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-590" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">O</span><span class="mo" id="MathJax-Span-591" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-592"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.461em, 1001.51em, 2.451em, -999.997em); top: -2.706em; left: 50%; margin-left: -0.779em;"><span class="mrow" id="MathJax-Span-593"><span class="mi" id="MathJax-Span-594" style="font-size: 70.7%; font-family: MathJax_Main;">ln</span><span class="mo" id="MathJax-Span-595" style="font-size: 70.7%;"></span><span class="mo" id="MathJax-Span-596" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-597" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-598" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-599" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.67em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.669em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-600" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>+</mo><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><mi>n</mi></mfrac><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-84">1+O(\frac{\ln(n)}{n})</script> for estimating any <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-85-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-601" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-602"><span class="mi" id="MathJax-Span-603" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-85">k</script>-way interaction model, regardless of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-86-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-604" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-605"><span class="mi" id="MathJax-Span-606" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-86">k</script>, and imply that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-87-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mstyle scriptlevel=&quot;0&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;&gt;(&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-607" style="width: 6.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.63em, 2.763em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-608"><span class="mi" id="MathJax-Span-609" style="font-family: MathJax_Math-italic;">O</span><span class="mstyle" id="MathJax-Span-610" style=""><span class="mrow" id="MathJax-Span-611"><span class="texatom" id="MathJax-Span-612" style=""><span class="mrow" id="MathJax-Span-613"><span class="mo" id="MathJax-Span-614" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span></span></span></span></span><span class="mi" id="MathJax-Span-615" style="font-family: MathJax_Math-italic;">k</span><span class="msubsup" id="MathJax-Span-616"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-617" style="font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.523em;"><span class="texatom" id="MathJax-Span-618"><span class="mrow" id="MathJax-Span-619"><span class="mn" id="MathJax-Span-620" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span class="mi" id="MathJax-Span-621" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mi" id="MathJax-Span-622" style="font-family: MathJax_Main; padding-left: 0.159em;">ln</span><span class="mo" id="MathJax-Span-623"></span><span class="mo" id="MathJax-Span-624" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-625" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-626" style="font-family: MathJax_Main;">)</span><span class="mstyle" id="MathJax-Span-627" style=""><span class="mrow" id="MathJax-Span-628"><span class="texatom" id="MathJax-Span-629" style=""><span class="mrow" id="MathJax-Span-630"><span class="mo" id="MathJax-Span-631" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mstyle scriptlevel="0"><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.2em" minsize="1.2em">(</mo></mrow></mstyle><mi>k</mi><msup><mi>p</mi><mrow class="MJX-TeXAtom-ORD"><mn>3</mn><mi>k</mi></mrow></msup><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mstyle scriptlevel="0"><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.2em" minsize="1.2em">)</mo></mrow></mstyle></math></span></span><script type="math/tex" id="MathJax-Element-87">O\big(kp^{3k}\ln(p)\big)</script> observations are required to accurately estimate this model. For the multi-round setting, we provide a near-optimal acquisition function that can be numerically optimized. We also explore several extensions of the design problem and finally validate our findings through simulations.</p>
            <p id="subjects-RL6d53a5jj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-RL6d53a5jj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-RL6d53a5jj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-RL6d53a5jj@OpenReview" onclick="foldPdfKimi('RL6d53a5jj@OpenReview', this)" class="hr hr-fold">
        </div><div id="4Z04wVQ9FY@OpenReview" class="panel paper" keywords="operators,neural,linearization,gaussian,currying,valued,pdes,function,luno,bayesian">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4Z04wVQ9FY" target="_blank" title="109/225"><span class="index notranslate">#109</span></a>
                <a id="title-4Z04wVQ9FY@OpenReview" class="title-link" href="/venue/4Z04wVQ9FY@OpenReview" target="_blank">Linearization Turns Neural Operators into Function-Valued Gaussian Processes</a>
                <a id="pdf-4Z04wVQ9FY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4Z04wVQ9FY@OpenReview', this)" data="https://openreview.net/pdf?id=4Z04wVQ9FY">[PDF<sup id="pdf-stars-4Z04wVQ9FY@OpenReview">2</sup>]</a>
                <a id="copy-4Z04wVQ9FY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4Z04wVQ9FY@OpenReview')">[Copy]</a>
                <a id="kimi-4Z04wVQ9FY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4Z04wVQ9FY@OpenReview', this)">[Kimi<sup id="kimi-stars-4Z04wVQ9FY@OpenReview">1</sup>]</a>
                <a id="rel-4Z04wVQ9FY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4Z04wVQ9FY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4Z04wVQ9FY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Emilia Magnani" target="_blank">Emilia Magnani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marvin Pförtner" target="_blank">Marvin Pförtner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tobias Weber" target="_blank">Tobias Weber</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Hennig" target="_blank">Philipp Hennig</a>
            </p>
            <p id="summary-4Z04wVQ9FY@OpenReview" class="summary">Neural operators generalize neural networks to learn mappings between function spaces from data. They are commonly used to learn solution operators of parametric partial differential equations (PDEs) or propagators of time-dependent PDEs. However, to make them useful in high-stakes simulation scenarios, their inherent predictive error must be quantified reliably. We introduce LUNO, a novel framework for approximate Bayesian uncertainty quantification in trained neural operators. Our approach leverages model linearization to push (Gaussian) weight-space uncertainty forward to the neural operator's predictions.We show that this can be interpreted as a probabilistic version of the concept of currying from functional programming, yielding a function-valued (Gaussian) random process belief. Our framework provides a practical yet theoretically sound way to apply existing Bayesian deep learning methods such as the linearized Laplace approximation to neural operators. Just as the underlying neural operator, our approach is resolution-agnostic by design.The method adds minimal prediction overhead, can be applied post-hoc without retraining the network, and scales to large models and datasets.We evaluate these aspects in a case study on Fourier neural operators.</p>
            <p id="subjects-4Z04wVQ9FY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4Z04wVQ9FY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4Z04wVQ9FY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4Z04wVQ9FY@OpenReview" onclick="foldPdfKimi('4Z04wVQ9FY@OpenReview', this)" class="hr hr-fold">
        </div><div id="IYOksPHJKT@OpenReview" class="panel paper" keywords="emotion,sepm,mllms,emotional,sharpening,visual,perception,multimodal,emotions,cues">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IYOksPHJKT" target="_blank" title="110/225"><span class="index notranslate">#110</span></a>
                <a id="title-IYOksPHJKT@OpenReview" class="title-link" href="/venue/IYOksPHJKT@OpenReview" target="_blank">Catch Your Emotion: Sharpening Emotion Perception in Multimodal Large Language Models</a>
                <a id="pdf-IYOksPHJKT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IYOksPHJKT@OpenReview', this)" data="https://openreview.net/pdf?id=IYOksPHJKT">[PDF<sup id="pdf-stars-IYOksPHJKT@OpenReview">9</sup>]</a>
                <a id="copy-IYOksPHJKT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IYOksPHJKT@OpenReview')">[Copy]</a>
                <a id="kimi-IYOksPHJKT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IYOksPHJKT@OpenReview', this)">[Kimi<sup id="kimi-stars-IYOksPHJKT@OpenReview">7</sup>]</a>
                <a id="rel-IYOksPHJKT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IYOksPHJKT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IYOksPHJKT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiyang Fang" target="_blank">Yiyang Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Liang" target="_blank">Jian Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenke Huang" target="_blank">Wenke Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Li" target="_blank">He Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kehua Su" target="_blank">Kehua Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mang Ye" target="_blank">Mang Ye</a>
            </p>
            <p id="summary-IYOksPHJKT@OpenReview" class="summary">Multimodal large language models (MLLMs) have achieved impressive progress in tasks such as visual question answering and visual understanding, but they still face significant challenges in emotional reasoning. Current methods to enhance emotional understanding typically rely on fine-tuning or manual annotations, which are resource-intensive and limit scalability. In this work, we focus on improving the ability of MLLMs to capture emotions during the inference phase. Specifically, MLLMs encounter two main issues: they struggle to distinguish between semantically similar emotions, leading to misclassification, and they are overwhelmed by redundant or irrelevant visual information, which distracts from key emotional cues. To address these, we propose Sharpening Emotion Perception in MLLMs (SEPM), which incorporates a Confidence-Guided Coarse-to-Fine Inference framework to refine emotion classification by guiding the model through simpler tasks. Additionally, SEPM employs Focus-on-Emotion Visual Augmentation to reduce visual redundancy by directing the attention of models to relevant emotional cues in images. Experimental results demonstrate that SEPM significantly improves MLLM performance on emotion-related tasks, providing a resource-efficient and scalable solution for emotion recognition.</p>
            <p id="subjects-IYOksPHJKT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IYOksPHJKT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IYOksPHJKT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IYOksPHJKT@OpenReview" onclick="foldPdfKimi('IYOksPHJKT@OpenReview', this)" class="hr hr-fold">
        </div><div id="Mlmpf4Izrj@OpenReview" class="panel paper" keywords="mcmc,vmap,fsms,chain,algorithms,tools,ups,vectorized,synchronization,vectorizing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Mlmpf4Izrj" target="_blank" title="111/225"><span class="index notranslate">#111</span></a>
                <a id="title-Mlmpf4Izrj@OpenReview" class="title-link" href="/venue/Mlmpf4Izrj@OpenReview" target="_blank">Efficiently Vectorized MCMC on Modern Accelerators</a>
                <a id="pdf-Mlmpf4Izrj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Mlmpf4Izrj@OpenReview', this)" data="https://openreview.net/pdf?id=Mlmpf4Izrj">[PDF<sup id="pdf-stars-Mlmpf4Izrj@OpenReview">1</sup>]</a>
                <a id="copy-Mlmpf4Izrj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Mlmpf4Izrj@OpenReview')">[Copy]</a>
                <a id="kimi-Mlmpf4Izrj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Mlmpf4Izrj@OpenReview', this)">[Kimi<sup id="kimi-stars-Mlmpf4Izrj@OpenReview">1</sup>]</a>
                <a id="rel-Mlmpf4Izrj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Mlmpf4Izrj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Mlmpf4Izrj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hugh Dance" target="_blank">Hugh Dance</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre Glaser" target="_blank">Pierre Glaser</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Orbanz" target="_blank">Peter Orbanz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan P. Adams" target="_blank">Ryan P. Adams</a>
            </p>
            <p id="summary-Mlmpf4Izrj@OpenReview" class="summary">With the advent of automatic vectorization tools (e.g., JAX's vmap), writing multi-chain MCMC algorithms is often now as simple as invoking those tools on single-chain code. Whilst convenient, for various MCMC algorithms this results in a synchronization problem---loosely speaking, at each iteration all chains running in parallel must wait until the last chain has finished drawing its sample. In this work, we show how to design single-chain MCMC algorithms in a way that avoids synchronization overheads when vectorizing with tools like vmap, by using the framework of finite state machines (FSMs). Using a simplified model, we derive an exact theoretical form of the obtainable speed-ups using our approach, and use it to make principled recommendations for optimal algorithm design. We implement several popular MCMC algorithms as FSMs, including Elliptical Slice Sampling, HMC-NUTS, and Delayed Rejection, demonstrating speed-ups of up to an order of magnitude in experiments.</p>
            <p id="subjects-Mlmpf4Izrj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Mlmpf4Izrj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mlmpf4Izrj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mlmpf4Izrj@OpenReview" onclick="foldPdfKimi('Mlmpf4Izrj@OpenReview', this)" class="hr hr-fold">
        </div><div id="5EbiopWH6e@OpenReview" class="panel paper" keywords="ssms,implicit,rnns,parallelization,expressivity,language,207b,languagemodels,transformers,convergence">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5EbiopWH6e" target="_blank" title="112/225"><span class="index notranslate">#112</span></a>
                <a id="title-5EbiopWH6e@OpenReview" class="title-link" href="/venue/5EbiopWH6e@OpenReview" target="_blank">Implicit Language Models are RNNs: Balancing Parallelization and Expressivity</a>
                <a id="pdf-5EbiopWH6e@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5EbiopWH6e@OpenReview', this)" data="https://openreview.net/pdf?id=5EbiopWH6e">[PDF<sup id="pdf-stars-5EbiopWH6e@OpenReview">1</sup>]</a>
                <a id="copy-5EbiopWH6e@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5EbiopWH6e@OpenReview')">[Copy]</a>
                <a id="kimi-5EbiopWH6e@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5EbiopWH6e@OpenReview', this)">[Kimi<sup id="kimi-stars-5EbiopWH6e@OpenReview">2</sup>]</a>
                <a id="rel-5EbiopWH6e@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5EbiopWH6e@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5EbiopWH6e@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Schoene" target="_blank">Mark Schoene</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Babak Rahmani" target="_blank">Babak Rahmani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heiner Kremer" target="_blank">Heiner Kremer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Falck" target="_blank">Fabian Falck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hitesh Ballani" target="_blank">Hitesh Ballani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jannes Gladrow" target="_blank">Jannes Gladrow</a>
            </p>
            <p id="summary-5EbiopWH6e@OpenReview" class="summary">State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks. Our code is publicly available at github.com/microsoft/implicit_languagemodels</p>
            <p id="subjects-5EbiopWH6e@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5EbiopWH6e@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5EbiopWH6e@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5EbiopWH6e@OpenReview" onclick="foldPdfKimi('5EbiopWH6e@OpenReview', this)" class="hr hr-fold">
        </div><div id="Qq5h78Eshy@OpenReview" class="panel paper" keywords="sgd,pass,sco,overfitting,eta,sqrt,sample,theta,smooth,schliserman">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Qq5h78Eshy" target="_blank" title="113/225"><span class="index notranslate">#113</span></a>
                <a id="title-Qq5h78Eshy@OpenReview" class="title-link" href="/venue/Qq5h78Eshy@OpenReview" target="_blank">Rapid Overfitting of Multi-Pass SGD in Stochastic Convex Optimization</a>
                <a id="pdf-Qq5h78Eshy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Qq5h78Eshy@OpenReview', this)" data="https://openreview.net/pdf?id=Qq5h78Eshy">[PDF<sup id="pdf-stars-Qq5h78Eshy@OpenReview">4</sup>]</a>
                <a id="copy-Qq5h78Eshy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Qq5h78Eshy@OpenReview')">[Copy]</a>
                <a id="kimi-Qq5h78Eshy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Qq5h78Eshy@OpenReview', this)">[Kimi<sup id="kimi-stars-Qq5h78Eshy@OpenReview">1</sup>]</a>
                <a id="rel-Qq5h78Eshy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Qq5h78Eshy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Qq5h78Eshy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shira Vansover-Hager" target="_blank">Shira Vansover-Hager</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomer Koren" target="_blank">Tomer Koren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roi Livni" target="_blank">Roi Livni</a>
            </p>
            <p id="summary-Qq5h78Eshy@OpenReview" class="summary">We study the out-of-sample performance of multi-pass stochastic gradient descent (SGD) in the fundamental stochastic convex optimization (SCO) model. While one-pass SGD is known to achieve an optimal <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-88-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0398;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-632" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.91em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-633"><span class="mi" id="MathJax-Span-634" style="font-family: MathJax_Main;">Θ</span><span class="mo" id="MathJax-Span-635" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-636" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-637"><span class="mrow" id="MathJax-Span-638"><span class="mo" id="MathJax-Span-639" style="font-family: MathJax_Main;">/</span></span></span><span class="msqrt" id="MathJax-Span-640"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-641"><span class="mi" id="MathJax-Span-642" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-643" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-88">\Theta(1/\sqrt{n})</script> excess population loss given a sample of size <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-89-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-644" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-645"><span class="mi" id="MathJax-Span-646" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-89">n</script>, much less is understood about the multi-pass version of the algorithm which is widely used in practice. Somewhat surprisingly, we show that in the general non-smooth case of SCO, just a few epochs of SGD can already hurt its out-of-sample performance significantly and lead to overfitting. In particular, using a step size <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-90-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B7;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0398;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-647" style="width: 7.034em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.73em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-648"><span class="mi" id="MathJax-Span-649" style="font-family: MathJax_Math-italic;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-650" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mi" id="MathJax-Span-651" style="font-family: MathJax_Main; padding-left: 0.263em;">Θ</span><span class="mo" id="MathJax-Span-652" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-653" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-654"><span class="mrow" id="MathJax-Span-655"><span class="mo" id="MathJax-Span-656" style="font-family: MathJax_Main;">/</span></span></span><span class="msqrt" id="MathJax-Span-657"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-658"><span class="mi" id="MathJax-Span-659" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-660" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>η</mi><mo>=</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-90">\eta = \Theta(1/\sqrt{n})</script>, which gives the optimal rate after one pass, can lead to population loss as large as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-91-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-661" style="width: 2.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.88em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-662"><span class="mi" id="MathJax-Span-663" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-664" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-665" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-666" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-91">\Omega(1)</script> after just one additional pass. More generally, we show that the population loss from the second pass onward is of the order <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-92-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0398;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B7;&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03B7;&lt;/mi&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-667" style="width: 9.326em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1007.66em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-668"><span class="mi" id="MathJax-Span-669" style="font-family: MathJax_Main;">Θ</span><span class="mo" id="MathJax-Span-670" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-671" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-672"><span class="mrow" id="MathJax-Span-673"><span class="mo" id="MathJax-Span-674" style="font-family: MathJax_Main;">/</span></span></span><span class="mo" id="MathJax-Span-675" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-676" style="font-family: MathJax_Math-italic;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-677" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-678" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-679" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-680" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msqrt" id="MathJax-Span-681"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-682"><span class="mi" id="MathJax-Span-683" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.68em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.003em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-684" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mi>η</mi><mi>T</mi><mo stretchy="false">)</mo><mo>+</mo><mi>η</mi><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-92">\Theta(1/(\eta T) + \eta \sqrt{T})</script>, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-93-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-685" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-686"><span class="mi" id="MathJax-Span-687" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-93">T</script> is the total number of steps. These results reveal a certain phase-transition in the out-of-sample behavior of SGD after the first epoch, as well as a sharp separation between the rates of overfitting in the smooth and non-smooth cases of SCO. Additionally, we extend our results to with-replacement SGD, proving that the same asymptotic bounds hold after <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-94-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-688" style="width: 5.211em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.22em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-689"><span class="mi" id="MathJax-Span-690" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-691" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-692" style="font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-693" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-694"></span><span class="mi" id="MathJax-Span-695" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">n</span><span class="mo" id="MathJax-Span-696" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-94">O(n \log n)</script> steps. Finally, we also prove a lower bound of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-95-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B7;&lt;/mi&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-697" style="width: 4.169em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.34em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-698"><span class="mi" id="MathJax-Span-699" style="font-family: MathJax_Main;">Ω</span><span class="mo" id="MathJax-Span-700" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-701" style="font-family: MathJax_Math-italic;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msqrt" id="MathJax-Span-702"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-703"><span class="mi" id="MathJax-Span-704" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-705" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>η</mi><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-95">\Omega(\eta \sqrt{n})</script> on the generalization gap of one-pass SGD in dimension <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-96-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-706" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.91em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-707"><span class="mi" id="MathJax-Span-708" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-709" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="texatom" id="MathJax-Span-710" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-711"><span class="texatom" id="MathJax-Span-712"><span class="mrow" id="MathJax-Span-713"><span class="munderover" id="MathJax-Span-714"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-715" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-716" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span></span></span><span class="mo" id="MathJax-Span-717" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-718" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-719" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow></mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-96">d = {\widetilde O}(n)</script>, improving on recent results of Koren et al. (2022) and Schliserman et al. (2024).</p>
            <p id="subjects-Qq5h78Eshy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Qq5h78Eshy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qq5h78Eshy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qq5h78Eshy@OpenReview" onclick="foldPdfKimi('Qq5h78Eshy@OpenReview', this)" class="hr hr-fold">
        </div><div id="WISfJyOA6M@OpenReview" class="panel paper" keywords="maxl,hsi,physpec,reconstruction,physical,hsis,rgb,physically,subspace,meta">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WISfJyOA6M" target="_blank" title="114/225"><span class="index notranslate">#114</span></a>
                <a id="title-WISfJyOA6M@OpenReview" class="title-link" href="/venue/WISfJyOA6M@OpenReview" target="_blank">PhySpec: Physically Consistent Spectral Reconstruction via Orthogonal Subspace Decomposition and Self-Supervised Meta-Auxiliary Learning</a>
                <a id="pdf-WISfJyOA6M@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WISfJyOA6M@OpenReview', this)" data="https://openreview.net/pdf?id=WISfJyOA6M">[PDF<sup id="pdf-stars-WISfJyOA6M@OpenReview">2</sup>]</a>
                <a id="copy-WISfJyOA6M@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WISfJyOA6M@OpenReview')">[Copy]</a>
                <a id="kimi-WISfJyOA6M@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WISfJyOA6M@OpenReview', this)">[Kimi<sup id="kimi-stars-WISfJyOA6M@OpenReview">1</sup>]</a>
                <a id="rel-WISfJyOA6M@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WISfJyOA6M@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WISfJyOA6M@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingxing Yang" target="_blank">Xingxing Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Chen" target="_blank">Jie Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaifeng Yang" target="_blank">Zaifeng Yang</a>
            </p>
            <p id="summary-WISfJyOA6M@OpenReview" class="summary">This paper presents a novel approach to hyperspectral image (HSI) reconstruction from RGB images, addressing fundamental limitations in existing learning-based methods from a physical perspective. We discuss and aim to address the ``colorimetric dilemma": failure to consistently reproduce ground-truth RGB from predicted HSI, thereby compromising physical integrity and reliability in practical applications. To tackle this issue, we propose PhySpec, a physically consistent framework for robust HSI reconstruction. Our approach fundamentally exploits the intrinsic physical relationship between HSIs and corresponding RGBs by employing orthogonal subspace decomposition, which enables explicit estimation of camera spectral sensitivity (CSS). This ensures that our reconstructed spectra align with well-established physical principles, enhancing their reliability and fidelity. Moreover, to efficiently use internal information from test samples, we propose a self-supervised meta-auxiliary learning (MAXL) strategy that rapidly adapts the trained parameters to unseen samples using only a few gradient descent steps at test time, while simultaneously constraining the generated HSIs to accurately recover ground-truth RGB values. Thus, MAXL reinforces the physical integrity of the reconstruction process. Extensive qualitative and quantitative evaluations validate the efficacy of our proposed framework, showing superior performance compared to SOTA methods.</p>
            <p id="subjects-WISfJyOA6M@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-WISfJyOA6M@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WISfJyOA6M@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WISfJyOA6M@OpenReview" onclick="foldPdfKimi('WISfJyOA6M@OpenReview', this)" class="hr hr-fold">
        </div><div id="WR0ahlhOoy@OpenReview" class="panel paper" keywords="adversarial,simplices,adversaries,clean,vlm,shot,vision,intermediate,decision,zero">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WR0ahlhOoy" target="_blank" title="115/225"><span class="index notranslate">#115</span></a>
                <a id="title-WR0ahlhOoy@OpenReview" class="title-link" href="/venue/WR0ahlhOoy@OpenReview" target="_blank">Improving Zero-Shot Adversarial Robustness in Vision-Language Models by Closed-form Alignment of Adversarial Path Simplices</a>
                <a id="pdf-WR0ahlhOoy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WR0ahlhOoy@OpenReview', this)" data="https://openreview.net/pdf?id=WR0ahlhOoy">[PDF<sup id="pdf-stars-WR0ahlhOoy@OpenReview">2</sup>]</a>
                <a id="copy-WR0ahlhOoy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WR0ahlhOoy@OpenReview')">[Copy]</a>
                <a id="kimi-WR0ahlhOoy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WR0ahlhOoy@OpenReview', this)">[Kimi<sup id="kimi-stars-WR0ahlhOoy@OpenReview">2</sup>]</a>
                <a id="rel-WR0ahlhOoy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WR0ahlhOoy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WR0ahlhOoy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junhao Dong" target="_blank">Junhao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Piotr Koniusz" target="_blank">Piotr Koniusz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Zhang" target="_blank">Yifei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhu" target="_blank">Hao Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiming Liu" target="_blank">Weiming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinghua Qu" target="_blank">Xinghua Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yew Soon ONG" target="_blank">Yew Soon ONG</a>
            </p>
            <p id="summary-WR0ahlhOoy@OpenReview" class="summary">Vision-Language Models (VLMs) such as CLIP excel at zero-shot classification due to large-scale pre-training but are vulnerable to adversarial examples. Adversarial fine-tuning robustifies zero-shot models by aligning prediction scores of individual adversaries with their clean counterparts, which typically overlooks intermediate adversarial samples along the adversarial trajectory crossing the decision boundary. Such intermediate adversaries and their vicinity produce informative representations capturing the decision boundary in detail. They can be improved by sampling adversarial candidates from simplices formed by joining two consecutive vertices on the adversarial trajectory and their clean counterpart. However, sampling simplices for adversaries is very costly. To train robust VLM, we overcome these limitations by Taylor expansion and formulating an upper-bound of alignment loss that depends on the Jacobian/Hessian obtained at clean samples. As regions between clean and intermediate adversarial samples capture a larger decision landscape, we robustify VLM by plausible adversaries from simplices by our closed-form formulation equivalent to infinite uniform sampling of the simplex. We obtain state-of-the-art robustness across 15 datasets and diverse vision-language tasks.</p>
            <p id="subjects-WR0ahlhOoy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-WR0ahlhOoy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WR0ahlhOoy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WR0ahlhOoy@OpenReview" onclick="foldPdfKimi('WR0ahlhOoy@OpenReview', this)" class="hr hr-fold">
        </div><div id="UKR3HsAFkC@OpenReview" class="panel paper" keywords="row,stochastic,decentralized,diag,optimization,speedup,pull,attain,achieving,lower">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UKR3HsAFkC" target="_blank" title="116/225"><span class="index notranslate">#116</span></a>
                <a id="title-UKR3HsAFkC@OpenReview" class="title-link" href="/venue/UKR3HsAFkC@OpenReview" target="_blank">Achieving Linear Speedup and Near-Optimal Complexity for Decentralized Optimization over Row-stochastic Networks</a>
                <a id="pdf-UKR3HsAFkC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UKR3HsAFkC@OpenReview', this)" data="https://openreview.net/pdf?id=UKR3HsAFkC">[PDF<sup id="pdf-stars-UKR3HsAFkC@OpenReview">3</sup>]</a>
                <a id="copy-UKR3HsAFkC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UKR3HsAFkC@OpenReview')">[Copy]</a>
                <a id="kimi-UKR3HsAFkC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UKR3HsAFkC@OpenReview', this)">[Kimi<sup id="kimi-stars-UKR3HsAFkC@OpenReview">1</sup>]</a>
                <a id="rel-UKR3HsAFkC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UKR3HsAFkC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UKR3HsAFkC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liyuan Liang" target="_blank">Liyuan Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyi Chen" target="_blank">Xinyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gan Luo" target="_blank">Gan Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Yuan" target="_blank">Kun Yuan</a>
            </p>
            <p id="summary-UKR3HsAFkC@OpenReview" class="summary">A key challenge in decentralized optimization is determining the optimal convergence rate and designing algorithms to achieve it. While this problem has been extensively addressed for doubly-stochastic and column-stochastic mixing matrices, the row-stochastic scenario remains unexplored. This paper bridges this gap by introducing effective metrics to capture the influence of row-stochastic mixing matrices and establishing the first convergence lower bound for decentralized learning over row-stochastic networks. However, existing algorithms fail to attain this lower bound due to two key issues: deviation in the descent direction caused by the adapted gradient tracking (GT) and instability introduced by the Pull-Diag protocol. To address descent deviation, we propose a novel analysis framework demonstrating that Pull-Diag-GT achieves linear speedup—the first such result for row-stochastic decentralized optimization. Moreover, by incorporating a multi-step gossip (MG) protocol, we resolve the instability issue and attain the lower bound, achieving near-optimal complexity for decentralized optimization over row-stochastic networks.</p>
            <p id="subjects-UKR3HsAFkC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-UKR3HsAFkC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UKR3HsAFkC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UKR3HsAFkC@OpenReview" onclick="foldPdfKimi('UKR3HsAFkC@OpenReview', this)" class="hr hr-fold">
        </div><div id="WbP2OwMULq@OpenReview" class="panel paper" keywords="healthgpt,comprehension,medical,generation,heterogeneous,language,dcdmllm,vision,adaptation,visual">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WbP2OwMULq" target="_blank" title="117/225"><span class="index notranslate">#117</span></a>
                <a id="title-WbP2OwMULq@OpenReview" class="title-link" href="/venue/WbP2OwMULq@OpenReview" target="_blank">HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation</a>
                <a id="pdf-WbP2OwMULq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WbP2OwMULq@OpenReview', this)" data="https://openreview.net/pdf?id=WbP2OwMULq">[PDF<sup id="pdf-stars-WbP2OwMULq@OpenReview">4</sup>]</a>
                <a id="copy-WbP2OwMULq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WbP2OwMULq@OpenReview')">[Copy]</a>
                <a id="kimi-WbP2OwMULq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WbP2OwMULq@OpenReview', this)">[Kimi<sup id="kimi-stars-WbP2OwMULq@OpenReview">2</sup>]</a>
                <a id="rel-WbP2OwMULq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WbP2OwMULq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WbP2OwMULq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianwei Lin" target="_blank">Tianwei Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqiao Zhang" target="_blank">Wenqiao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sijing Li" target="_blank">Sijing Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqian Yuan" target="_blank">Yuqian Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Binhe Yu" target="_blank">Binhe Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyuan Li" target="_blank">Haoyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanggui He" target="_blank">Wanggui He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Jiang" target="_blank">Hao Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengze Li" target="_blank">Mengze Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song xiaohui" target="_blank">Song xiaohui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siliang Tang" target="_blank">Siliang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Xiao" target="_blank">Jun Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Lin" target="_blank">Hui Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueting Zhuang" target="_blank">Yueting Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beng Chin Ooi" target="_blank">Beng Chin Ooi</a>
            </p>
            <p id="summary-WbP2OwMULq@OpenReview" class="summary">We present **HealthGPT**, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation **(H-LoRA)** technique, which is complemented by a tailored hierarchical visual perception **(HVP)** approach and a three-stage learning strategy **(TLS)**. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called **VL-Health**. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.</p>
            <p id="subjects-WbP2OwMULq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-WbP2OwMULq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WbP2OwMULq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WbP2OwMULq@OpenReview" onclick="foldPdfKimi('WbP2OwMULq@OpenReview', this)" class="hr hr-fold">
        </div><div id="WxY61MmHYo@OpenReview" class="panel paper" keywords="primate,ventral,scaling,alignment,brain,stream,visual,models,datasets,laws">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WxY61MmHYo" target="_blank" title="118/225"><span class="index notranslate">#118</span></a>
                <a id="title-WxY61MmHYo@OpenReview" class="title-link" href="/venue/WxY61MmHYo@OpenReview" target="_blank">Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream</a>
                <a id="pdf-WxY61MmHYo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WxY61MmHYo@OpenReview', this)" data="https://openreview.net/pdf?id=WxY61MmHYo">[PDF<sup id="pdf-stars-WxY61MmHYo@OpenReview">2</sup>]</a>
                <a id="copy-WxY61MmHYo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WxY61MmHYo@OpenReview')">[Copy]</a>
                <a id="kimi-WxY61MmHYo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WxY61MmHYo@OpenReview', this)">[Kimi<sup id="kimi-stars-WxY61MmHYo@OpenReview">1</sup>]</a>
                <a id="rel-WxY61MmHYo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WxY61MmHYo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WxY61MmHYo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Abdulkadir Gokce" target="_blank">Abdulkadir Gokce</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Schrimpf" target="_blank">Martin Schrimpf</a>
            </p>
            <p id="summary-WxY61MmHYo@OpenReview" class="summary">When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition behaviors and neural response patterns in the primate brain. While recent machine learning advances suggest that scaling compute, model size, and dataset size improves task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate visual ventral stream by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and behavior. We find that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive biases and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Our results suggest that while scaling current architectures and datasets might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream, highlighting the need for novel strategies in building brain models.</p>
            <p id="subjects-WxY61MmHYo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-WxY61MmHYo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WxY61MmHYo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WxY61MmHYo@OpenReview" onclick="foldPdfKimi('WxY61MmHYo@OpenReview', this)" class="hr hr-fold">
        </div><div id="XXFBqfwnUp@OpenReview" class="panel paper" keywords="string,encodings,robotics,position,textbf,controllers,ropes,token,translationally,rotary">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XXFBqfwnUp" target="_blank" title="119/225"><span class="index notranslate">#119</span></a>
                <a id="title-XXFBqfwnUp@OpenReview" class="title-link" href="/venue/XXFBqfwnUp@OpenReview" target="_blank">Learning the RoPEs: Better 2D and 3D Position Encodings with STRING</a>
                <a id="pdf-XXFBqfwnUp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XXFBqfwnUp@OpenReview', this)" data="https://openreview.net/pdf?id=XXFBqfwnUp">[PDF<sup id="pdf-stars-XXFBqfwnUp@OpenReview">3</sup>]</a>
                <a id="copy-XXFBqfwnUp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XXFBqfwnUp@OpenReview')">[Copy]</a>
                <a id="kimi-XXFBqfwnUp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XXFBqfwnUp@OpenReview', this)">[Kimi<sup id="kimi-stars-XXFBqfwnUp@OpenReview">1</sup>]</a>
                <a id="rel-XXFBqfwnUp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XXFBqfwnUp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XXFBqfwnUp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Connor Schenck" target="_blank">Connor Schenck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Isaac Reid" target="_blank">Isaac Reid</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mithun Jacob" target="_blank">Mithun Jacob</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Bewley" target="_blank">Alex Bewley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua Ainslie" target="_blank">Joshua Ainslie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Rendleman" target="_blank">David Rendleman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deepali Jain" target="_blank">Deepali Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohit Sharma" target="_blank">Mohit Sharma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kumar Avinava Dubey" target="_blank">Kumar Avinava Dubey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayzaan Wahid" target="_blank">Ayzaan Wahid</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sumeet Singh" target="_blank">Sumeet Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=René Wagner" target="_blank">René Wagner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianli Ding" target="_blank">Tianli Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuyuan Fu" target="_blank">Chuyuan Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arunkumar Byravan" target="_blank">Arunkumar Byravan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob J Varley" target="_blank">Jacob J Varley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexey Gritsenko" target="_blank">Alexey Gritsenko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthias Minderer" target="_blank">Matthias Minderer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dmitry Kalashnikov" target="_blank">Dmitry Kalashnikov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Tompson" target="_blank">Jonathan Tompson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vikas Sindhwani" target="_blank">Vikas Sindhwani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Krzysztof Choromanski" target="_blank">Krzysztof Choromanski</a>
            </p>
            <p id="summary-XXFBqfwnUp@OpenReview" class="summary">We introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-97-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;STRING&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-720" style="width: 5.471em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.534em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1004.48em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-721"><span class="texatom" id="MathJax-Span-722"><span class="mrow" id="MathJax-Span-723"><span class="mtext" id="MathJax-Span-724" style="font-family: MathJax_Main-bold;">STRING</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">STRING</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-97">\textbf{STRING}</script>: Separable Translationally Invariant Position Encodings. STRING extends Rotary Position Encodings, a recently proposed and widely used algorithm in large language models, via a unifying theoretical framework. Importantly, STRING still provides <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-98-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;exact&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-725" style="width: 3.232em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1002.61em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-726"><span class="texatom" id="MathJax-Span-727"><span class="mrow" id="MathJax-Span-728"><span class="mtext" id="MathJax-Span-729" style="font-family: MathJax_Main-bold;">exact</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">exact</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-98">\textbf{exact}</script> translation invariance, including token coordinates of arbitrary dimensionality, whilst maintaining a low computational footprint. These properties are especially important in robotics, where efficient 3D token representation is key. We integrate STRING into Vision Transformers with RGB(-D) inputs (color plus optional depth), showing substantial gains, e.g. in open-vocabulary object detection and for robotics controllers.We complement our experiments with a rigorous mathematical analysis, proving the universality of our methods. Videos of STRING-based robotics controllers can be found here: https://sites.google.com/view/string-robotics.</p>
            <p id="subjects-XXFBqfwnUp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-XXFBqfwnUp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XXFBqfwnUp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XXFBqfwnUp@OpenReview" onclick="foldPdfKimi('XXFBqfwnUp@OpenReview', this)" class="hr hr-fold">
        </div><div id="Y19ngWhN0b@OpenReview" class="panel paper" keywords="contrastive,supervised,wsc,imprecise,speechless,10308,learning,weakly,labels,similarity">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Y19ngWhN0b" target="_blank" title="120/225"><span class="index notranslate">#120</span></a>
                <a id="title-Y19ngWhN0b@OpenReview" class="title-link" href="/venue/Y19ngWhN0b@OpenReview" target="_blank">Weakly-Supervised Contrastive Learning for Imprecise Class Labels</a>
                <a id="pdf-Y19ngWhN0b@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Y19ngWhN0b@OpenReview', this)" data="https://openreview.net/pdf?id=Y19ngWhN0b">[PDF<sup id="pdf-stars-Y19ngWhN0b@OpenReview">7</sup>]</a>
                <a id="copy-Y19ngWhN0b@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Y19ngWhN0b@OpenReview')">[Copy]</a>
                <a id="kimi-Y19ngWhN0b@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Y19ngWhN0b@OpenReview', this)">[Kimi<sup id="kimi-stars-Y19ngWhN0b@OpenReview">3</sup>]</a>
                <a id="rel-Y19ngWhN0b@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Y19ngWhN0b@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Y19ngWhN0b@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zi-Hao Zhou" target="_blank">Zi-Hao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun-Jie Wang" target="_blank">Jun-Jie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Wei" target="_blank">Tong Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min-Ling Zhang" target="_blank">Min-Ling Zhang</a>
            </p>
            <p id="summary-Y19ngWhN0b@OpenReview" class="summary">Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at [https://github.com/Speechless-10308/WSC](https://github.com/Speechless-10308/WSC).</p>
            <p id="subjects-Y19ngWhN0b@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Y19ngWhN0b@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Y19ngWhN0b@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Y19ngWhN0b@OpenReview" onclick="foldPdfKimi('Y19ngWhN0b@OpenReview', this)" class="hr hr-fold">
        </div><div id="YucuAuXMpT@OpenReview" class="panel paper" keywords="representational,similarity,functional,networks,representations,analysable,connectionism,dissociation,neural,analytical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YucuAuXMpT" target="_blank" title="121/225"><span class="index notranslate">#121</span></a>
                <a id="title-YucuAuXMpT@OpenReview" class="title-link" href="/venue/YucuAuXMpT@OpenReview" target="_blank">Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks</a>
                <a id="pdf-YucuAuXMpT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YucuAuXMpT@OpenReview', this)" data="https://openreview.net/pdf?id=YucuAuXMpT">[PDF<sup id="pdf-stars-YucuAuXMpT@OpenReview">2</sup>]</a>
                <a id="copy-YucuAuXMpT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YucuAuXMpT@OpenReview')">[Copy]</a>
                <a id="kimi-YucuAuXMpT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YucuAuXMpT@OpenReview', this)">[Kimi<sup id="kimi-stars-YucuAuXMpT@OpenReview">1</sup>]</a>
                <a id="rel-YucuAuXMpT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YucuAuXMpT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YucuAuXMpT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lukas Braun" target="_blank">Lukas Braun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erin Grant" target="_blank">Erin Grant</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Saxe" target="_blank">Andrew Saxe</a>
            </p>
            <p id="summary-YucuAuXMpT@OpenReview" class="summary">A foundational principle of connectionism is that perception, action, and cognition emerge from parallel computations among simple, interconnected units that generate and rely on neural representations. Accordingly, researchers employ multivariate pattern analysis to decode and compare the neural codes of artificial and biological networks, aiming to uncover their functions. However, there is limited analytical understanding of how a network’s representation and function relate, despite this being essential to any quantitative notion of underlying function or functional similarity. We address this question using fully analysable two-layer linear networks and numerical simulations in nonlinear networks. We find that function and representation are dissociated, allowing representational similarity without functional similarity and vice versa. Further, we show that neither robustness to input noise nor the level of generalisation error constrain representations to the task. In contrast, networks robust to parameter noise have limited representational flexibility and must employ task-specific representations. Our findings suggest that representational alignment reflects computational advantages beyond functional alignment alone, with significant implications for interpreting and comparing the representations of connectionist systems</p>
            <p id="subjects-YucuAuXMpT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-YucuAuXMpT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YucuAuXMpT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YucuAuXMpT@OpenReview" onclick="foldPdfKimi('YucuAuXMpT@OpenReview', this)" class="hr hr-fold">
        </div><div id="Yv416IYTFp@OpenReview" class="panel paper" keywords="pass,private,attributes,protection,services,substitution,soundly,substantiates,data,inadvertently">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Yv416IYTFp" target="_blank" title="122/225"><span class="index notranslate">#122</span></a>
                <a id="title-Yv416IYTFp@OpenReview" class="title-link" href="/venue/Yv416IYTFp@OpenReview" target="_blank">PASS: Private Attributes Protection with Stochastic Data Substitution</a>
                <a id="pdf-Yv416IYTFp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Yv416IYTFp@OpenReview', this)" data="https://openreview.net/pdf?id=Yv416IYTFp">[PDF<sup id="pdf-stars-Yv416IYTFp@OpenReview">1</sup>]</a>
                <a id="copy-Yv416IYTFp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Yv416IYTFp@OpenReview')">[Copy]</a>
                <a id="kimi-Yv416IYTFp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Yv416IYTFp@OpenReview', this)">[Kimi<sup id="kimi-stars-Yv416IYTFp@OpenReview"></sup>]</a>
                <a id="rel-Yv416IYTFp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Yv416IYTFp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Yv416IYTFp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhuo Chen" target="_blank">Yizhuo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun-Fu (Richard) Chen" target="_blank">Chun-Fu (Richard) Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hsiang Hsu" target="_blank">Hsiang Hsu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohan Hu" target="_blank">Shaohan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tarek Abdelzaher" target="_blank">Tarek Abdelzaher</a>
            </p>
            <p id="summary-Yv416IYTFp@OpenReview" class="summary">The growing Machine Learning (ML) services require extensive collections of user data, which may inadvertently include people's private information irrelevant to the services. Various studies have been proposed to protect private attributes by removing them from the data while maintaining the utilities of the data for downstream tasks. Nevertheless, as we theoretically and empirically show in the paper, these methods reveal severe vulnerability because of a common weakness rooted in their adversarial training based strategies. To overcome this limitation, we propose a novel approach, PASS, designed to stochastically substitute the original sample with another one according to certain probabilities, which is trained with a novel loss function soundly derived from information-theoretic objective defined for utility-preserving private attributes protection. The comprehensive evaluation of PASS on various datasets of different modalities, including facial images, human activity sensory signals, and voice recording datasets, substantiates PASS's effectiveness and generalizability.</p>
            <p id="subjects-Yv416IYTFp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Yv416IYTFp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yv416IYTFp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yv416IYTFp@OpenReview" onclick="foldPdfKimi('Yv416IYTFp@OpenReview', this)" class="hr hr-fold">
        </div><div id="Zm2M92TZyO@OpenReview" class="panel paper" keywords="agdiff,glad,graph,anomalous,pseudo,diffusion,anomalies,graphs,generating,normality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Zm2M92TZyO" target="_blank" title="123/225"><span class="index notranslate">#123</span></a>
                <a id="title-Zm2M92TZyO@OpenReview" class="title-link" href="/venue/Zm2M92TZyO@OpenReview" target="_blank">Leveraging Diffusion Model as Pseudo-Anomalous Graph Generator for Graph-Level Anomaly Detection</a>
                <a id="pdf-Zm2M92TZyO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Zm2M92TZyO@OpenReview', this)" data="https://openreview.net/pdf?id=Zm2M92TZyO">[PDF<sup id="pdf-stars-Zm2M92TZyO@OpenReview">2</sup>]</a>
                <a id="copy-Zm2M92TZyO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Zm2M92TZyO@OpenReview')">[Copy]</a>
                <a id="kimi-Zm2M92TZyO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Zm2M92TZyO@OpenReview', this)">[Kimi<sup id="kimi-stars-Zm2M92TZyO@OpenReview">2</sup>]</a>
                <a id="rel-Zm2M92TZyO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Zm2M92TZyO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Zm2M92TZyO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinyu Cai" target="_blank">Jinyu Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhe Zhang" target="_blank">Yunhe Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fusheng Liu" target="_blank">Fusheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=See-Kiong Ng" target="_blank">See-Kiong Ng</a>
            </p>
            <p id="summary-Zm2M92TZyO@OpenReview" class="summary">A fundamental challenge in graph-level anomaly detection (GLAD) is the scarcity of anomalous graph data, as the training dataset typically contains only normal graphs or very few anomalies. This imbalance hinders the development of robust detection models. In this paper, we propose **A**nomalous **G**raph **Diff**usion (AGDiff), a framework that explores the potential of diffusion models in generating pseudo-anomalous graphs for GLAD. Unlike existing diffusion-based methods that focus on modeling data normality, AGDiff leverages the latent diffusion framework to incorporate subtle perturbations into graph representations, thereby generating pseudo-anomalous graphs that closely resemble normal ones. By jointly training a classifier to distinguish these generated graph anomalies from normal graphs, AGDiff learns more discriminative decision boundaries. The shift from solely modeling normality to explicitly generating and learning from pseudo graph anomalies enables AGDiff to effectively identify complex anomalous patterns that other approaches might overlook. Comprehensive experimental results demonstrate that the proposed AGDiff significantly outperforms several state-of-the-art GLAD baselines.</p>
            <p id="subjects-Zm2M92TZyO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Zm2M92TZyO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zm2M92TZyO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zm2M92TZyO@OpenReview" onclick="foldPdfKimi('Zm2M92TZyO@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZrhGq664om@OpenReview" class="panel paper" keywords="nc1,loss,landscape,unconstrained,gradient,mean,neural,collapse,layer,occurrence">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZrhGq664om" target="_blank" title="124/225"><span class="index notranslate">#124</span></a>
                <a id="title-ZrhGq664om@OpenReview" class="title-link" href="/venue/ZrhGq664om@OpenReview" target="_blank">Neural Collapse Beyond the Unconstrained Features Model: Landscape, Dynamics, and Generalization in the Mean-Field Regime</a>
                <a id="pdf-ZrhGq664om@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZrhGq664om@OpenReview', this)" data="https://openreview.net/pdf?id=ZrhGq664om">[PDF<sup id="pdf-stars-ZrhGq664om@OpenReview">2</sup>]</a>
                <a id="copy-ZrhGq664om@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZrhGq664om@OpenReview')">[Copy]</a>
                <a id="kimi-ZrhGq664om@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZrhGq664om@OpenReview', this)">[Kimi<sup id="kimi-stars-ZrhGq664om@OpenReview">1</sup>]</a>
                <a id="rel-ZrhGq664om@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZrhGq664om@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZrhGq664om@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Diyuan Wu" target="_blank">Diyuan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Mondelli" target="_blank">Marco Mondelli</a>
            </p>
            <p id="summary-ZrhGq664om@OpenReview" class="summary">Neural Collapse is a phenomenon where the last-layer representations of a well-trained neural network converge to a highly structured geometry. In this paper, we focus on its first (and most basic) property, known as NC1: the within-class variability vanishes. While prior theoretical studies establish the occurrence of NC1 via the data-agnostic unconstrained features model, our work adopts a data-specific perspective, analyzing NC1 in a three-layer neural network, with the first two layers operating in the mean-field regime and followed by a linear layer. In particular, we establish a fundamental connection between NC1 and the loss landscape: we prove that points with small empirical loss and gradient norm (thus, close to being stationary) approximately satisfy NC1, and the closeness to NC1 is controlled by the residual loss and gradient norm. We then show that (i) gradient flow on the mean squared error converges to NC1 solutions with small empirical loss, and (ii) for well-separated data distributions, both NC1 and vanishing test loss are achieved simultaneously. This aligns with the empirical observation that NC1 emerges during training while models attain near-zero test error. Overall, our results demonstrate that NC1 arises from gradient training due to the properties of the loss landscape, and they show the co-occurrence of NC1 and small test error for certain data distributions.</p>
            <p id="subjects-ZrhGq664om@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ZrhGq664om@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZrhGq664om@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZrhGq664om@OpenReview" onclick="foldPdfKimi('ZrhGq664om@OpenReview', this)" class="hr hr-fold">
        </div><div id="a6Cagkpmgz@OpenReview" class="panel paper" keywords="moreau,stochastic,algorithms,primal,dual,constraints,envelope,smoothed,nonconvex,inequality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=a6Cagkpmgz" target="_blank" title="125/225"><span class="index notranslate">#125</span></a>
                <a id="title-a6Cagkpmgz@OpenReview" class="title-link" href="/venue/a6Cagkpmgz@OpenReview" target="_blank">Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints</a>
                <a id="pdf-a6Cagkpmgz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('a6Cagkpmgz@OpenReview', this)" data="https://openreview.net/pdf?id=a6Cagkpmgz">[PDF<sup id="pdf-stars-a6Cagkpmgz@OpenReview">3</sup>]</a>
                <a id="copy-a6Cagkpmgz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('a6Cagkpmgz@OpenReview')">[Copy]</a>
                <a id="kimi-a6Cagkpmgz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('a6Cagkpmgz@OpenReview', this)">[Kimi<sup id="kimi-stars-a6Cagkpmgz@OpenReview">1</sup>]</a>
                <a id="rel-a6Cagkpmgz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('a6Cagkpmgz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-a6Cagkpmgz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruichuan Huang" target="_blank">Ruichuan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Zhang" target="_blank">Jiawei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ahmet Alacaoglu" target="_blank">Ahmet Alacaoglu</a>
            </p>
            <p id="summary-a6Cagkpmgz@OpenReview" class="summary">We propose smoothed primal-dual algorithms for solving stochastic nonconvex optimization problems with linear \emph{inequality} constraints. Our algorithms are single-loop and only require a single (or two) samples of stochastic gradients at each iteration. A defining feature of our algorithm is that it is based on an inexact gradient descent framework for the Moreau envelope, where the gradient of the Moreau envelope is estimated using one step of a stochastic primal-dual (linearized) augmented Lagrangian algorithm. To handle inequality constraints and stochasticity, we combine the recently established global error bounds in constrained optimization with a Moreau envelope-based analysis of stochastic proximal algorithms. We establish the optimal (in their respective cases) <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-99-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-730" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.87em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-731"><span class="mi" id="MathJax-Span-732" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-733" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-734"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-735" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-736"><span class="mrow" id="MathJax-Span-737"><span class="mo" id="MathJax-Span-738" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-739" style="font-size: 70.7%; font-family: MathJax_Main;">4</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-740" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>4</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-99">O(\varepsilon^{-4})</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-100-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-741" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.87em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-742"><span class="mi" id="MathJax-Span-743" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-744" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-745"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-746" style="font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="texatom" id="MathJax-Span-747"><span class="mrow" id="MathJax-Span-748"><span class="mo" id="MathJax-Span-749" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-750" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-751" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>ε</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>3</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-100">O(\varepsilon^{-3})</script> sample complexity guarantees for our algorithms and provide extensions to stochastic linear constraints. Unlike existing methods, iterations of our algorithms are free of subproblems, large batch sizes or increasing penalty parameters in their iterations and they use dual variable updates to ensure feasibility.</p>
            <p id="subjects-a6Cagkpmgz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-a6Cagkpmgz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-a6Cagkpmgz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-a6Cagkpmgz@OpenReview" onclick="foldPdfKimi('a6Cagkpmgz@OpenReview', this)" class="hr hr-fold">
        </div><div id="a7UM5c1CEa@OpenReview" class="panel paper" keywords="training,pre,transfer,unsupervised,sample,complexity,learning,provable,settings,grants">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=a7UM5c1CEa" target="_blank" title="126/225"><span class="index notranslate">#126</span></a>
                <a id="title-a7UM5c1CEa@OpenReview" class="title-link" href="/venue/a7UM5c1CEa@OpenReview" target="_blank">Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models</a>
                <a id="pdf-a7UM5c1CEa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('a7UM5c1CEa@OpenReview', this)" data="https://openreview.net/pdf?id=a7UM5c1CEa">[PDF<sup id="pdf-stars-a7UM5c1CEa@OpenReview">1</sup>]</a>
                <a id="copy-a7UM5c1CEa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('a7UM5c1CEa@OpenReview')">[Copy]</a>
                <a id="kimi-a7UM5c1CEa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('a7UM5c1CEa@OpenReview', this)">[Kimi<sup id="kimi-stars-a7UM5c1CEa@OpenReview">2</sup>]</a>
                <a id="rel-a7UM5c1CEa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('a7UM5c1CEa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-a7UM5c1CEa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Taj Jones-McCormick" target="_blank">Taj Jones-McCormick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aukosh Jagannath" target="_blank">Aukosh Jagannath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Subhabrata Sen" target="_blank">Subhabrata Sen</a>
            </p>
            <p id="summary-a7UM5c1CEa@OpenReview" class="summary">Unsupervised pre-training and transfer learning are commonly used techniques to initialize training algorithms for neural networks, particularly in settings with limited labeled data. In this paper, we study the effects of unsupervised pre-training and transfer learning on the sample complexity of high-dimensional supervised learning. Specifically, we consider the problem of training a single-layer neural network via online stochastic gradient descent. We establish that pre-training and transfer learning (under concept shift) reduce sample complexity by polynomial factors (in the dimension) under very general assumptions. We also uncover some surprising settings where pre-training grants exponential improvement over random initialization in terms of sample complexity.</p>
            <p id="subjects-a7UM5c1CEa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-a7UM5c1CEa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-a7UM5c1CEa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-a7UM5c1CEa@OpenReview" onclick="foldPdfKimi('a7UM5c1CEa@OpenReview', this)" class="hr hr-fold">
        </div><div id="aFNq67ilos@OpenReview" class="panel paper" keywords="training,attention,context,descent,dynamics,linear,icl,merged,query,parametrization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aFNq67ilos" target="_blank" title="127/225"><span class="index notranslate">#127</span></a>
                <a id="title-aFNq67ilos@OpenReview" class="title-link" href="/venue/aFNq67ilos@OpenReview" target="_blank">Training Dynamics of In-Context Learning in Linear Attention</a>
                <a id="pdf-aFNq67ilos@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aFNq67ilos@OpenReview', this)" data="https://openreview.net/pdf?id=aFNq67ilos">[PDF<sup id="pdf-stars-aFNq67ilos@OpenReview">6</sup>]</a>
                <a id="copy-aFNq67ilos@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aFNq67ilos@OpenReview')">[Copy]</a>
                <a id="kimi-aFNq67ilos@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aFNq67ilos@OpenReview', this)">[Kimi<sup id="kimi-stars-aFNq67ilos@OpenReview">3</sup>]</a>
                <a id="rel-aFNq67ilos@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aFNq67ilos@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aFNq67ilos@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yedi Zhang" target="_blank">Yedi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaditya Singh" target="_blank">Aaditya Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Latham" target="_blank">Peter Latham</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Saxe" target="_blank">Andrew Saxe</a>
            </p>
            <p id="summary-aFNq67ilos@OpenReview" class="summary">While attention-based models have demonstrated the remarkable ability of in-context learning (ICL), the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show that the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show that the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we provide a theoretical description of how ICL abilities evolve during gradient descent training of linear attention, revealing abrupt acquisition or progressive improvements depending on how the key and query are parametrized.</p>
            <p id="subjects-aFNq67ilos@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-aFNq67ilos@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aFNq67ilos@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aFNq67ilos@OpenReview" onclick="foldPdfKimi('aFNq67ilos@OpenReview', this)" class="hr hr-fold">
        </div><div id="aJeLhLcsh0@OpenReview" class="panel paper" keywords="code,turn,rewards,feedback,generation,execution,multi,step,single,recoverable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aJeLhLcsh0" target="_blank" title="128/225"><span class="index notranslate">#128</span></a>
                <a id="title-aJeLhLcsh0@OpenReview" class="title-link" href="/venue/aJeLhLcsh0@OpenReview" target="_blank">Multi-Turn Code Generation Through Single-Step Rewards</a>
                <a id="pdf-aJeLhLcsh0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aJeLhLcsh0@OpenReview', this)" data="https://openreview.net/pdf?id=aJeLhLcsh0">[PDF<sup id="pdf-stars-aJeLhLcsh0@OpenReview">3</sup>]</a>
                <a id="copy-aJeLhLcsh0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aJeLhLcsh0@OpenReview')">[Copy]</a>
                <a id="kimi-aJeLhLcsh0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aJeLhLcsh0@OpenReview', this)">[Kimi<sup id="kimi-stars-aJeLhLcsh0@OpenReview">5</sup>]</a>
                <a id="rel-aJeLhLcsh0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aJeLhLcsh0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aJeLhLcsh0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Arnav Kumar Jain" target="_blank">Arnav Kumar Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gonzalo Gonzalez-Pumariega" target="_blank">Gonzalo Gonzalez-Pumariega</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wayne Chen" target="_blank">Wayne Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Rush" target="_blank">Alexander Rush</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenting Zhao" target="_blank">Wenting Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjiban Choudhury" target="_blank">Sanjiban Choudhury</a>
            </p>
            <p id="summary-aJeLhLcsh0@OpenReview" class="summary">We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards.We propose a simple yet scalable approach, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-101-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-752" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-753"><span class="mi" id="MathJax-Span-754" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-101">\mu</script>CODE, that solves multi-turn code generation using only single-step rewards.Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn.<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-102-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-755" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-756"><span class="mi" id="MathJax-Span-757" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-102">\mu</script>CODE iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code.Experimental evaluations show that our approach achieves significant improvements over state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-103-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-758" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-759"><span class="mi" id="MathJax-Span-760" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-103">\mu</script>CODE at utilizing the execution feedback.</p>
            <p id="subjects-aJeLhLcsh0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-aJeLhLcsh0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aJeLhLcsh0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aJeLhLcsh0@OpenReview" onclick="foldPdfKimi('aJeLhLcsh0@OpenReview', this)" class="hr hr-fold">
        </div><div id="aQUUUAcAw1@OpenReview" class="panel paper" keywords="addad,cohen,algorithm,clustering,maggiori,node,pivot,insertions,added,lattanzi">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aQUUUAcAw1" target="_blank" title="129/225"><span class="index notranslate">#129</span></a>
                <a id="title-aQUUUAcAw1@OpenReview" class="title-link" href="/venue/aQUUUAcAw1@OpenReview" target="_blank">Sparse-pivot: Dynamic correlation clustering for node insertions</a>
                <a id="pdf-aQUUUAcAw1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aQUUUAcAw1@OpenReview', this)" data="https://openreview.net/pdf?id=aQUUUAcAw1">[PDF<sup id="pdf-stars-aQUUUAcAw1@OpenReview">2</sup>]</a>
                <a id="copy-aQUUUAcAw1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aQUUUAcAw1@OpenReview')">[Copy]</a>
                <a id="kimi-aQUUUAcAw1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aQUUUAcAw1@OpenReview', this)">[Kimi<sup id="kimi-stars-aQUUUAcAw1@OpenReview">1</sup>]</a>
                <a id="rel-aQUUUAcAw1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aQUUUAcAw1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aQUUUAcAw1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mina Dalirrooyfard" target="_blank">Mina Dalirrooyfard</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Konstantin Makarychev" target="_blank">Konstantin Makarychev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Slobodan Mitrovic" target="_blank">Slobodan Mitrovic</a>
            </p>
            <p id="summary-aQUUUAcAw1@OpenReview" class="summary">We present a new Correlation Clustering algorithm for a dynamic setting where nodes are added one at a time. In this model, proposed by Cohen-Addad, Lattanzi, Maggiori, and Parotsidis (ICML 2024), the algorithm uses database queries to access the input graph and updates the clustering as each new node is added.Our algorithm has the amortized update time of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-104-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-761" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1004.12em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-762"><span class="msubsup" id="MathJax-Span-763"><span style="display: inline-block; position: relative; width: 2.815em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-764" style="font-family: MathJax_Main;">log</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 1.305em;"><span class="texatom" id="MathJax-Span-765"><span class="mrow" id="MathJax-Span-766"><span class="mi" id="MathJax-Span-767" style="font-size: 70.7%; font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-768" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-769" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-770" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-771"></span><span class="mo" id="MathJax-Span-772" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-773" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-774" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>log</mi><mrow class="MJX-TeXAtom-ORD"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-104">\log^{O(1)}(n)</script>. Its approximation factor is <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-105-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-775" style="width: 3.232em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.61em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-776"><span class="mn" id="MathJax-Span-777" style="font-family: MathJax_Main;">20</span><span class="mo" id="MathJax-Span-778" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-779" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">ε</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>20</mn><mo>+</mo><mi>ε</mi></math></span></span><script type="math/tex" id="MathJax-Element-105">20+\varepsilon</script>, which is a substantial improvement over the approximation factor of the algorithm by Cohen-Addad et al. We complement our theoretical findings by empirically evaluating the approximation guarantee of our algorithm. The results show that it outperforms the algorithm by Cohen-Addad et al.~in practice.</p>
            <p id="subjects-aQUUUAcAw1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-aQUUUAcAw1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aQUUUAcAw1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aQUUUAcAw1@OpenReview" onclick="foldPdfKimi('aQUUUAcAw1@OpenReview', this)" class="hr hr-fold">
        </div><div id="afpc1MFMYU@OpenReview" class="panel paper" keywords="nsdiff,lsnm,diffusion,uncertainty,anm,stationary,probabilistic,noise,forecasting,denoising">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=afpc1MFMYU" target="_blank" title="130/225"><span class="index notranslate">#130</span></a>
                <a id="title-afpc1MFMYU@OpenReview" class="title-link" href="/venue/afpc1MFMYU@OpenReview" target="_blank">Non-stationary Diffusion For Probabilistic Time Series Forecasting</a>
                <a id="pdf-afpc1MFMYU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('afpc1MFMYU@OpenReview', this)" data="https://openreview.net/pdf?id=afpc1MFMYU">[PDF<sup id="pdf-stars-afpc1MFMYU@OpenReview">5</sup>]</a>
                <a id="copy-afpc1MFMYU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('afpc1MFMYU@OpenReview')">[Copy]</a>
                <a id="kimi-afpc1MFMYU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('afpc1MFMYU@OpenReview', this)">[Kimi<sup id="kimi-stars-afpc1MFMYU@OpenReview">2</sup>]</a>
                <a id="rel-afpc1MFMYU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('afpc1MFMYU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-afpc1MFMYU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weiwei Ye" target="_blank">Weiwei Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuopeng Xu" target="_blank">Zhuopeng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ning Gui" target="_blank">Ning Gui</a>
            </p>
            <p id="summary-afpc1MFMYU@OpenReview" class="summary">Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time. However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM). In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM. A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty. Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling. Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process. Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches. Code is available at https://github.com/wwy155/NsDiff.</p>
            <p id="subjects-afpc1MFMYU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-afpc1MFMYU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-afpc1MFMYU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-afpc1MFMYU@OpenReview" onclick="foldPdfKimi('afpc1MFMYU@OpenReview', this)" class="hr hr-fold">
        </div><div id="bLcXkIasck@OpenReview" class="panel paper" keywords="membership,text,gram,duplicates,verbatim,completed,llm,completion,gamed,definitions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bLcXkIasck" target="_blank" title="131/225"><span class="index notranslate">#131</span></a>
                <a id="title-bLcXkIasck@OpenReview" class="title-link" href="/venue/bLcXkIasck@OpenReview" target="_blank">Language Models May Verbatim Complete Text They Were Not Explicitly Trained On</a>
                <a id="pdf-bLcXkIasck@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bLcXkIasck@OpenReview', this)" data="https://openreview.net/pdf?id=bLcXkIasck">[PDF<sup id="pdf-stars-bLcXkIasck@OpenReview">3</sup>]</a>
                <a id="copy-bLcXkIasck@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bLcXkIasck@OpenReview')">[Copy]</a>
                <a id="kimi-bLcXkIasck@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bLcXkIasck@OpenReview', this)">[Kimi<sup id="kimi-stars-bLcXkIasck@OpenReview">2</sup>]</a>
                <a id="rel-bLcXkIasck@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bLcXkIasck@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bLcXkIasck@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ken Ziyu Liu" target="_blank">Ken Ziyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher A. Choquette Choo" target="_blank">Christopher A. Choquette Choo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Jagielski" target="_blank">Matthew Jagielski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Kairouz" target="_blank">Peter Kairouz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanmi Koyejo" target="_blank">Sanmi Koyejo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Percy Liang" target="_blank">Percy Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Papernot" target="_blank">Nicolas Papernot</a>
            </p>
            <p id="summary-bLcXkIasck@OpenReview" class="summary">An important question today is whether a given text was used to train a large language model (LLM). A completion test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the n-gram overlap between the target text and any text in the dataset. In this work, we demonstrate that this n-gram based membership definition can be effectively gamed. We study scenarios where sequences are non-members for a given n and we find that completion tests still succeed. We find many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of n for membership definitions. Using these insights, we design adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of n. Our findings highlight the inadequacy of n-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.</p>
            <p id="subjects-bLcXkIasck@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-bLcXkIasck@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bLcXkIasck@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bLcXkIasck@OpenReview" onclick="foldPdfKimi('bLcXkIasck@OpenReview', this)" class="hr hr-fold">
        </div><div id="beeNgQEfe2@OpenReview" class="panel paper" keywords="traces,test,compute,scaling,verifier,verification,distilling,search,rewards,budget">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=beeNgQEfe2" target="_blank" title="132/225"><span class="index notranslate">#132</span></a>
                <a id="title-beeNgQEfe2@OpenReview" class="title-link" href="/venue/beeNgQEfe2@OpenReview" target="_blank">Scaling Test-Time Compute Without Verification or RL is Suboptimal</a>
                <a id="pdf-beeNgQEfe2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('beeNgQEfe2@OpenReview', this)" data="https://openreview.net/pdf?id=beeNgQEfe2">[PDF<sup id="pdf-stars-beeNgQEfe2@OpenReview">4</sup>]</a>
                <a id="copy-beeNgQEfe2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('beeNgQEfe2@OpenReview')">[Copy]</a>
                <a id="kimi-beeNgQEfe2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('beeNgQEfe2@OpenReview', this)">[Kimi<sup id="kimi-stars-beeNgQEfe2@OpenReview">4</sup>]</a>
                <a id="rel-beeNgQEfe2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('beeNgQEfe2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-beeNgQEfe2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Amrith Setlur" target="_blank">Amrith Setlur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nived Rajaraman" target="_blank">Nived Rajaraman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Levine" target="_blank">Sergey Levine</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aviral Kumar" target="_blank">Aviral Kumar</a>
            </p>
            <p id="summary-beeNgQEfe2@OpenReview" class="summary">Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: (i) distilling successful search or thinking traces; and (ii), using verification (e.g., 0/1 outcome rewards, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erdős 1945], implying a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF widening as test-time budget grows.We corroborate our theory empirically on didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.</p>
            <p id="subjects-beeNgQEfe2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-beeNgQEfe2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-beeNgQEfe2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-beeNgQEfe2@OpenReview" onclick="foldPdfKimi('beeNgQEfe2@OpenReview', this)" class="hr hr-fold">
        </div><div id="bkauyuzBN4@OpenReview" class="panel paper" keywords="privacy,forecasting,subsampling,amplification,structured,private,series,differentially,hospital,batches">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bkauyuzBN4" target="_blank" title="133/225"><span class="index notranslate">#133</span></a>
                <a id="title-bkauyuzBN4@OpenReview" class="title-link" href="/venue/bkauyuzBN4@OpenReview" target="_blank">Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting</a>
                <a id="pdf-bkauyuzBN4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bkauyuzBN4@OpenReview', this)" data="https://openreview.net/pdf?id=bkauyuzBN4">[PDF<sup id="pdf-stars-bkauyuzBN4@OpenReview">2</sup>]</a>
                <a id="copy-bkauyuzBN4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bkauyuzBN4@OpenReview')">[Copy]</a>
                <a id="kimi-bkauyuzBN4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bkauyuzBN4@OpenReview', this)">[Kimi<sup id="kimi-stars-bkauyuzBN4@OpenReview">2</sup>]</a>
                <a id="rel-bkauyuzBN4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bkauyuzBN4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bkauyuzBN4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Schuchardt" target="_blank">Jan Schuchardt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mina Dalirrooyfard" target="_blank">Mina Dalirrooyfard</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jed Guzelkabaagac" target="_blank">Jed Guzelkabaagac</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anderson Schneider" target="_blank">Anderson Schneider</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuriy Nevmyvaka" target="_blank">Yuriy Nevmyvaka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Günnemann" target="_blank">Stephan Günnemann</a>
            </p>
            <p id="summary-bkauyuzBN4@OpenReview" class="summary">Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time series specific tasks like forecasting, since they rely on the *privacy amplification* attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this *structured subsampling* to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.</p>
            <p id="subjects-bkauyuzBN4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-bkauyuzBN4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bkauyuzBN4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bkauyuzBN4@OpenReview" onclick="foldPdfKimi('bkauyuzBN4@OpenReview', this)" class="hr hr-fold">
        </div><div id="c0dhw1du33@OpenReview" class="panel paper" keywords="vdms,video,policy,generalist,vpp,representations,robot,future,prediction,visual">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=c0dhw1du33" target="_blank" title="134/225"><span class="index notranslate">#134</span></a>
                <a id="title-c0dhw1du33@OpenReview" class="title-link" href="/venue/c0dhw1du33@OpenReview" target="_blank">Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations</a>
                <a id="pdf-c0dhw1du33@OpenReview" class="title-pdf notranslate" onclick="togglePdf('c0dhw1du33@OpenReview', this)" data="https://openreview.net/pdf?id=c0dhw1du33">[PDF<sup id="pdf-stars-c0dhw1du33@OpenReview">4</sup>]</a>
                <a id="copy-c0dhw1du33@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('c0dhw1du33@OpenReview')">[Copy]</a>
                <a id="kimi-c0dhw1du33@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('c0dhw1du33@OpenReview', this)">[Kimi<sup id="kimi-stars-c0dhw1du33@OpenReview">3</sup>]</a>
                <a id="rel-c0dhw1du33@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('c0dhw1du33@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-c0dhw1du33@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yucheng Hu" target="_blank">Yucheng Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanjiang Guo" target="_blank">Yanjiang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengchao Wang" target="_blank">Pengchao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Chen" target="_blank">Xiaoyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yen-Jen Wang" target="_blank">Yen-Jen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianke Zhang" target="_blank">Jianke Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Koushil Sreenath" target="_blank">Koushil Sreenath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaochao Lu" target="_blank">Chaochao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyu Chen" target="_blank">Jianyu Chen</a>
            </p>
            <p id="summary-c0dhw1du33@OpenReview" class="summary">Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data.In experiments, VPP achieves a 18.6\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\% increase in success rates for complex real-world dexterous manipulation tasks. For your convenience, videos can be found at https://video-prediction-policy.github.io/</p>
            <p id="subjects-c0dhw1du33@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-c0dhw1du33@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-c0dhw1du33@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-c0dhw1du33@OpenReview" onclick="foldPdfKimi('c0dhw1du33@OpenReview', this)" class="hr hr-fold">
        </div><div id="cKaUC1PeJA@OpenReview" class="panel paper" keywords="authentication,steganography,hiding,image,secret,recipients,separate,images,multiple,cover">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cKaUC1PeJA" target="_blank" title="135/225"><span class="index notranslate">#135</span></a>
                <a id="title-cKaUC1PeJA@OpenReview" class="title-link" href="/venue/cKaUC1PeJA@OpenReview" target="_blank">Efficient and Separate Authentication Image Steganography Network</a>
                <a id="pdf-cKaUC1PeJA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cKaUC1PeJA@OpenReview', this)" data="https://openreview.net/pdf?id=cKaUC1PeJA">[PDF<sup id="pdf-stars-cKaUC1PeJA@OpenReview">4</sup>]</a>
                <a id="copy-cKaUC1PeJA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cKaUC1PeJA@OpenReview')">[Copy]</a>
                <a id="kimi-cKaUC1PeJA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cKaUC1PeJA@OpenReview', this)">[Kimi<sup id="kimi-stars-cKaUC1PeJA@OpenReview">2</sup>]</a>
                <a id="rel-cKaUC1PeJA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cKaUC1PeJA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cKaUC1PeJA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junchao Zhou" target="_blank">Junchao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Lu" target="_blank">Yao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Wen" target="_blank">Jie Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangming Lu" target="_blank">Guangming Lu</a>
            </p>
            <p id="summary-cKaUC1PeJA@OpenReview" class="summary">Image steganography hides multiple images for multiple recipients into a single cover image. All secret images are usually revealed without authentication, which reduces security among multiple recipients. It is elegant to design an authentication mechanism for isolated reception. We explore such mechanism through sufficient experiments, and uncover that additional authentication information will affect the distribution of hidden information and occupy more hiding space of the cover image. This severely decreases effectiveness and efficiency in large-capacity hiding. To overcome such a challenge, we first prove the authentication feasibility within image steganography. Then, this paper proposes an image steganography network collaborating with separate authentication and efficient scheme. Specifically, multiple pairs of lock-key are generated during hiding and revealing. Unlike traditional methods, our method has two stages to make appropriate distribution adaptation between locks and secret images, simultaneously extracting more reasonable primary information from secret images, which can release hiding space of the cover image to some extent. Furthermore, due to separate authentication, fused information can be hidden in parallel with a single network rather than traditional serial hiding with multiple networks, which can largely decrease the model size. Extensive experiments demonstrate that the proposed method achieves more secure, effective, and efficient image steganography. Code is available at https://github.com/Revive624/Authentication-Image-Steganography.</p>
            <p id="subjects-cKaUC1PeJA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-cKaUC1PeJA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cKaUC1PeJA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cKaUC1PeJA@OpenReview" onclick="foldPdfKimi('cKaUC1PeJA@OpenReview', this)" class="hr hr-fold">
        </div><div id="clJIQ4TKR0@OpenReview" class="panel paper" keywords="tournaments,rankings,robin,llm,transitivity,round,transitive,judge,preferences,rightarrow">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=clJIQ4TKR0" target="_blank" title="136/225"><span class="index notranslate">#136</span></a>
                <a id="title-clJIQ4TKR0@OpenReview" class="title-link" href="/venue/clJIQ4TKR0@OpenReview" target="_blank">Investigating Non-Transitivity in LLM-as-a-Judge</a>
                <a id="pdf-clJIQ4TKR0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('clJIQ4TKR0@OpenReview', this)" data="https://openreview.net/pdf?id=clJIQ4TKR0">[PDF<sup id="pdf-stars-clJIQ4TKR0@OpenReview">1</sup>]</a>
                <a id="copy-clJIQ4TKR0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('clJIQ4TKR0@OpenReview')">[Copy]</a>
                <a id="kimi-clJIQ4TKR0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('clJIQ4TKR0@OpenReview', this)">[Kimi<sup id="kimi-stars-clJIQ4TKR0@OpenReview">1</sup>]</a>
                <a id="rel-clJIQ4TKR0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('clJIQ4TKR0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-clJIQ4TKR0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Xu" target="_blank">Yi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laura Ruis" target="_blank">Laura Ruis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Rocktäschel" target="_blank">Tim Rocktäschel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Kirk" target="_blank">Robert Kirk</a>
            </p>
            <p id="summary-clJIQ4TKR0@OpenReview" class="summary">Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0\% <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-106-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-780" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-781"><span class="mo" id="MathJax-Span-782" style="font-family: MathJax_Main;">→</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-106">\rightarrow</script> 96.4\% and 82.1\% <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-107-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-783" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-784"><span class="mo" id="MathJax-Span-785" style="font-family: MathJax_Main;">→</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></span></span><script type="math/tex" id="MathJax-Element-107">\rightarrow</script> 86.3\% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency.</p>
            <p id="subjects-clJIQ4TKR0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-clJIQ4TKR0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-clJIQ4TKR0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-clJIQ4TKR0@OpenReview" onclick="foldPdfKimi('clJIQ4TKR0@OpenReview', this)" class="hr hr-fold">
        </div><div id="dhRXGWJ027@OpenReview" class="panel paper" keywords="cognitive,animal,symbolic,human,hypotheses,romera,funsearch,cognition,paredes,effort">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dhRXGWJ027" target="_blank" title="137/225"><span class="index notranslate">#137</span></a>
                <a id="title-dhRXGWJ027@OpenReview" class="title-link" href="/venue/dhRXGWJ027@OpenReview" target="_blank">Discovering Symbolic Cognitive Models from Human and Animal Behavior</a>
                <a id="pdf-dhRXGWJ027@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dhRXGWJ027@OpenReview', this)" data="https://openreview.net/pdf?id=dhRXGWJ027">[PDF<sup id="pdf-stars-dhRXGWJ027@OpenReview">2</sup>]</a>
                <a id="copy-dhRXGWJ027@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dhRXGWJ027@OpenReview')">[Copy]</a>
                <a id="kimi-dhRXGWJ027@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dhRXGWJ027@OpenReview', this)">[Kimi<sup id="kimi-stars-dhRXGWJ027@OpenReview">2</sup>]</a>
                <a id="rel-dhRXGWJ027@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dhRXGWJ027@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dhRXGWJ027@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pablo Samuel Castro" target="_blank">Pablo Samuel Castro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nenad Tomasev" target="_blank">Nenad Tomasev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ankit Anand" target="_blank">Ankit Anand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Navodita Sharma" target="_blank">Navodita Sharma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishika Mohanta" target="_blank">Rishika Mohanta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aparna Dev" target="_blank">Aparna Dev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuba Perlin" target="_blank">Kuba Perlin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddhant Jain" target="_blank">Siddhant Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyle Levin" target="_blank">Kyle Levin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noemi Elteto" target="_blank">Noemi Elteto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Will Dabney" target="_blank">Will Dabney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Novikov" target="_blank">Alexander Novikov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Glenn Turner" target="_blank">Glenn Turner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maria Eckstein" target="_blank">Maria Eckstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathaniel Daw" target="_blank">Nathaniel Daw</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Miller" target="_blank">Kevin Miller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kimberly Stachenfeld" target="_blank">Kimberly Stachenfeld</a>
            </p>
            <p id="summary-dhRXGWJ027@OpenReview" class="summary">Symbolic models play a key role in cognitive science, expressing computationally precise hypotheses about how the brain implements a cognitive process. Identifying an appropriate model typically requires a great deal of effort and ingenuity on the part of a human scientist.Here, we adapt FunSearch (Romera-Paredes et al. 2024), a recently developed tool that uses Large Language Models (LLMs) in an evolutionary algorithm, to automatically discover symbolic cognitive models that accurately capture human and animal behavior.We consider datasets from three species performing a classic reward-learning task that has been the focus of substantial modeling effort, and find that the discovered programs outperform state-of-the-art cognitive models for each.The discovered programs can readily be interpreted as hypotheses about human and animal cognition, instantiating interpretable symbolic learning and decision-making algorithms. Broadly, these results demonstrate the viability of using LLM-powered program synthesis to propose novel scientific hypotheses regarding mechanisms of human and animal cognition.</p>
            <p id="subjects-dhRXGWJ027@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-dhRXGWJ027@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dhRXGWJ027@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dhRXGWJ027@OpenReview" onclick="foldPdfKimi('dhRXGWJ027@OpenReview', this)" class="hr hr-fold">
        </div><div id="e46xNZhwl8@OpenReview" class="panel paper" keywords="invariances,exact,kernel,polynomial,averaging,regression,diaz,canonicalization,2025,time">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=e46xNZhwl8" target="_blank" title="138/225"><span class="index notranslate">#138</span></a>
                <a id="title-e46xNZhwl8@OpenReview" class="title-link" href="/venue/e46xNZhwl8@OpenReview" target="_blank">Learning with Exact Invariances in Polynomial Time</a>
                <a id="pdf-e46xNZhwl8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('e46xNZhwl8@OpenReview', this)" data="https://openreview.net/pdf?id=e46xNZhwl8">[PDF<sup id="pdf-stars-e46xNZhwl8@OpenReview">1</sup>]</a>
                <a id="copy-e46xNZhwl8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('e46xNZhwl8@OpenReview')">[Copy]</a>
                <a id="kimi-e46xNZhwl8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('e46xNZhwl8@OpenReview', this)">[Kimi<sup id="kimi-stars-e46xNZhwl8@OpenReview">1</sup>]</a>
                <a id="rel-e46xNZhwl8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('e46xNZhwl8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-e46xNZhwl8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ashkan Soleymani" target="_blank">Ashkan Soleymani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Behrooz Tahmasebi" target="_blank">Behrooz Tahmasebi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefanie Jegelka" target="_blank">Stefanie Jegelka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patrick Jaillet" target="_blank">Patrick Jaillet</a>
            </p>
            <p id="summary-e46xNZhwl8@OpenReview" class="summary">We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with \emph{exact} invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem. To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (as opposed to approximate) invariances in this setting, partially addressing a question posed by Diaz (2025) regarding the avoidance of prohibitively large and computationally intensive group averaging methods in kernel regression with exact invariances. Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest.</p>
            <p id="subjects-e46xNZhwl8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-e46xNZhwl8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-e46xNZhwl8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-e46xNZhwl8@OpenReview" onclick="foldPdfKimi('e46xNZhwl8@OpenReview', this)" class="hr hr-fold">
        </div><div id="evb9dNxCN5@OpenReview" class="panel paper" keywords="continual,confounders,confounded,dataset,confounding,spurious,concon,getting,easily,clevr">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=evb9dNxCN5" target="_blank" title="139/225"><span class="index notranslate">#139</span></a>
                <a id="title-evb9dNxCN5@OpenReview" class="title-link" href="/venue/evb9dNxCN5@OpenReview" target="_blank">Where is the Truth? The Risk of Getting Confounded in a Continual World</a>
                <a id="pdf-evb9dNxCN5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('evb9dNxCN5@OpenReview', this)" data="https://openreview.net/pdf?id=evb9dNxCN5">[PDF<sup id="pdf-stars-evb9dNxCN5@OpenReview">1</sup>]</a>
                <a id="copy-evb9dNxCN5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('evb9dNxCN5@OpenReview')">[Copy]</a>
                <a id="kimi-evb9dNxCN5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('evb9dNxCN5@OpenReview', this)">[Kimi<sup id="kimi-stars-evb9dNxCN5@OpenReview">2</sup>]</a>
                <a id="rel-evb9dNxCN5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('evb9dNxCN5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-evb9dNxCN5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Peter Busch" target="_blank">Florian Peter Busch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roshni Ramanna Kamath" target="_blank">Roshni Ramanna Kamath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rupert Mitchell" target="_blank">Rupert Mitchell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wolfgang Stammer" target="_blank">Wolfgang Stammer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kristian Kersting" target="_blank">Kristian Kersting</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Mundt" target="_blank">Martin Mundt</a>
            </p>
            <p id="summary-evb9dNxCN5@OpenReview" class="summary">A dataset is confounded if it is most easily solved via a spurious correlation which fails to generalize to new data. In this work, we show that, in a continual learning setting where confounders may vary in time across tasks, the challenge of mitigating the effect of confounders far exceeds the standard forgetting problem normally considered. In particular, we provide a formal description of such continual confounders and identify that, in general, spurious correlations are easily ignored when training for all tasks jointly, but it is harder to avoid confounding when they are considered sequentially. These descriptions serve as a basis for constructing a novel CLEVR-based continually confounded dataset, which we term the ConCon dataset. Our evaluations demonstrate that standard continual learning methods fail to ignore the dataset's confounders. Overall, our work highlights the challenges of confounding factors, particularly in continual learning settings, and demonstrates the need for developing continual learning methods to robustly tackle these.</p>
            <p id="subjects-evb9dNxCN5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-evb9dNxCN5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-evb9dNxCN5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-evb9dNxCN5@OpenReview" onclick="foldPdfKimi('evb9dNxCN5@OpenReview', this)" class="hr hr-fold">
        </div><div id="fPOkujQBVb@OpenReview" class="panel paper" keywords="ntk,covariate,regression,nonparametric,neural,spherical,parameterized,network,distributional,trained">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fPOkujQBVb" target="_blank" title="140/225"><span class="index notranslate">#140</span></a>
                <a id="title-fPOkujQBVb@OpenReview" class="title-link" href="/venue/fPOkujQBVb@OpenReview" target="_blank">Sharp Generalization for Nonparametric Regression by Over-Parameterized Neural Networks: A Distribution-Free Analysis in Spherical Covariate</a>
                <a id="pdf-fPOkujQBVb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fPOkujQBVb@OpenReview', this)" data="https://openreview.net/pdf?id=fPOkujQBVb">[PDF<sup id="pdf-stars-fPOkujQBVb@OpenReview">3</sup>]</a>
                <a id="copy-fPOkujQBVb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fPOkujQBVb@OpenReview')">[Copy]</a>
                <a id="kimi-fPOkujQBVb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fPOkujQBVb@OpenReview', this)">[Kimi<sup id="kimi-stars-fPOkujQBVb@OpenReview">1</sup>]</a>
                <a id="rel-fPOkujQBVb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fPOkujQBVb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fPOkujQBVb@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yingzhen Yang" target="_blank">Yingzhen Yang</a>
            </p>
            <p id="summary-fPOkujQBVb@OpenReview" class="summary">Sharp generalization bound for neural networks trained by gradient descent (GD) is of central interest in statistical learning theory and deep learning. In this paper, we consider nonparametric regressionby an over-parameterized two-layer NN trained by GD. We show that, if the neural network is trained by GD with early stopping, then the trained network renders a sharp rate of the nonparametric regression risk of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-108-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-786" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-787"><span class="mi" id="MathJax-Span-788" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-789" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-790"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-791" style="font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.497em; left: 0.419em;"><span class="mn" id="MathJax-Span-792" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.47em, 2.294em, -999.997em); top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-793" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-794" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msubsup><mi>ϵ</mi><mi>n</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-108">O(\epsilon_n^2)</script>, which is the same rate as that for the classical kernel regression trained by GD with early stopping, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-109-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-795" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-796"><span class="msubsup" id="MathJax-Span-797"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-798" style="font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-799" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ϵ</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-109">\epsilon_n</script> is the critical population rate of the Neural Tangent Kernel (NTK) associated with the network and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-110-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-800" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-801"><span class="mi" id="MathJax-Span-802" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-110">n</script> is the size of the training data. It is remarked that our result does not require distributional assumptions on the covariate as long as the covariate lies on the unit sphere, in a strong contrast with many existing results which rely on specific distributions such as the spherical uniform data distribution or distributions satisfying certain restrictive conditions.As a special case of our general result, when the eigenvalues of the associated NTKdecay at a rate of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-111-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03BB;&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x224D;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-803" style="width: 5.419em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.482em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.836em, 1004.48em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-804"><span class="msubsup" id="MathJax-Span-805"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-806" style="font-family: MathJax_Math-italic;">λ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.576em;"><span class="mi" id="MathJax-Span-807" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-808" style="font-family: MathJax_Main; padding-left: 0.263em;">≍</span><span class="msubsup" id="MathJax-Span-809" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.42em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-810" style="font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.602em; left: 0.419em;"><span class="texatom" id="MathJax-Span-811"><span class="mrow" id="MathJax-Span-812"><span class="mo" id="MathJax-Span-813" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mfrac" id="MathJax-Span-814"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.617em, 1000.26em, 2.294em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.102em;"><span class="mi" id="MathJax-Span-815" style="font-size: 50%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.617em, 1000.84em, 2.346em, -999.997em); top: -1.82em; left: 50%; margin-left: -0.414em;"><span class="mrow" id="MathJax-Span-816"><span class="mi" id="MathJax-Span-817" style="font-size: 50%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-818" style="font-size: 50%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-819" style="font-size: 50%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.99em, 1.201em, -999.997em); top: -1.195em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.992em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>λ</mi><mi>j</mi></msub><mo>≍</mo><msup><mi>j</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mfrac><mi>d</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-111">\lambda_j \asymp j^{-\frac{d}{d-1}}</script> for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-112-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;&amp;#x2265;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-820" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.19em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-821"><span class="mi" id="MathJax-Span-822" style="font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-823" style="font-family: MathJax_Main; padding-left: 0.263em;">≥</span><span class="mn" id="MathJax-Span-824" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi><mo>≥</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-112">j \ge 1</script> which happens under certain distributional assumption such as the training features follow the spherical uniform distribution, we immediately obtain the minimax optimal rate of<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-113-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-825" style="width: 5.159em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.273em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.836em, 1004.17em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-826"><span class="mi" id="MathJax-Span-827" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-828" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-829"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-830" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.602em; left: 0.628em;"><span class="texatom" id="MathJax-Span-831"><span class="mrow" id="MathJax-Span-832"><span class="mo" id="MathJax-Span-833" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mfrac" id="MathJax-Span-834"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.617em, 1000.26em, 2.294em, -999.997em); top: -2.497em; left: 50%; margin-left: -0.102em;"><span class="mi" id="MathJax-Span-835" style="font-size: 50%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.617em, 1001.1em, 2.346em, -999.997em); top: -1.82em; left: 50%; margin-left: -0.57em;"><span class="mrow" id="MathJax-Span-836"><span class="mn" id="MathJax-Span-837" style="font-size: 50%; font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-838" style="font-size: 50%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-839" style="font-size: 50%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-840" style="font-size: 50%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.25em, 1.201em, -999.997em); top: -1.195em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.253em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-841" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mfrac><mi>d</mi><mrow><mn>2</mn><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-113">O(n^{-\frac{d}{2d-1}})</script>, which is the major results of several existing works in this direction. The neural network width in our general result is lower bounded by a function of only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-114-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-842" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-843"><span class="mi" id="MathJax-Span-844" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-114">d</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-115-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-845" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-846"><span class="msubsup" id="MathJax-Span-847"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-848" style="font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-849" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ϵ</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-115">\epsilon_n</script>, and such width does not depend on the minimum eigenvalue of the empirical NTK matrix whose lower bound usually requires additional assumptions on the training data.Our results are built upon two significant technical results which are of independent interest. First, uniform convergence to the NTK is established during the training process by GD, so that we can have a nice decomposition of the neural network function at any step of the GD into a function in the ReproducingKernel Hilbert Space associated with the NTK and an error function with a small <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-116-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-850" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-851"><span class="msubsup" id="MathJax-Span-852"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-853" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.68em;"><span class="texatom" id="MathJax-Span-854"><span class="mrow" id="MathJax-Span-855"><span class="mi" id="MathJax-Span-856" style="font-size: 70.7%; font-family: MathJax_Main;">∞</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">∞</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-116">L^{\infty}</script>-norm. Second, local Rademacher complexity is employedto tightly bound the Rademacher complexity of the function class comprising all the possible neural network functions obtained by GD. Our resultformally fills the gap between training a classical kernel regression model and training an over-parameterized but finite-width neural network by GD for nonparametric regression without distributional assumptions about the spherical covariate.</p>
            <p id="subjects-fPOkujQBVb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-fPOkujQBVb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fPOkujQBVb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fPOkujQBVb@OpenReview" onclick="foldPdfKimi('fPOkujQBVb@OpenReview', this)" class="hr hr-fold">
        </div><div id="gcgzQSKR7y@OpenReview" class="panel paper" keywords="neyman,regret,adaptive,designs,widetilde,clipogd,multigroup,design,guarantees,anytime">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gcgzQSKR7y" target="_blank" title="141/225"><span class="index notranslate">#141</span></a>
                <a id="title-gcgzQSKR7y@OpenReview" class="title-link" href="/venue/gcgzQSKR7y@OpenReview" target="_blank">Stronger Neyman Regret Guarantees for Adaptive Experimental Design</a>
                <a id="pdf-gcgzQSKR7y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gcgzQSKR7y@OpenReview', this)" data="https://openreview.net/pdf?id=gcgzQSKR7y">[PDF<sup id="pdf-stars-gcgzQSKR7y@OpenReview">1</sup>]</a>
                <a id="copy-gcgzQSKR7y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gcgzQSKR7y@OpenReview')">[Copy]</a>
                <a id="kimi-gcgzQSKR7y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gcgzQSKR7y@OpenReview', this)">[Kimi<sup id="kimi-stars-gcgzQSKR7y@OpenReview">1</sup>]</a>
                <a id="rel-gcgzQSKR7y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gcgzQSKR7y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gcgzQSKR7y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Georgy Noarov" target="_blank">Georgy Noarov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Riccardo Fogliato" target="_blank">Riccardo Fogliato</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin A Bertran" target="_blank">Martin A Bertran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Roth" target="_blank">Aaron Roth</a>
            </p>
            <p id="summary-gcgzQSKR7y@OpenReview" class="summary">We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering *sublinear Neyman regret*, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design.Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-117-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-857" style="width: 3.701em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1002.97em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-858"><span class="texatom" id="MathJax-Span-859"><span class="mrow" id="MathJax-Span-860"><span class="munderover" id="MathJax-Span-861"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-862" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-863" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-864" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-865"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-866"><span class="mi" id="MathJax-Span-867" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.68em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.003em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-868" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-117">\widetilde{O}(\sqrt{T})</script> expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-118-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-869" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.6em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-870"><span class="texatom" id="MathJax-Span-871"><span class="mrow" id="MathJax-Span-872"><span class="munderover" id="MathJax-Span-873"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-874" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-875" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-876" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-877" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-878"></span><span class="mi" id="MathJax-Span-879" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-880" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mi>T</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-118">\widetilde{O}(\log T)</script> Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual ``multigroup'' Neyman regret guarantees: Given a set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-119-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-881" style="width: 3.701em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1002.97em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-882"><span class="texatom" id="MathJax-Span-883"><span class="mrow" id="MathJax-Span-884"><span class="munderover" id="MathJax-Span-885"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-886" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-887" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-888" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-889"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-890"><span class="mi" id="MathJax-Span-891" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.68em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.003em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-892" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-119">\widetilde{O}(\sqrt{T})</script> anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments.</p>
            <p id="subjects-gcgzQSKR7y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gcgzQSKR7y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gcgzQSKR7y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gcgzQSKR7y@OpenReview" onclick="foldPdfKimi('gcgzQSKR7y@OpenReview', this)" class="hr hr-fold">
        </div><div id="glLqTK9En3@OpenReview" class="panel paper" keywords="stitching,stitched,functionally,mislead,aligned,different,functional,capture,similarity,examining">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=glLqTK9En3" target="_blank" title="142/225"><span class="index notranslate">#142</span></a>
                <a id="title-glLqTK9En3@OpenReview" class="title-link" href="/venue/glLqTK9En3@OpenReview" target="_blank">Functional Alignment Can Mislead: Examining Model Stitching</a>
                <a id="pdf-glLqTK9En3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('glLqTK9En3@OpenReview', this)" data="https://openreview.net/pdf?id=glLqTK9En3">[PDF<sup id="pdf-stars-glLqTK9En3@OpenReview">2</sup>]</a>
                <a id="copy-glLqTK9En3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('glLqTK9En3@OpenReview')">[Copy]</a>
                <a id="kimi-glLqTK9En3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('glLqTK9En3@OpenReview', this)">[Kimi<sup id="kimi-stars-glLqTK9En3@OpenReview">2</sup>]</a>
                <a id="rel-glLqTK9En3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('glLqTK9En3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-glLqTK9En3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Damian Smith" target="_blank">Damian Smith</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harvey Mannering" target="_blank">Harvey Mannering</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antonia Marcu" target="_blank">Antonia Marcu</a>
            </p>
            <p id="summary-glLqTK9En3@OpenReview" class="summary">A common belief in the representational comparison literature is that if two representations can be functionally aligned, they must capture similar information. In this paper we focus on model stitching and show that models can be functionally aligned, but represent very different information. Firstly, we show that discriminative models with very different biases can be stitched together. We then show that models trained to solve entirely different tasks on different data modalities, and even clustered random noise, can be successfully stitched into MNIST or ImageNet-trained models. We end with a discussion of the wider impact of our results on the community's current beliefs. Overall, our paper draws attention to the need to correctly interpret the results of such functional similarity measures and highlights the need for approaches that capture informational similarity.</p>
            <p id="subjects-glLqTK9En3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-glLqTK9En3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-glLqTK9En3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-glLqTK9En3@OpenReview" onclick="foldPdfKimi('glLqTK9En3@OpenReview', this)" class="hr hr-fold">
        </div><div id="h2oNQOzbc5@OpenReview" class="panel paper" keywords="actionpiece,action,tokenizing,sequences,recommendation,contextually,generative,tokens,context,sets">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=h2oNQOzbc5" target="_blank" title="143/225"><span class="index notranslate">#143</span></a>
                <a id="title-h2oNQOzbc5@OpenReview" class="title-link" href="/venue/h2oNQOzbc5@OpenReview" target="_blank">ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation</a>
                <a id="pdf-h2oNQOzbc5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('h2oNQOzbc5@OpenReview', this)" data="https://openreview.net/pdf?id=h2oNQOzbc5">[PDF<sup id="pdf-stars-h2oNQOzbc5@OpenReview">3</sup>]</a>
                <a id="copy-h2oNQOzbc5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('h2oNQOzbc5@OpenReview')">[Copy]</a>
                <a id="kimi-h2oNQOzbc5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('h2oNQOzbc5@OpenReview', this)">[Kimi<sup id="kimi-stars-h2oNQOzbc5@OpenReview">1</sup>]</a>
                <a id="rel-h2oNQOzbc5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('h2oNQOzbc5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-h2oNQOzbc5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yupeng Hou" target="_blank">Yupeng Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianmo Ni" target="_blank">Jianmo Ni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhankui He" target="_blank">Zhankui He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noveen Sachdeva" target="_blank">Noveen Sachdeva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wang-Cheng Kang" target="_blank">Wang-Cheng Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ed Chi" target="_blank">Ed Chi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julian McAuley" target="_blank">Julian McAuley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Derek Cheng" target="_blank">Derek Cheng</a>
            </p>
            <p id="summary-h2oNQOzbc5@OpenReview" class="summary">Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a *set* of item features. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Our code is available at: https://github.com/google-deepmind/action_piece.</p>
            <p id="subjects-h2oNQOzbc5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-h2oNQOzbc5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-h2oNQOzbc5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-h2oNQOzbc5@OpenReview" onclick="foldPdfKimi('h2oNQOzbc5@OpenReview', this)" class="hr hr-fold">
        </div><div id="hRQyqtcjVv@OpenReview" class="panel paper" keywords="jailbreak,jailbreaks,tax,outputs,instructions,guardrails,bomb,utility,refuse,bypass">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hRQyqtcjVv" target="_blank" title="144/225"><span class="index notranslate">#144</span></a>
                <a id="title-hRQyqtcjVv@OpenReview" class="title-link" href="/venue/hRQyqtcjVv@OpenReview" target="_blank">The Jailbreak Tax: How Useful are Your Jailbreak Outputs?</a>
                <a id="pdf-hRQyqtcjVv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hRQyqtcjVv@OpenReview', this)" data="https://openreview.net/pdf?id=hRQyqtcjVv">[PDF<sup id="pdf-stars-hRQyqtcjVv@OpenReview">4</sup>]</a>
                <a id="copy-hRQyqtcjVv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hRQyqtcjVv@OpenReview')">[Copy]</a>
                <a id="kimi-hRQyqtcjVv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hRQyqtcjVv@OpenReview', this)">[Kimi<sup id="kimi-stars-hRQyqtcjVv@OpenReview">3</sup>]</a>
                <a id="rel-hRQyqtcjVv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hRQyqtcjVv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hRQyqtcjVv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kristina Nikolić" target="_blank">Kristina Nikolić</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luze Sun" target="_blank">Luze Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Zhang" target="_blank">Jie Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Tramer" target="_blank">Florian Tramer</a>
            </p>
            <p id="summary-hRQyqtcjVv@OpenReview" class="summary">Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs.In this paper, we ask whether the model outputs produced by existing jailbreaks are actually *useful*. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions?Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math).Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the *jailbreak tax*. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy.Overall, our work proposes jailbreak utility as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax</p>
            <p id="subjects-hRQyqtcjVv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hRQyqtcjVv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hRQyqtcjVv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hRQyqtcjVv@OpenReview" onclick="foldPdfKimi('hRQyqtcjVv@OpenReview', this)" class="hr hr-fold">
        </div><div id="hS2Ed5XYRq@OpenReview" class="panel paper" keywords="mapeval,foundation,reasoning,models,map,geospatial,evaluation,spatial,api,geo">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hS2Ed5XYRq" target="_blank" title="145/225"><span class="index notranslate">#145</span></a>
                <a id="title-hS2Ed5XYRq@OpenReview" class="title-link" href="/venue/hS2Ed5XYRq@OpenReview" target="_blank">MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models</a>
                <a id="pdf-hS2Ed5XYRq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hS2Ed5XYRq@OpenReview', this)" data="https://openreview.net/pdf?id=hS2Ed5XYRq">[PDF<sup id="pdf-stars-hS2Ed5XYRq@OpenReview">3</sup>]</a>
                <a id="copy-hS2Ed5XYRq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hS2Ed5XYRq@OpenReview')">[Copy]</a>
                <a id="kimi-hS2Ed5XYRq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hS2Ed5XYRq@OpenReview', this)">[Kimi<sup id="kimi-stars-hS2Ed5XYRq@OpenReview">1</sup>]</a>
                <a id="rel-hS2Ed5XYRq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hS2Ed5XYRq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hS2Ed5XYRq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mahir Labib Dihan" target="_blank">Mahir Labib Dihan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tanvir Hassan" target="_blank">Tanvir Hassan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Md Tanvir Parvez" target="_blank">Md Tanvir Parvez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hasebul Hasan" target="_blank">Hasebul Hasan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Almash Alam" target="_blank">Almash Alam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Aamir Cheema" target="_blank">Muhammad Aamir Cheema</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammed Eunus Ali" target="_blank">Mohammed Eunus Ali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Md Rizwan Parvez" target="_blank">Md Rizwan Parvez</a>
            </p>
            <p id="summary-hS2Ed5XYRq@OpenReview" class="summary">Recent advancements in foundation models have improved autonomous tool usage and reasoning, but their capabilities in map-based reasoning remain underexplored. To address this, we introduce MapEval, a benchmark designed to assess foundation models across three distinct tasks—textual, API-based, and visual reasoning—through 700 multiple-choice questions spanning 180 cities and 54 countries, covering spatial relationships, navigation, travel planning, and real-world map interactions. Unlike prior benchmarks that focus on simple location queries, MapEval requires models to handle long-context reasoning, API interactions and visual map analysis, making it the most comprehensive evaluation framework for geospatial AI. On evaluation of 30 foundation models, including Claude-3.5-Sonnet, GPT-4o, Gemini-1.5-Pro, none surpasses 67% accuracy, with open-source models performing significantly worse and all models lagging over 20% behind human performance. These results expose critical gaps in spatial inference, as models struggle with distances, directions, route planning, and place-specific reasoning, highlighting the need for better geospatial AI to bridge the gap between foundation models and real-world navigation.</p>
            <p id="subjects-hS2Ed5XYRq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hS2Ed5XYRq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hS2Ed5XYRq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hS2Ed5XYRq@OpenReview" onclick="foldPdfKimi('hS2Ed5XYRq@OpenReview', this)" class="hr hr-fold">
        </div><div id="hrdLhNDAzp@OpenReview" class="panel paper" keywords="mcu,ended,open,agents,minecraft,evaluation,tasks,craftjarvis,game,framework">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hrdLhNDAzp" target="_blank" title="146/225"><span class="index notranslate">#146</span></a>
                <a id="title-hrdLhNDAzp@OpenReview" class="title-link" href="/venue/hrdLhNDAzp@OpenReview" target="_blank">MCU: An Evaluation Framework for Open-Ended Game Agents</a>
                <a id="pdf-hrdLhNDAzp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hrdLhNDAzp@OpenReview', this)" data="https://openreview.net/pdf?id=hrdLhNDAzp">[PDF<sup id="pdf-stars-hrdLhNDAzp@OpenReview">1</sup>]</a>
                <a id="copy-hrdLhNDAzp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hrdLhNDAzp@OpenReview')">[Copy]</a>
                <a id="kimi-hrdLhNDAzp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hrdLhNDAzp@OpenReview', this)">[Kimi<sup id="kimi-stars-hrdLhNDAzp@OpenReview">2</sup>]</a>
                <a id="rel-hrdLhNDAzp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hrdLhNDAzp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hrdLhNDAzp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyue Zheng" target="_blank">Xinyue Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haowei Lin" target="_blank">Haowei Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaichen He" target="_blank">Kaichen He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihao Wang" target="_blank">Zihao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Fu" target="_blank">Qiang Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haobo Fu" target="_blank">Haobo Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zilong Zheng" target="_blank">Zilong Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yitao Liang" target="_blank">Yitao Liang</a>
            </p>
            <p id="summary-hrdLhNDAzp@OpenReview" class="summary">Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce \textit{Minecraft Universe} (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 11 major categories and 41 subcategories of challenges; (2) a task composition mechanism capable of generating infinite diverse tasks with varying difficulty; and (3) a general evaluation framework that achieves 91.5\% alignment with human ratings for open-ended task assessment. Empirical results reveal that even state-of-the-art foundation agents struggle with the increasing diversity and complexity of tasks. These findings highlight the necessity of MCU as a robust benchmark to drive progress in AI agent development within open-ended environments. Our evaluation code and scripts are available at https://github.com/CraftJarvis/MCU.</p>
            <p id="subjects-hrdLhNDAzp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hrdLhNDAzp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hrdLhNDAzp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hrdLhNDAzp@OpenReview" onclick="foldPdfKimi('hrdLhNDAzp@OpenReview', this)" class="hr hr-fold">
        </div><div id="hwTKGdM4TK@OpenReview" class="panel paper" keywords="naive,bayes,attributes,icgnb,gnb,icg,graph,instance,correlation,vgae">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hwTKGdM4TK" target="_blank" title="147/225"><span class="index notranslate">#147</span></a>
                <a id="title-hwTKGdM4TK@OpenReview" class="title-link" href="/venue/hwTKGdM4TK@OpenReview" target="_blank">Instance Correlation Graph-based Naive Bayes</a>
                <a id="pdf-hwTKGdM4TK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hwTKGdM4TK@OpenReview', this)" data="https://openreview.net/pdf?id=hwTKGdM4TK">[PDF<sup id="pdf-stars-hwTKGdM4TK@OpenReview">2</sup>]</a>
                <a id="copy-hwTKGdM4TK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hwTKGdM4TK@OpenReview')">[Copy]</a>
                <a id="kimi-hwTKGdM4TK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hwTKGdM4TK@OpenReview', this)">[Kimi<sup id="kimi-stars-hwTKGdM4TK@OpenReview">1</sup>]</a>
                <a id="rel-hwTKGdM4TK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hwTKGdM4TK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hwTKGdM4TK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chengyuan Li" target="_blank">Chengyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangxiao Jiang" target="_blank">Liangxiao Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjun Zhang" target="_blank">Wenjun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangjun Yu" target="_blank">Liangjun Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Zhang" target="_blank">Huan Zhang</a>
            </p>
            <p id="summary-hwTKGdM4TK@OpenReview" class="summary">Due to its simplicity, effectiveness and robustness, naive Bayes (NB) has continued to be one of the top 10 data mining algorithms. To improve its performance, a large number of improved algorithms have been proposed in the last few decades. However, in addition to Gaussian naive Bayes (GNB), there is little work on numerical attributes. At the same time, none of them takes into account the correlations among instances. To fill this gap, we propose a novel algorithm called instance correlation graph-based naive Bayes (ICGNB). Specifically, it first uses original attributes to construct an instance correlation graph (ICG) to represent the correlations among instances. Then, it employs a variational graph auto-encoder (VGAE) to generate new attributes from the constructed ICG and uses them to augment original attributes.Finally, it weights each augmented attribute to alleviate the attribute redundancy and builds GNB on the weighted attributes. The experimental results on tens of datasets show that ICGNB significantly outperforms its deserved competitors.Our codes and datasets are available at https://github.com/jiangliangxiao/ICGNB.</p>
            <p id="subjects-hwTKGdM4TK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hwTKGdM4TK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hwTKGdM4TK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hwTKGdM4TK@OpenReview" onclick="foldPdfKimi('hwTKGdM4TK@OpenReview', this)" class="hr hr-fold">
        </div><div id="iBpkzB5LEr@OpenReview" class="panel paper" keywords="primal,nar,reasoning,algorithms,dual,algorithmic,neural,classical,approximation,simulates">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=iBpkzB5LEr" target="_blank" title="148/225"><span class="index notranslate">#148</span></a>
                <a id="title-iBpkzB5LEr@OpenReview" class="title-link" href="/venue/iBpkzB5LEr@OpenReview" target="_blank">Primal-Dual Neural Algorithmic Reasoning</a>
                <a id="pdf-iBpkzB5LEr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('iBpkzB5LEr@OpenReview', this)" data="https://openreview.net/pdf?id=iBpkzB5LEr">[PDF<sup id="pdf-stars-iBpkzB5LEr@OpenReview">2</sup>]</a>
                <a id="copy-iBpkzB5LEr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('iBpkzB5LEr@OpenReview')">[Copy]</a>
                <a id="kimi-iBpkzB5LEr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('iBpkzB5LEr@OpenReview', this)">[Kimi<sup id="kimi-stars-iBpkzB5LEr@OpenReview">1</sup>]</a>
                <a id="rel-iBpkzB5LEr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('iBpkzB5LEr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-iBpkzB5LEr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yu He" target="_blank">Yu He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ellen Vitercik" target="_blank">Ellen Vitercik</a>
            </p>
            <p id="summary-iBpkzB5LEr@OpenReview" class="summary">Neural Algorithmic Reasoning (NAR) trains neural networks to simulate classical algorithms, enabling structured and interpretable reasoning over complex data. While prior research has predominantly focused on learning exact algorithms for polynomial-time-solvable problems, extending NAR to harder problems remains an open challenge. In this work, we introduce a general NAR framework grounded in the primal-dual paradigm, a classical method for designing efficient approximation algorithms. By leveraging a bipartite representation between primal and dual variables, we establish an alignment between primal-dual algorithms and Graph Neural Networks. Furthermore, we incorporate optimal solutions from small instances to greatly enhance the model’s reasoning capabilities. Our empirical results demonstrate that our model not only simulates but also outperforms approximation algorithms for multiple tasks, exhibiting robust generalization to larger and out-of-distribution graphs. Moreover, we highlight the framework’s practical utility by integrating it with commercial solvers and applying it to real-world datasets.</p>
            <p id="subjects-iBpkzB5LEr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-iBpkzB5LEr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-iBpkzB5LEr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-iBpkzB5LEr@OpenReview" onclick="foldPdfKimi('iBpkzB5LEr@OpenReview', this)" class="hr hr-fold">
        </div><div id="iFOXz5H2gB@OpenReview" class="panel paper" keywords="airmvc,noisy,identification,clustering,rectification,contrastive,view,scenarios,xihongyang1999,multi">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=iFOXz5H2gB" target="_blank" title="149/225"><span class="index notranslate">#149</span></a>
                <a id="title-iFOXz5H2gB@OpenReview" class="title-link" href="/venue/iFOXz5H2gB@OpenReview" target="_blank">Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios</a>
                <a id="pdf-iFOXz5H2gB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('iFOXz5H2gB@OpenReview', this)" data="https://openreview.net/pdf?id=iFOXz5H2gB">[PDF<sup id="pdf-stars-iFOXz5H2gB@OpenReview">3</sup>]</a>
                <a id="copy-iFOXz5H2gB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('iFOXz5H2gB@OpenReview')">[Copy]</a>
                <a id="kimi-iFOXz5H2gB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('iFOXz5H2gB@OpenReview', this)">[Kimi<sup id="kimi-stars-iFOXz5H2gB@OpenReview">2</sup>]</a>
                <a id="rel-iFOXz5H2gB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('iFOXz5H2gB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-iFOXz5H2gB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=xihong yang" target="_blank">xihong yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Wang" target="_blank">Siwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangdi Wang" target="_blank">Fangdi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Jin" target="_blank">Jiaqi Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suyuan Liu" target="_blank">Suyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Liu" target="_blank">Yue Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=En Zhu" target="_blank">En Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinwang Liu" target="_blank">Xinwang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueming Jin" target="_blank">Yueming Jin</a>
            </p>
            <p id="summary-iFOXz5H2gB@OpenReview" class="summary">Leveraging the powerful representation learning capabilities, deep multi-view clustering methods have demonstrated reliable performance by effectively integrating multi-source information from diverse views in recent years. Most existing methods rely on the assumption of clean views. However, noise is pervasive in real-world scenarios, leading to a significant degradation in performance. To tackle this problem, we propose a novel multi-view clustering framework for the automatic identification and rectification of noisy data, termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly identification problem using GMM. We then design a hybrid rectification strategy to mitigate the adverse effects of noisy data based on the identification results. Furthermore, we introduce a noise-robust contrastive mechanism to generate reliable representations. Additionally, we provide a theoretical proof demonstrating that these representations can discard noisy information, thereby improving the performance of downstream tasks. Extensive experiments on six benchmark datasets demonstrate that AIRMVC outperforms state-of-the-art algorithms in terms of robustness in noisy scenarios. The code of AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github.</p>
            <p id="subjects-iFOXz5H2gB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-iFOXz5H2gB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-iFOXz5H2gB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-iFOXz5H2gB@OpenReview" onclick="foldPdfKimi('iFOXz5H2gB@OpenReview', this)" class="hr hr-fold">
        </div><div id="jEcQP3lGlq@OpenReview" class="panel paper" keywords="plms,multimodal,dplm,protein,token,design,folding,650m,elucidating,structural">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jEcQP3lGlq" target="_blank" title="150/225"><span class="index notranslate">#150</span></a>
                <a id="title-jEcQP3lGlq@OpenReview" class="title-link" href="/venue/jEcQP3lGlq@OpenReview" target="_blank">Elucidating the Design Space of Multimodal Protein Language Models</a>
                <a id="pdf-jEcQP3lGlq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jEcQP3lGlq@OpenReview', this)" data="https://openreview.net/pdf?id=jEcQP3lGlq">[PDF<sup id="pdf-stars-jEcQP3lGlq@OpenReview">5</sup>]</a>
                <a id="copy-jEcQP3lGlq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jEcQP3lGlq@OpenReview')">[Copy]</a>
                <a id="kimi-jEcQP3lGlq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jEcQP3lGlq@OpenReview', this)">[Kimi<sup id="kimi-stars-jEcQP3lGlq@OpenReview">2</sup>]</a>
                <a id="rel-jEcQP3lGlq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jEcQP3lGlq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jEcQP3lGlq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng-Yen Hsieh" target="_blank">Cheng-Yen Hsieh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyou Wang" target="_blank">Xinyou Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daiheng Zhang" target="_blank">Daiheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongyu Xue" target="_blank">Dongyu Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei YE" target="_blank">Fei YE</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shujian Huang" target="_blank">Shujian Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaixiang Zheng" target="_blank">Zaixiang Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanquan Gu" target="_blank">Quanquan Gu</a>
            </p>
            <p id="summary-jEcQP3lGlq@OpenReview" class="summary">Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks.To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling.The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models.Project page and code: https://bytedance.github.io/dplm/dplm-2.1.</p>
            <p id="subjects-jEcQP3lGlq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jEcQP3lGlq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jEcQP3lGlq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jEcQP3lGlq@OpenReview" onclick="foldPdfKimi('jEcQP3lGlq@OpenReview', this)" class="hr hr-fold">
        </div><div id="jJRkkPr474@OpenReview" class="panel paper" keywords="hyena,equivariant,geometric,equivariance,context,global,quadratic,convolutional,30k,sacrifice">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jJRkkPr474" target="_blank" title="151/225"><span class="index notranslate">#151</span></a>
                <a id="title-jJRkkPr474@OpenReview" class="title-link" href="/venue/jJRkkPr474@OpenReview" target="_blank">Geometric Hyena Networks for Large-scale Equivariant Learning</a>
                <a id="pdf-jJRkkPr474@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jJRkkPr474@OpenReview', this)" data="https://openreview.net/pdf?id=jJRkkPr474">[PDF<sup id="pdf-stars-jJRkkPr474@OpenReview">2</sup>]</a>
                <a id="copy-jJRkkPr474@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jJRkkPr474@OpenReview')">[Copy]</a>
                <a id="kimi-jJRkkPr474@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jJRkkPr474@OpenReview', this)">[Kimi<sup id="kimi-stars-jJRkkPr474@OpenReview">1</sup>]</a>
                <a id="rel-jJRkkPr474@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jJRkkPr474@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jJRkkPr474@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Artem Moskalev" target="_blank">Artem Moskalev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mangal Prakash" target="_blank">Mangal Prakash</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Xu" target="_blank">Junjie Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Cui" target="_blank">Tianyu Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Liao" target="_blank">Rui Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommaso Mansi" target="_blank">Tommaso Mansi</a>
            </p>
            <p id="summary-jJRkkPr474@OpenReview" class="summary">Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-120-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;30&lt;/mn&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-893" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.51em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-894"><span class="mn" id="MathJax-Span-895" style="font-family: MathJax_Main;">30</span><span class="mi" id="MathJax-Span-896" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>30</mn><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-120">30k</script> tokens <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-121-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-897" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-898"><span class="mn" id="MathJax-Span-899" style="font-family: MathJax_Main;">20</span><span class="mo" id="MathJax-Span-900" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>20</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-121">20 \times</script> faster than the equivariant transformer and allows <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-122-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;72&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-901" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-902"><span class="mn" id="MathJax-Span-903" style="font-family: MathJax_Main;">72</span><span class="mo" id="MathJax-Span-904" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>72</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-122">72 \times</script> longer context within the same budget.</p>
            <p id="subjects-jJRkkPr474@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jJRkkPr474@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jJRkkPr474@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jJRkkPr474@OpenReview" onclick="foldPdfKimi('jJRkkPr474@OpenReview', this)" class="hr hr-fold">
        </div><div id="jnPHZqcUdn@OpenReview" class="panel paper" keywords="cell,ssl,scssl,bench,single,scgpt,modal,scvi,data,batch">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jnPHZqcUdn" target="_blank" title="152/225"><span class="index notranslate">#152</span></a>
                <a id="title-jnPHZqcUdn@OpenReview" class="title-link" href="/venue/jnPHZqcUdn@OpenReview" target="_blank">scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data</a>
                <a id="pdf-jnPHZqcUdn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jnPHZqcUdn@OpenReview', this)" data="https://openreview.net/pdf?id=jnPHZqcUdn">[PDF<sup id="pdf-stars-jnPHZqcUdn@OpenReview">3</sup>]</a>
                <a id="copy-jnPHZqcUdn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jnPHZqcUdn@OpenReview')">[Copy]</a>
                <a id="kimi-jnPHZqcUdn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jnPHZqcUdn@OpenReview', this)">[Kimi<sup id="kimi-stars-jnPHZqcUdn@OpenReview">4</sup>]</a>
                <a id="rel-jnPHZqcUdn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jnPHZqcUdn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jnPHZqcUdn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Olga Ovcharenko" target="_blank">Olga Ovcharenko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Barkmann" target="_blank">Florian Barkmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Toma" target="_blank">Philip Toma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Imant Daunhawer" target="_blank">Imant Daunhawer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julia Vogt" target="_blank">Julia Vogt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Schelter" target="_blank">Sebastian Schelter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Valentina Boeva" target="_blank">Valentina Boeva</a>
            </p>
            <p id="summary-jnPHZqcUdn@OpenReview" class="summary">Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL methods. Our evaluation spans nine datasets and focuses on three common downstream tasks: batch correction, cell type annotation, and missing modality prediction. Furthermore, we systematically assess various data augmentation strategies. Our analysis reveals task-specific trade-offs: the specialized single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at uni-modal batch correction, while generic SSL methods, such as VICReg and SimCLR, demonstrate superior performance in cell typing and multi-modal data integration. Random masking emerges as the most effective augmentation technique across all tasks, surpassing domain-specific augmentations. Notably, our results indicate the need for a specialized single-cell multi-modal data integration framework. scSSL-Bench provides a standardized evaluation platform and concrete recommendations for applying SSL to single-cell analysis, advancing the convergence of deep learning and single-cell genomics.</p>
            <p id="subjects-jnPHZqcUdn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jnPHZqcUdn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jnPHZqcUdn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jnPHZqcUdn@OpenReview" onclick="foldPdfKimi('jnPHZqcUdn@OpenReview', this)" class="hr hr-fold">
        </div><div id="js3gePctLu@OpenReview" class="panel paper" keywords="sellers,auctioneer,submodular,auctions,descending,procurement,optimization,services,frameworks,nas">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=js3gePctLu" target="_blank" title="153/225"><span class="index notranslate">#153</span></a>
                <a id="title-js3gePctLu@OpenReview" class="title-link" href="/venue/js3gePctLu@OpenReview" target="_blank">Procurement Auctions via Approximately Optimal Submodular Optimization</a>
                <a id="pdf-js3gePctLu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('js3gePctLu@OpenReview', this)" data="https://openreview.net/pdf?id=js3gePctLu">[PDF<sup id="pdf-stars-js3gePctLu@OpenReview">1</sup>]</a>
                <a id="copy-js3gePctLu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('js3gePctLu@OpenReview')">[Copy]</a>
                <a id="kimi-js3gePctLu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('js3gePctLu@OpenReview', this)">[Kimi<sup id="kimi-stars-js3gePctLu@OpenReview">1</sup>]</a>
                <a id="rel-js3gePctLu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('js3gePctLu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-js3gePctLu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Deng" target="_blank">Yuan Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amin Karbasi" target="_blank">Amin Karbasi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vahab Mirrokni" target="_blank">Vahab Mirrokni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renato Leme" target="_blank">Renato Leme</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Grigoris Velegkas" target="_blank">Grigoris Velegkas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Zuo" target="_blank">Song Zuo</a>
            </p>
            <p id="summary-js3gePctLu@OpenReview" class="summary">We study the problem of procurement auctions, in which an auctioneer seeks to acquire services from a group of strategic sellers with private costs. The quality of the services is measured through some submodular function that is known to the auctioneer. Our goal is to design computationally efficient procurement auctions that (approximately) maximize the difference between the quality of the acquired services and the total cost of the sellers, in a way that is incentive compatible (IC) and individual rational (IR) for the sellers, and generates non-negative surplus (NAS) for the auctioneer. {Our contribution is twofold: \textbf{i)} we provide an improved analysis of existing algorithms for non-positive submodular function maximization and \textbf{ii)} we design computationally efficient frameworks that transform submodular function optimization algorithms to mechanisms that are IC and IR for the sellers, NAS for the auctioneer, and approximation-preserving.} Our frameworks are general and work both in the offline setting where the auctioneer can observe the bids and the services of all the sellers simultaneously, and in the online setting where the sellers arrive in an adversarial order and the auctioneer has to make an irrevocable decision whether to purchase their service or not. We further investigate whether it is possible to convert state-of-art submodular optimization algorithms into descending auctions. We focus on the adversarial setting, meaning that the schedule of the descending prices is determined by an adversary. We show that a submodular optimization algorithm satisfying bi-criteria <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-123-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-905" style="width: 3.909em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.232em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.13em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-906"><span class="mo" id="MathJax-Span-907" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-908" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-909"><span class="mrow" id="MathJax-Span-910"><span class="mo" id="MathJax-Span-911" style="font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-912" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-913" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-914" style="font-family: MathJax_Main; padding-left: 0.159em;">1</span><span class="mo" id="MathJax-Span-915" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-123">(1/2,1)</script>-approximation in welfare can be effectively converted to a descending auction in this setting. We further establish a connection between descending auctions and online submodular optimization. Finally, we demonstrate the practical applications of our frameworks by instantiating them with different state-of-the-art submodular optimization algorithms and comparing their welfare performance through empirical experiments on publicly available datasets that consist of thousands of sellers.</p>
            <p id="subjects-js3gePctLu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-js3gePctLu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-js3gePctLu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-js3gePctLu@OpenReview" onclick="foldPdfKimi('js3gePctLu@OpenReview', this)" class="hr hr-fold">
        </div><div id="jvP1wbD0xh@OpenReview" class="panel paper" keywords="dgss,agent,queries,scene,segmentation,semantic,querydiff,domain,guidance,diffusion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jvP1wbD0xh" target="_blank" title="154/225"><span class="index notranslate">#154</span></a>
                <a id="title-jvP1wbD0xh@OpenReview" class="title-link" href="/venue/jvP1wbD0xh@OpenReview" target="_blank">Better to Teach than to Give: Domain Generalized Semantic Segmentation via Agent Queries with Diffusion Model Guidance</a>
                <a id="pdf-jvP1wbD0xh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jvP1wbD0xh@OpenReview', this)" data="https://openreview.net/pdf?id=jvP1wbD0xh">[PDF<sup id="pdf-stars-jvP1wbD0xh@OpenReview">3</sup>]</a>
                <a id="copy-jvP1wbD0xh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jvP1wbD0xh@OpenReview')">[Copy]</a>
                <a id="kimi-jvP1wbD0xh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jvP1wbD0xh@OpenReview', this)">[Kimi<sup id="kimi-stars-jvP1wbD0xh@OpenReview">1</sup>]</a>
                <a id="rel-jvP1wbD0xh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jvP1wbD0xh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jvP1wbD0xh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Li" target="_blank">Fan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Wang" target="_blank">Xuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Qi" target="_blank">Min Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxiang Zhang" target="_blank">Zhaoxiang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=yuelei xu" target="_blank">yuelei xu</a>
            </p>
            <p id="summary-jvP1wbD0xh@OpenReview" class="summary">Domain Generalized Semantic Segmentation (DGSS) trains a model on a labeled source domain to generalize to unseen target domains with consistent contextual distribution and varying visual appearance.Most existing methods rely on domain randomization or data generation but struggle to capture the underlying scene distribution, resulting in the loss of useful semantic information. Inspired by the diffusion model's capability to generate diverse variations within a given scene context, we consider harnessing its rich prior knowledge of scene distribution to tackle the challenging DGSS task.In this paper, we propose a novel agent \textbf{Query}-driven learning framework based on \textbf{Diff}usion model guidance for DGSS, named QueryDiff. Our recipe comprises three key ingredients: (1) generating agent queries from segmentation features to aggregate semantic information about instances within the scene; (2) learning the inherent semantic distribution of the scene through agent queries guided by diffusion features; (3) refining segmentation features using optimized agent queries for robust mask predictions.Extensive experiments across various settings demonstrate that our method significantly outperforms previous state-of-the-art methods. Notably, it enhances the model's ability to generalize effectively to extreme domains, such as cubist art styles. Code is available at https://github.com/FanLiHub/QueryDiff.</p>
            <p id="subjects-jvP1wbD0xh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-jvP1wbD0xh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jvP1wbD0xh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jvP1wbD0xh@OpenReview" onclick="foldPdfKimi('jvP1wbD0xh@OpenReview', this)" class="hr hr-fold">
        </div><div id="kfYxyvCYQ4@OpenReview" class="panel paper" keywords="hyperspherical,simbav2,normalization,reinforcement,reward,lesson,scaling,optimization,scalable,learning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kfYxyvCYQ4" target="_blank" title="155/225"><span class="index notranslate">#155</span></a>
                <a id="title-kfYxyvCYQ4@OpenReview" class="title-link" href="/venue/kfYxyvCYQ4@OpenReview" target="_blank">Hyperspherical Normalization for Scalable Deep Reinforcement Learning</a>
                <a id="pdf-kfYxyvCYQ4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kfYxyvCYQ4@OpenReview', this)" data="https://openreview.net/pdf?id=kfYxyvCYQ4">[PDF<sup id="pdf-stars-kfYxyvCYQ4@OpenReview">5</sup>]</a>
                <a id="copy-kfYxyvCYQ4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kfYxyvCYQ4@OpenReview')">[Copy]</a>
                <a id="kimi-kfYxyvCYQ4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kfYxyvCYQ4@OpenReview', this)">[Kimi<sup id="kimi-stars-kfYxyvCYQ4@OpenReview">4</sup>]</a>
                <a id="rel-kfYxyvCYQ4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kfYxyvCYQ4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kfYxyvCYQ4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hojoon Lee" target="_blank">Hojoon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youngdo Lee" target="_blank">Youngdo Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Takuma Seno" target="_blank">Takuma Seno</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Donghu Kim" target="_blank">Donghu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Stone" target="_blank">Peter Stone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaegul Choo" target="_blank">Jaegul Choo</a>
            </p>
            <p id="summary-kfYxyvCYQ4@OpenReview" class="summary">Scaling up the model size and computation has brought consistent performance improvements in supervised learning. However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization.In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by hyperspherical normalization; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains.</p>
            <p id="subjects-kfYxyvCYQ4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-kfYxyvCYQ4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kfYxyvCYQ4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kfYxyvCYQ4@OpenReview" onclick="foldPdfKimi('kfYxyvCYQ4@OpenReview', this)" class="hr hr-fold">
        </div><div id="kmg7hweySi@OpenReview" class="panel paper" keywords="fastica,ica,inputs,gaussian,feature,learnt,sgd,gtrsim,non,independent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kmg7hweySi" target="_blank" title="156/225"><span class="index notranslate">#156</span></a>
                <a id="title-kmg7hweySi@OpenReview" class="title-link" href="/venue/kmg7hweySi@OpenReview" target="_blank">Feature learning from non-Gaussian inputs: the case of Independent Component Analysis in high dimensions</a>
                <a id="pdf-kmg7hweySi@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kmg7hweySi@OpenReview', this)" data="https://openreview.net/pdf?id=kmg7hweySi">[PDF<sup id="pdf-stars-kmg7hweySi@OpenReview"></sup>]</a>
                <a id="copy-kmg7hweySi@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kmg7hweySi@OpenReview')">[Copy]</a>
                <a id="kimi-kmg7hweySi@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kmg7hweySi@OpenReview', this)">[Kimi<sup id="kimi-stars-kmg7hweySi@OpenReview"></sup>]</a>
                <a id="rel-kmg7hweySi@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kmg7hweySi@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kmg7hweySi@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fabiola Ricci" target="_blank">Fabiola Ricci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Bardone" target="_blank">Lorenzo Bardone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Goldt" target="_blank">Sebastian Goldt</a>
            </p>
            <p id="summary-kmg7hweySi@OpenReview" class="summary">Deep neural networks learn structured features from complex, non-Gaussian inputs, but the mechanisms behind this process remain poorly understood. Our work is motivated by the observation that the first-layer filters learnt by deep convolutional neural networks from natural images resemble those learnt by independent component analysis (ICA), a simple unsupervised method that seeks the most non-Gaussian projections of its inputs. This similarity suggests that ICA provides a simple, yet principled model for studying feature learning. Here, we leverage this connection to investigate the interplay between data structure and optimisation in feature learning for the most popular ICA algorithm, FastICA, and stochastic gradient descent (SGD), which is used to train deep networks. We rigorously establish that FastICA requires at least <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-124-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x2273;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-916" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1002.92em, 2.815em, -999.997em); top: -2.445em; left: 0em;"><span class="mrow" id="MathJax-Span-917"><span class="mi" id="MathJax-Span-918" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-919" style="font-family: MathJax_AMS; padding-left: 0.263em;">≳</span><span class="msubsup" id="MathJax-Span-920" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-921" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="mn" id="MathJax-Span-922" style="font-size: 70.7%; font-family: MathJax_Main;">4</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>≳</mo><msup><mi>d</mi><mn>4</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-124">n\gtrsim d^4</script> samples to recover a single non-Gaussian direction from <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-125-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-923" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-924"><span class="mi" id="MathJax-Span-925" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-125">d</script>-dimensional inputs on a simple synthetic data model. We show that vanilla online SGD outperforms FastICA, and prove that the optimal sample complexity <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-126-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&amp;#x2273;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-926" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1002.92em, 2.815em, -999.997em); top: -2.445em; left: 0em;"><span class="mrow" id="MathJax-Span-927"><span class="mi" id="MathJax-Span-928" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-929" style="font-family: MathJax_AMS; padding-left: 0.263em;">≳</span><span class="msubsup" id="MathJax-Span-930" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-931" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="mn" id="MathJax-Span-932" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.451em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>≳</mo><msup><mi>d</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-126">n\gtrsim d^2</script> can be reached by smoothing the loss, albeit in a data-dependent way. We finally demonstrate the existence of a search phase for FastICA on ImageNet, and discuss how the strong non-Gaussianity of said images compensates for the poor sample complexity of FastICA.</p>
            <p id="subjects-kmg7hweySi@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-kmg7hweySi@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kmg7hweySi@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kmg7hweySi@OpenReview" onclick="foldPdfKimi('kmg7hweySi@OpenReview', this)" class="hr hr-fold">
        </div><div id="mEV0nvHcK3@OpenReview" class="panel paper" keywords="kbi,code,defect,merge,review,context,oversimplify,codebases,snippet,text">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mEV0nvHcK3" target="_blank" title="157/225"><span class="index notranslate">#157</span></a>
                <a id="title-mEV0nvHcK3@OpenReview" class="title-link" href="/venue/mEV0nvHcK3@OpenReview" target="_blank">Towards Practical Defect-Focused Automated Code Review</a>
                <a id="pdf-mEV0nvHcK3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mEV0nvHcK3@OpenReview', this)" data="https://openreview.net/pdf?id=mEV0nvHcK3">[PDF<sup id="pdf-stars-mEV0nvHcK3@OpenReview"></sup>]</a>
                <a id="copy-mEV0nvHcK3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mEV0nvHcK3@OpenReview')">[Copy]</a>
                <a id="kimi-mEV0nvHcK3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mEV0nvHcK3@OpenReview', this)">[Kimi<sup id="kimi-stars-mEV0nvHcK3@OpenReview">1</sup>]</a>
                <a id="rel-mEV0nvHcK3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mEV0nvHcK3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mEV0nvHcK3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junyi Lu" target="_blank">Junyi Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lili Jiang" target="_blank">Lili Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojia Li" target="_blank">Xiaojia Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianbing Fang" target="_blank">Jianbing Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengjun Zhang" target="_blank">Fengjun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Yang" target="_blank">Li Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun Zuo" target="_blank">Chun Zuo</a>
            </p>
            <p id="summary-mEV0nvHcK3@OpenReview" class="summary">The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2× improvement over standard LLMs and a 10× gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.</p>
            <p id="subjects-mEV0nvHcK3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mEV0nvHcK3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mEV0nvHcK3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mEV0nvHcK3@OpenReview" onclick="foldPdfKimi('mEV0nvHcK3@OpenReview', this)" class="hr hr-fold">
        </div><div id="mruyFvKDKq@OpenReview" class="panel paper" keywords="uplift,idum,textbf,marketing,online,distribution,modeling,invariant,odeling,sufficiency">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mruyFvKDKq" target="_blank" title="158/225"><span class="index notranslate">#158</span></a>
                <a id="title-mruyFvKDKq@OpenReview" class="title-link" href="/venue/mruyFvKDKq@OpenReview" target="_blank">Invariant Deep Uplift Modeling for Incentive Assignment in Online Marketing via Probability of Necessity and Sufficiency</a>
                <a id="pdf-mruyFvKDKq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mruyFvKDKq@OpenReview', this)" data="https://openreview.net/pdf?id=mruyFvKDKq">[PDF<sup id="pdf-stars-mruyFvKDKq@OpenReview">1</sup>]</a>
                <a id="copy-mruyFvKDKq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mruyFvKDKq@OpenReview')">[Copy]</a>
                <a id="kimi-mruyFvKDKq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mruyFvKDKq@OpenReview', this)">[Kimi<sup id="kimi-stars-mruyFvKDKq@OpenReview">1</sup>]</a>
                <a id="rel-mruyFvKDKq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mruyFvKDKq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mruyFvKDKq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zexu Sun" target="_blank">Zexu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiyu Han" target="_blank">Qiyu Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Yang" target="_blank">Hao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anpeng Wu" target="_blank">Anpeng Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minqin Zhu" target="_blank">Minqin Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dugang Liu" target="_blank">Dugang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Ma" target="_blank">Chen Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunpeng Weng" target="_blank">Yunpeng Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Tang" target="_blank">Xing Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=xiuqiang He" target="_blank">xiuqiang He</a>
            </p>
            <p id="summary-mruyFvKDKq@OpenReview" class="summary">In online platforms, incentives (\textit{e.g}., discounts, coupons) are used to boost user engagement and revenue. Uplift modeling methods are developed to estimate user responses from observational data, often incorporating distribution balancing to address selection bias. However, these methods are limited by in-distribution testing data, which mirrors the training data distribution. In reality, user features change continuously due to time, geography, and other factors, especially on complex online marketing platforms. Thus, effective uplift modeling method for out-of-distribution data is crucial. To address this, we propose a novel uplift modeling method \textbf{I}nvariant \textbf{D}eep \textbf{U}plift \textbf{M}odeling, namely \textbf{IDUM}, which uses invariant learning to enhance out-of-distribution generalization by identifying causal factors that remain consistent across domains. IDUM further refines these features into necessary and sufficient factors and employs a masking component to reduce computational costs by selecting the most informative invariant features. A balancing discrepancy component is also introduced to mitigate selection bias in observational data. We conduct extensive experiments on public and real-world datasets to demonstrate IDUM's effectiveness in both in-distribution and out-of-distribution scenarios in online marketing. Furthermore, we also provide theoretical analysis and related proofs to support our IDUM's generalizability.</p>
            <p id="subjects-mruyFvKDKq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mruyFvKDKq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mruyFvKDKq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mruyFvKDKq@OpenReview" onclick="foldPdfKimi('mruyFvKDKq@OpenReview', this)" class="hr hr-fold">
        </div><div id="mwSBIlNLdQ@OpenReview" class="panel paper" keywords="constraints,geometric,dynamics,learning,environmental,bundle,constraint,sensing,measurement,satisfaction">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mwSBIlNLdQ" target="_blank" title="159/225"><span class="index notranslate">#159</span></a>
                <a id="title-mwSBIlNLdQ@OpenReview" class="title-link" href="/venue/mwSBIlNLdQ@OpenReview" target="_blank">Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures</a>
                <a id="pdf-mwSBIlNLdQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mwSBIlNLdQ@OpenReview', this)" data="https://openreview.net/pdf?id=mwSBIlNLdQ">[PDF<sup id="pdf-stars-mwSBIlNLdQ@OpenReview">2</sup>]</a>
                <a id="copy-mwSBIlNLdQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mwSBIlNLdQ@OpenReview')">[Copy]</a>
                <a id="kimi-mwSBIlNLdQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mwSBIlNLdQ@OpenReview', this)">[Kimi<sup id="kimi-stars-mwSBIlNLdQ@OpenReview">3</sup>]</a>
                <a id="rel-mwSBIlNLdQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mwSBIlNLdQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mwSBIlNLdQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongzhe Zheng" target="_blank">Dongzhe Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjie Mei" target="_blank">Wenjie Mei</a>
            </p>
            <p id="summary-mwSBIlNLdQ@OpenReview" class="summary">Learning unknown dynamics under environmental (or external) constraints is fundamental to many fields (e.g., modern robotics), particularly challenging when constraint information is only locally available and uncertain. Existing approaches requiring global constraints or using probabilistic filtering fail to fully exploit the geometric structure inherent in local measurements (by using, e.g., sensors) and constraints. This paper presents a geometric framework unifying measurements, constraints, and dynamics learning through a fiber bundle structure over the state space. This naturally induced geometric structure enables measurement-aware Control Barrier Functions that adapt to local sensing (or measurement) conditions. By integrating Neural ODEs, our framework learns continuous-time dynamics while preserving geometric constraints, with theoretical guarantees of learning convergence and constraint satisfaction dependent on sensing quality. The geometric framework not only enables efficient dynamics learning but also suggests promising directions for integration with reinforcement learning approaches. Extensive simulations demonstrate significant improvements in both learning efficiency and constraint satisfaction over traditional methods, especially under limited and uncertain sensing conditions.</p>
            <p id="subjects-mwSBIlNLdQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mwSBIlNLdQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mwSBIlNLdQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mwSBIlNLdQ@OpenReview" onclick="foldPdfKimi('mwSBIlNLdQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="mzSwYvwYdC@OpenReview" class="panel paper" keywords="models,independent,initializations,test,values,llama,assumptions,unconstrained,copies,tests">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mzSwYvwYdC" target="_blank" title="160/225"><span class="index notranslate">#160</span></a>
                <a id="title-mzSwYvwYdC@OpenReview" class="title-link" href="/venue/mzSwYvwYdC@OpenReview" target="_blank">Independence Tests for Language Models</a>
                <a id="pdf-mzSwYvwYdC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mzSwYvwYdC@OpenReview', this)" data="https://openreview.net/pdf?id=mzSwYvwYdC">[PDF<sup id="pdf-stars-mzSwYvwYdC@OpenReview">4</sup>]</a>
                <a id="copy-mzSwYvwYdC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mzSwYvwYdC@OpenReview')">[Copy]</a>
                <a id="kimi-mzSwYvwYdC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mzSwYvwYdC@OpenReview', this)">[Kimi<sup id="kimi-stars-mzSwYvwYdC@OpenReview">3</sup>]</a>
                <a id="rel-mzSwYvwYdC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mzSwYvwYdC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mzSwYvwYdC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sally Zhu" target="_blank">Sally Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ahmed Ahmed" target="_blank">Ahmed Ahmed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohith Kuditipudi" target="_blank">Rohith Kuditipudi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Percy Liang" target="_blank">Percy Liang</a>
            </p>
            <p id="summary-mzSwYvwYdC@OpenReview" class="summary">Motivated by liability and intellectual property concerns over open-weight models we consider the following problem: given the weights of two models, can we test whether they were trained independently---i.e., from independent random initializations? We consider two settings: *constrained* and *unconstrained*. In the constrained setting, we make assumptions about model architecture and training and propose statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. We compute the p-values by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures between the original two models versus these copies. We report p-values on pairs of 21 open-weight models (210 total pairs) and find we correctly identify all pairs of non-independent models. In the unconstrained setting we make none of the prior assumptions and allow for adversarial evasion attacks that do not change model output. We thus propose a new test which matches hidden activations between two models, which is robust to these transformations and to changes in model architecture and can also identify specific non-independent components of models. Though we no longer obtain exact p-values from this test, empirically we find it reliably distinguishes non-independent models like a p-value. Notably, we can use the test to identify specific parts of one model that are derived from another (e.g., how Llama 3.1-8B was pruned to initialize Llama 3.2-3B, or shared layers between Mistral-7B and StripedHyena-7B), and it is even robust to retraining individual layers of either model from scratch.</p>
            <p id="subjects-mzSwYvwYdC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mzSwYvwYdC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mzSwYvwYdC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mzSwYvwYdC@OpenReview" onclick="foldPdfKimi('mzSwYvwYdC@OpenReview', this)" class="hr hr-fold">
        </div><div id="nVD7KoU09V@OpenReview" class="panel paper" keywords="graphode,static,causal,generalization,coupled,dynamical,initialization,cmcd,rethink,dynamic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=nVD7KoU09V" target="_blank" title="161/225"><span class="index notranslate">#161</span></a>
                <a id="title-nVD7KoU09V@OpenReview" class="title-link" href="/venue/nVD7KoU09V@OpenReview" target="_blank">Rethink GraphODE Generalization within Coupled Dynamical System</a>
                <a id="pdf-nVD7KoU09V@OpenReview" class="title-pdf notranslate" onclick="togglePdf('nVD7KoU09V@OpenReview', this)" data="https://openreview.net/pdf?id=nVD7KoU09V">[PDF<sup id="pdf-stars-nVD7KoU09V@OpenReview">2</sup>]</a>
                <a id="copy-nVD7KoU09V@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('nVD7KoU09V@OpenReview')">[Copy]</a>
                <a id="kimi-nVD7KoU09V@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('nVD7KoU09V@OpenReview', this)">[Kimi<sup id="kimi-stars-nVD7KoU09V@OpenReview">1</sup>]</a>
                <a id="rel-nVD7KoU09V@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('nVD7KoU09V@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-nVD7KoU09V@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guancheng Wan" target="_blank">Guancheng Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zijie Huang" target="_blank">Zijie Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanjia Zhao" target="_blank">Wanjia Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Luo" target="_blank">Xiao Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhou Sun" target="_blank">Yizhou Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Wang" target="_blank">Wei Wang</a>
            </p>
            <p id="summary-nVD7KoU09V@OpenReview" class="summary">Coupled dynamical systems govern essential phenomena across physics, biology, and engineering, where components interact through complex dependencies. While Graph Ordinary Differential Equations (GraphODE) offer a powerful framework to model these systems, their **generalization** capabilities degrade severely under limited observational training data due to two fundamental flaws: (i) the entanglement of static attributes and dynamic states in the initialization process, and (ii) the reliance on context-specific coupling patterns during training, which hinders performance in unseen scenarios. In this paper, we propose a Generalizable GraphODE with disentanglement and regularization (GREAT) to address these challenges. Through systematic analysis via the Structural Causal Model, we identify backdoor paths that undermine generalization and design two key modules to mitigate their effects. The *Dynamic-Static Equilibrium Decoupler (DyStaED)* disentangles static and dynamic states via orthogonal subspace projections, ensuring robust initialization. Furthermore, the *Causal Mediation for Coupled Dynamics (CMCD)* employs variational inference to estimate latent causal factors, reducing spurious correlations and enhancing universal coupling dynamics. Extensive experiments across diverse dynamical systems demonstrate that ours outperforms state-of-the-art methods within both in-distribution and out-of-distribution.</p>
            <p id="subjects-nVD7KoU09V@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-nVD7KoU09V@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-nVD7KoU09V@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-nVD7KoU09V@OpenReview" onclick="foldPdfKimi('nVD7KoU09V@OpenReview', this)" class="hr hr-fold">
        </div><div id="oYyaVSqEFu@OpenReview" class="panel paper" keywords="asynchronous,anomaly,driving,cameras,millisecond,detection,time,multimodal,event,network">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oYyaVSqEFu" target="_blank" title="162/225"><span class="index notranslate">#162</span></a>
                <a id="title-oYyaVSqEFu@OpenReview" class="title-link" href="/venue/oYyaVSqEFu@OpenReview" target="_blank">When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network</a>
                <a id="pdf-oYyaVSqEFu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oYyaVSqEFu@OpenReview', this)" data="https://openreview.net/pdf?id=oYyaVSqEFu">[PDF<sup id="pdf-stars-oYyaVSqEFu@OpenReview">3</sup>]</a>
                <a id="copy-oYyaVSqEFu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oYyaVSqEFu@OpenReview')">[Copy]</a>
                <a id="kimi-oYyaVSqEFu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oYyaVSqEFu@OpenReview', this)">[Kimi<sup id="kimi-stars-oYyaVSqEFu@OpenReview">3</sup>]</a>
                <a id="rel-oYyaVSqEFu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oYyaVSqEFu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oYyaVSqEFu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Xiao" target="_blank">Dong Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangyao Chen" target="_blank">Guangyao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peixi Peng" target="_blank">Peixi Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yangru Huang" target="_blank">Yangru Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Zhao" target="_blank">Yifan Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxing Dai" target="_blank">Yongxing Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonghong Tian" target="_blank">Yonghong Tian</a>
            </p>
            <p id="summary-oYyaVSqEFu@OpenReview" class="summary">Anomaly detection is essential for the safety and reliability of autonomous driving systems. Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios. In this paper, we introduce real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy. We propose a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images. This combination effectively captures both the temporal dynamics and spatial details of the driving environment, enabling swift and precise anomaly detection. Extensive experiments on benchmark datasets show that our approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance.</p>
            <p id="subjects-oYyaVSqEFu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-oYyaVSqEFu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oYyaVSqEFu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oYyaVSqEFu@OpenReview" onclick="foldPdfKimi('oYyaVSqEFu@OpenReview', this)" class="hr hr-fold">
        </div><div id="oa7MYAO6h6@OpenReview" class="panel paper" keywords="shadowkv,cache,throughput,context,inference,decoding,gpu,batch,memory,llm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=oa7MYAO6h6" target="_blank" title="163/225"><span class="index notranslate">#163</span></a>
                <a id="title-oa7MYAO6h6@OpenReview" class="title-link" href="/venue/oa7MYAO6h6@OpenReview" target="_blank">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a>
                <a id="pdf-oa7MYAO6h6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('oa7MYAO6h6@OpenReview', this)" data="https://openreview.net/pdf?id=oa7MYAO6h6">[PDF<sup id="pdf-stars-oa7MYAO6h6@OpenReview">12</sup>]</a>
                <a id="copy-oa7MYAO6h6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('oa7MYAO6h6@OpenReview')">[Copy]</a>
                <a id="kimi-oa7MYAO6h6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('oa7MYAO6h6@OpenReview', this)">[Kimi<sup id="kimi-stars-oa7MYAO6h6@OpenReview">7</sup>]</a>
                <a id="rel-oa7MYAO6h6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('oa7MYAO6h6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-oa7MYAO6h6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanshi Sun" target="_blank">Hanshi Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li-Wen Chang" target="_blank">Li-Wen Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenlei Bao" target="_blank">Wenlei Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Size Zheng" target="_blank">Size Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ningxin Zheng" target="_blank">Ningxin Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Liu" target="_blank">Xin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harry Dong" target="_blank">Harry Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuejie Chi" target="_blank">Yuejie Chi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beidi Chen" target="_blank">Beidi Chen</a>
            </p>
            <p id="summary-oa7MYAO6h6@OpenReview" class="summary">With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for decoding both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to accelerate inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory usage or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on benchmarks like RULER, LongBench, and models such as Llama-3.1-8B and GLM-4-9B-1M, we demonstrate that it achieves up to 6<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-127-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-933" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-934"><span class="mo" id="MathJax-Span-935" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-127">\times</script> larger batch sizes and 3.04<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-128-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-936" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-937"><span class="mo" id="MathJax-Span-938" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-128">\times</script> higher throughput on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory.</p>
            <p id="subjects-oa7MYAO6h6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-oa7MYAO6h6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-oa7MYAO6h6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-oa7MYAO6h6@OpenReview" onclick="foldPdfKimi('oa7MYAO6h6@OpenReview', this)" class="hr hr-fold">
        </div><div id="otNB7BzsiR@OpenReview" class="panel paper" keywords="sparsity,wise,layer,theoretical,determining,sparsification,llms,reconstruction,layers,perspective">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=otNB7BzsiR" target="_blank" title="164/225"><span class="index notranslate">#164</span></a>
                <a id="title-otNB7BzsiR@OpenReview" class="title-link" href="/venue/otNB7BzsiR@OpenReview" target="_blank">Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective</a>
                <a id="pdf-otNB7BzsiR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('otNB7BzsiR@OpenReview', this)" data="https://openreview.net/pdf?id=otNB7BzsiR">[PDF<sup id="pdf-stars-otNB7BzsiR@OpenReview">3</sup>]</a>
                <a id="copy-otNB7BzsiR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('otNB7BzsiR@OpenReview')">[Copy]</a>
                <a id="kimi-otNB7BzsiR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('otNB7BzsiR@OpenReview', this)">[Kimi<sup id="kimi-stars-otNB7BzsiR@OpenReview">6</sup>]</a>
                <a id="rel-otNB7BzsiR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('otNB7BzsiR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-otNB7BzsiR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weizhong Huang" target="_blank">Weizhong Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxin Zhang" target="_blank">Yuxin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiawu Zheng" target="_blank">Xiawu Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Chao" target="_blank">Fei Chao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rongrong Ji" target="_blank">Rongrong Ji</a>
            </p>
            <p id="summary-otNB7BzsiR@OpenReview" class="summary">In this paper, we address the challenge of determining the layer-wise sparsity rates of large language models (LLMs) through a theoretical perspective. Specifically, we identify a critical issue of **"reconstruction error explosion"** in existing LLMs sparsification methods. This refers to the cumulative effect of reconstruction errors throughout the sparsification process, where errors from earlier layers propagate and amplify in subsequent layers. As a result, the overall reconstruction error increases significantly, leading to a substantial degradation in model performance. Through theoretical analysis, we derive a simple yet effective approach to layer-wise sparsity allocation that mitigates this issue. Our method uses a monotonically increasing arithmetic progression, reducing the process of determining sparsity rates for multiple layers to the determination of a single common difference hyperparameter. Remarkably, this allows for the optimal layer-wise sparsity rates to be identified with just a few trials. Both our theoretical analysis and experimental results demonstrate that this sparsity allocation scheme is near optimal. Extensive experiments show that our method significantly improves the performance of sparse LLMs across various architectures, outperforming existing layer-wise sparsity methods. Furthermore, it enhances the performance of various compression techniques and is applicable to vision and multimodal models. Notably, our method achieves a reduction of 52.10 in perplexity for the 70% sparse LLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by 10.50%, and delivers speedups of 2.63<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-129-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-939" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-940"><span class="mo" id="MathJax-Span-941" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-129">\times</script> and 2.23<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-130-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-942" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-943"><span class="mo" id="MathJax-Span-944" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-130">\times</script> on CPU and GPU, respectively. Code is available at https://github.com/wzhuang-xmu/ATP.</p>
            <p id="subjects-otNB7BzsiR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-otNB7BzsiR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-otNB7BzsiR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-otNB7BzsiR@OpenReview" onclick="foldPdfKimi('otNB7BzsiR@OpenReview', this)" class="hr hr-fold">
        </div><div id="pKaNgFzJBy@OpenReview" class="panel paper" keywords="guidance,matching,flow,westlakeu,ai4science,general,methods,abbreviated,different,predecessor">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pKaNgFzJBy" target="_blank" title="165/225"><span class="index notranslate">#165</span></a>
                <a id="title-pKaNgFzJBy@OpenReview" class="title-link" href="/venue/pKaNgFzJBy@OpenReview" target="_blank">On the Guidance of Flow Matching</a>
                <a id="pdf-pKaNgFzJBy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pKaNgFzJBy@OpenReview', this)" data="https://openreview.net/pdf?id=pKaNgFzJBy">[PDF<sup id="pdf-stars-pKaNgFzJBy@OpenReview">7</sup>]</a>
                <a id="copy-pKaNgFzJBy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pKaNgFzJBy@OpenReview')">[Copy]</a>
                <a id="kimi-pKaNgFzJBy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pKaNgFzJBy@OpenReview', this)">[Kimi<sup id="kimi-stars-pKaNgFzJBy@OpenReview">7</sup>]</a>
                <a id="rel-pKaNgFzJBy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pKaNgFzJBy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pKaNgFzJBy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiqi Feng" target="_blank">Ruiqi Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenglei Yu" target="_blank">Chenglei Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Deng" target="_blank">Wenhao Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiyan Hu" target="_blank">Peiyan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tailin Wu" target="_blank">Tailin Wu</a>
            </p>
            <p id="summary-pKaNgFzJBy@OpenReview" class="summary">Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.</p>
            <p id="subjects-pKaNgFzJBy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pKaNgFzJBy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pKaNgFzJBy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pKaNgFzJBy@OpenReview" onclick="foldPdfKimi('pKaNgFzJBy@OpenReview', this)" class="hr hr-fold">
        </div><div id="pWs925fKyK@OpenReview" class="panel paper" keywords="rtl,timing,rtldistil,eda,prediction,distillation,layout,teacher,gnn,level">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pWs925fKyK" target="_blank" title="166/225"><span class="index notranslate">#166</span></a>
                <a id="title-pWs925fKyK@OpenReview" class="title-link" href="/venue/pWs925fKyK@OpenReview" target="_blank">Bridging Layout and RTL: Knowledge Distillation based Timing Prediction</a>
                <a id="pdf-pWs925fKyK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pWs925fKyK@OpenReview', this)" data="https://openreview.net/pdf?id=pWs925fKyK">[PDF<sup id="pdf-stars-pWs925fKyK@OpenReview">5</sup>]</a>
                <a id="copy-pWs925fKyK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pWs925fKyK@OpenReview')">[Copy]</a>
                <a id="kimi-pWs925fKyK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pWs925fKyK@OpenReview', this)">[Kimi<sup id="kimi-stars-pWs925fKyK@OpenReview">1</sup>]</a>
                <a id="rel-pWs925fKyK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pWs925fKyK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pWs925fKyK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mingjun Wang" target="_blank">Mingjun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yihan Wen" target="_blank">Yihan Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Sun" target="_blank">Bin Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianan Mu" target="_blank">Jianan Mu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juan Li" target="_blank">Juan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyi Wang" target="_blank">Xiaoyi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Ye" target="_blank">Jing Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bei Yu" target="_blank">Bei Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huawei Li" target="_blank">Huawei Li</a>
            </p>
            <p id="summary-pWs925fKyK@OpenReview" class="summary">Accurate and efficient timing prediction at the register-transfer level (RTL) remains a fundamental challenge in electronic design automation (EDA), particularly in striking a balance between accuracy and computational efficiency. While static timing analysis (STA) provides high-fidelity results through comprehensive physical parameters, its computational overhead makes it impractical for rapid design iterations. Conversely, existing RTL-level approaches sacrifice accuracy due to the limited physical information available. We propose RTLDistil, a novel cross-stage knowledge distillation framework that bridges this gap by transferring precise physical characteristics from a layout-aware teacher model (Teacher GNN) to an efficient RTL-level student model (Student GNN), both implemented as graph neural networks (GNNs). RTLDistil efficiently predicts key timing metrics, such as arrival time (AT), and employs a multi-granularity distillation strategy that captures timing-critical features at node, subgraph, and global levels. Experimental results demonstrate that RTLDistil achieves significant improvement in RTL-level timing prediction error reduction, compared to state-of-the-art prediction models. This framework enables accurate early-stage timing prediction, advancing EDA's ``left-shift'' paradigm while maintaining computational efficiency. Our code and dataset will be publicly available at https://github.com/sklp-eda-lab/RTLDistil.</p>
            <p id="subjects-pWs925fKyK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pWs925fKyK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pWs925fKyK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pWs925fKyK@OpenReview" onclick="foldPdfKimi('pWs925fKyK@OpenReview', this)" class="hr hr-fold">
        </div><div id="tNGdLEL4R0@OpenReview" class="panel paper" keywords="robustness,scaling,defense,attack,models,adversarially,adversarial,attacks,alignmentresearch,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tNGdLEL4R0" target="_blank" title="167/225"><span class="index notranslate">#167</span></a>
                <a id="title-tNGdLEL4R0@OpenReview" class="title-link" href="/venue/tNGdLEL4R0@OpenReview" target="_blank">Scaling Trends in Language Model Robustness</a>
                <a id="pdf-tNGdLEL4R0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tNGdLEL4R0@OpenReview', this)" data="https://openreview.net/pdf?id=tNGdLEL4R0">[PDF<sup id="pdf-stars-tNGdLEL4R0@OpenReview">2</sup>]</a>
                <a id="copy-tNGdLEL4R0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tNGdLEL4R0@OpenReview')">[Copy]</a>
                <a id="kimi-tNGdLEL4R0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tNGdLEL4R0@OpenReview', this)">[Kimi<sup id="kimi-stars-tNGdLEL4R0@OpenReview">3</sup>]</a>
                <a id="rel-tNGdLEL4R0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tNGdLEL4R0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tNGdLEL4R0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nikolaus Howe" target="_blank">Nikolaus Howe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ian McKenzie" target="_blank">Ian McKenzie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oskar Hollinsworth" target="_blank">Oskar Hollinsworth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michał Zając" target="_blank">Michał Zając</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Tseng" target="_blank">Tom Tseng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Tucker" target="_blank">Aaron Tucker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre-Luc Bacon" target="_blank">Pierre-Luc Bacon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Gleave" target="_blank">Adam Gleave</a>
            </p>
            <p id="summary-tNGdLEL4R0@OpenReview" class="summary">Increasing model size has unlocked a dazzling array of capabilities in language models.At the same time, even frontier models remain vulnerable to jailbreaks and prompt injections, despite concerted efforts to make them robust.As both attackers and defenders gain access to more compute, and as models become larger, what will be the effect on robustness?We argue that to answer this question requires a *scaling lens*, which we adopt in an extensive study of language model robustness across several classification tasks, model families, and adversarial attacks.We find that in the absence of explicit safety training, larger models are not consistently more robust; however, scale improves sample efficiency in adversarial training, though it worsens compute efficiency.Further, we find that increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models.Finally, after exploring robustness transfer across attacks and threat models, we combine attack and defense scaling rates to study the offense-defense balance.We find that while attack scaling outpaces adversarial training across all models studied, larger adversarially trained models might give defense the advantage in the long run.These results underscore the utility of the scaling lens, and provide a paradigm for evaluating future attacks and defenses on frontier models.Code for this project is available at https://github.com/AlignmentResearch/scaling-llm-robustness-paper.</p>
            <p id="subjects-tNGdLEL4R0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tNGdLEL4R0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tNGdLEL4R0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tNGdLEL4R0@OpenReview" onclick="foldPdfKimi('tNGdLEL4R0@OpenReview', this)" class="hr hr-fold">
        </div><div id="ps3aO9MHJv@OpenReview" class="panel paper" keywords="missing,missingness,values,interpretability,reliance,test,models,tree,unregularized,imputed">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ps3aO9MHJv" target="_blank" title="168/225"><span class="index notranslate">#168</span></a>
                <a id="title-ps3aO9MHJv@OpenReview" class="title-link" href="/venue/ps3aO9MHJv@OpenReview" target="_blank">Prediction models that learn to avoid missing values</a>
                <a id="pdf-ps3aO9MHJv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ps3aO9MHJv@OpenReview', this)" data="https://openreview.net/pdf?id=ps3aO9MHJv">[PDF<sup id="pdf-stars-ps3aO9MHJv@OpenReview">1</sup>]</a>
                <a id="copy-ps3aO9MHJv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ps3aO9MHJv@OpenReview')">[Copy]</a>
                <a id="kimi-ps3aO9MHJv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ps3aO9MHJv@OpenReview', this)">[Kimi<sup id="kimi-stars-ps3aO9MHJv@OpenReview">1</sup>]</a>
                <a id="rel-ps3aO9MHJv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ps3aO9MHJv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ps3aO9MHJv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lena Stempfle" target="_blank">Lena Stempfle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anton Matsson" target="_blank">Anton Matsson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Newton Mwai" target="_blank">Newton Mwai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fredrik Johansson" target="_blank">Fredrik Johansson</a>
            </p>
            <p id="summary-ps3aO9MHJv@OpenReview" class="summary">Handling missing values at test time is challenging for machine learning models, especially when aiming for both high accuracy and interpretability. Established approaches often add bias through imputation or excessive model complexity via missingness indicators. Moreover, either method can obscure interpretability, making it harder to understand how the model utilizes the observed variables in predictions. We propose *missingness-avoiding* (MA) machine learning, a general framework for training models to rarely require the values of missing (or imputed) features at test time. We create tailored MA learning algorithms for decision trees, tree ensembles, and sparse linear models by incorporating classifier-specific regularization terms in their learning objectives. The tree-based models leverage contextual missingness by reducing reliance on missing values based on the observed context. Experiments on real-world datasets demonstrate that **MA-DT, MA-LASSO, MA-RF**, and **MA-GBT** effectively reduce the reliance on features with missing values while maintaining predictive performance competitive with their unregularized counterparts. This shows that our framework gives practitioners a powerful tool to maintain interpretability in predictions with test-time missing values.</p>
            <p id="subjects-ps3aO9MHJv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ps3aO9MHJv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ps3aO9MHJv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ps3aO9MHJv@OpenReview" onclick="foldPdfKimi('ps3aO9MHJv@OpenReview', this)" class="hr hr-fold">
        </div><div id="pyIXyl4qFx@OpenReview" class="panel paper" keywords="mesh,adaptivity,relocation,meshing,classical,fems,gnn,solution,methods,element">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pyIXyl4qFx" target="_blank" title="169/225"><span class="index notranslate">#169</span></a>
                <a id="title-pyIXyl4qFx@OpenReview" class="title-link" href="/venue/pyIXyl4qFx@OpenReview" target="_blank">G-Adaptivity: optimised graph-based mesh relocation for finite element methods</a>
                <a id="pdf-pyIXyl4qFx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pyIXyl4qFx@OpenReview', this)" data="https://openreview.net/pdf?id=pyIXyl4qFx">[PDF<sup id="pdf-stars-pyIXyl4qFx@OpenReview">2</sup>]</a>
                <a id="copy-pyIXyl4qFx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pyIXyl4qFx@OpenReview')">[Copy]</a>
                <a id="kimi-pyIXyl4qFx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pyIXyl4qFx@OpenReview', this)">[Kimi<sup id="kimi-stars-pyIXyl4qFx@OpenReview">2</sup>]</a>
                <a id="rel-pyIXyl4qFx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pyIXyl4qFx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pyIXyl4qFx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=James Rowbottom" target="_blank">James Rowbottom</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georg Maierhofer" target="_blank">Georg Maierhofer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Teo Deveney" target="_blank">Teo Deveney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eike Müller" target="_blank">Eike Müller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alberto Paganini" target="_blank">Alberto Paganini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katharina Schratz" target="_blank">Katharina Schratz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pietro Lió" target="_blank">Pietro Lió</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carola-Bibiane Schönlieb" target="_blank">Carola-Bibiane Schönlieb</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chris Budd" target="_blank">Chris Budd</a>
            </p>
            <p id="summary-pyIXyl4qFx@OpenReview" class="summary">We present a novel, and effective, approach to achieve optimal mesh relocation in finite element methods (FEMs). The cost and accuracy of FEMs is critically dependent on the choice of mesh points. Mesh relocation (r-adaptivity) seeks to optimise the mesh geometry to obtain the best solution accuracy at given computational budget. Classical r-adaptivity relies on the solution of a separate nonlinear ``meshing'' PDE to determine mesh point locations. This incurs significant cost at remeshing, and relies on estimates that relate interpolation- and FEM-error. Recent machine learning approaches have focused on the construction of fast surrogates for such classical methods. Instead, our new approach trains a graph neural network (GNN) to determine mesh point locations by directly minimising the FE solution error from the PDE system Firedrake to achieve higher solution accuracy. Our GNN architecture closely aligns the mesh solution space to that of classical meshing methodologies, thus replacing classical estimates for optimality with a learnable strategy. This allows for rapid and robust training and results in an extremely efficient and effective GNN approach to online r-adaptivity. Our method outperforms both classical, and prior ML, approaches to r-adaptive meshing. In particular, it achieves lower FE solution error, whilst retaining the significant speed-up over classical methods observed in prior ML work.</p>
            <p id="subjects-pyIXyl4qFx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-pyIXyl4qFx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pyIXyl4qFx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pyIXyl4qFx@OpenReview" onclick="foldPdfKimi('pyIXyl4qFx@OpenReview', this)" class="hr hr-fold">
        </div><div id="qOgKMqv9T7@OpenReview" class="panel paper" keywords="timing,xai,series,temporality,gradients,metrics,attribution,suboptimal,integrated,points">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qOgKMqv9T7" target="_blank" title="170/225"><span class="index notranslate">#170</span></a>
                <a id="title-qOgKMqv9T7@OpenReview" class="title-link" href="/venue/qOgKMqv9T7@OpenReview" target="_blank">TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation</a>
                <a id="pdf-qOgKMqv9T7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qOgKMqv9T7@OpenReview', this)" data="https://openreview.net/pdf?id=qOgKMqv9T7">[PDF<sup id="pdf-stars-qOgKMqv9T7@OpenReview">2</sup>]</a>
                <a id="copy-qOgKMqv9T7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qOgKMqv9T7@OpenReview')">[Copy]</a>
                <a id="kimi-qOgKMqv9T7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qOgKMqv9T7@OpenReview', this)">[Kimi<sup id="kimi-stars-qOgKMqv9T7@OpenReview">2</sup>]</a>
                <a id="rel-qOgKMqv9T7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qOgKMqv9T7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qOgKMqv9T7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hyeongwon Jang" target="_blank">Hyeongwon Jang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changhun Kim" target="_blank">Changhun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eunho Yang" target="_blank">Eunho Yang</a>
            </p>
            <p id="summary-qOgKMqv9T7@OpenReview" class="summary">Recent explainable artificial intelligence (XAI) methods for time series primarily estimate point-wise attribution magnitudes, while overlooking the directional impact on predictions, leading to suboptimal identification of significant points. Our analysis shows that conventional Integrated Gradients (IG) effectively capture critical points with both positive and negative impacts on predictions. However, current evaluation metrics fail to assess this capability, as they inadvertently cancel out opposing feature contributions. To address this limitation, we propose novel evaluation metrics—Cumulative Prediction Difference (CPD) and Cumulative Prediction Preservation(CPP)—to systematically assess whether attribution methods accurately identify significant positive and negative points in time series XAI. Under these metrics, conventional IG outperforms recent counterparts. However, directly applying IG to time series data may lead to suboptimal outcomes, as generated paths ignore temporal relationships and introduce out-of-distribution samples. To overcome these challenges, we introduce TIMING, which enhances IG by incorporating temporal awareness while maintaining its theoretical properties. Extensive experiments on synthetic and real-world time series benchmarks demonstrate that TIMING outperforms existing timeseries XAI baselines. Our code is available at https://github.com/drumpt/TIMING.</p>
            <p id="subjects-qOgKMqv9T7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qOgKMqv9T7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qOgKMqv9T7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qOgKMqv9T7@OpenReview" onclick="foldPdfKimi('qOgKMqv9T7@OpenReview', this)" class="hr hr-fold">
        </div><div id="qtuxDy2qEB@OpenReview" class="panel paper" keywords="log,sampling,parallel,concave,widetilde,simulation,reducing,mathcal,adaptive,complexity">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qtuxDy2qEB" target="_blank" title="171/225"><span class="index notranslate">#171</span></a>
                <a id="title-qtuxDy2qEB@OpenReview" class="title-link" href="/venue/qtuxDy2qEB@OpenReview" target="_blank">Parallel Simulation for Log-concave Sampling and Score-based Diffusion Models</a>
                <a id="pdf-qtuxDy2qEB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qtuxDy2qEB@OpenReview', this)" data="https://openreview.net/pdf?id=qtuxDy2qEB">[PDF<sup id="pdf-stars-qtuxDy2qEB@OpenReview">1</sup>]</a>
                <a id="copy-qtuxDy2qEB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qtuxDy2qEB@OpenReview')">[Copy]</a>
                <a id="kimi-qtuxDy2qEB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qtuxDy2qEB@OpenReview', this)">[Kimi<sup id="kimi-stars-qtuxDy2qEB@OpenReview">1</sup>]</a>
                <a id="rel-qtuxDy2qEB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qtuxDy2qEB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qtuxDy2qEB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huanjian Zhou" target="_blank">Huanjian Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Masashi Sugiyama" target="_blank">Masashi Sugiyama</a>
            </p>
            <p id="summary-qtuxDy2qEB@OpenReview" class="summary">Sampling from high-dimensional probability distributions is fundamental in machine learning and statistics. As datasets grow larger, computational efficiency becomes increasingly important, particularly in reducing *adaptive complexity*, namely the number of sequential rounds required for sampling algorithms. While recent works have introduced several parallelizable techniques, they often exhibit suboptimal convergence rates and remain significantly weaker than the latest lower bounds for log-concave sampling.To address this, we propose a novel parallel sampling method that improves adaptive complexity dependence on dimension <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-131-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-945" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-946"><span class="mi" id="MathJax-Span-947" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-131">d</script> reducing it from <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-132-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-948" style="width: 4.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.86em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-949"><span class="texatom" id="MathJax-Span-950"><span class="mrow" id="MathJax-Span-951"><span class="munderover" id="MathJax-Span-952"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="texatom" id="MathJax-Span-953"><span class="mrow" id="MathJax-Span-954"><span class="mi" id="MathJax-Span-955" style="font-family: MathJax_Caligraphic;">O</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-956" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-957" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-958"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-959" style="font-family: MathJax_Main;">log</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 1.305em;"><span class="mn" id="MathJax-Span-960" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-961"></span><span class="mi" id="MathJax-Span-962" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-963" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><msup><mi>log</mi><mn>2</mn></msup><mo>⁡</mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-132">\widetilde{\mathcal{O}}(\log^2 d)</script> to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-133-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-964" style="width: 4.273em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.544em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.44em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-965"><span class="texatom" id="MathJax-Span-966"><span class="mrow" id="MathJax-Span-967"><span class="munderover" id="MathJax-Span-968"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="texatom" id="MathJax-Span-969"><span class="mrow" id="MathJax-Span-970"><span class="mi" id="MathJax-Span-971" style="font-family: MathJax_Caligraphic;">O</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-972" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-973" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-974" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-975"></span><span class="mi" id="MathJax-Span-976" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-977" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-133">\widetilde{\mathcal{O}}(\log d)</script>. Our approach builds on parallel simulation techniques from scientific computing.</p>
            <p id="subjects-qtuxDy2qEB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qtuxDy2qEB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qtuxDy2qEB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qtuxDy2qEB@OpenReview" onclick="foldPdfKimi('qtuxDy2qEB@OpenReview', this)" class="hr hr-fold">
        </div><div id="r9HlTuCQfr@OpenReview" class="panel paper" keywords="dro,bas,ambiguity,posterior,bayesian,distributionally,dgp,optimisation,uncertainty,sets">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=r9HlTuCQfr" target="_blank" title="172/225"><span class="index notranslate">#172</span></a>
                <a id="title-r9HlTuCQfr@OpenReview" class="title-link" href="/venue/r9HlTuCQfr@OpenReview" target="_blank">Decision Making under the Exponential Family: Distributionally Robust Optimisation with Bayesian Ambiguity Sets</a>
                <a id="pdf-r9HlTuCQfr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('r9HlTuCQfr@OpenReview', this)" data="https://openreview.net/pdf?id=r9HlTuCQfr">[PDF<sup id="pdf-stars-r9HlTuCQfr@OpenReview">1</sup>]</a>
                <a id="copy-r9HlTuCQfr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('r9HlTuCQfr@OpenReview')">[Copy]</a>
                <a id="kimi-r9HlTuCQfr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('r9HlTuCQfr@OpenReview', this)">[Kimi<sup id="kimi-stars-r9HlTuCQfr@OpenReview">1</sup>]</a>
                <a id="rel-r9HlTuCQfr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('r9HlTuCQfr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-r9HlTuCQfr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Charita Dellaporta" target="_blank">Charita Dellaporta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patrick O&amp;#x27;Hara" target="_blank">Patrick O&amp;#x27;Hara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Theodoros Damoulas" target="_blank">Theodoros Damoulas</a>
            </p>
            <p id="summary-r9HlTuCQfr@OpenReview" class="summary">Decision making under uncertainty is challenging as the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs on the model’s parameters. However, minimising the expected risk under these beliefs can lead to suboptimal decisions due to model uncertainty or limited, noisy observations. To address this, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DRO-BAS) which hedges against model uncertainty by optimising the worst-case risk over a posterior-informed ambiguity set. We provide two such sets, based on the posterior expectation (DRO-BAS(PE)) or the posterior predictive (DRO-BAS(PP)) and prove that both admit, under conditions, strong dual formulations leading to efficient single-stage stochastic programs which are solved with a sample average approximation. For DRO-BAS(PE), this covers all conjugate exponential family members while for DRO-BAS(PP) this is shown under conditions on the predictive's moment generating function. Our DRO-BAS formulations outperform existing Bayesian DRO on the Newsvendor problem and achieve faster solve times with comparable robustness on the Portfolio problem.</p>
            <p id="subjects-r9HlTuCQfr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-r9HlTuCQfr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-r9HlTuCQfr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-r9HlTuCQfr@OpenReview" onclick="foldPdfKimi('r9HlTuCQfr@OpenReview', this)" class="hr hr-fold">
        </div><div id="sDK6bSmHgM@OpenReview" class="panel paper" keywords="drag,flowdrag,editing,mesh,vfd,edits,dragbench,deformation,user,truth">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sDK6bSmHgM" target="_blank" title="173/225"><span class="index notranslate">#173</span></a>
                <a id="title-sDK6bSmHgM@OpenReview" class="title-link" href="/venue/sDK6bSmHgM@OpenReview" target="_blank">FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields</a>
                <a id="pdf-sDK6bSmHgM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sDK6bSmHgM@OpenReview', this)" data="https://openreview.net/pdf?id=sDK6bSmHgM">[PDF<sup id="pdf-stars-sDK6bSmHgM@OpenReview">1</sup>]</a>
                <a id="copy-sDK6bSmHgM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sDK6bSmHgM@OpenReview')">[Copy]</a>
                <a id="kimi-sDK6bSmHgM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sDK6bSmHgM@OpenReview', this)">[Kimi<sup id="kimi-stars-sDK6bSmHgM@OpenReview">1</sup>]</a>
                <a id="rel-sDK6bSmHgM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sDK6bSmHgM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sDK6bSmHgM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gwanhyeong Koo" target="_blank">Gwanhyeong Koo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sunjae Yoon" target="_blank">Sunjae Yoon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Younghwan Lee" target="_blank">Younghwan Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ji Woo Hong" target="_blank">Ji Woo Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Yoo" target="_blank">Chang Yoo</a>
            </p>
            <p id="summary-sDK6bSmHgM@OpenReview" class="summary">Drag-based editing allows precise object manipulation through point-based control, offering user convenience. However, current methods often suffer from a geometric inconsistency problem by focusing exclusively on matching user-defined points, neglecting the broader geometry and leading to artifacts or unstable edits. We propose FlowDrag, which leverages geometric information for more accurate and coherent transformations. Our approach constructs a 3D mesh from the image, using an energy function to guide mesh deformation based on user-defined drag points. The resulting mesh displacements are projected into 2D and incorporated into a UNet denoising process, enabling precise handle-to-target point alignment while preserving structural integrity. Additionally, existing drag-editing benchmarks provide no ground truth, making it difficult to assess how accurately the edits match the intended transformations. To address this, we present VFD (VidFrameDrag) benchmark dataset, which provides ground-truth frames using consecutive shots in a video dataset. FlowDrag outperforms existing drag-based editing methods on both VFD Bench and DragBench.</p>
            <p id="subjects-sDK6bSmHgM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-sDK6bSmHgM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sDK6bSmHgM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sDK6bSmHgM@OpenReview" onclick="foldPdfKimi('sDK6bSmHgM@OpenReview', this)" class="hr hr-fold">
        </div><div id="sEBfiF8JBu@OpenReview" class="panel paper" keywords="pandas,jailbreaking,shot,affirmation,fabricated,exchanges,harmful,many,affirmations,prompt">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sEBfiF8JBu" target="_blank" title="174/225"><span class="index notranslate">#174</span></a>
                <a id="title-sEBfiF8JBu@OpenReview" class="title-link" href="/venue/sEBfiF8JBu@OpenReview" target="_blank">PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling</a>
                <a id="pdf-sEBfiF8JBu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sEBfiF8JBu@OpenReview', this)" data="https://openreview.net/pdf?id=sEBfiF8JBu">[PDF<sup id="pdf-stars-sEBfiF8JBu@OpenReview">1</sup>]</a>
                <a id="copy-sEBfiF8JBu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sEBfiF8JBu@OpenReview')">[Copy]</a>
                <a id="kimi-sEBfiF8JBu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sEBfiF8JBu@OpenReview', this)">[Kimi<sup id="kimi-stars-sEBfiF8JBu@OpenReview">3</sup>]</a>
                <a id="rel-sEBfiF8JBu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sEBfiF8JBu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sEBfiF8JBu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Avery Ma" target="_blank">Avery Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yangchen Pan" target="_blank">Yangchen Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amir-massoud Farahmand" target="_blank">Amir-massoud Farahmand</a>
            </p>
            <p id="summary-sEBfiF8JBu@OpenReview" class="summary">Many-shot jailbreaking circumvents the safety alignment of LLMs by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational exchanges between the user and the model. These exchanges are randomly sampled from a pool of unsafe question-answer pairs, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with Positive Affirmations, Negative Demonstrations, and an optimized Adaptive Sampling method tailored to the target prompt's topic. We also introduce ManyHarm, a dataset of harmful question–answer pairs, and demonstrate through extensive experiments that PANDAS significantly outperforms baseline methods in long-context scenarios. Through attention analysis, we provide insights into how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.</p>
            <p id="subjects-sEBfiF8JBu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-sEBfiF8JBu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sEBfiF8JBu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sEBfiF8JBu@OpenReview" onclick="foldPdfKimi('sEBfiF8JBu@OpenReview', this)" class="hr hr-fold">
        </div><div id="sQS0roNQZR@OpenReview" class="panel paper" keywords="language,character,token,prompt,models,strings,level,tokenized,approximate,tokens">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sQS0roNQZR" target="_blank" title="175/225"><span class="index notranslate">#175</span></a>
                <a id="title-sQS0roNQZR@OpenReview" class="title-link" href="/venue/sQS0roNQZR@OpenReview" target="_blank">From Language Models over Tokens to Language Models over Characters</a>
                <a id="pdf-sQS0roNQZR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sQS0roNQZR@OpenReview', this)" data="https://openreview.net/pdf?id=sQS0roNQZR">[PDF<sup id="pdf-stars-sQS0roNQZR@OpenReview">2</sup>]</a>
                <a id="copy-sQS0roNQZR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sQS0roNQZR@OpenReview')">[Copy]</a>
                <a id="kimi-sQS0roNQZR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sQS0roNQZR@OpenReview', this)">[Kimi<sup id="kimi-stars-sQS0roNQZR@OpenReview">2</sup>]</a>
                <a id="rel-sQS0roNQZR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sQS0roNQZR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sQS0roNQZR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Vieira" target="_blank">Tim Vieira</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin LeBrun" target="_blank">Benjamin LeBrun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mario Giulianelli" target="_blank">Mario Giulianelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juan Luis Gastaldi" target="_blank">Juan Luis Gastaldi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian DuSell" target="_blank">Brian DuSell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Terilla" target="_blank">John Terilla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Timothy O&amp;#x27;Donnell" target="_blank">Timothy O&amp;#x27;Donnell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Cotterell" target="_blank">Ryan Cotterell</a>
            </p>
            <p id="summary-sQS0roNQZR@OpenReview" class="summary">Modern language models are internally—and mathematically—distributions over *token* strings rather than *character* strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent processing are very sensitive to the specification of the prompt (e.g., whether the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. Across four publicly available language models, we find that—even with a small computation budget—our method is able to accurately approximate the character-level distribution at reasonably fast speeds, and that a significant improvement in the language model's compression rate (bits/byte) is achieved.</p>
            <p id="subjects-sQS0roNQZR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-sQS0roNQZR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sQS0roNQZR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sQS0roNQZR@OpenReview" onclick="foldPdfKimi('sQS0roNQZR@OpenReview', this)" class="hr hr-fold">
        </div><div id="teUg2pMrF0@OpenReview" class="panel paper" keywords="lns,milp,neighborhood,llm,large,scale,strategies,layer,milpopt,search">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=teUg2pMrF0" target="_blank" title="176/225"><span class="index notranslate">#176</span></a>
                <a id="title-teUg2pMrF0@OpenReview" class="title-link" href="/venue/teUg2pMrF0@OpenReview" target="_blank">Large Language Model-driven Large Neighborhood Search for Large-Scale MILP Problems</a>
                <a id="pdf-teUg2pMrF0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('teUg2pMrF0@OpenReview', this)" data="https://openreview.net/pdf?id=teUg2pMrF0">[PDF<sup id="pdf-stars-teUg2pMrF0@OpenReview">3</sup>]</a>
                <a id="copy-teUg2pMrF0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('teUg2pMrF0@OpenReview')">[Copy]</a>
                <a id="kimi-teUg2pMrF0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('teUg2pMrF0@OpenReview', this)">[Kimi<sup id="kimi-stars-teUg2pMrF0@OpenReview">2</sup>]</a>
                <a id="rel-teUg2pMrF0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('teUg2pMrF0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-teUg2pMrF0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huigen Ye" target="_blank">Huigen Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hua Xu" target="_blank">Hua Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=An Yan" target="_blank">An Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaoyang Cheng" target="_blank">Yaoyang Cheng</a>
            </p>
            <p id="summary-teUg2pMrF0@OpenReview" class="summary">Large Neighborhood Search (LNS) is a widely used method for solving large-scale Mixed Integer Linear Programming (MILP) problems. The effectiveness of LNS crucially depends on the choice of the search neighborhood. However, existing strategies either rely on expert knowledge or computationally expensive Machine Learning (ML) approaches, both of which struggle to scale effectively for large problems. To address this, we propose LLM-LNS, a novel Large Language Model (LLM)-driven LNS framework for large-scale MILP problems. Our approach introduces a dual-layer self-evolutionary LLM agent to automate neighborhood selection, discovering effective strategies with scant small-scale training data that generalize well to large-scale MILPs. The inner layer evolves heuristic strategies to ensure convergence, while the outer layer evolves evolutionary prompt strategies to maintain diversity. Experimental results demonstrate that the proposed dual-layer agent outperforms state-of-the-art agents such as FunSearch and EOH. Furthermore, the full LLM-LNS framework surpasses manually designed LNS algorithms like ACP, ML-based LNS methods like CL-LNS, and large-scale solvers such as Gurobi and SCIP. It also achieves superior performance compared to advanced ML-based MILP optimization frameworks like GNN&amp;GBDT and Light-MILPopt, further validating the effectiveness of our approach.</p>
            <p id="subjects-teUg2pMrF0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-teUg2pMrF0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-teUg2pMrF0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-teUg2pMrF0@OpenReview" onclick="foldPdfKimi('teUg2pMrF0@OpenReview', this)" class="hr hr-fold">
        </div><div id="tnyxtaSve5@OpenReview" class="panel paper" keywords="medical,clinical,mvqa,neurocognitive,lvlms,professional,reasoning,benchmark,questions,visual">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tnyxtaSve5" target="_blank" title="177/225"><span class="index notranslate">#177</span></a>
                <a id="title-tnyxtaSve5@OpenReview" class="title-link" href="/venue/tnyxtaSve5@OpenReview" target="_blank">Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning</a>
                <a id="pdf-tnyxtaSve5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tnyxtaSve5@OpenReview', this)" data="https://openreview.net/pdf?id=tnyxtaSve5">[PDF<sup id="pdf-stars-tnyxtaSve5@OpenReview">4</sup>]</a>
                <a id="copy-tnyxtaSve5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tnyxtaSve5@OpenReview')">[Copy]</a>
                <a id="kimi-tnyxtaSve5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tnyxtaSve5@OpenReview', this)">[Kimi<sup id="kimi-stars-tnyxtaSve5@OpenReview">1</sup>]</a>
                <a id="rel-tnyxtaSve5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tnyxtaSve5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tnyxtaSve5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rina Bao" target="_blank">Rina Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shilong Dong" target="_blank">Shilong Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenfang Chen" target="_blank">Zhenfang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng He" target="_blank">Sheng He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patricia Ellen Grant" target="_blank">Patricia Ellen Grant</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yangming Ou" target="_blank">Yangming Ou</a>
            </p>
            <p id="summary-tnyxtaSve5@OpenReview" class="summary">Medical Visual Question Answering (MVQA) requires AI models to answer questions related to medical images, offering significant potential to assist medical professionals in evaluating and diagnosing diseases, thereby improving early interventions. However, existing MVQA datasets primarily focus on basic questions regarding visual perception and pattern recognition, without addressing the more complex questions that are critical in clinical diagnosis and decision-making. This paper introduces a new benchmark designed for professional-level medical reasoning, simulating the decision-making process. We achieve this by collecting MRI and clinical data related to Hypoxic-Ischemic Encephalopathy, enriched with expert annotations and insights. Building on this data, we generate clinical question-answer pairs and MRI interpretations to enable comprehensive diagnosis, interpretation, and prediction of neurocognitive outcomes. Our evaluation of current large vision-language models (LVLMs) shows limited performance on this benchmark, highlighting both the challenges of the task and the importance of this benchmark for advancing medical AI. Furthermore, we propose a novel ``Clinical Graph of Thoughts" model, which integrates domain-specific medical knowledge and clinical reasoning processes with the interpretive abilities of LVLMs. The model demonstrates promising results, achieving around 15\% absolute gain on the most important neurocognitive outcome task, while the benchmark still reveals substantial opportunities for further research innovation.</p>
            <p id="subjects-tnyxtaSve5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tnyxtaSve5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tnyxtaSve5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tnyxtaSve5@OpenReview" onclick="foldPdfKimi('tnyxtaSve5@OpenReview', this)" class="hr hr-fold">
        </div><div id="tqL8gJsuS5@OpenReview" class="panel paper" keywords="unlearning,dsda,source,data,discrimination,multitask,synthesis,aware,free,retain">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tqL8gJsuS5" target="_blank" title="178/225"><span class="index notranslate">#178</span></a>
                <a id="title-tqL8gJsuS5@OpenReview" class="title-link" href="/venue/tqL8gJsuS5@OpenReview" target="_blank">Efficient Source-free Unlearning via Energy-Guided Data Synthesis and Discrimination-Aware Multitask Optimization</a>
                <a id="pdf-tqL8gJsuS5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tqL8gJsuS5@OpenReview', this)" data="https://openreview.net/pdf?id=tqL8gJsuS5">[PDF<sup id="pdf-stars-tqL8gJsuS5@OpenReview">2</sup>]</a>
                <a id="copy-tqL8gJsuS5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tqL8gJsuS5@OpenReview')">[Copy]</a>
                <a id="kimi-tqL8gJsuS5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tqL8gJsuS5@OpenReview', this)">[Kimi<sup id="kimi-stars-tqL8gJsuS5@OpenReview">2</sup>]</a>
                <a id="rel-tqL8gJsuS5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tqL8gJsuS5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tqL8gJsuS5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiuyuan Wang" target="_blank">Xiuyuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaochao Chen" target="_blank">Chaochao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiming Liu" target="_blank">Weiming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinting Liao" target="_blank">Xinting Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Wang" target="_blank">Fan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaolin Zheng" target="_blank">Xiaolin Zheng</a>
            </p>
            <p id="summary-tqL8gJsuS5@OpenReview" class="summary">With growing privacy concerns and the enforcement of data protection regulations, machine unlearning has emerged as a promising approach for removing the influence of forget data while maintaining model performance on retain data. However, most existing unlearning methods require access to the original training data, which is often impractical due to privacy policies, storage constraints, and other limitations. This gives rise to the challenging task of source-free unlearning, where unlearning must be accomplished without accessing the original training data. Few existing source-free unlearning methods rely on knowledge distillation and model retraining, which impose substantial computational costs. In this work, we propose the Data Synthesis-based Discrimination-Aware (DSDA) unlearning framework, which enables efficient source-free unlearning in two stages: (1) Accelerated Energy-Guided Data Synthesis (AEGDS), which employs Langevin dynamics to model the training data distribution while integrating Runge–Kutta methods and momentum to enhance efficiency. (2) Discrimination-Aware Multitask Optimization (DAMO), which refines the feature distribution of retain data and mitigates the gradient conflicts among multiple unlearning objectives. Extensive experiments on three benchmark datasets demonstrate that DSDA outperforms existing unlearning methods, validating its effectiveness and efficiency in source-free unlearning.</p>
            <p id="subjects-tqL8gJsuS5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tqL8gJsuS5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tqL8gJsuS5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tqL8gJsuS5@OpenReview" onclick="foldPdfKimi('tqL8gJsuS5@OpenReview', this)" class="hr hr-fold">
        </div><div id="u3n5wuRGTa@OpenReview" class="panel paper" keywords="metta,classes,class,learning,vector,zero,application,discovering,machine,corresponds">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=u3n5wuRGTa" target="_blank" title="179/225"><span class="index notranslate">#179</span></a>
                <a id="title-u3n5wuRGTa@OpenReview" class="title-link" href="/venue/u3n5wuRGTa@OpenReview" target="_blank">Discovering a Zero (Zero-Vector Class of Machine Learning)</a>
                <a id="pdf-u3n5wuRGTa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('u3n5wuRGTa@OpenReview', this)" data="https://openreview.net/pdf?id=u3n5wuRGTa">[PDF<sup id="pdf-stars-u3n5wuRGTa@OpenReview">4</sup>]</a>
                <a id="copy-u3n5wuRGTa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('u3n5wuRGTa@OpenReview')">[Copy]</a>
                <a id="kimi-u3n5wuRGTa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('u3n5wuRGTa@OpenReview', this)">[Kimi<sup id="kimi-stars-u3n5wuRGTa@OpenReview">4</sup>]</a>
                <a id="rel-u3n5wuRGTa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('u3n5wuRGTa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-u3n5wuRGTa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Harikrishna Metta" target="_blank">Harikrishna Metta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Venkatesh Babu Radhakrishnan" target="_blank">Venkatesh Babu Radhakrishnan</a>
            </p>
            <p id="summary-u3n5wuRGTa@OpenReview" class="summary">In Machine learning, separating data into classes is a very fundamental problem. A mathematical framework around the classes is presented in this work to deepen the understanding of classes. The classes are defined as vectors in a Vector Space, where addition corresponds to the union of classes, and scalar multiplication resembles set complement of classes. The Zero-Vector in the vector space corresponds to a class referred to as the Metta-Class. This discovery enables numerous applications. One such application, termed 'clear learning' in this work, focuses on learning the true nature (manifold) of the data instead of merely learning a boundary sufficient for classification. Another application, called 'unary class learning', involves learning a single class in isolation rather than learning by comparing two or more classes. Additionally, 'set operations on classes' is another application highlighted in this work. Furthermore, Continual Learning of classes is facilitated by smaller networks. The Metta-Class enables neural networks to learn only the data manifold; therefore, it can also be used for generation of new data. Results for the key applications are shown using the MNIST dataset. To further strengthen the claims, some results are also produced using the CIFAR-10 and ImageNet-1k embeddings. The code supporting these applications is publicly available at: github.com/hm-4/Metta-Class.</p>
            <p id="subjects-u3n5wuRGTa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-u3n5wuRGTa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-u3n5wuRGTa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-u3n5wuRGTa@OpenReview" onclick="foldPdfKimi('u3n5wuRGTa@OpenReview', this)" class="hr hr-fold">
        </div><div id="u6xeKVHS6K@OpenReview" class="panel paper" keywords="generated,images,gmail,modality,generative,image,vision,captioning,alignment,real">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=u6xeKVHS6K" target="_blank" title="180/225"><span class="index notranslate">#180</span></a>
                <a id="title-u6xeKVHS6K@OpenReview" class="title-link" href="/venue/u6xeKVHS6K@OpenReview" target="_blank">GMAIL: Generative Modality Alignment for generated Image Learning</a>
                <a id="pdf-u6xeKVHS6K@OpenReview" class="title-pdf notranslate" onclick="togglePdf('u6xeKVHS6K@OpenReview', this)" data="https://openreview.net/pdf?id=u6xeKVHS6K">[PDF<sup id="pdf-stars-u6xeKVHS6K@OpenReview">1</sup>]</a>
                <a id="copy-u6xeKVHS6K@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('u6xeKVHS6K@OpenReview')">[Copy]</a>
                <a id="kimi-u6xeKVHS6K@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('u6xeKVHS6K@OpenReview', this)">[Kimi<sup id="kimi-stars-u6xeKVHS6K@OpenReview">2</sup>]</a>
                <a id="rel-u6xeKVHS6K@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('u6xeKVHS6K@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-u6xeKVHS6K@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shentong Mo" target="_blank">Shentong Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sukmin Yun" target="_blank">Sukmin Yun</a>
            </p>
            <p id="summary-u6xeKVHS6K@OpenReview" class="summary">Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined \textit{GMAIL}, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.</p>
            <p id="subjects-u6xeKVHS6K@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-u6xeKVHS6K@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-u6xeKVHS6K@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-u6xeKVHS6K@OpenReview" onclick="foldPdfKimi('u6xeKVHS6K@OpenReview', this)" class="hr hr-fold">
        </div><div id="uqCfoVXb67@OpenReview" class="panel paper" keywords="unidb,soc,bridge,diffusion,terminal,doob,control,restoration,bridges,framework">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uqCfoVXb67" target="_blank" title="181/225"><span class="index notranslate">#181</span></a>
                <a id="title-uqCfoVXb67@OpenReview" class="title-link" href="/venue/uqCfoVXb67@OpenReview" target="_blank">UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control</a>
                <a id="pdf-uqCfoVXb67@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uqCfoVXb67@OpenReview', this)" data="https://openreview.net/pdf?id=uqCfoVXb67">[PDF<sup id="pdf-stars-uqCfoVXb67@OpenReview">2</sup>]</a>
                <a id="copy-uqCfoVXb67@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uqCfoVXb67@OpenReview')">[Copy]</a>
                <a id="kimi-uqCfoVXb67@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uqCfoVXb67@OpenReview', this)">[Kimi<sup id="kimi-stars-uqCfoVXb67@OpenReview">2</sup>]</a>
                <a id="rel-uqCfoVXb67@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uqCfoVXb67@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uqCfoVXb67@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaizhen Zhu" target="_blank">Kaizhen Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mokai Pan" target="_blank">Mokai Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuexin Ma" target="_blank">Yuexin Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Fu" target="_blank">Yanwei Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Yu" target="_blank">Jingyi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingya Wang" target="_blank">Jingya Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Shi" target="_blank">Ye Shi</a>
            </p>
            <p id="summary-uqCfoVXb67@OpenReview" class="summary">Recent advances in diffusion bridge models leverage Doob’s <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-134-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-978" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-979"><span class="mi" id="MathJax-Span-980" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-134">h</script>-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches frequently produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified framework for diffusion bridges based on Stochastic Optimal Control (SOC). UniDB formulates the problem through an SOC-based optimization and derives a closed-form solution for the optimal controller, thereby unifying and generalizing existing diffusion bridge models. We demonstrate that existing diffusion bridges employing Doob’s <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-135-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-981" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-982"><span class="mi" id="MathJax-Span-983" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-135">h</script>-transform constitute a special case of our framework, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. Notably, UniDB seamlessly integrates with existing diffusion bridge models, requiring only minimal code modifications. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework. Our code is available at https://github.com/UniDB-SOC/UniDB/.</p>
            <p id="subjects-uqCfoVXb67@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-uqCfoVXb67@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uqCfoVXb67@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uqCfoVXb67@OpenReview" onclick="foldPdfKimi('uqCfoVXb67@OpenReview', this)" class="hr hr-fold">
        </div><div id="v4DWXM93VV@OpenReview" class="panel paper" keywords="mfl,distributional,sda,langevin,minimax,convergence,descent,ascent,rate,mean">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=v4DWXM93VV" target="_blank" title="182/225"><span class="index notranslate">#182</span></a>
                <a id="title-v4DWXM93VV@OpenReview" class="title-link" href="/venue/v4DWXM93VV@OpenReview" target="_blank">Convergence of Mean-Field Langevin Stochastic Descent-Ascent for Distributional Minimax Optimization</a>
                <a id="pdf-v4DWXM93VV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('v4DWXM93VV@OpenReview', this)" data="https://openreview.net/pdf?id=v4DWXM93VV">[PDF<sup id="pdf-stars-v4DWXM93VV@OpenReview">2</sup>]</a>
                <a id="copy-v4DWXM93VV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('v4DWXM93VV@OpenReview')">[Copy]</a>
                <a id="kimi-v4DWXM93VV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('v4DWXM93VV@OpenReview', this)">[Kimi<sup id="kimi-stars-v4DWXM93VV@OpenReview">2</sup>]</a>
                <a id="rel-v4DWXM93VV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('v4DWXM93VV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-v4DWXM93VV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhangyi Liu" target="_blank">Zhangyi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Liu" target="_blank">Feng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Gao" target="_blank">Rui Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuang Li" target="_blank">Shuang Li</a>
            </p>
            <p id="summary-v4DWXM93VV@OpenReview" class="summary">We study convergence properties of the discrete-time Mean-Field Langevin Stochastic Descent-Ascent (MFL-SDA) algorithm for solving distributional minimax optimization. These problems arise in various applications, such as zero-sum games, generative adversarial networks and distributionally robust learning. Despite the significance of MFL-SDA in these contexts, the discrete-time convergence rate remains underexplored.To address this gap, we establish a last-iterate convergence rate of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-136-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/mfrac&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-984" style="width: 5.211em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.326em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1004.22em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-985"><span class="mi" id="MathJax-Span-986" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-987" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-988"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-989" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.32em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.154em;"><span class="mi" id="MathJax-Span-990" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mi" id="MathJax-Span-991" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-992"></span><span class="mfrac" id="MathJax-Span-993" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-994" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.32em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.154em;"><span class="mi" id="MathJax-Span-995" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ϵ</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-996" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mfrac><mn>1</mn><mi>ϵ</mi></mfrac><mi>log</mi><mo>⁡</mo><mfrac><mn>1</mn><mi>ϵ</mi></mfrac><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-136">O(\frac{1}{\epsilon}\log\frac{1}{\epsilon})</script> for MFL-SDA. This rate is nearly optimal when compared to the complexity lower bound of its Euclidean counterpart. This rate also matches the complexity of mean-field Langevin stochastic gradient descent for distributional minimization and the outer-loop iteration complexity of an existing double-loop algorithm for distributional minimax problems.By leveraging an elementary analysis framework that avoids PDE-based techniques, we overcome previous limitations and achieve a faster convergence rate.</p>
            <p id="subjects-v4DWXM93VV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-v4DWXM93VV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-v4DWXM93VV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-v4DWXM93VV@OpenReview" onclick="foldPdfKimi('v4DWXM93VV@OpenReview', this)" class="hr hr-fold">
        </div><div id="vOdz3zhSCj@OpenReview" class="panel paper" keywords="neds,decoding,neural,activity,encoding,behavior,brain,scale,animals,animal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vOdz3zhSCj" target="_blank" title="183/225"><span class="index notranslate">#183</span></a>
                <a id="title-vOdz3zhSCj@OpenReview" class="title-link" href="/venue/vOdz3zhSCj@OpenReview" target="_blank">Neural Encoding and Decoding at Scale</a>
                <a id="pdf-vOdz3zhSCj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vOdz3zhSCj@OpenReview', this)" data="https://openreview.net/pdf?id=vOdz3zhSCj">[PDF<sup id="pdf-stars-vOdz3zhSCj@OpenReview">3</sup>]</a>
                <a id="copy-vOdz3zhSCj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vOdz3zhSCj@OpenReview')">[Copy]</a>
                <a id="kimi-vOdz3zhSCj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vOdz3zhSCj@OpenReview', this)">[Kimi<sup id="kimi-stars-vOdz3zhSCj@OpenReview">3</sup>]</a>
                <a id="rel-vOdz3zhSCj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vOdz3zhSCj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vOdz3zhSCj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yizi Zhang" target="_blank">Yizi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanchen Wang" target="_blank">Yanchen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mehdi Azabou" target="_blank">Mehdi Azabou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre Andre" target="_blank">Alexandre Andre</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Wang" target="_blank">Zixuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanrui Lyu" target="_blank">Hanrui Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=International Brain Laboratory" target="_blank">International Brain Laboratory</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eva Dyer" target="_blank">Eva Dyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Department of Statistics Liam Paninski" target="_blank">Department of Statistics Liam Paninski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cole Hurwitz" target="_blank">Cole Hurwitz</a>
            </p>
            <p id="summary-vOdz3zhSCj@OpenReview" class="summary">Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the visual decision-making task. In comparison to other large-scale modeling approaches, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior.</p>
            <p id="subjects-vOdz3zhSCj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-vOdz3zhSCj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vOdz3zhSCj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vOdz3zhSCj@OpenReview" onclick="foldPdfKimi('vOdz3zhSCj@OpenReview', this)" class="hr hr-fold">
        </div><div id="vhACnRfuYh@OpenReview" class="panel paper" keywords="ldp,imitation,latent,planning,leverage,diffusion,demonstrations,planner,suboptimal,action">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vhACnRfuYh" target="_blank" title="184/225"><span class="index notranslate">#184</span></a>
                <a id="title-vhACnRfuYh@OpenReview" class="title-link" href="/venue/vhACnRfuYh@OpenReview" target="_blank">Latent Diffusion Planning for Imitation Learning</a>
                <a id="pdf-vhACnRfuYh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vhACnRfuYh@OpenReview', this)" data="https://openreview.net/pdf?id=vhACnRfuYh">[PDF<sup id="pdf-stars-vhACnRfuYh@OpenReview">5</sup>]</a>
                <a id="copy-vhACnRfuYh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vhACnRfuYh@OpenReview')">[Copy]</a>
                <a id="kimi-vhACnRfuYh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vhACnRfuYh@OpenReview', this)">[Kimi<sup id="kimi-stars-vhACnRfuYh@OpenReview">3</sup>]</a>
                <a id="rel-vhACnRfuYh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vhACnRfuYh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vhACnRfuYh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Amber Xie" target="_blank">Amber Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oleh Rybkin" target="_blank">Oleh Rybkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dorsa Sadigh" target="_blank">Dorsa Sadigh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chelsea Finn" target="_blank">Chelsea Finn</a>
            </p>
            <p id="summary-vhACnRfuYh@OpenReview" class="summary">Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations.To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data.On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data.</p>
            <p id="subjects-vhACnRfuYh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-vhACnRfuYh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vhACnRfuYh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vhACnRfuYh@OpenReview" onclick="foldPdfKimi('vhACnRfuYh@OpenReview', this)" class="hr hr-fold">
        </div><div id="w0xYx9CJhY@OpenReview" class="panel paper" keywords="lvlms,marine,guidance,grounded,vision,hallucination,object,hallucinations,mitigating,api">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=w0xYx9CJhY" target="_blank" title="185/225"><span class="index notranslate">#185</span></a>
                <a id="title-w0xYx9CJhY@OpenReview" class="title-link" href="/venue/w0xYx9CJhY@OpenReview" target="_blank">Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance</a>
                <a id="pdf-w0xYx9CJhY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('w0xYx9CJhY@OpenReview', this)" data="https://openreview.net/pdf?id=w0xYx9CJhY">[PDF<sup id="pdf-stars-w0xYx9CJhY@OpenReview">8</sup>]</a>
                <a id="copy-w0xYx9CJhY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('w0xYx9CJhY@OpenReview')">[Copy]</a>
                <a id="kimi-w0xYx9CJhY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('w0xYx9CJhY@OpenReview', this)">[Kimi<sup id="kimi-stars-w0xYx9CJhY@OpenReview">5</sup>]</a>
                <a id="rel-w0xYx9CJhY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('w0xYx9CJhY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-w0xYx9CJhY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linxi Zhao" target="_blank">Linxi Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yihe Deng" target="_blank">Yihe Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weitong Zhang" target="_blank">Weitong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanquan Gu" target="_blank">Quanquan Gu</a>
            </p>
            <p id="summary-w0xYx9CJhY@OpenReview" class="summary">The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.</p>
            <p id="subjects-w0xYx9CJhY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-w0xYx9CJhY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-w0xYx9CJhY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-w0xYx9CJhY@OpenReview" onclick="foldPdfKimi('w0xYx9CJhY@OpenReview', this)" class="hr hr-fold">
        </div><div id="wBJIO15pBV@OpenReview" class="panel paper" keywords="symmetry,permutation,transformers,rotation,fusion,mlps,parameter,model,permuting,matrices">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wBJIO15pBV" target="_blank" title="186/225"><span class="index notranslate">#186</span></a>
                <a id="title-wBJIO15pBV@OpenReview" class="title-link" href="/venue/wBJIO15pBV@OpenReview" target="_blank">Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion</a>
                <a id="pdf-wBJIO15pBV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wBJIO15pBV@OpenReview', this)" data="https://openreview.net/pdf?id=wBJIO15pBV">[PDF<sup id="pdf-stars-wBJIO15pBV@OpenReview">4</sup>]</a>
                <a id="copy-wBJIO15pBV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wBJIO15pBV@OpenReview')">[Copy]</a>
                <a id="kimi-wBJIO15pBV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wBJIO15pBV@OpenReview', this)">[Kimi<sup id="kimi-stars-wBJIO15pBV@OpenReview">1</sup>]</a>
                <a id="rel-wBJIO15pBV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wBJIO15pBV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wBJIO15pBV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Binchi Zhang" target="_blank">Binchi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaiyi Zheng" target="_blank">Zaiyi Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengzhang Chen" target="_blank">Zhengzhang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jundong Li" target="_blank">Jundong Li</a>
            </p>
            <p id="summary-wBJIO15pBV@OpenReview" class="summary">Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry</p>
            <p id="subjects-wBJIO15pBV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wBJIO15pBV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wBJIO15pBV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wBJIO15pBV@OpenReview" onclick="foldPdfKimi('wBJIO15pBV@OpenReview', this)" class="hr hr-fold">
        </div><div id="wXfuOj9C7L@OpenReview" class="panel paper" keywords="ttt,hidden,mlp,expressive,layers,instantiations,mamba,context,linear,test">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wXfuOj9C7L" target="_blank" title="187/225"><span class="index notranslate">#187</span></a>
                <a id="title-wXfuOj9C7L@OpenReview" class="title-link" href="/venue/wXfuOj9C7L@OpenReview" target="_blank">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a>
                <a id="pdf-wXfuOj9C7L@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wXfuOj9C7L@OpenReview', this)" data="https://openreview.net/pdf?id=wXfuOj9C7L">[PDF<sup id="pdf-stars-wXfuOj9C7L@OpenReview">3</sup>]</a>
                <a id="copy-wXfuOj9C7L@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wXfuOj9C7L@OpenReview')">[Copy]</a>
                <a id="kimi-wXfuOj9C7L@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wXfuOj9C7L@OpenReview', this)">[Kimi<sup id="kimi-stars-wXfuOj9C7L@OpenReview">4</sup>]</a>
                <a id="rel-wXfuOj9C7L@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wXfuOj9C7L@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wXfuOj9C7L@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Sun" target="_blank">Yu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinhao Li" target="_blank">Xinhao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karan Dalal" target="_blank">Karan Dalal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiarui Xu" target="_blank">Jiarui Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arjun Vikram" target="_blank">Arjun Vikram</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Genghan Zhang" target="_blank">Genghan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yann Dubois" target="_blank">Yann Dubois</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlei Chen" target="_blank">Xinlei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaolong Wang" target="_blank">Xiaolong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanmi Koyejo" target="_blank">Sanmi Koyejo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tatsunori Hashimoto" target="_blank">Tatsunori Hashimoto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carlos Guestrin" target="_blank">Carlos Guestrin</a>
            </p>
            <p id="summary-wXfuOj9C7L@OpenReview" class="summary">Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.</p>
            <p id="subjects-wXfuOj9C7L@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wXfuOj9C7L@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wXfuOj9C7L@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wXfuOj9C7L@OpenReview" onclick="foldPdfKimi('wXfuOj9C7L@OpenReview', this)" class="hr hr-fold">
        </div><div id="wbvshlfyB0@OpenReview" class="panel paper" keywords="teaching,graph,nonparametric,learners,gcns,grant,property,descent,level,reinterprets">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wbvshlfyB0" target="_blank" title="188/225"><span class="index notranslate">#188</span></a>
                <a id="title-wbvshlfyB0@OpenReview" class="title-link" href="/venue/wbvshlfyB0@OpenReview" target="_blank">Nonparametric Teaching for Graph Property Learners</a>
                <a id="pdf-wbvshlfyB0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wbvshlfyB0@OpenReview', this)" data="https://openreview.net/pdf?id=wbvshlfyB0">[PDF<sup id="pdf-stars-wbvshlfyB0@OpenReview">3</sup>]</a>
                <a id="copy-wbvshlfyB0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wbvshlfyB0@OpenReview')">[Copy]</a>
                <a id="kimi-wbvshlfyB0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wbvshlfyB0@OpenReview', this)">[Kimi<sup id="kimi-stars-wbvshlfyB0@OpenReview">2</sup>]</a>
                <a id="rel-wbvshlfyB0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wbvshlfyB0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wbvshlfyB0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Zhang" target="_blank">Chen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weixin Bu" target="_blank">Weixin Bu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyi Ren" target="_blank">Zeyi Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengwu Liu" target="_blank">Zhengwu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yik-Chung WU" target="_blank">Yik-Chung WU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ngai Wong" target="_blank">Ngai Wong</a>
            </p>
            <p id="summary-wbvshlfyB0@OpenReview" class="summary">Inferring properties of graph-structured data, *e.g.*, the solubility of molecules, essentially involves learning the implicit mapping from graphs to their properties. This learning process is often costly for graph property learners like Graph Convolutional Networks (GCNs). To address this, we propose a paradigm called Graph Nonparametric Teaching (GraNT) that reinterprets the learning process through a novel nonparametric teaching perspective. Specifically, the latter offers a theoretical framework for teaching implicitly defined (*i.e.*, nonparametric) mappings via example selection. Such an implicit mapping is realized by a dense set of graph-property pairs, with the GraNT teacher selecting a subset of them to promote faster convergence in GCN training. By analytically examining the impact of graph structure on parameter-based gradient descent during training, and recasting the evolution of GCNs—shaped by parameter updates—through functional gradient descent in nonparametric teaching, we show *for the first time* that teaching graph property learners (*i.e.*, GCNs) is consistent with teaching structure-aware nonparametric learners. These new findings readily commit GraNT to enhancing learning efficiency of the graph property learner, showing significant reductions in training time for graph-level regression (-36.62\%), graph-level classification (-38.19\%), node-level regression (-30.97\%) and node-level classification (-47.30\%), all while maintaining its generalization performance.</p>
            <p id="subjects-wbvshlfyB0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wbvshlfyB0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wbvshlfyB0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wbvshlfyB0@OpenReview" onclick="foldPdfKimi('wbvshlfyB0@OpenReview', this)" class="hr hr-fold">
        </div><div id="wiQe95BPaB@OpenReview" class="panel paper" keywords="flashtp,times,mlips,interatomic,tensor,product,sevennet,e3nn,potentials,snu">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wiQe95BPaB" target="_blank" title="189/225"><span class="index notranslate">#189</span></a>
                <a id="title-wiQe95BPaB@OpenReview" class="title-link" href="/venue/wiQe95BPaB@OpenReview" target="_blank">FlashTP: Fused, Sparsity-Aware Tensor Product for Machine Learning Interatomic Potentials</a>
                <a id="pdf-wiQe95BPaB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wiQe95BPaB@OpenReview', this)" data="https://openreview.net/pdf?id=wiQe95BPaB">[PDF<sup id="pdf-stars-wiQe95BPaB@OpenReview">2</sup>]</a>
                <a id="copy-wiQe95BPaB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wiQe95BPaB@OpenReview')">[Copy]</a>
                <a id="kimi-wiQe95BPaB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wiQe95BPaB@OpenReview', this)">[Kimi<sup id="kimi-stars-wiQe95BPaB@OpenReview">1</sup>]</a>
                <a id="rel-wiQe95BPaB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wiQe95BPaB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wiQe95BPaB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seung Lee" target="_blank">Seung Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hojoon Kim" target="_blank">Hojoon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yutack Park" target="_blank">Yutack Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawoon Jeong" target="_blank">Dawoon Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungwu Han" target="_blank">Seungwu Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yeonhong Park" target="_blank">Yeonhong Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jae W. Lee" target="_blank">Jae W. Lee</a>
            </p>
            <p id="summary-wiQe95BPaB@OpenReview" class="summary">Machine Learning Interatomic Potentials (MLIPs) enable efficient molecular dynamics (MD) simulations with high accuracy. While equivariant MLIPs achieve state-of-the-art accuracy, they face significant computational bottlenecks centered around their Tensor-Product layer, which account for up to 75\% of training time and cause substantial memory overhead. We present FlashTP, a highly optimized tensor-product library that addresses these inefficiencies through kernel fusion, sparse computation, and path-aggregated execution. FlashTP achieves up to 41.6<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-137-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-997" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-998"><span class="mo" id="MathJax-Span-999" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-137">\times</script> and 60.8<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-138-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1000" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1001"><span class="mo" id="MathJax-Span-1002" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-138">\times</script> kernel speedups over _e3nn_ and NVIDIA cuEquivariance, respectively. For SevenNet-l3i5, it delivers 4.2<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-139-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1003" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1004"><span class="mo" id="MathJax-Span-1005" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-139">\times</script> and 3.5<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-140-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1006" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1007"><span class="mo" id="MathJax-Span-1008" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-140">\times</script> speedup while reducing peak memory usage by 6.3<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-141-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1009" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1010"><span class="mo" id="MathJax-Span-1011" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-141">\times</script> and 6.2<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-142-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1012" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1013"><span class="mo" id="MathJax-Span-1014" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-142">\times</script> for inference and training, respectively. The code is available at https://github.com/SNU-ARC/flashTP.</p>
            <p id="subjects-wiQe95BPaB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-wiQe95BPaB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wiQe95BPaB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wiQe95BPaB@OpenReview" onclick="foldPdfKimi('wiQe95BPaB@OpenReview', this)" class="hr hr-fold">
        </div><div id="xVBfdltHST@OpenReview" class="panel paper" keywords="solvation,rilood,relational,invariant,prediction,ood,free,energy,robust,diverse">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xVBfdltHST" target="_blank" title="190/225"><span class="index notranslate">#190</span></a>
                <a id="title-xVBfdltHST@OpenReview" class="title-link" href="/venue/xVBfdltHST@OpenReview" target="_blank">Relational Invariant Learning for Robust Solvation Free Energy Prediction</a>
                <a id="pdf-xVBfdltHST@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xVBfdltHST@OpenReview', this)" data="https://openreview.net/pdf?id=xVBfdltHST">[PDF<sup id="pdf-stars-xVBfdltHST@OpenReview">1</sup>]</a>
                <a id="copy-xVBfdltHST@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xVBfdltHST@OpenReview')">[Copy]</a>
                <a id="kimi-xVBfdltHST@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xVBfdltHST@OpenReview', this)">[Kimi<sup id="kimi-stars-xVBfdltHST@OpenReview">2</sup>]</a>
                <a id="rel-xVBfdltHST@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xVBfdltHST@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xVBfdltHST@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yeyun Chen" target="_blank">Yeyun Chen</a>
            </p>
            <p id="summary-xVBfdltHST@OpenReview" class="summary">Predicting the solvation free energy of molecules using graph neural networks holds significant potential for advancing drug discovery and the design of novel materials. While previous methods have demonstrated success on independent and identically distributed (IID) datasets, their performance in out-of-distribution (OOD) scenarios remains largely unexplored. We propose a novel Relational Invariant Learning framework (RILOOD) to enhance OOD generalization in solvation free energy prediction. RILOOD comprises three key components: (i) a mixup-based conditional modeling module that integrates diverse environments, (ii) a novel multi-granularity refinement strategy that extends beyond core substructures to enable context-aware representation learning for capturing multi-level interactions, and (iii) an invariant learning mechanism that identifies robust patterns generalizable to unseen environments. Extensive experiments demonstrate that RILOOD significantly outperforms state-of-the-art methods across various distribution shifts, highlighting its effectiveness in improving solvation free energy prediction under diverse conditions.</p>
            <p id="subjects-xVBfdltHST@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xVBfdltHST@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xVBfdltHST@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xVBfdltHST@OpenReview" onclick="foldPdfKimi('xVBfdltHST@OpenReview', this)" class="hr hr-fold">
        </div><div id="xtlixzbcfV@OpenReview" class="panel paper" keywords="novelty,world,novelties,detection,agent,sudden,ofour,reinforcement,ina,transitions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xtlixzbcfV" target="_blank" title="191/225"><span class="index notranslate">#191</span></a>
                <a id="title-xtlixzbcfV@OpenReview" class="title-link" href="/venue/xtlixzbcfV@OpenReview" target="_blank">Novelty Detection in Reinforcement Learning with World Models</a>
                <a id="pdf-xtlixzbcfV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xtlixzbcfV@OpenReview', this)" data="https://openreview.net/pdf?id=xtlixzbcfV">[PDF<sup id="pdf-stars-xtlixzbcfV@OpenReview">7</sup>]</a>
                <a id="copy-xtlixzbcfV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xtlixzbcfV@OpenReview')">[Copy]</a>
                <a id="kimi-xtlixzbcfV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xtlixzbcfV@OpenReview', this)">[Kimi<sup id="kimi-stars-xtlixzbcfV@OpenReview">4</sup>]</a>
                <a id="rel-xtlixzbcfV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xtlixzbcfV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xtlixzbcfV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Geigh Zollicoffer" target="_blank">Geigh Zollicoffer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kenneth Eaton" target="_blank">Kenneth Eaton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Balloch" target="_blank">Jonathan Balloch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julia Kim" target="_blank">Julia Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Zhou" target="_blank">Wei Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Wright" target="_blank">Robert Wright</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Riedl" target="_blank">Mark Riedl</a>
            </p>
            <p id="summary-xtlixzbcfV@OpenReview" class="summary">Reinforcement learning (RL) using world models has found significant recent successes.However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline.We refer to the sudden change in visual properties or state transitions as novelties.Implementing novelty detection within generated world model frameworks is a crucialtask for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents by utilizing the misalignment of the world model's hallucinated states and the true observed states as a novelty score. We provideeffective approaches to detecting novelties in a distribution of transitions learned by an agent ina world model. Finally, we show the advantage ofour work in a novel environment compared to traditional machine learning novelty detection methods as well as currently accepted RL-focused novelty detection algorithms.</p>
            <p id="subjects-xtlixzbcfV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xtlixzbcfV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xtlixzbcfV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xtlixzbcfV@OpenReview" onclick="foldPdfKimi('xtlixzbcfV@OpenReview', this)" class="hr hr-fold">
        </div><div id="xvLVYrYQ8a@OpenReview" class="panel paper" keywords="mpnns,graph,generalization,abilities,aggregation,covered,forest,overlook,loss,grained">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xvLVYrYQ8a" target="_blank" title="192/225"><span class="index notranslate">#192</span></a>
                <a id="title-xvLVYrYQ8a@OpenReview" class="title-link" href="/venue/xvLVYrYQ8a@OpenReview" target="_blank">Covered Forest: Fine-grained generalization analysis of graph neural networks</a>
                <a id="pdf-xvLVYrYQ8a@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xvLVYrYQ8a@OpenReview', this)" data="https://openreview.net/pdf?id=xvLVYrYQ8a">[PDF<sup id="pdf-stars-xvLVYrYQ8a@OpenReview">4</sup>]</a>
                <a id="copy-xvLVYrYQ8a@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xvLVYrYQ8a@OpenReview')">[Copy]</a>
                <a id="kimi-xvLVYrYQ8a@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xvLVYrYQ8a@OpenReview', this)">[Kimi<sup id="kimi-stars-xvLVYrYQ8a@OpenReview">1</sup>]</a>
                <a id="rel-xvLVYrYQ8a@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xvLVYrYQ8a@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xvLVYrYQ8a@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Antonis Vasileiou" target="_blank">Antonis Vasileiou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Finkelshtein" target="_blank">Ben Finkelshtein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Floris Geerts" target="_blank">Floris Geerts</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ron Levie" target="_blank">Ron Levie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Morris" target="_blank">Christopher Morris</a>
            </p>
            <p id="summary-xvLVYrYQ8a@OpenReview" class="summary">The expressive power of message-passing graph neural networks (MPNNs) is reasonably well understood, primarily through combinatorial techniques from graph isomorphism testing. However, MPNNs' generalization abilities---making meaningful predictions beyond the training set---remain less explored. Current generalization analyses often overlook graph structure, limit the focus to specific aggregation functions, and assume the impractical, hard-to-optimize <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-143-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1015" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1016"><span class="mn" id="MathJax-Span-1017" style="font-family: MathJax_Main;">0</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-143">0</script>-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-144-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1018" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1019"><span class="mn" id="MathJax-Span-1020" style="font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-144">1</script> loss function. Here, we extend recent advances in graph similarity theory to assess the influence of graph structure, aggregation, and loss functions on MPNNs' generalization abilities. Our empirical study supports our theoretical insights, improving our understanding of MPNNs' generalization properties.</p>
            <p id="subjects-xvLVYrYQ8a@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-xvLVYrYQ8a@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xvLVYrYQ8a@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xvLVYrYQ8a@OpenReview" onclick="foldPdfKimi('xvLVYrYQ8a@OpenReview', this)" class="hr hr-fold">
        </div><div id="yXRixu0ONY@OpenReview" class="panel paper" keywords="protein,pallatom,atom,textit,generation,unlocking,coordinates,tokenizes,designability,atom14">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=yXRixu0ONY" target="_blank" title="193/225"><span class="index notranslate">#193</span></a>
                <a id="title-yXRixu0ONY@OpenReview" class="title-link" href="/venue/yXRixu0ONY@OpenReview" target="_blank">P(all-atom) Is Unlocking New Path For Protein Design</a>
                <a id="pdf-yXRixu0ONY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('yXRixu0ONY@OpenReview', this)" data="https://openreview.net/pdf?id=yXRixu0ONY">[PDF<sup id="pdf-stars-yXRixu0ONY@OpenReview">5</sup>]</a>
                <a id="copy-yXRixu0ONY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('yXRixu0ONY@OpenReview')">[Copy]</a>
                <a id="kimi-yXRixu0ONY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('yXRixu0ONY@OpenReview', this)">[Kimi<sup id="kimi-stars-yXRixu0ONY@OpenReview">2</sup>]</a>
                <a id="rel-yXRixu0ONY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('yXRixu0ONY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-yXRixu0ONY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Qu" target="_blank">Wei Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Guan" target="_blank">Jiawei Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Ma" target="_blank">Rui Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=kezhai" target="_blank">kezhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=weikun wu" target="_blank">weikun wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=haobo Wang" target="_blank">haobo Wang</a>
            </p>
            <p id="summary-yXRixu0ONY@OpenReview" class="summary">We introduce Pallatom, an innovative protein generation model capable of producing protein structures with all-atom coordinates. Pallatom directly learns and models the joint distribution <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-145-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;structure&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;seq&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1021" style="width: 8.648em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.19em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1007.09em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1022"><span class="mi" id="MathJax-Span-1023" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-1024" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-1025"><span class="mrow" id="MathJax-Span-1026"><span class="mtext" id="MathJax-Span-1027" style="font-family: MathJax_Main-italic;">structure</span></span></span><span class="mo" id="MathJax-Span-1028" style="font-family: MathJax_Main;">,</span><span class="texatom" id="MathJax-Span-1029" style="padding-left: 0.159em;"><span class="mrow" id="MathJax-Span-1030"><span class="mtext" id="MathJax-Span-1031" style="font-family: MathJax_Main-italic;">seq</span></span></span><span class="mo" id="MathJax-Span-1032" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">structure</mtext></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">seq</mtext></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-145">P(\textit{structure}, \textit{seq})</script> by focusing on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-146-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;all-atom&lt;/mtext&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1033" style="width: 6.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.107em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1034"><span class="mi" id="MathJax-Span-1035" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-1036" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-1037"><span class="mrow" id="MathJax-Span-1038"><span class="mtext" id="MathJax-Span-1039" style="font-family: MathJax_Main-italic;">all-atom</span></span></span><span class="mo" id="MathJax-Span-1040" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">all-atom</mtext></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-146">P(\textit{all-atom})</script>, effectively addressing the interdependence between sequence and structure in protein generation. To achieve this, we propose a novel network architecture specifically designed for all-atom protein generation. Our model employs a dual-track framework that tokenizes proteins into token-level and atomic-level representations, integrating them through a multi-layer decoding process with "traversing" representations and recycling mechanism. We also introduce the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-147-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;atom14&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1041" style="width: 3.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.08em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1042"><span class="texatom" id="MathJax-Span-1043"><span class="mrow" id="MathJax-Span-1044"><span class="mtext" id="MathJax-Span-1045" style="font-family: MathJax_Typewriter;">atom14</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">atom14</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-147">\texttt{atom14}</script> representation method, which unifies the description of unknown side-chain coordinates, ensuring high fidelity between the generated all-atom conformation and its physical structure. Experimental results demonstrate that Pallatom excels in key metrics of protein design, including designability, diversity, and novelty, showing significant improvements across the board. Our model not only enhances the accuracy of protein generation but also exhibits excellent sampling efficiency, paving the way for future applications in larger and more complex systems.</p>
            <p id="subjects-yXRixu0ONY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-yXRixu0ONY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-yXRixu0ONY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-yXRixu0ONY@OpenReview" onclick="foldPdfKimi('yXRixu0ONY@OpenReview', this)" class="hr hr-fold">
        </div><div id="zIGIvysR1H@OpenReview" class="panel paper" keywords="sandbox,suite,multimodal,juicer,data,development,vbench,feedback,models,centric">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zIGIvysR1H" target="_blank" title="194/225"><span class="index notranslate">#194</span></a>
                <a id="title-zIGIvysR1H@OpenReview" class="title-link" href="/venue/zIGIvysR1H@OpenReview" target="_blank">Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development</a>
                <a id="pdf-zIGIvysR1H@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zIGIvysR1H@OpenReview', this)" data="https://openreview.net/pdf?id=zIGIvysR1H">[PDF<sup id="pdf-stars-zIGIvysR1H@OpenReview">1</sup>]</a>
                <a id="copy-zIGIvysR1H@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zIGIvysR1H@OpenReview')">[Copy]</a>
                <a id="kimi-zIGIvysR1H@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zIGIvysR1H@OpenReview', this)">[Kimi<sup id="kimi-stars-zIGIvysR1H@OpenReview">2</sup>]</a>
                <a id="rel-zIGIvysR1H@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zIGIvysR1H@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zIGIvysR1H@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daoyuan Chen" target="_blank">Daoyuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haibin Wang" target="_blank">Haibin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilun Huang" target="_blank">Yilun Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ce Ge" target="_blank">Ce Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaliang Li" target="_blank">Yaliang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bolin Ding" target="_blank">Bolin Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingren Zhou" target="_blank">Jingren Zhou</a>
            </p>
            <p id="summary-zIGIvysR1H@OpenReview" class="summary">The emergence of multimodal large models has advanced artificial intelligence, introducing unprecedented levels of performance and functionality. However, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource utilization. In response, we present a new sandbox suite tailored for integrated data-model co-development. This sandbox provides a feedback-driven experimental platform, enabling cost-effective iteration and guided refinement of both data and models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through practical use cases on multimodal tasks such as image-text pre-training with CLIP, image-to-text generation with LLaVA-like models, and text-to-video generation with DiT-based models, yields transferable and notable performance boosts, such as topping the VBench leaderboard. A comprehensive set of over 100 experiments demonstrated the suite's usability and extensibility, while also uncovering insights into the interplay between data quality, diversity, model behavior, and computational costs. All codes, datasets, and models are open-sourced to foster future research and applications that would otherwise be infeasible due to the lack of a dedicated co-development infrastructure.</p>
            <p id="subjects-zIGIvysR1H@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-zIGIvysR1H@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zIGIvysR1H@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zIGIvysR1H@OpenReview" onclick="foldPdfKimi('zIGIvysR1H@OpenReview', this)" class="hr hr-fold">
        </div><div id="zL6ljQvPzZ@OpenReview" class="panel paper" keywords="varepsilon,ldp,quantile,shuffle,protocols,adaptive,private,privacy,frac,users">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zL6ljQvPzZ" target="_blank" title="195/225"><span class="index notranslate">#195</span></a>
                <a id="title-zL6ljQvPzZ@OpenReview" class="title-link" href="/venue/zL6ljQvPzZ@OpenReview" target="_blank">Lightweight Protocols for Distributed Private Quantile Estimation</a>
                <a id="pdf-zL6ljQvPzZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zL6ljQvPzZ@OpenReview', this)" data="https://openreview.net/pdf?id=zL6ljQvPzZ">[PDF<sup id="pdf-stars-zL6ljQvPzZ@OpenReview">1</sup>]</a>
                <a id="copy-zL6ljQvPzZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zL6ljQvPzZ@OpenReview')">[Copy]</a>
                <a id="kimi-zL6ljQvPzZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zL6ljQvPzZ@OpenReview', this)">[Kimi<sup id="kimi-stars-zL6ljQvPzZ@OpenReview">1</sup>]</a>
                <a id="rel-zL6ljQvPzZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zL6ljQvPzZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zL6ljQvPzZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anders Aamand" target="_blank">Anders Aamand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabrizio Boninsegna" target="_blank">Fabrizio Boninsegna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abigail Gentle" target="_blank">Abigail Gentle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Imola" target="_blank">Jacob Imola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rasmus Pagh" target="_blank">Rasmus Pagh</a>
            </p>
            <p id="summary-zL6ljQvPzZ@OpenReview" class="summary">Distributed data analysis is a large and growing field driven by a massive proliferation of user devices, and by privacy concerns surrounding the centralised storage of data. We consider two \emph{adaptive} algorithms for estimating one quantile (e.g.~the median) when each user holds a single data point lying in a domain <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-148-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1046" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.2em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1047"><span class="mo" id="MathJax-Span-1048" style="font-family: MathJax_Main;">[</span><span class="mi" id="MathJax-Span-1049" style="font-family: MathJax_Math-italic;">B</span><span class="mo" id="MathJax-Span-1050" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mi>B</mi><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-148">[B]</script> that can be queried once through a private mechanism; one under local differential privacy (LDP) and another for shuffle differential privacy (shuffle-DP). In the adaptive setting we present an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-149-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1051" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1052"><span class="mi" id="MathJax-Span-1053" style="font-family: MathJax_Math-italic;">ε</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi></math></span></span><script type="math/tex" id="MathJax-Element-149">\varepsilon</script>-LDP algorithm which can estimate any quantile within error <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-150-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1054" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1055"><span class="mi" id="MathJax-Span-1056" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-150">\alpha</script> only requiring <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-151-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1057" style="width: 4.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.492em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.94em, 1003.39em, 2.763em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1058"><span class="mi" id="MathJax-Span-1059" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-1060" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-1061"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1001.62em, 2.451em, -999.997em); top: -2.706em; left: 50%; margin-left: -0.779em;"><span class="mrow" id="MathJax-Span-1062"><span class="mi" id="MathJax-Span-1063" style="font-size: 70.7%; font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-1064" style="font-size: 70.7%;"></span><span class="mi" id="MathJax-Span-1065" style="font-size: 70.7%; font-family: MathJax_Math-italic; padding-left: 0.263em;">B</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.336em, 1001.41em, 4.169em, -999.997em); top: -3.539em; left: 50%; margin-left: -0.727em;"><span class="mrow" id="MathJax-Span-1066"><span class="msubsup" id="MathJax-Span-1067"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1068" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.341em; left: 0.315em;"><span class="mn" id="MathJax-Span-1069" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-1070"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1071" style="font-size: 70.7%; font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.341em; left: 0.471em;"><span class="mn" id="MathJax-Span-1072" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.72em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.721em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-1073" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.622em; border-left: 0px solid; width: 0px; height: 1.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>log</mi><mo>⁡</mo><mi>B</mi></mrow><mrow><msup><mi>ε</mi><mn>2</mn></msup><msup><mi>α</mi><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-151">O(\frac{\log B}{\varepsilon^2\alpha^2})</script> users, and an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-152-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x03B4;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1074" style="width: 2.607em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.03em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1075"><span class="mo" id="MathJax-Span-1076" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1077" style="font-family: MathJax_Math-italic;">ε</span><span class="mo" id="MathJax-Span-1078" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-1079" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">δ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-1080" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>ε</mi><mo>,</mo><mi>δ</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-152">(\varepsilon,\delta)</script>-shuffle DP algorithm requiring only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-153-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1081" style="width: 9.586em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1007.87em, 2.763em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1082"><span class="texatom" id="MathJax-Span-1083"><span class="mrow" id="MathJax-Span-1084"><span class="munderover" id="MathJax-Span-1085"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1086" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-1087" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-1088" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-1089" style="font-family: MathJax_Main;">(</span><span class="mfrac" id="MathJax-Span-1090"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-1091" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.336em, 1000.63em, 4.169em, -999.997em); top: -3.539em; left: 50%; margin-left: -0.31em;"><span class="msubsup" id="MathJax-Span-1092"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1093" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ε</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.341em; left: 0.315em;"><span class="mn" id="MathJax-Span-1094" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.73em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.732em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-1095" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mfrac" id="MathJax-Span-1096" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-1097" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.336em, 1000.78em, 4.169em, -999.997em); top: -3.539em; left: 50%; margin-left: -0.414em;"><span class="msubsup" id="MathJax-Span-1098"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1099" style="font-size: 70.7%; font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.341em; left: 0.471em;"><span class="mn" id="MathJax-Span-1100" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.89em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.888em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-1101" style="font-family: MathJax_Main;">)</span><span class="mi" id="MathJax-Span-1102" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-1103"></span><span class="mi" id="MathJax-Span-1104" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">B</span><span class="mo" id="MathJax-Span-1105" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.622em; border-left: 0px solid; width: 0px; height: 1.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mfrac><mn>1</mn><msup><mi>ε</mi><mn>2</mn></msup></mfrac><mo>+</mo><mfrac><mn>1</mn><msup><mi>α</mi><mn>2</mn></msup></mfrac><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mi>B</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-153">\widetilde{O}((\frac{1}{\varepsilon^2}+\frac{1}{\alpha^2})\log B)</script> users. Prior (nonadaptive) algorithms require more users by several logarithmic factors in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-154-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1106" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1107"><span class="mi" id="MathJax-Span-1108" style="font-family: MathJax_Math-italic;">B</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></span></span><script type="math/tex" id="MathJax-Element-154">B</script>. We further provide a matching lower bound for adaptive protocols, showing that our LDP algorithm is optimal in the low-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-155-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1109" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1110"><span class="mi" id="MathJax-Span-1111" style="font-family: MathJax_Math-italic;">ε</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi></math></span></span><script type="math/tex" id="MathJax-Element-155">\varepsilon</script> regime. Additionally, we establish lower bounds against non-adaptive protocols which paired with our understanding of the adaptive case, proves a fundamental separation between these models.</p>
            <p id="subjects-zL6ljQvPzZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-zL6ljQvPzZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zL6ljQvPzZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zL6ljQvPzZ@OpenReview" onclick="foldPdfKimi('zL6ljQvPzZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="zU4VCPHYRC@OpenReview" class="panel paper" keywords="byzantine,workers,aggregators,brdl,robust,robustness,tension,accuracy,attack,faulty">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zU4VCPHYRC" target="_blank" title="196/225"><span class="index notranslate">#196</span></a>
                <a id="title-zU4VCPHYRC@OpenReview" class="title-link" href="/venue/zU4VCPHYRC@OpenReview" target="_blank">On the Tension between Byzantine Robustness and No-Attack Accuracy in Distributed Learning</a>
                <a id="pdf-zU4VCPHYRC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zU4VCPHYRC@OpenReview', this)" data="https://openreview.net/pdf?id=zU4VCPHYRC">[PDF<sup id="pdf-stars-zU4VCPHYRC@OpenReview">1</sup>]</a>
                <a id="copy-zU4VCPHYRC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zU4VCPHYRC@OpenReview')">[Copy]</a>
                <a id="kimi-zU4VCPHYRC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zU4VCPHYRC@OpenReview', this)">[Kimi<sup id="kimi-stars-zU4VCPHYRC@OpenReview">1</sup>]</a>
                <a id="rel-zU4VCPHYRC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zU4VCPHYRC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zU4VCPHYRC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi-Rui Yang" target="_blank">Yi-Rui Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang-Wei Shi" target="_blank">Chang-Wei Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wu-Jun Li" target="_blank">Wu-Jun Li</a>
            </p>
            <p id="summary-zU4VCPHYRC@OpenReview" class="summary">Byzantine-robust distributed learning (BRDL), which refers to distributed learning that can work with potential faulty or malicious workers (also known as Byzantine workers), has recently attracted much research attention. Robust aggregators are widely used in existing BRDL methods to obtain robustness against Byzantine workers. However, Byzantine workers do not always exist in applications. As far as we know, there is almost no existing work theoretically investigating the effect of using robust aggregators when there are no Byzantine workers. To bridge this knowledge gap, we theoretically analyze the aggregation error for robust aggregators when there are no Byzantine workers. Specifically, we show that the worst-case aggregation error without Byzantine workers increases with the increase of the number of Byzantine workers that a robust aggregator can tolerate. The theoretical result reveals the tension between Byzantine robustness and no-attack accuracy, which refers to accuracy without faulty workers and malicious workers in this paper. Furthermore, we provide lower bounds for the convergence rate of gradient descent with robust aggregators for non-convex objective functions and objective functions that satisfy the Polyak-Lojasiewicz (PL) condition, respectively. We also prove the tightness of the lower bounds. The lower bounds for convergence rate reveal similar tension between Byzantine robustness and no-attack accuracy. Empirical results further support our theoretical findings.</p>
            <p id="subjects-zU4VCPHYRC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-zU4VCPHYRC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zU4VCPHYRC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zU4VCPHYRC@OpenReview" onclick="foldPdfKimi('zU4VCPHYRC@OpenReview', this)" class="hr hr-fold">
        </div><div id="zdOGBRQEbz@OpenReview" class="panel paper" keywords="protein,plms,sae,mechanistic,features,biology,plm,autoencoders,thermostability,sparse">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zdOGBRQEbz" target="_blank" title="197/225"><span class="index notranslate">#197</span></a>
                <a id="title-zdOGBRQEbz@OpenReview" class="title-link" href="/venue/zdOGBRQEbz@OpenReview" target="_blank">From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models</a>
                <a id="pdf-zdOGBRQEbz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zdOGBRQEbz@OpenReview', this)" data="https://openreview.net/pdf?id=zdOGBRQEbz">[PDF<sup id="pdf-stars-zdOGBRQEbz@OpenReview">5</sup>]</a>
                <a id="copy-zdOGBRQEbz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zdOGBRQEbz@OpenReview')">[Copy]</a>
                <a id="kimi-zdOGBRQEbz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zdOGBRQEbz@OpenReview', this)">[Kimi<sup id="kimi-stars-zdOGBRQEbz@OpenReview">1</sup>]</a>
                <a id="rel-zdOGBRQEbz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zdOGBRQEbz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zdOGBRQEbz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Etowah Adams" target="_blank">Etowah Adams</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liam Bai" target="_blank">Liam Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minji Lee" target="_blank">Minji Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiyang Yu" target="_blank">Yiyang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammed AlQuraishi" target="_blank">Mohammed AlQuraishi</a>
            </p>
            <p id="summary-zdOGBRQEbz@OpenReview" class="summary">Protein language models (pLMs) are powerful predictors of protein structure and function, learning through unsupervised training on millions of protein sequences. pLMs are thought to capture common motifs in protein sequences, but the specifics of pLM features are not well understood. Identifying these features would not only shed light on how pLMs work, but potentially uncover novel protein biology––studying the model to study the biology. Motivated by this, we train sparse autoencoders (SAEs) on the residual stream of a pLM, ESM-2. By characterizing SAE features, we determine that pLMs use a combination of generic features and family-specific features to represent a protein. In addition, we demonstrate how known sequence determinants of properties such as thermostability and subcellular localization can be identified by linear probing of SAE features. For predictive features without known functional associations, we hypothesize their role in unknown mechanisms and provide visualization tools to aid their interpretation. Our study gives a better understanding of the limitations of pLMs, and demonstrates how SAE features can be used to help generate hypotheses for biological mechanisms. We release our code, model weights, and feature visualizer.</p>
            <p id="subjects-zdOGBRQEbz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-zdOGBRQEbz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zdOGBRQEbz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zdOGBRQEbz@OpenReview" onclick="foldPdfKimi('zdOGBRQEbz@OpenReview', this)" class="hr hr-fold">
        </div><div id="42Au7FoD8F@OpenReview" class="panel paper" keywords="mysterious,generalization,phenomena,hypothesis,overfitting,deep,position,classes,defying,overparametrization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=42Au7FoD8F" target="_blank" title="198/225"><span class="index notranslate">#198</span></a>
                <a id="title-42Au7FoD8F@OpenReview" class="title-link" href="/venue/42Au7FoD8F@OpenReview" target="_blank">Position: Deep Learning is Not So Mysterious or Different</a>
                <a id="pdf-42Au7FoD8F@OpenReview" class="title-pdf notranslate" onclick="togglePdf('42Au7FoD8F@OpenReview', this)" data="https://openreview.net/pdf?id=42Au7FoD8F">[PDF<sup id="pdf-stars-42Au7FoD8F@OpenReview">3</sup>]</a>
                <a id="copy-42Au7FoD8F@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('42Au7FoD8F@OpenReview')">[Copy]</a>
                <a id="kimi-42Au7FoD8F@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('42Au7FoD8F@OpenReview', this)">[Kimi<sup id="kimi-stars-42Au7FoD8F@OpenReview">1</sup>]</a>
                <a id="rel-42Au7FoD8F@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('42Au7FoD8F@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-42Au7FoD8F@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Wilson" target="_blank">Andrew Wilson</a>
            </p>
            <p id="summary-42Au7FoD8F@OpenReview" class="summary">Deep neural networks are often seen as different from other model classes by defying conventional notions of generalization. Popular examples of anomalous generalization behaviour include benign overfitting, double descent, and the success of overparametrization.This position paper argues that these phenomena are not distinct to neural networks, or particularly mysterious. Moreover, this generalization behaviour can be intuitively understood, and rigorously characterized, using long-standing generalization frameworks such as PAC-Bayes and countable hypothesis bounds. We present soft inductive biases as a key unifying principle in explaining these phenomena: rather than restricting the hypothesis space to avoid overfitting, embrace a flexible hypothesis space, with a soft preference for simpler solutions that are consistent with the data. This principle can be encoded in many model classes, and thus deep learning is not as mysterious or different from other model classes as it might seem. However, we also highlight how deep learning is relatively distinct in other ways, such as its ability for representation learning, phenomena such as mode connectivity, and its relative universality.</p>
            <p id="subjects-42Au7FoD8F@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-42Au7FoD8F@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-42Au7FoD8F@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-42Au7FoD8F@OpenReview" onclick="foldPdfKimi('42Au7FoD8F@OpenReview', this)" class="hr hr-fold">
        </div><div id="ACzL62Jp4E@OpenReview" class="panel paper" keywords="gpai,flaw,flaws,reporting,disclosure,systems,evaluation,security,interventions,providers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ACzL62Jp4E" target="_blank" title="199/225"><span class="index notranslate">#199</span></a>
                <a id="title-ACzL62Jp4E@OpenReview" class="title-link" href="/venue/ACzL62Jp4E@OpenReview" target="_blank">Position: In-House Evaluation Is Not Enough. Towards Robust Third-Party Evaluation and Flaw Disclosure for General-Purpose AI</a>
                <a id="pdf-ACzL62Jp4E@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ACzL62Jp4E@OpenReview', this)" data="https://openreview.net/pdf?id=ACzL62Jp4E">[PDF<sup id="pdf-stars-ACzL62Jp4E@OpenReview">1</sup>]</a>
                <a id="copy-ACzL62Jp4E@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ACzL62Jp4E@OpenReview')">[Copy]</a>
                <a id="kimi-ACzL62Jp4E@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ACzL62Jp4E@OpenReview', this)">[Kimi<sup id="kimi-stars-ACzL62Jp4E@OpenReview">1</sup>]</a>
                <a id="rel-ACzL62Jp4E@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ACzL62Jp4E@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ACzL62Jp4E@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shayne Longpre" target="_blank">Shayne Longpre</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Klyman" target="_blank">Kevin Klyman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruth Elisabeth Appel" target="_blank">Ruth Elisabeth Appel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sayash Kapoor" target="_blank">Sayash Kapoor</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishi Bommasani" target="_blank">Rishi Bommasani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michelle Sahar" target="_blank">Michelle Sahar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sean McGregor" target="_blank">Sean McGregor</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Avijit Ghosh" target="_blank">Avijit Ghosh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Borhane Blili-Hamelin" target="_blank">Borhane Blili-Hamelin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Butters" target="_blank">Nathan Butters</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alondra Nelson" target="_blank">Alondra Nelson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Elazari" target="_blank">Amit Elazari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Sellars" target="_blank">Andrew Sellars</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Casey Ellis" target="_blank">Casey Ellis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dane Sherrets" target="_blank">Dane Sherrets</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawn Song" target="_blank">Dawn Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harley Geiger" target="_blank">Harley Geiger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ilona Cohen" target="_blank">Ilona Cohen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lauren McIlvenny" target="_blank">Lauren McIlvenny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Madhulika Srikumar" target="_blank">Madhulika Srikumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Jaycox" target="_blank">Mark Jaycox</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Markus Anderljung" target="_blank">Markus Anderljung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nadine Johnson" target="_blank">Nadine Johnson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Carlini" target="_blank">Nicholas Carlini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Miailhe" target="_blank">Nicolas Miailhe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nik Marda" target="_blank">Nik Marda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Henderson" target="_blank">Peter Henderson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rebecca Portnoff" target="_blank">Rebecca Portnoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rebecca Weiss" target="_blank">Rebecca Weiss</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Victoria Westerhoff" target="_blank">Victoria Westerhoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yacine Jernite" target="_blank">Yacine Jernite</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rumman Chowdhury" target="_blank">Rumman Chowdhury</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Percy Liang" target="_blank">Percy Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arvind Narayanan" target="_blank">Arvind Narayanan</a>
            </p>
            <p id="summary-ACzL62Jp4E@OpenReview" class="summary">The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped, lagging far behind more established fields like software security. Based on a collaboration between experts from the fields of software security, machine learning, law, social science, and policy, we identify key gaps in the evaluation and reporting of flaws in GPAI systems. We call for three interventions to advance system safety. First, we propose using standardized AI flaw reports and rules of engagement for researchers in order to ease the process of submitting, reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system providers adopt broadly-scoped flaw disclosure programs, borrowing from bug bounties, with legal safe harbors to protect researchers. Third, we advocate for the development of improved infrastructure to coordinate distribution of flaw reports across the many stakeholders who may be impacted. These interventions are increasingly urgent, as evidenced by the prevalence of jailbreaks and other flaws that can transfer across different providers' GPAI systems. By promoting robust reporting and coordination in the AI ecosystem, these proposals could significantly improve the safety, security, and accountability of GPAI systems.</p>
            <p id="subjects-ACzL62Jp4E@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-ACzL62Jp4E@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ACzL62Jp4E@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ACzL62Jp4E@OpenReview" onclick="foldPdfKimi('ACzL62Jp4E@OpenReview', this)" class="hr hr-fold">
        </div><div id="HuvAM5x2xG@OpenReview" class="panel paper" keywords="formal,reasoning,ai4math,mathematical,position,math,intellectually,frontier,verification,mirrored">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HuvAM5x2xG" target="_blank" title="200/225"><span class="index notranslate">#200</span></a>
                <a id="title-HuvAM5x2xG@OpenReview" class="title-link" href="/venue/HuvAM5x2xG@OpenReview" target="_blank">Position: Formal Mathematical Reasoning—A New Frontier in AI</a>
                <a id="pdf-HuvAM5x2xG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HuvAM5x2xG@OpenReview', this)" data="https://openreview.net/pdf?id=HuvAM5x2xG">[PDF<sup id="pdf-stars-HuvAM5x2xG@OpenReview">2</sup>]</a>
                <a id="copy-HuvAM5x2xG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HuvAM5x2xG@OpenReview')">[Copy]</a>
                <a id="kimi-HuvAM5x2xG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HuvAM5x2xG@OpenReview', this)">[Kimi<sup id="kimi-stars-HuvAM5x2xG@OpenReview">3</sup>]</a>
                <a id="rel-HuvAM5x2xG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HuvAM5x2xG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HuvAM5x2xG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiyu Yang" target="_blank">Kaiyu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Poesia" target="_blank">Gabriel Poesia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingxuan He" target="_blank">Jingxuan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenda Li" target="_blank">Wenda Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kristin Lauter" target="_blank">Kristin Lauter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Swarat Chaudhuri" target="_blank">Swarat Chaudhuri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawn Song" target="_blank">Dawn Song</a>
            </p>
            <p id="summary-HuvAM5x2xG@OpenReview" class="summary">AI for Mathematics (AI4Math) is intellectually intriguing and is crucial for AI-driven system design and verification. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. This position paper advocates formal mathematical reasoning as an indispensable component in future AI for math, formal verification, and verifiable generation. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success.</p>
            <p id="subjects-HuvAM5x2xG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-HuvAM5x2xG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HuvAM5x2xG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HuvAM5x2xG@OpenReview" onclick="foldPdfKimi('HuvAM5x2xG@OpenReview', this)" class="hr hr-fold">
        </div><div id="QMgCDPWL9Y@OpenReview" class="panel paper" keywords="reasoning,aui,reason,agi,intelligence,pretraining,llms,transferability,pretaining,algorithmic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QMgCDPWL9Y" target="_blank" title="201/225"><span class="index notranslate">#201</span></a>
                <a id="title-QMgCDPWL9Y@OpenReview" class="title-link" href="/venue/QMgCDPWL9Y@OpenReview" target="_blank">Position: General Intelligence Requires Reward-based Pretraining</a>
                <a id="pdf-QMgCDPWL9Y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QMgCDPWL9Y@OpenReview', this)" data="https://openreview.net/pdf?id=QMgCDPWL9Y">[PDF<sup id="pdf-stars-QMgCDPWL9Y@OpenReview">4</sup>]</a>
                <a id="copy-QMgCDPWL9Y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QMgCDPWL9Y@OpenReview')">[Copy]</a>
                <a id="kimi-QMgCDPWL9Y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QMgCDPWL9Y@OpenReview', this)">[Kimi<sup id="kimi-stars-QMgCDPWL9Y@OpenReview">2</sup>]</a>
                <a id="rel-QMgCDPWL9Y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QMgCDPWL9Y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QMgCDPWL9Y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seungwook Han" target="_blank">Seungwook Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jyothish Pari" target="_blank">Jyothish Pari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Gershman" target="_blank">Samuel Gershman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pulkit Agrawal" target="_blank">Pulkit Agrawal</a>
            </p>
            <p id="summary-QMgCDPWL9Y@OpenReview" class="summary">Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs. To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a \textit{reasoning prior} for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.</p>
            <p id="subjects-QMgCDPWL9Y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-QMgCDPWL9Y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QMgCDPWL9Y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QMgCDPWL9Y@OpenReview" onclick="foldPdfKimi('QMgCDPWL9Y@OpenReview', this)" class="hr hr-fold">
        </div><div id="YhZ2PY2nZa@OpenReview" class="panel paper" keywords="clt,evals,llm,uncertainty,bars,datapoints,position,hundred,evaluations,rely">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YhZ2PY2nZa" target="_blank" title="202/225"><span class="index notranslate">#202</span></a>
                <a id="title-YhZ2PY2nZa@OpenReview" class="title-link" href="/venue/YhZ2PY2nZa@OpenReview" target="_blank">Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints</a>
                <a id="pdf-YhZ2PY2nZa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YhZ2PY2nZa@OpenReview', this)" data="https://openreview.net/pdf?id=YhZ2PY2nZa">[PDF<sup id="pdf-stars-YhZ2PY2nZa@OpenReview">1</sup>]</a>
                <a id="copy-YhZ2PY2nZa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YhZ2PY2nZa@OpenReview')">[Copy]</a>
                <a id="kimi-YhZ2PY2nZa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YhZ2PY2nZa@OpenReview', this)">[Kimi<sup id="kimi-stars-YhZ2PY2nZa@OpenReview">2</sup>]</a>
                <a id="rel-YhZ2PY2nZa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YhZ2PY2nZa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YhZ2PY2nZa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sam Bowyer" target="_blank">Sam Bowyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laurence Aitchison" target="_blank">Laurence Aitchison</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Desi Ivanova" target="_blank">Desi Ivanova</a>
            </p>
            <p id="summary-YhZ2PY2nZa@OpenReview" class="summary">Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios.</p>
            <p id="subjects-YhZ2PY2nZa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-YhZ2PY2nZa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YhZ2PY2nZa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YhZ2PY2nZa@OpenReview" onclick="foldPdfKimi('YhZ2PY2nZa@OpenReview', this)" class="hr hr-fold">
        </div><div id="asQJx56NqB@OpenReview" class="panel paper" keywords="neologisms,neologism,concepts,vocabulary,machines,understand,human,andnot,position,machine">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=asQJx56NqB" target="_blank" title="203/225"><span class="index notranslate">#203</span></a>
                <a id="title-asQJx56NqB@OpenReview" class="title-link" href="/venue/asQJx56NqB@OpenReview" target="_blank">Position: We Can’t Understand AI Using our Existing Vocabulary</a>
                <a id="pdf-asQJx56NqB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('asQJx56NqB@OpenReview', this)" data="https://openreview.net/pdf?id=asQJx56NqB">[PDF<sup id="pdf-stars-asQJx56NqB@OpenReview">3</sup>]</a>
                <a id="copy-asQJx56NqB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('asQJx56NqB@OpenReview')">[Copy]</a>
                <a id="kimi-asQJx56NqB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('asQJx56NqB@OpenReview', this)">[Kimi<sup id="kimi-stars-asQJx56NqB@OpenReview">3</sup>]</a>
                <a id="rel-asQJx56NqB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('asQJx56NqB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-asQJx56NqB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=John Hewitt" target="_blank">John Hewitt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Geirhos" target="_blank">Robert Geirhos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Been Kim" target="_blank">Been Kim</a>
            </p>
            <p id="summary-asQJx56NqB@OpenReview" class="summary">This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we shouldstrive to develop neologisms: new words thatrepresent precise human concepts that we wantto teach machines, or machine concepts that weneed to learn. We start from the premise thathumans and machines have differing concepts.This means interpretability can be framed as acommunication problem: humans must be able toreference and control machine concepts, and communicate human concepts to machines. Creatinga shared human-machine language through developing neologisms, we believe, could solve thiscommunication problem. Successful neologismsachieve a useful amount of abstraction: not toodetailed, so they’re reusable in many contexts, andnot too high-level, so they convey precise information. As a proof of concept, we demonstrate howa “length neologism” enables controlling LLMresponse length, while a “diversity neologism” allows sampling more variable responses. Takentogether, we argue that we cannot understand AIusing our existing vocabulary, and expanding itthrough neologisms creates opportunities for bothcontrolling and understanding machines better.</p>
            <p id="subjects-asQJx56NqB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-asQJx56NqB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-asQJx56NqB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-asQJx56NqB@OpenReview" onclick="foldPdfKimi('asQJx56NqB@OpenReview', this)" class="hr hr-fold">
        </div><div id="eI8KegpPyX@OpenReview" class="panel paper" keywords="race,racial,flawed,essentialist,position,categorization,premise,oversimplify,critiques,social">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eI8KegpPyX" target="_blank" title="204/225"><span class="index notranslate">#204</span></a>
                <a id="title-eI8KegpPyX@OpenReview" class="title-link" href="/venue/eI8KegpPyX@OpenReview" target="_blank">Position: The Categorization of Race in ML is a Flawed Premise</a>
                <a id="pdf-eI8KegpPyX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eI8KegpPyX@OpenReview', this)" data="https://openreview.net/pdf?id=eI8KegpPyX">[PDF<sup id="pdf-stars-eI8KegpPyX@OpenReview">1</sup>]</a>
                <a id="copy-eI8KegpPyX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eI8KegpPyX@OpenReview')">[Copy]</a>
                <a id="kimi-eI8KegpPyX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eI8KegpPyX@OpenReview', this)">[Kimi<sup id="kimi-stars-eI8KegpPyX@OpenReview">1</sup>]</a>
                <a id="rel-eI8KegpPyX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eI8KegpPyX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eI8KegpPyX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Miriam Doh" target="_blank">Miriam Doh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benedikt Höltgen" target="_blank">Benedikt Höltgen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Piera Riccio" target="_blank">Piera Riccio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nuria Oliver" target="_blank">Nuria Oliver</a>
            </p>
            <p id="summary-eI8KegpPyX@OpenReview" class="summary">This position paper critiques the reliance on rigid racial taxonomies in machine learning, exposing their U.S.-centric nature and lack of global applicability—particularly in Europe, where race categories are not commonly used. These classifications oversimplify racial identity, erasing the experiences of mixed-race individuals and reinforcing outdated essentialist views that contradict the social construction of race. We suggest research agendas in machine learning that move beyond categorical variables to better address discrimination and social inequality.</p>
            <p id="subjects-eI8KegpPyX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-eI8KegpPyX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eI8KegpPyX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eI8KegpPyX@OpenReview" onclick="foldPdfKimi('eI8KegpPyX@OpenReview', this)" class="hr hr-fold">
        </div><div id="eax2ixyeQL@OpenReview" class="panel paper" keywords="algorithmic,algorithms,algeval,llms,understanding,position,actually,solve,emergent,hypotheses">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eax2ixyeQL" target="_blank" title="205/225"><span class="index notranslate">#205</span></a>
                <a id="title-eax2ixyeQL@OpenReview" class="title-link" href="/venue/eax2ixyeQL@OpenReview" target="_blank">Position: We Need An Algorithmic Understanding of Generative AI</a>
                <a id="pdf-eax2ixyeQL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eax2ixyeQL@OpenReview', this)" data="https://openreview.net/pdf?id=eax2ixyeQL">[PDF<sup id="pdf-stars-eax2ixyeQL@OpenReview">1</sup>]</a>
                <a id="copy-eax2ixyeQL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eax2ixyeQL@OpenReview')">[Copy]</a>
                <a id="kimi-eax2ixyeQL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eax2ixyeQL@OpenReview', this)">[Kimi<sup id="kimi-stars-eax2ixyeQL@OpenReview">2</sup>]</a>
                <a id="rel-eax2ixyeQL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eax2ixyeQL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eax2ixyeQL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Eberle" target="_blank">Oliver Eberle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas McGee" target="_blank">Thomas McGee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hamza Giaffar" target="_blank">Hamza Giaffar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taylor Webb" target="_blank">Taylor Webb</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ida Momennejad" target="_blank">Ida Momennejad</a>
            </p>
            <p id="summary-eax2ixyeQL@OpenReview" class="summary">What algorithms do LLMs actually learn and use to solve problems? Studies addressing this question are sparse, as research priorities are focused on improving performance through scale, leaving a theoretical and empirical gap in understanding emergent algorithms. This position paper proposes AlgEval: a framework for systematic research into the algorithms that LLMs learn and use. AlgEval aims to uncover algorithmic primitives, reflected in latent representations, attention, and inference-time compute, and their algorithmic composition to solve task-specific problems. We highlight potential methodological paths and a case study toward this goal, focusing on emergent search algorithms. Our case study illustrates both the formation of top-down hypotheses about candidate algorithms, and bottom-up tests of these hypotheses via circuit-level analysis of attention patterns and hidden states. The rigorous, systematic evaluation of how LLMs actually solve tasks provides an alternative to resource-intensive scaling, reorienting the field toward a principled understanding of underlying computations. Such algorithmic explanations offer a pathway to human-understandable interpretability, enabling comprehension of the model's internal reasoning performance measures. This can in turn lead to more sample-efficient methods for training and improving performance, as well as novel architectures for end-to-end and multi-agent systems.</p>
            <p id="subjects-eax2ixyeQL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-eax2ixyeQL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eax2ixyeQL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eax2ixyeQL@OpenReview" onclick="foldPdfKimi('eax2ixyeQL@OpenReview', this)" class="hr hr-fold">
        </div><div id="gwhPvu97Gm@OpenReview" class="panel paper" keywords="checklist,baselines,human,reporting,recommendations,baselining,evaluations,kevinlwei,rigorous,policymakers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gwhPvu97Gm" target="_blank" title="206/225"><span class="index notranslate">#206</span></a>
                <a id="title-gwhPvu97Gm@OpenReview" class="title-link" href="/venue/gwhPvu97Gm@OpenReview" target="_blank">Position: Human Baselines in Model Evaluations Need Rigor and Transparency (With Recommendations &amp; Reporting Checklist)</a>
                <a id="pdf-gwhPvu97Gm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gwhPvu97Gm@OpenReview', this)" data="https://openreview.net/pdf?id=gwhPvu97Gm">[PDF<sup id="pdf-stars-gwhPvu97Gm@OpenReview">1</sup>]</a>
                <a id="copy-gwhPvu97Gm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gwhPvu97Gm@OpenReview')">[Copy]</a>
                <a id="kimi-gwhPvu97Gm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gwhPvu97Gm@OpenReview', this)">[Kimi<sup id="kimi-stars-gwhPvu97Gm@OpenReview">1</sup>]</a>
                <a id="rel-gwhPvu97Gm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gwhPvu97Gm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gwhPvu97Gm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Wei" target="_blank">Kevin Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Patricia Paskov" target="_blank">Patricia Paskov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sunishchal Dev" target="_blank">Sunishchal Dev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Byun" target="_blank">Michael Byun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anka Reuel" target="_blank">Anka Reuel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xavier Roberts-Gaal" target="_blank">Xavier Roberts-Gaal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rachel Calcott" target="_blank">Rachel Calcott</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Evie Coxon" target="_blank">Evie Coxon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chinmay Deshpande" target="_blank">Chinmay Deshpande</a>
            </p>
            <p id="summary-gwhPvu97Gm@OpenReview" class="summary">**In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end.** Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: [https://github.com/kevinlwei/human-baselines](https://github.com/kevinlwei/human-baselines).</p>
            <p id="subjects-gwhPvu97Gm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-gwhPvu97Gm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gwhPvu97Gm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gwhPvu97Gm@OpenReview" onclick="foldPdfKimi('gwhPvu97Gm@OpenReview', this)" class="hr hr-fold">
        </div><div id="mzc1KPkIMJ@OpenReview" class="panel paper" keywords="neuroalgebraic,algebraic,geometry,invitation,position,unveils,varieties,learning,algebro,spaces">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mzc1KPkIMJ" target="_blank" title="207/225"><span class="index notranslate">#207</span></a>
                <a id="title-mzc1KPkIMJ@OpenReview" class="title-link" href="/venue/mzc1KPkIMJ@OpenReview" target="_blank">Position: Algebra Unveils Deep Learning - An Invitation to Neuroalgebraic Geometry</a>
                <a id="pdf-mzc1KPkIMJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mzc1KPkIMJ@OpenReview', this)" data="https://openreview.net/pdf?id=mzc1KPkIMJ">[PDF<sup id="pdf-stars-mzc1KPkIMJ@OpenReview"></sup>]</a>
                <a id="copy-mzc1KPkIMJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mzc1KPkIMJ@OpenReview')">[Copy]</a>
                <a id="kimi-mzc1KPkIMJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mzc1KPkIMJ@OpenReview', this)">[Kimi<sup id="kimi-stars-mzc1KPkIMJ@OpenReview"></sup>]</a>
                <a id="rel-mzc1KPkIMJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mzc1KPkIMJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mzc1KPkIMJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Giovanni Luca Marchetti" target="_blank">Giovanni Luca Marchetti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vahid Shahverdi" target="_blank">Vahid Shahverdi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Mereta" target="_blank">Stefano Mereta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Trager" target="_blank">Matthew Trager</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kathlén Kohn" target="_blank">Kathlén Kohn</a>
            </p>
            <p id="summary-mzc1KPkIMJ@OpenReview" class="summary">In this position paper, we promote the study of function spaces parameterized by machine learning models through the lens of algebraic geometry. To this end, we focus on algebraic models, such as neural networks with polynomial activations, whose associated function spaces are semi-algebraic varieties. We outline a dictionary between algebro-geometric invariants of these varieties, such as dimension, degree, and singularities, and fundamental aspects of machine learning, such as sample complexity, expressivity, training dynamics, and implicit bias. Along the way, we review the literature and discuss ideas beyond the algebraic domain. This work lays the foundations of a research direction bridging algebraic geometry and deep learning, that we refer to as neuroalgebraic geometry.</p>
            <p id="subjects-mzc1KPkIMJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mzc1KPkIMJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mzc1KPkIMJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mzc1KPkIMJ@OpenReview" onclick="foldPdfKimi('mzc1KPkIMJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="tctWi7I5wd@OpenReview" class="panel paper" keywords="bias,llm,probes,social,sciences,probing,lessons,conflicting,rethinking,appropriate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tctWi7I5wd" target="_blank" title="208/225"><span class="index notranslate">#208</span></a>
                <a id="title-tctWi7I5wd@OpenReview" class="title-link" href="/venue/tctWi7I5wd@OpenReview" target="_blank">Position: Rethinking LLM Bias Probing Using Lessons from the Social Sciences</a>
                <a id="pdf-tctWi7I5wd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tctWi7I5wd@OpenReview', this)" data="https://openreview.net/pdf?id=tctWi7I5wd">[PDF<sup id="pdf-stars-tctWi7I5wd@OpenReview">1</sup>]</a>
                <a id="copy-tctWi7I5wd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tctWi7I5wd@OpenReview')">[Copy]</a>
                <a id="kimi-tctWi7I5wd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tctWi7I5wd@OpenReview', this)">[Kimi<sup id="kimi-stars-tctWi7I5wd@OpenReview">1</sup>]</a>
                <a id="rel-tctWi7I5wd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tctWi7I5wd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tctWi7I5wd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kirsten Morehouse" target="_blank">Kirsten Morehouse</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddharth Swaroop" target="_blank">Siddharth Swaroop</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiwei Pan" target="_blank">Weiwei Pan</a>
            </p>
            <p id="summary-tctWi7I5wd@OpenReview" class="summary">The proliferation of LLM bias probes introduces three challenges: we lack (1) principled criteria for selecting appropriate probes, (2) a system for reconciling conflicting results across probes, and (3) formal frameworks for reasoning about when and why experimental findings will generalize to real user behavior. In response, we propose a systematic approach to LLM social bias probing, drawing on insights from the social sciences. Central to this approach is EcoLevels—a novel framework that helps (a) identify appropriate bias probes (b) reconcile conflicting results, and (c) generate predictions about bias generalization. We ground our framework in the social sciences, as many LLM probes are adapted from human studies, and these fields have faced similar challenges when studying bias in humans. Finally, we outline five lessons that demonstrate how LLM bias probing can (and should) benefit from decades of social science research</p>
            <p id="subjects-tctWi7I5wd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-tctWi7I5wd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tctWi7I5wd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tctWi7I5wd@OpenReview" onclick="foldPdfKimi('tctWi7I5wd@OpenReview', this)" class="hr hr-fold">
        </div><div id="hfLqdquVt3@OpenReview" class="panel paper" keywords="mil,pretraining,transfer,pretrained,cancer,pathology,pan,tissue,models,instance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hfLqdquVt3" target="_blank" title="209/225"><span class="index notranslate">#209</span></a>
                <a id="title-hfLqdquVt3@OpenReview" class="title-link" href="/venue/hfLqdquVt3@OpenReview" target="_blank">Do Multiple Instance Learning Models Transfer?</a>
                <a id="pdf-hfLqdquVt3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hfLqdquVt3@OpenReview', this)" data="https://openreview.net/pdf?id=hfLqdquVt3">[PDF<sup id="pdf-stars-hfLqdquVt3@OpenReview">1</sup>]</a>
                <a id="copy-hfLqdquVt3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hfLqdquVt3@OpenReview')">[Copy]</a>
                <a id="kimi-hfLqdquVt3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hfLqdquVt3@OpenReview', this)">[Kimi<sup id="kimi-stars-hfLqdquVt3@OpenReview"></sup>]</a>
                <a id="rel-hfLqdquVt3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hfLqdquVt3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hfLqdquVt3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Shao" target="_blank">Daniel Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Chen" target="_blank">Richard Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Song" target="_blank">Andrew Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joel Runevic" target="_blank">Joel Runevic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Y. Lu" target="_blank">Ming Y. Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Ding" target="_blank">Tong Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Faisal Mahmood" target="_blank">Faisal Mahmood</a>
            </p>
            <p id="summary-hfLqdquVt3@OpenReview" class="summary">Multiple Instance Learning (MIL) is a cornerstone approach in computational pathology for distilling embeddings from gigapixel tissue images into patient-level representations to predict clinical outcomes. However, MIL is frequently challenged by the constraints of working with small, weakly-supervised clinical datasets. Unlike fields such as natural language processing and computer vision, which effectively use transfer learning to improve model quality in data-scarce environments, the transferability of MIL models remains largely unexplored. We conduct the first comprehensive investigation into transfer learning capabilities of pretrained MIL models, evaluating 11 MIL models across 19 pretraining tasks spanning tissue subtyping, cancer grading, and molecular subtype prediction. We observe a substantial performance boost with finetuning pretrained models over training from randomly initialized weights, even with domain differences between pretraining and target tasks. Pretraining on pan-cancer datasets enables consistent generalization across organs and task types compared to single-disease pretraining. Remarkably, this pan-cancer pretraining leads to better transfer than that of a state-of-the-art slide-level foundation model, while using only 6.5\% of the training data. These findings indicate that MIL architectures exhibit robust adaptability, offering insights into the benefits of leveraging pretrained models to enhance performance in computational pathology.</p>
            <p id="subjects-hfLqdquVt3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hfLqdquVt3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hfLqdquVt3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hfLqdquVt3@OpenReview" onclick="foldPdfKimi('hfLqdquVt3@OpenReview', this)" class="hr hr-fold">
        </div><div id="92oBV5HAGl@OpenReview" class="panel paper" keywords="unlearning,mechanistic,editing,edits,knowledge,components,robust,edit,counterfact,relearn">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=92oBV5HAGl" target="_blank" title="210/225"><span class="index notranslate">#210</span></a>
                <a id="title-92oBV5HAGl@OpenReview" class="title-link" href="/venue/92oBV5HAGl@OpenReview" target="_blank">Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization</a>
                <a id="pdf-92oBV5HAGl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('92oBV5HAGl@OpenReview', this)" data="https://openreview.net/pdf?id=92oBV5HAGl">[PDF<sup id="pdf-stars-92oBV5HAGl@OpenReview">3</sup>]</a>
                <a id="copy-92oBV5HAGl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('92oBV5HAGl@OpenReview')">[Copy]</a>
                <a id="kimi-92oBV5HAGl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('92oBV5HAGl@OpenReview', this)">[Kimi<sup id="kimi-stars-92oBV5HAGl@OpenReview">2</sup>]</a>
                <a id="rel-92oBV5HAGl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('92oBV5HAGl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-92oBV5HAGl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Phillip Guo" target="_blank">Phillip Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaquib Syed" target="_blank">Aaquib Syed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhay Sheshadri" target="_blank">Abhay Sheshadri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aidan Ewart" target="_blank">Aidan Ewart</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gintare Karolina Dziugaite" target="_blank">Gintare Karolina Dziugaite</a>
            </p>
            <p id="summary-92oBV5HAGl@OpenReview" class="summary">Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability---which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability---can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the *lookup-table mechanism* for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models.We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.</p>
            <p id="subjects-92oBV5HAGl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-92oBV5HAGl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-92oBV5HAGl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-92oBV5HAGl@OpenReview" onclick="foldPdfKimi('92oBV5HAGl@OpenReview', this)" class="hr hr-fold">
        </div><div id="XrCbBdycDc@OpenReview" class="panel paper" keywords="diffusion,mctd,mcts,tree,monte,carlo,reconceptualizes,planning,denoised,search">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XrCbBdycDc" target="_blank" title="211/225"><span class="index notranslate">#211</span></a>
                <a id="title-XrCbBdycDc@OpenReview" class="title-link" href="/venue/XrCbBdycDc@OpenReview" target="_blank">Monte Carlo Tree Diffusion for System 2 Planning</a>
                <a id="pdf-XrCbBdycDc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XrCbBdycDc@OpenReview', this)" data="https://openreview.net/pdf?id=XrCbBdycDc">[PDF<sup id="pdf-stars-XrCbBdycDc@OpenReview">3</sup>]</a>
                <a id="copy-XrCbBdycDc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XrCbBdycDc@OpenReview')">[Copy]</a>
                <a id="kimi-XrCbBdycDc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XrCbBdycDc@OpenReview', this)">[Kimi<sup id="kimi-stars-XrCbBdycDc@OpenReview">1</sup>]</a>
                <a id="rel-XrCbBdycDc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XrCbBdycDc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XrCbBdycDc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jaesik Yoon" target="_blank">Jaesik Yoon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyeonseo Cho" target="_blank">Hyeonseo Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Doojin Baek" target="_blank">Doojin Baek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yoshua Bengio" target="_blank">Yoshua Bengio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sungjin Ahn" target="_blank">Sungjin Ahn</a>
            </p>
            <p id="summary-XrCbBdycDc@OpenReview" class="summary">Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)—whose performance naturally improves with inference-time computation scaling—standard diffusion‐based planners offer only limited avenues for the scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree‐structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long‐horizon tasks show that MCTD outperforms diffusion baselines, yielding higher‐quality solutions as inference-time computation increases.</p>
            <p id="subjects-XrCbBdycDc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-XrCbBdycDc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XrCbBdycDc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XrCbBdycDc@OpenReview" onclick="foldPdfKimi('XrCbBdycDc@OpenReview', this)" class="hr hr-fold">
        </div><div id="qLfo1sef50@OpenReview" class="panel paper" keywords="rlhf,preference,ppl,policy,human,learning,labeled,reward,preferences,regret">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qLfo1sef50" target="_blank" title="212/225"><span class="index notranslate">#212</span></a>
                <a id="title-qLfo1sef50@OpenReview" class="title-link" href="/venue/qLfo1sef50@OpenReview" target="_blank">Policy-labeled Preference Learning: Is Preference Enough for RLHF?</a>
                <a id="pdf-qLfo1sef50@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qLfo1sef50@OpenReview', this)" data="https://openreview.net/pdf?id=qLfo1sef50">[PDF<sup id="pdf-stars-qLfo1sef50@OpenReview">3</sup>]</a>
                <a id="copy-qLfo1sef50@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qLfo1sef50@OpenReview')">[Copy]</a>
                <a id="kimi-qLfo1sef50@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qLfo1sef50@OpenReview', this)">[Kimi<sup id="kimi-stars-qLfo1sef50@OpenReview">4</sup>]</a>
                <a id="rel-qLfo1sef50@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qLfo1sef50@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qLfo1sef50@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Taehyun Cho" target="_blank">Taehyun Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seokhun Ju" target="_blank">Seokhun Ju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungyub Han" target="_blank">Seungyub Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dohyeong Kim" target="_blank">Dohyeong Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyungjae Lee" target="_blank">Kyungjae Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jungwoo Lee" target="_blank">Jungwoo Lee</a>
            </p>
            <p id="summary-qLfo1sef50@OpenReview" class="summary">To design reward that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing models using reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. To address this, we propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Additionally, we introduce a contrastive KL regularization term derived from regret-based principles to enhance sequential contrastive learning. Experiments in high-dimensional continuous control environments demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings.</p>
            <p id="subjects-qLfo1sef50@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-qLfo1sef50@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qLfo1sef50@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qLfo1sef50@OpenReview" onclick="foldPdfKimi('qLfo1sef50@OpenReview', this)" class="hr hr-fold">
        </div><div id="IfWKVF6LfY@OpenReview" class="panel paper" keywords="rto,ppo,token,texttt,dpo,rlhf,optimization,reinforced,wise,zkshan2002">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=IfWKVF6LfY" target="_blank" title="213/225"><span class="index notranslate">#213</span></a>
                <a id="title-IfWKVF6LfY@OpenReview" class="title-link" href="/venue/IfWKVF6LfY@OpenReview" target="_blank">DPO Meets PPO: Reinforced Token Optimization for RLHF</a>
                <a id="pdf-IfWKVF6LfY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('IfWKVF6LfY@OpenReview', this)" data="https://openreview.net/pdf?id=IfWKVF6LfY">[PDF<sup id="pdf-stars-IfWKVF6LfY@OpenReview">8</sup>]</a>
                <a id="copy-IfWKVF6LfY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('IfWKVF6LfY@OpenReview')">[Copy]</a>
                <a id="kimi-IfWKVF6LfY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('IfWKVF6LfY@OpenReview', this)">[Kimi<sup id="kimi-stars-IfWKVF6LfY@OpenReview">5</sup>]</a>
                <a id="rel-IfWKVF6LfY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('IfWKVF6LfY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-IfWKVF6LfY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Han Zhong" target="_blank">Han Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zikang Shan" target="_blank">Zikang Shan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guhao Feng" target="_blank">Guhao Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Xiong" target="_blank">Wei Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinle Cheng" target="_blank">Xinle Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Zhao" target="_blank">Li Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di He" target="_blank">Di He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiang Bian" target="_blank">Jiang Bian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Wang" target="_blank">Liwei Wang</a>
            </p>
            <p id="summary-IfWKVF6LfY@OpenReview" class="summary">In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards---a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. We conduct extensive experiments to evaluate \texttt{RTO} against PPO and other direct preference learning algorithms. The results highlight the effectiveness of RTO, with the algorithm outperforming PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code and models are available at \href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.</p>
            <p id="subjects-IfWKVF6LfY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-IfWKVF6LfY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-IfWKVF6LfY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-IfWKVF6LfY@OpenReview" onclick="foldPdfKimi('IfWKVF6LfY@OpenReview', this)" class="hr hr-fold">
        </div><div id="4HQaMUYWAT@OpenReview" class="panel paper" keywords="initialization,reasoning,tasks,language,bias,training,llms,models,revolutionized,scales">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4HQaMUYWAT" target="_blank" title="214/225"><span class="index notranslate">#214</span></a>
                <a id="title-4HQaMUYWAT@OpenReview" class="title-link" href="/venue/4HQaMUYWAT@OpenReview" target="_blank">An Analysis for Reasoning Bias of Language Models with Small Initialization</a>
                <a id="pdf-4HQaMUYWAT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4HQaMUYWAT@OpenReview', this)" data="https://openreview.net/pdf?id=4HQaMUYWAT">[PDF<sup id="pdf-stars-4HQaMUYWAT@OpenReview">2</sup>]</a>
                <a id="copy-4HQaMUYWAT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4HQaMUYWAT@OpenReview')">[Copy]</a>
                <a id="kimi-4HQaMUYWAT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4HQaMUYWAT@OpenReview', this)">[Kimi<sup id="kimi-stars-4HQaMUYWAT@OpenReview">1</sup>]</a>
                <a id="rel-4HQaMUYWAT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4HQaMUYWAT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4HQaMUYWAT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Yao" target="_blank">Junjie Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=zhongwang zhang" target="_blank">zhongwang zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhi-Qin John Xu" target="_blank">Zhi-Qin John Xu</a>
            </p>
            <p id="summary-4HQaMUYWAT@OpenReview" class="summary">Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.</p>
            <p id="subjects-4HQaMUYWAT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-4HQaMUYWAT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4HQaMUYWAT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4HQaMUYWAT@OpenReview" onclick="foldPdfKimi('4HQaMUYWAT@OpenReview', this)" class="hr hr-fold">
        </div><div id="5QAKPBVdFH@OpenReview" class="panel paper" keywords="sharpness,symmetries,transformers,riemannian,transformer,hide,obscure,generalization,correlation,quotient">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5QAKPBVdFH" target="_blank" title="215/225"><span class="index notranslate">#215</span></a>
                <a id="title-5QAKPBVdFH@OpenReview" class="title-link" href="/venue/5QAKPBVdFH@OpenReview" target="_blank">Hide &amp; Seek: Transformer Symmetries Obscure Sharpness &amp; Riemannian Geometry Finds It</a>
                <a id="pdf-5QAKPBVdFH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5QAKPBVdFH@OpenReview', this)" data="https://openreview.net/pdf?id=5QAKPBVdFH">[PDF<sup id="pdf-stars-5QAKPBVdFH@OpenReview"></sup>]</a>
                <a id="copy-5QAKPBVdFH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5QAKPBVdFH@OpenReview')">[Copy]</a>
                <a id="kimi-5QAKPBVdFH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5QAKPBVdFH@OpenReview', this)">[Kimi<sup id="kimi-stars-5QAKPBVdFH@OpenReview">2</sup>]</a>
                <a id="rel-5QAKPBVdFH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5QAKPBVdFH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5QAKPBVdFH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marvin F, da Silva" target="_blank">Marvin F, da Silva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Dangel" target="_blank">Felix Dangel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sageev Oore" target="_blank">Sageev Oore</a>
            </p>
            <p id="summary-5QAKPBVdFH@OpenReview" class="summary">The concept of sharpness has been successfully applied to traditional architectures like MLPs and CNNs to predict their generalization. For transformers, however, recent work reported weak correlation between flatness and generalization. We argue that existing sharpness measures fail for transformers, because they have much richer symmetries in their attention mechanism that induce directions in parameter space along which the network or its loss remain identical. We posit that sharpness must account fully for these symmetries, and thus we redefine it on a quotient manifold that results from quotienting out the transformer symmetries, thereby removing their ambiguities. Leveraging tools from Riemannian geometry, we propose a fully general notion of sharpness, in terms of a geodesic ball on the symmetry-corrected quotient manifold. In practice, we need to resort to approximating the geodesics. Doing so up to first order yields existing adaptive sharpness measures, and we demonstrate that including higher-order terms is crucial to recover correlation with generalization. We present results on diagonal networks with synthetic data, and show that our geodesic sharpness reveals strong correlation for real-world transformers on both text and image classification tasks.</p>
            <p id="subjects-5QAKPBVdFH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-5QAKPBVdFH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5QAKPBVdFH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5QAKPBVdFH@OpenReview" onclick="foldPdfKimi('5QAKPBVdFH@OpenReview', this)" class="hr hr-fold">
        </div><div id="GbJqQsIwJu@OpenReview" class="panel paper" keywords="preferences,distributions,samples,parametric,preference,estimators,underscored,feedback,unknown,deterministic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GbJqQsIwJu" target="_blank" title="216/225"><span class="index notranslate">#216</span></a>
                <a id="title-GbJqQsIwJu@OpenReview" class="title-link" href="/venue/GbJqQsIwJu@OpenReview" target="_blank">Learning Parametric Distributions from Samples and Preferences</a>
                <a id="pdf-GbJqQsIwJu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GbJqQsIwJu@OpenReview', this)" data="https://openreview.net/pdf?id=GbJqQsIwJu">[PDF<sup id="pdf-stars-GbJqQsIwJu@OpenReview">1</sup>]</a>
                <a id="copy-GbJqQsIwJu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GbJqQsIwJu@OpenReview')">[Copy]</a>
                <a id="kimi-GbJqQsIwJu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GbJqQsIwJu@OpenReview', this)">[Kimi<sup id="kimi-stars-GbJqQsIwJu@OpenReview">1</sup>]</a>
                <a id="rel-GbJqQsIwJu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GbJqQsIwJu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GbJqQsIwJu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Jourdan" target="_blank">Marc Jourdan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gizem Yüce" target="_blank">Gizem Yüce</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Flammarion" target="_blank">Nicolas Flammarion</a>
            </p>
            <p id="summary-GbJqQsIwJu@OpenReview" class="summary">Recent advances in language modeling have underscored the role of preference feedback in enhancing model performance. This paper investigates the conditions under which preference feedback improves parameter estimation in classes of continuous parametric distributions. In our framework, the learner observes pairs of samples from an unknown distribution along with their relative preferences depending on the same unknown parameter. We show that preferences-based M-estimators achieve a better asymptotic variance than sample-only M-estimators, further improved by deterministic preferences. Leveraging the hard constraints revealed by deterministic preferences, we propose an estimator achieving an estimation error scaling of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-156-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1112" style="width: 3.857em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.08em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1113"><span class="texatom" id="MathJax-Span-1114"><span class="mrow" id="MathJax-Span-1115"><span class="mi" id="MathJax-Span-1116" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-1117" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-1118" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-1119"><span class="mrow" id="MathJax-Span-1120"><span class="mo" id="MathJax-Span-1121" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-1122" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-1123" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-156">\mathcal{O}(1/n)</script>---a significant improvement over the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-157-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0398;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1124" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.91em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1125"><span class="mi" id="MathJax-Span-1126" style="font-family: MathJax_Main;">Θ</span><span class="mo" id="MathJax-Span-1127" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-1128" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-1129"><span class="mrow" id="MathJax-Span-1130"><span class="mo" id="MathJax-Span-1131" style="font-family: MathJax_Main;">/</span></span></span><span class="msqrt" id="MathJax-Span-1132"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-1133"><span class="mi" id="MathJax-Span-1134" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.63em, 3.961em, -999.997em); top: -4.424em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.049em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1135" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-157">\Theta(1/\sqrt{n})</script> rate attainable with samples alone. Next, we establish a lower bound that matches this accelerated rate; up to problem-dependent constants. While the assumptions underpinning our analysis are restrictive, they are satisfied by notable cases such as Gaussian or Laplace distributions for preferences based on the log-probability reward.</p>
            <p id="subjects-GbJqQsIwJu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-GbJqQsIwJu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GbJqQsIwJu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GbJqQsIwJu@OpenReview" onclick="foldPdfKimi('GbJqQsIwJu@OpenReview', this)" class="hr hr-fold">
        </div><div id="Hrp6jRIKdX@OpenReview" class="panel paper" keywords="denoisers,diffusion,network,mechanistic,denoising,generalization,behaviour,comparing,operations,training">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Hrp6jRIKdX" target="_blank" title="217/225"><span class="index notranslate">#217</span></a>
                <a id="title-Hrp6jRIKdX@OpenReview" class="title-link" href="/venue/Hrp6jRIKdX@OpenReview" target="_blank">Towards a Mechanistic Explanation of Diffusion Model Generalization</a>
                <a id="pdf-Hrp6jRIKdX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Hrp6jRIKdX@OpenReview', this)" data="https://openreview.net/pdf?id=Hrp6jRIKdX">[PDF<sup id="pdf-stars-Hrp6jRIKdX@OpenReview">1</sup>]</a>
                <a id="copy-Hrp6jRIKdX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Hrp6jRIKdX@OpenReview')">[Copy]</a>
                <a id="kimi-Hrp6jRIKdX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Hrp6jRIKdX@OpenReview', this)">[Kimi<sup id="kimi-stars-Hrp6jRIKdX@OpenReview">1</sup>]</a>
                <a id="rel-Hrp6jRIKdX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Hrp6jRIKdX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Hrp6jRIKdX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Niedoba" target="_blank">Matthew Niedoba</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Berend Zwartsenberg" target="_blank">Berend Zwartsenberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Murphy" target="_blank">Kevin Murphy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Frank Wood" target="_blank">Frank Wood</a>
            </p>
            <p id="summary-Hrp6jRIKdX@OpenReview" class="summary">We propose a simple, training-free mechanism which explains the generalization behaviour of diffusion models. By comparing pre-trained diffusion models to their theoretically optimal empirical counterparts, we identify a shared local inductive bias across a variety of network architectures. From this observation, we hypothesize that network denoisers generalize through localized denoising operations, as these operations approximate the training objective well over much of the training distribution. To validate our hypothesis, we introduce novel denoising algorithms which aggregate local empirical denoisers to replicate network behaviour. Comparing these algorithms to network denoisers across forward and reverse diffusion processes, our approach exhibits consistent visual similarity to neural network outputs, with lower mean squared error than previously proposed methods.</p>
            <p id="subjects-Hrp6jRIKdX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Hrp6jRIKdX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hrp6jRIKdX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hrp6jRIKdX@OpenReview" onclick="foldPdfKimi('Hrp6jRIKdX@OpenReview', this)" class="hr hr-fold">
        </div><div id="Ukjl86EsIk@OpenReview" class="panel paper" keywords="averse,decision,risk,makers,prediction,rac,uncertainty,optimal,sets,quantification">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Ukjl86EsIk" target="_blank" title="218/225"><span class="index notranslate">#218</span></a>
                <a id="title-Ukjl86EsIk@OpenReview" class="title-link" href="/venue/Ukjl86EsIk@OpenReview" target="_blank">Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents</a>
                <a id="pdf-Ukjl86EsIk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Ukjl86EsIk@OpenReview', this)" data="https://openreview.net/pdf?id=Ukjl86EsIk">[PDF<sup id="pdf-stars-Ukjl86EsIk@OpenReview">1</sup>]</a>
                <a id="copy-Ukjl86EsIk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Ukjl86EsIk@OpenReview')">[Copy]</a>
                <a id="kimi-Ukjl86EsIk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Ukjl86EsIk@OpenReview', this)">[Kimi<sup id="kimi-stars-Ukjl86EsIk@OpenReview">3</sup>]</a>
                <a id="rel-Ukjl86EsIk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Ukjl86EsIk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Ukjl86EsIk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shayan Kiyani" target="_blank">Shayan Kiyani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=George Pappas" target="_blank">George Pappas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Roth" target="_blank">Aaron Roth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hamed Hassani" target="_blank">Hamed Hassani</a>
            </p>
            <p id="summary-Ukjl86EsIk@OpenReview" class="summary">A fundamental question in data-driven decision making is how to quantify the uncertainty of predictions to inform risk-sensitive downstream actions, as often required in domains such as medicine. We develop a decision-theoretic foundation linking prediction sets to risk-averse decision-making, addressing three questions: (1) What is the correct notion of uncertainty quantification for risk-averse decision makers? We prove that prediction sets are optimal for decision makers who wish to optimize their value at risk. (2) What is the optimal policy that a risk averse decision maker should use to map prediction sets to actions? We show that a simple max-min decision policy is optimal for risk-averse decision makers. Finally, (3) How can we derive prediction sets that are optimal for such decision makers? We provide an exact characterization in the population regime and a distribution free finite-sample construction. These insights leads to *Risk-Averse Calibration (RAC)*, a principled algorithm that is both *practical*—exploiting black-box predictions to enhance downstream utility—and *safe*—adhering to user-defined risk thresholds. We experimentally demonstrate RAC's advantages in medical diagnosis and recommendation systems, showing that it substantially improves the trade-off between safety and utility, delivering higher utility than existing methods while avoiding critical errors.</p>
            <p id="subjects-Ukjl86EsIk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Ukjl86EsIk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ukjl86EsIk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ukjl86EsIk@OpenReview" onclick="foldPdfKimi('Ukjl86EsIk@OpenReview', this)" class="hr hr-fold">
        </div><div id="Z1qZoHa6ql@OpenReview" class="panel paper" keywords="counterfactual,constraints,graphical,causal,calculus,independences,ctf,inference,quantities,diagram">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Z1qZoHa6ql" target="_blank" title="219/225"><span class="index notranslate">#219</span></a>
                <a id="title-Z1qZoHa6ql@OpenReview" class="title-link" href="/venue/Z1qZoHa6ql@OpenReview" target="_blank">Counterfactual Graphical Models: Constraints and Inference</a>
                <a id="pdf-Z1qZoHa6ql@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Z1qZoHa6ql@OpenReview', this)" data="https://openreview.net/pdf?id=Z1qZoHa6ql">[PDF<sup id="pdf-stars-Z1qZoHa6ql@OpenReview">3</sup>]</a>
                <a id="copy-Z1qZoHa6ql@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Z1qZoHa6ql@OpenReview')">[Copy]</a>
                <a id="kimi-Z1qZoHa6ql@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Z1qZoHa6ql@OpenReview', this)">[Kimi<sup id="kimi-stars-Z1qZoHa6ql@OpenReview">2</sup>]</a>
                <a id="rel-Z1qZoHa6ql@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Z1qZoHa6ql@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Z1qZoHa6ql@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Juan Correa" target="_blank">Juan Correa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elias Bareinboim" target="_blank">Elias Bareinboim</a>
            </p>
            <p id="summary-Z1qZoHa6ql@OpenReview" class="summary">Graphical models have been widely used as parsimonious encoders of constraints of the underlying probability models. When organized in a structured way, these models can facilitate the derivation of non-trivial constraints, the inference of quantities of interest, and the optimization of their estimands. In particular, causal diagrams allow for the efficient representation of structural constraints of the underlying causal system. In this paper, we introduce an efficient graphical construction called Ancestral Multi-world Networks that is sound and complete for reading counterfactual independences from a causal diagram using d-separation. Moreover, we introduce the counterfactual (ctf-) calculus, which can be used to transform counterfactual quantities using three rules licensed by the constraints encoded in the diagram. This result generalizes Pearl’s celebrated do-calculus from interventional to counterfactual reasoning.</p>
            <p id="subjects-Z1qZoHa6ql@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-Z1qZoHa6ql@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Z1qZoHa6ql@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Z1qZoHa6ql@OpenReview" onclick="foldPdfKimi('Z1qZoHa6ql@OpenReview', this)" class="hr hr-fold">
        </div><div id="c16m2kUTLZ@OpenReview" class="panel paper" keywords="soundness,verifiers,floating,verification,deployed,bounding,deployment,networks,goal,precision">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=c16m2kUTLZ" target="_blank" title="220/225"><span class="index notranslate">#220</span></a>
                <a id="title-c16m2kUTLZ@OpenReview" class="title-link" href="/venue/c16m2kUTLZ@OpenReview" target="_blank">No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks</a>
                <a id="pdf-c16m2kUTLZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('c16m2kUTLZ@OpenReview', this)" data="https://openreview.net/pdf?id=c16m2kUTLZ">[PDF<sup id="pdf-stars-c16m2kUTLZ@OpenReview">1</sup>]</a>
                <a id="copy-c16m2kUTLZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('c16m2kUTLZ@OpenReview')">[Copy]</a>
                <a id="kimi-c16m2kUTLZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('c16m2kUTLZ@OpenReview', this)">[Kimi<sup id="kimi-stars-c16m2kUTLZ@OpenReview">1</sup>]</a>
                <a id="rel-c16m2kUTLZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('c16m2kUTLZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-c16m2kUTLZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Attila Szász" target="_blank">Attila Szász</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Balázs Bánhelyi" target="_blank">Balázs Bánhelyi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Jelasity" target="_blank">Mark Jelasity</a>
            </p>
            <p id="summary-c16m2kUTLZ@OpenReview" class="summary">The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound.</p>
            <p id="subjects-c16m2kUTLZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-c16m2kUTLZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-c16m2kUTLZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-c16m2kUTLZ@OpenReview" onclick="foldPdfKimi('c16m2kUTLZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="d60cmFf89H@OpenReview" class="panel paper" keywords="tabflex,tabpfn,tabular,datasets,attention,millions,speedup,scaling,diverse,poker">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=d60cmFf89H" target="_blank" title="221/225"><span class="index notranslate">#221</span></a>
                <a id="title-d60cmFf89H@OpenReview" class="title-link" href="/venue/d60cmFf89H@OpenReview" target="_blank">TabFlex: Scaling Tabular Learning to Millions with Linear Attention</a>
                <a id="pdf-d60cmFf89H@OpenReview" class="title-pdf notranslate" onclick="togglePdf('d60cmFf89H@OpenReview', this)" data="https://openreview.net/pdf?id=d60cmFf89H">[PDF<sup id="pdf-stars-d60cmFf89H@OpenReview">1</sup>]</a>
                <a id="copy-d60cmFf89H@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('d60cmFf89H@OpenReview')">[Copy]</a>
                <a id="kimi-d60cmFf89H@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('d60cmFf89H@OpenReview', this)">[Kimi<sup id="kimi-stars-d60cmFf89H@OpenReview">1</sup>]</a>
                <a id="rel-d60cmFf89H@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('d60cmFf89H@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-d60cmFf89H@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Zeng" target="_blank">Yuchen Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuan Dinh" target="_blank">Tuan Dinh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wonjun Kang" target="_blank">Wonjun Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Mueller" target="_blank">Andreas Mueller</a>
            </p>
            <p id="summary-d60cmFf89H@OpenReview" class="summary">Leveraging the in-context learning (ICL) capability of Large Language Models (LLMs) for tabular classification has gained significant attention for its training-free adaptability across diverse datasets. Recent advancements, like TabPFN, excel in small-scale tabular datasets but struggle to scale for large and complex datasets. Our work enhances the efficiency and scalability of TabPFN for larger datasets by incorporating linear attention mechanisms as a scalable alternative to complexity-quadratic self-attention. Our model, TabFlex, efficiently handles tabular datasets with thousands of features and hundreds of classes, scaling seamlessly to millions of samples. For instance, TabFlex processes the poker-hand dataset with over a million samples in just 5 seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a 2× speedup compared to TabPFN and a 1.5× speedup over XGBoost, outperforming 25 tested baselines in terms of efficiency across a diverse range of datasets. Furthermore, TabFlex remains highly effective on large-scale datasets, delivering strong performance with significantly reduced computational costs, especially when combined with data-efficient techniques such as dimensionality reduction and data sampling.</p>
            <p id="subjects-d60cmFf89H@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-d60cmFf89H@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-d60cmFf89H@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-d60cmFf89H@OpenReview" onclick="foldPdfKimi('d60cmFf89H@OpenReview', this)" class="hr hr-fold">
        </div><div id="hYHczNrKoX@OpenReview" class="panel paper" keywords="collection,covariance,active,operator,decay,strategies,data,kernels,convergence,rate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hYHczNrKoX" target="_blank" title="222/225"><span class="index notranslate">#222</span></a>
                <a id="title-hYHczNrKoX@OpenReview" class="title-link" href="/venue/hYHczNrKoX@OpenReview" target="_blank">On the Benefits of Active Data Collection in Operator Learning</a>
                <a id="pdf-hYHczNrKoX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hYHczNrKoX@OpenReview', this)" data="https://openreview.net/pdf?id=hYHczNrKoX">[PDF<sup id="pdf-stars-hYHczNrKoX@OpenReview"></sup>]</a>
                <a id="copy-hYHczNrKoX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hYHczNrKoX@OpenReview')">[Copy]</a>
                <a id="kimi-hYHczNrKoX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hYHczNrKoX@OpenReview', this)">[Kimi<sup id="kimi-stars-hYHczNrKoX@OpenReview">1</sup>]</a>
                <a id="rel-hYHczNrKoX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hYHczNrKoX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hYHczNrKoX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Unique Subedi" target="_blank">Unique Subedi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ambuj Tewari" target="_blank">Ambuj Tewari</a>
            </p>
            <p id="summary-hYHczNrKoX@OpenReview" class="summary">We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with thepassive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-158-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1136" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.61em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1137"><span class="mo" id="MathJax-Span-1138" style="font-family: MathJax_Main;">∼</span><span class="msubsup" id="MathJax-Span-1139" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1140" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-1141"><span class="mrow" id="MathJax-Span-1142"><span class="mo" id="MathJax-Span-1143" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-1144" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mn>1</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-158">\sim n^{-1}</script>). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts.</p>
            <p id="subjects-hYHczNrKoX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-hYHczNrKoX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hYHczNrKoX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hYHczNrKoX@OpenReview" onclick="foldPdfKimi('hYHczNrKoX@OpenReview', this)" class="hr hr-fold">
        </div><div id="mQeZEsdODh@OpenReview" class="panel paper" keywords="online,continual,crl,world,forgetting,planning,solve,tasks,agent,reinforcement">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mQeZEsdODh" target="_blank" title="223/225"><span class="index notranslate">#223</span></a>
                <a id="title-mQeZEsdODh@OpenReview" class="title-link" href="/venue/mQeZEsdODh@OpenReview" target="_blank">Continual Reinforcement Learning by Planning with Online World Models</a>
                <a id="pdf-mQeZEsdODh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mQeZEsdODh@OpenReview', this)" data="https://openreview.net/pdf?id=mQeZEsdODh">[PDF<sup id="pdf-stars-mQeZEsdODh@OpenReview">4</sup>]</a>
                <a id="copy-mQeZEsdODh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mQeZEsdODh@OpenReview')">[Copy]</a>
                <a id="kimi-mQeZEsdODh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mQeZEsdODh@OpenReview', this)">[Kimi<sup id="kimi-stars-mQeZEsdODh@OpenReview">3</sup>]</a>
                <a id="rel-mQeZEsdODh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mQeZEsdODh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mQeZEsdODh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Liu" target="_blank">Zichen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guoji Fu" target="_blank">Guoji Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Du" target="_blank">Chao Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wee Sun Lee" target="_blank">Wee Sun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Lin" target="_blank">Min Lin</a>
            </p>
            <p id="summary-mQeZEsdODh@OpenReview" class="summary">Continual reinforcement learning (CRL) refers to a naturalistic setting where an agent needs to endlessly evolve, by trial and error, to solve multiple tasks that are presented sequentially. One of the largest obstacles to CRL is that the agent may forget how to solve previous tasks when learning a new task, known as catastrophic forgetting. In this paper, we propose to address this challenge by planning with online world models. Specifically, we learn a Follow-The-Leader shallow model online to capture the world dynamics, in which we plan using model predictive control to solve a set of tasks specified by any reward functions. The online world model is immune to forgetting by construction with a proven regret bound of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-159-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1145" style="width: 9.273em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1007.61em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-1146"><span class="texatom" id="MathJax-Span-1147"><span class="mrow" id="MathJax-Span-1148"><span class="mi" id="MathJax-Span-1149" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-1150" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-1151"><span style="display: inline-block; position: relative; width: 6.148em; height: 0px;"><span style="position: absolute; clip: rect(1.201em, 1005.05em, 2.555em, -999.997em); top: -2.133em; left: 0.992em;"><span class="mrow" id="MathJax-Span-1152"><span class="msubsup" id="MathJax-Span-1153"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-1154" style="font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.445em; left: 0.94em;"><span class="mn" id="MathJax-Span-1155" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mi" id="MathJax-Span-1156" style="font-family: MathJax_Math-italic;">D</span><span class="mi" id="MathJax-Span-1157" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-1158"></span><span class="mo" id="MathJax-Span-1159" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-1160" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-1161" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1005.11em, 3.961em, -999.997em); top: -4.581em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 5.107em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 4.43em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.419em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.94em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.409em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.93em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 2.451em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 2.971em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 3.492em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 3.961em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.023em, 1000.99em, 4.534em, -999.997em); top: -4.008em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-1162" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msqrt><msup><mi>K</mi><mn>2</mn></msup><mi>D</mi><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-159">\mathcal{O}(\sqrt{K^2D\log(T)})</script> under mild assumptions. The planner searches actions solely based on the latest online model, thus forming a FTL Online Agent (OA) that updates incrementally. To assess OA, we further design Continual Bench, a dedicated environment for CRL, and compare with several strong baselines under the same model-planning algorithmic framework. The empirical results show that OA learns continuously to solve new tasks while not forgetting old skills, outperforming agents built on deep world models with various continual learning techniques.</p>
            <p id="subjects-mQeZEsdODh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-mQeZEsdODh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mQeZEsdODh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mQeZEsdODh@OpenReview" onclick="foldPdfKimi('mQeZEsdODh@OpenReview', this)" class="hr hr-fold">
        </div><div id="n1cqQK4hhC@OpenReview" class="panel paper" keywords="skill,abstractions,rotation,codebook,otation,star,skills,libero,augmented,quantization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=n1cqQK4hhC" target="_blank" title="224/225"><span class="index notranslate">#224</span></a>
                <a id="title-n1cqQK4hhC@OpenReview" class="title-link" href="/venue/n1cqQK4hhC@OpenReview" target="_blank">STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization</a>
                <a id="pdf-n1cqQK4hhC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('n1cqQK4hhC@OpenReview', this)" data="https://openreview.net/pdf?id=n1cqQK4hhC">[PDF<sup id="pdf-stars-n1cqQK4hhC@OpenReview">2</sup>]</a>
                <a id="copy-n1cqQK4hhC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('n1cqQK4hhC@OpenReview')">[Copy]</a>
                <a id="kimi-n1cqQK4hhC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('n1cqQK4hhC@OpenReview', this)">[Kimi<sup id="kimi-stars-n1cqQK4hhC@OpenReview">5</sup>]</a>
                <a id="rel-n1cqQK4hhC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('n1cqQK4hhC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-n1cqQK4hhC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Li" target="_blank">Hao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Lv" target="_blank">Qi Lv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Shao" target="_blank">Rui Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Deng" target="_blank">Xiang Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinchuan Li" target="_blank">Yinchuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianye Hao" target="_blank">Jianye Hao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liqiang Nie" target="_blank">Liqiang Nie</a>
            </p>
            <p id="summary-n1cqQK4hhC@OpenReview" class="summary">Transforming complex actions into discrete skill abstractions has demonstrated strong potential for robotic manipulation.Existing approaches mainly leverage latent variable models, e.g., VQ-VAE, to learn skill abstractions through learned vectors (codebooks), while they suffer from codebook collapse and modeling the causal relationship between learned skills. To address these limitations, we present **S**kill **T**raining with **A**ugmented **R**otation (**STAR**), a framework that advances both skill learning and composition to complete complex behaviors. Specifically, to prevent codebook collapse, we devise rotation-augmented residual skill quantization (RaRSQ).It encodes relative angles between encoder outputs into the gradient flow by rotation-based gradient mechanism. Points within the same skill code are forced to be either pushed apart or pulled closer together depending on gradient directions.Further, to capture the casual relationship between skills, we present causal skill transformer (CST) which explicitly models dependencies between skill representations through an autoregressive mechanism for coherent action generation.Extensive experiments demonstrate the superiority of STAR on both LIBERO benchmark and realworld tasks, with around 12% improvement over the baselines.</p>
            <p id="subjects-n1cqQK4hhC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-n1cqQK4hhC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-n1cqQK4hhC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-n1cqQK4hhC@OpenReview" onclick="foldPdfKimi('n1cqQK4hhC@OpenReview', this)" class="hr hr-fold">
        </div><div id="vES22INUKm@OpenReview" class="panel paper" keywords="cnfs,matching,generative,target,analysis,error,flow,modeling,end,deep">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vES22INUKm" target="_blank" title="225/225"><span class="index notranslate">#225</span></a>
                <a id="title-vES22INUKm@OpenReview" class="title-link" href="/venue/vES22INUKm@OpenReview" target="_blank">An Error Analysis of Flow Matching for Deep Generative Modeling</a>
                <a id="pdf-vES22INUKm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vES22INUKm@OpenReview', this)" data="https://openreview.net/pdf?id=vES22INUKm">[PDF<sup id="pdf-stars-vES22INUKm@OpenReview">4</sup>]</a>
                <a id="copy-vES22INUKm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vES22INUKm@OpenReview')">[Copy]</a>
                <a id="kimi-vES22INUKm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vES22INUKm@OpenReview', this)">[Kimi<sup id="kimi-stars-vES22INUKm@OpenReview">4</sup>]</a>
                <a id="rel-vES22INUKm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vES22INUKm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vES22INUKm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyu Zhou" target="_blank">Zhengyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiwei Liu" target="_blank">Weiwei Liu</a>
            </p>
            <p id="summary-vES22INUKm@OpenReview" class="summary">Continuous Normalizing Flows (CNFs) have proven to be a highly efficient technique for generative modeling of complex data since the introduction of Flow Matching (FM). The core of FM is to learn the constructed velocity fields of CNFs through deep least squares regression. Despite its empirical effectiveness, theoretical investigations of FM remain limited. In this paper, we present the first end-to-end error analysis of CNFs built upon FM. Our analysis shows that for general target distributions with bounded support, the generated distribution of FM is guaranteed to converge to the target distribution in the sense of the Wasserstein-2 distance. Furthermore, the convergence rate is significantly improved under an additional mild Lipschitz condition of the target score function.</p>
            <p id="subjects-vES22INUKm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Spotlight" target="_blank">ICML.2025 - Spotlight</a>
            </p>
            <div id="pdf-container-vES22INUKm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vES22INUKm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vES22INUKm@OpenReview" onclick="foldPdfKimi('vES22INUKm@OpenReview', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-Hp53p5AU7X@OpenReview" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#Hp53p5AU7X@OpenReview">Reducing Variance of Stochastic Optimization for Approximating Nash Equilibria in Normal-Form Games</a>
                    <a class="i-star" onclick="toggleAppStar('Hp53p5AU7X@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hp53p5AU7X@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-B9DOjtj9xK@OpenReview" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#B9DOjtj9xK@OpenReview">Learning Soft Sparse Shapes for Efficient Time-Series Classification</a>
                    <a class="i-star" onclick="toggleAppStar('B9DOjtj9xK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('B9DOjtj9xK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-XEyGcrhxB8@OpenReview" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#XEyGcrhxB8@OpenReview">A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO</a>
                    <a class="i-star" onclick="toggleAppStar('XEyGcrhxB8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XEyGcrhxB8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-il3KRr4H9u@OpenReview" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#il3KRr4H9u@OpenReview">BaxBench: Can LLMs Generate Correct and Secure Backends?</a>
                    <a class="i-star" onclick="toggleAppStar('il3KRr4H9u@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('il3KRr4H9u@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Vhc0KrcqWu@OpenReview" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#Vhc0KrcqWu@OpenReview">Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</a>
                    <a class="i-star" onclick="toggleAppStar('Vhc0KrcqWu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Vhc0KrcqWu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-gKdjHLrHDS@OpenReview" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#gKdjHLrHDS@OpenReview">Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry</a>
                    <a class="i-star" onclick="toggleAppStar('gKdjHLrHDS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gKdjHLrHDS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-m3zrHhiCCj@OpenReview" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#m3zrHhiCCj@OpenReview">Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator</a>
                    <a class="i-star" onclick="toggleAppStar('m3zrHhiCCj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('m3zrHhiCCj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-dzwUOiBlQW@OpenReview" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#dzwUOiBlQW@OpenReview">Masked Autoencoders Are Effective Tokenizers for Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('dzwUOiBlQW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dzwUOiBlQW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Ossg1IbHDT@OpenReview" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#Ossg1IbHDT@OpenReview">Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Ossg1IbHDT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ossg1IbHDT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Vf9f7eNX6T@OpenReview" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#Vf9f7eNX6T@OpenReview">A Closer Look at Multimodal Representation Collapse</a>
                    <a class="i-star" onclick="toggleAppStar('Vf9f7eNX6T@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Vf9f7eNX6T@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-TyArXyYnvz@OpenReview" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#TyArXyYnvz@OpenReview">GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for Generalized Low-Rank Trace Regression</a>
                    <a class="i-star" onclick="toggleAppStar('TyArXyYnvz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TyArXyYnvz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-zk5k2NQcEA@OpenReview" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#zk5k2NQcEA@OpenReview">Score-of-Mixture Training: One-Step Generative Model Training Made Simple via Score Estimation of Mixture Distributions</a>
                    <a class="i-star" onclick="toggleAppStar('zk5k2NQcEA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zk5k2NQcEA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-VpBBw1bL47@OpenReview" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#VpBBw1bL47@OpenReview">InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('VpBBw1bL47@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VpBBw1bL47@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-79O2XccGXZ@OpenReview" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#79O2XccGXZ@OpenReview">Geometric Representation Condition Improves Equivariant Molecule Generation</a>
                    <a class="i-star" onclick="toggleAppStar('79O2XccGXZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('79O2XccGXZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-jNCTdUsQaC@OpenReview" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#jNCTdUsQaC@OpenReview">On Learning Parallel Pancakes with Mostly Uniform Weights</a>
                    <a class="i-star" onclick="toggleAppStar('jNCTdUsQaC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jNCTdUsQaC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-GFsMJKt9Kp@OpenReview" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#GFsMJKt9Kp@OpenReview">Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety</a>
                    <a class="i-star" onclick="toggleAppStar('GFsMJKt9Kp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GFsMJKt9Kp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-J5MmGPWKfb@OpenReview" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#J5MmGPWKfb@OpenReview">Position: Language model developers should report train-test overlap</a>
                    <a class="i-star" onclick="toggleAppStar('J5MmGPWKfb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('J5MmGPWKfb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-0ysC6VS0y3@OpenReview" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#0ysC6VS0y3@OpenReview">Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('0ysC6VS0y3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0ysC6VS0y3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-10l1pGeOcK@OpenReview" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#10l1pGeOcK@OpenReview">SAFE: Finding Sparse and Flat Minima to Improve Pruning</a>
                    <a class="i-star" onclick="toggleAppStar('10l1pGeOcK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('10l1pGeOcK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-1w0Zp99dnX@OpenReview" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#1w0Zp99dnX@OpenReview">Generalized Random Forests Using Fixed-Point Trees</a>
                    <a class="i-star" onclick="toggleAppStar('1w0Zp99dnX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1w0Zp99dnX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2Oqm2IzTy9@OpenReview" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#2Oqm2IzTy9@OpenReview">Training Deep Learning Models with Norm-Constrained LMOs</a>
                    <a class="i-star" onclick="toggleAppStar('2Oqm2IzTy9@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2Oqm2IzTy9@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2PWn1LtCwP@OpenReview" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#2PWn1LtCwP@OpenReview">Doubly Robust Conformalized Survival Analysis with Right-Censored Data</a>
                    <a class="i-star" onclick="toggleAppStar('2PWn1LtCwP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2PWn1LtCwP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2dz6psiiA0@OpenReview" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#2dz6psiiA0@OpenReview">Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner</a>
                    <a class="i-star" onclick="toggleAppStar('2dz6psiiA0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2dz6psiiA0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-3Z827FtMNe@OpenReview" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#3Z827FtMNe@OpenReview">Great Models Think Alike and this Undermines AI Oversight</a>
                    <a class="i-star" onclick="toggleAppStar('3Z827FtMNe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3Z827FtMNe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-3rB0bVU6z6@OpenReview" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#3rB0bVU6z6@OpenReview">RE-Bench: Evaluating Frontier AI R&amp;D Capabilities of Language Model Agents against Human Experts</a>
                    <a class="i-star" onclick="toggleAppStar('3rB0bVU6z6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3rB0bVU6z6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-4gWE7CMOlH@OpenReview" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#4gWE7CMOlH@OpenReview">Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration</a>
                    <a class="i-star" onclick="toggleAppStar('4gWE7CMOlH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4gWE7CMOlH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4qIP1sXcR1@OpenReview" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#4qIP1sXcR1@OpenReview">ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals</a>
                    <a class="i-star" onclick="toggleAppStar('4qIP1sXcR1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4qIP1sXcR1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4vAa0A98xI@OpenReview" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#4vAa0A98xI@OpenReview">CoPINN: Cognitive Physics-Informed Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('4vAa0A98xI@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4vAa0A98xI@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4yHWV3B6g4@OpenReview" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#4yHWV3B6g4@OpenReview">Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models</a>
                    <a class="i-star" onclick="toggleAppStar('4yHWV3B6g4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4yHWV3B6g4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5IpVe9PH14@OpenReview" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#5IpVe9PH14@OpenReview">Catoni Contextual Bandits are Robust to Heavy-tailed Rewards</a>
                    <a class="i-star" onclick="toggleAppStar('5IpVe9PH14@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5IpVe9PH14@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5hZCK4Wbex@OpenReview" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#5hZCK4Wbex@OpenReview">Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition</a>
                    <a class="i-star" onclick="toggleAppStar('5hZCK4Wbex@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5hZCK4Wbex@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5hyfZ2jYfI@OpenReview" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#5hyfZ2jYfI@OpenReview">The Synergy of LLMs &amp; RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data</a>
                    <a class="i-star" onclick="toggleAppStar('5hyfZ2jYfI@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5hyfZ2jYfI@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-6gX4rP6QJW@OpenReview" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#6gX4rP6QJW@OpenReview">Self-supervised Masked Graph Autoencoder via Structure-aware Curriculum</a>
                    <a class="i-star" onclick="toggleAppStar('6gX4rP6QJW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6gX4rP6QJW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-71Mm8GDGYd@OpenReview" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#71Mm8GDGYd@OpenReview">$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting</a>
                    <a class="i-star" onclick="toggleAppStar('71Mm8GDGYd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('71Mm8GDGYd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-73mDARqOtQ@OpenReview" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#73mDARqOtQ@OpenReview">RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding</a>
                    <a class="i-star" onclick="toggleAppStar('73mDARqOtQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('73mDARqOtQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8JGwoZceQs@OpenReview" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#8JGwoZceQs@OpenReview">Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs</a>
                    <a class="i-star" onclick="toggleAppStar('8JGwoZceQs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8JGwoZceQs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9hFQvmCl7P@OpenReview" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#9hFQvmCl7P@OpenReview">FedSSI: Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence</a>
                    <a class="i-star" onclick="toggleAppStar('9hFQvmCl7P@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9hFQvmCl7P@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9hd5WA6QCn@OpenReview" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#9hd5WA6QCn@OpenReview">MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('9hd5WA6QCn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9hd5WA6QCn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9vYGZX4OVN@OpenReview" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#9vYGZX4OVN@OpenReview">Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?</a>
                    <a class="i-star" onclick="toggleAppStar('9vYGZX4OVN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9vYGZX4OVN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-AiaVCVDuxF@OpenReview" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#AiaVCVDuxF@OpenReview">Robust ML Auditing using Prior Knowledge</a>
                    <a class="i-star" onclick="toggleAppStar('AiaVCVDuxF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('AiaVCVDuxF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-AnoIgkc6WS@OpenReview" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#AnoIgkc6WS@OpenReview">Exogenous Isomorphism for Counterfactual Identifiability</a>
                    <a class="i-star" onclick="toggleAppStar('AnoIgkc6WS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('AnoIgkc6WS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-AsODat0dkE@OpenReview" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#AsODat0dkE@OpenReview">Optimizing Adaptive Attacks against Watermarks for Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('AsODat0dkE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('AsODat0dkE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-W2Fe1hT7Ks@OpenReview" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#W2Fe1hT7Ks@OpenReview">The Role of Randomness in Stability</a>
                    <a class="i-star" onclick="toggleAppStar('W2Fe1hT7Ks@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('W2Fe1hT7Ks@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-BkdAnSKNoX@OpenReview" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#BkdAnSKNoX@OpenReview">TLLC: Transfer Learning-based Label Completion for Crowdsourcing</a>
                    <a class="i-star" onclick="toggleAppStar('BkdAnSKNoX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BkdAnSKNoX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-BkrIQPREkn@OpenReview" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#BkrIQPREkn@OpenReview">Not All Wrong is Bad: Using Adversarial Examples for Unlearning</a>
                    <a class="i-star" onclick="toggleAppStar('BkrIQPREkn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BkrIQPREkn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bm706VlAtU@OpenReview" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#Bm706VlAtU@OpenReview">Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</a>
                    <a class="i-star" onclick="toggleAppStar('Bm706VlAtU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bm706VlAtU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DDIGCk25BO@OpenReview" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#DDIGCk25BO@OpenReview">Robust Automatic Modulation Classification with Fuzzy Regularization</a>
                    <a class="i-star" onclick="toggleAppStar('DDIGCk25BO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DDIGCk25BO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DJcEoC9JpQ@OpenReview" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#DJcEoC9JpQ@OpenReview">Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger</a>
                    <a class="i-star" onclick="toggleAppStar('DJcEoC9JpQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DJcEoC9JpQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DUGFTH9W8B@OpenReview" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#DUGFTH9W8B@OpenReview">Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport</a>
                    <a class="i-star" onclick="toggleAppStar('DUGFTH9W8B@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DUGFTH9W8B@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DoDXFkF10S@OpenReview" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#DoDXFkF10S@OpenReview">Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation</a>
                    <a class="i-star" onclick="toggleAppStar('DoDXFkF10S@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DoDXFkF10S@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DzLP43CbiX@OpenReview" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#DzLP43CbiX@OpenReview">Flopping for FLOPs: Leveraging Equivariance for Computational Efficiency</a>
                    <a class="i-star" onclick="toggleAppStar('DzLP43CbiX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DzLP43CbiX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EW2JR5aVLm@OpenReview" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#EW2JR5aVLm@OpenReview">Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes</a>
                    <a class="i-star" onclick="toggleAppStar('EW2JR5aVLm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EW2JR5aVLm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-F1ff8zcjPp@OpenReview" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#F1ff8zcjPp@OpenReview">Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('F1ff8zcjPp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('F1ff8zcjPp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-F8NTPAz5HH@OpenReview" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#F8NTPAz5HH@OpenReview">Is Complex Query Answering Really Complex?</a>
                    <a class="i-star" onclick="toggleAppStar('F8NTPAz5HH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('F8NTPAz5HH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FKi6yjXwCN@OpenReview" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#FKi6yjXwCN@OpenReview">LOCATE 3D: Real-World Object Localization via Self-Supervised Learning in 3D</a>
                    <a class="i-star" onclick="toggleAppStar('FKi6yjXwCN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FKi6yjXwCN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FSVdEzR4To@OpenReview" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#FSVdEzR4To@OpenReview">Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data</a>
                    <a class="i-star" onclick="toggleAppStar('FSVdEzR4To@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FSVdEzR4To@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ffpc7vx6qq@OpenReview" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#Ffpc7vx6qq@OpenReview">Learning Safety Constraints for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Ffpc7vx6qq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ffpc7vx6qq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FuGps5Zyia@OpenReview" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#FuGps5Zyia@OpenReview">Ad-Hoc Human-AI Coordination Challenge</a>
                    <a class="i-star" onclick="toggleAppStar('FuGps5Zyia@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FuGps5Zyia@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GazlTYxZss@OpenReview" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#GazlTYxZss@OpenReview">Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</a>
                    <a class="i-star" onclick="toggleAppStar('GazlTYxZss@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GazlTYxZss@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GhTdNOMfOD@OpenReview" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#GhTdNOMfOD@OpenReview">TimeBase: The Power of Minimalism in Efficient Long-term Time Series Forecasting</a>
                    <a class="i-star" onclick="toggleAppStar('GhTdNOMfOD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GhTdNOMfOD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gn6L4QRKf7@OpenReview" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#Gn6L4QRKf7@OpenReview">On the Power of Context-Enhanced Learning in LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('Gn6L4QRKf7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gn6L4QRKf7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Go0DdhjATH@OpenReview" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#Go0DdhjATH@OpenReview">Policy Regularization on Globally Accessible States in Cross-Dynamics Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Go0DdhjATH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Go0DdhjATH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GoGuB1yFko@OpenReview" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#GoGuB1yFko@OpenReview">Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection</a>
                    <a class="i-star" onclick="toggleAppStar('GoGuB1yFko@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GoGuB1yFko@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gp7NfP7Erm@OpenReview" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#Gp7NfP7Erm@OpenReview">Towards Robustness and Explainability of Automatic Algorithm Selection</a>
                    <a class="i-star" onclick="toggleAppStar('Gp7NfP7Erm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gp7NfP7Erm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-HxCuvx2uUi@OpenReview" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#HxCuvx2uUi@OpenReview">Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning</a>
                    <a class="i-star" onclick="toggleAppStar('HxCuvx2uUi@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HxCuvx2uUi@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-I4jNAbqHnM@OpenReview" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#I4jNAbqHnM@OpenReview">The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes</a>
                    <a class="i-star" onclick="toggleAppStar('I4jNAbqHnM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('I4jNAbqHnM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IKCfxWtTsu@OpenReview" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#IKCfxWtTsu@OpenReview">PCEvolve: Private Contrastive Evolution for Synthetic Dataset Generation via Few-Shot Private Data and Generative APIs</a>
                    <a class="i-star" onclick="toggleAppStar('IKCfxWtTsu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IKCfxWtTsu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-JxnOZwFNcU@OpenReview" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#JxnOZwFNcU@OpenReview">Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection</a>
                    <a class="i-star" onclick="toggleAppStar('JxnOZwFNcU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('JxnOZwFNcU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-K2CckZjNy0@OpenReview" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#K2CckZjNy0@OpenReview">AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders</a>
                    <a class="i-star" onclick="toggleAppStar('K2CckZjNy0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('K2CckZjNy0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KRosBwvhDx@OpenReview" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#KRosBwvhDx@OpenReview">Do We Really Need Message Passing in Brain Network Modeling?</a>
                    <a class="i-star" onclick="toggleAppStar('KRosBwvhDx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KRosBwvhDx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KZo2XhcSg6@OpenReview" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#KZo2XhcSg6@OpenReview">LipsNet++: Unifying Filter and Controller into a Policy Network</a>
                    <a class="i-star" onclick="toggleAppStar('KZo2XhcSg6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KZo2XhcSg6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IYLNdCII48@OpenReview" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#IYLNdCII48@OpenReview">CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation</a>
                    <a class="i-star" onclick="toggleAppStar('IYLNdCII48@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IYLNdCII48@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lie2rOCgkh@OpenReview" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#Lie2rOCgkh@OpenReview">Causal Attribution Analysis for Continuous Outcomes</a>
                    <a class="i-star" onclick="toggleAppStar('Lie2rOCgkh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lie2rOCgkh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lktwi30g63@OpenReview" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#Lktwi30g63@OpenReview">When and How Does CLIP Enable Domain and Compositional Generalization?</a>
                    <a class="i-star" onclick="toggleAppStar('Lktwi30g63@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lktwi30g63@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GKqoqGCHTq@OpenReview" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#GKqoqGCHTq@OpenReview">Improving Consistency Models with Generator-Augmented Flows</a>
                    <a class="i-star" onclick="toggleAppStar('GKqoqGCHTq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GKqoqGCHTq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-M7mVzCV6uU@OpenReview" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#M7mVzCV6uU@OpenReview">Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework</a>
                    <a class="i-star" onclick="toggleAppStar('M7mVzCV6uU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('M7mVzCV6uU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-MHaSq1LlTe@OpenReview" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#MHaSq1LlTe@OpenReview">Signed Laplacians for Constrained Graph Clustering</a>
                    <a class="i-star" onclick="toggleAppStar('MHaSq1LlTe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MHaSq1LlTe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-MkCnPNOLMk@OpenReview" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#MkCnPNOLMk@OpenReview">Towards Better-than-2 Approximation for Constrained Correlation Clustering</a>
                    <a class="i-star" onclick="toggleAppStar('MkCnPNOLMk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('MkCnPNOLMk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-O0oe7hPtbl@OpenReview" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#O0oe7hPtbl@OpenReview">Gridded Transformer Neural Processes for Spatio-Temporal Data</a>
                    <a class="i-star" onclick="toggleAppStar('O0oe7hPtbl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('O0oe7hPtbl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-O6q2BHK1BL@OpenReview" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#O6q2BHK1BL@OpenReview">Local Identifying Causal Relations in the Presence of Latent Variables</a>
                    <a class="i-star" onclick="toggleAppStar('O6q2BHK1BL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('O6q2BHK1BL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OJ6WE7F8tK@OpenReview" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#OJ6WE7F8tK@OpenReview">Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator</a>
                    <a class="i-star" onclick="toggleAppStar('OJ6WE7F8tK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OJ6WE7F8tK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Obet2x6GNl@OpenReview" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#Obet2x6GNl@OpenReview">Algorithms with Calibrated Machine Learning Predictions</a>
                    <a class="i-star" onclick="toggleAppStar('Obet2x6GNl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Obet2x6GNl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OpineZj5bj@OpenReview" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#OpineZj5bj@OpenReview">Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning</a>
                    <a class="i-star" onclick="toggleAppStar('OpineZj5bj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OpineZj5bj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PUzNwYmb3l@OpenReview" style="display:none">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#PUzNwYmb3l@OpenReview">Efficient First-Order Optimization on the Pareto Set for Multi-Objective Learning under Preference Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('PUzNwYmb3l@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PUzNwYmb3l@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ppcf30NGL0@OpenReview" style="display:none">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#Ppcf30NGL0@OpenReview">New Bounds for Sparse Variational Gaussian Processes</a>
                    <a class="i-star" onclick="toggleAppStar('Ppcf30NGL0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ppcf30NGL0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PzSG5nKe1q@OpenReview" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#PzSG5nKe1q@OpenReview">RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('PzSG5nKe1q@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PzSG5nKe1q@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Q3rGQUGgWo@OpenReview" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#Q3rGQUGgWo@OpenReview">SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('Q3rGQUGgWo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Q3rGQUGgWo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QC4dfobOLQ@OpenReview" style="display:none">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#QC4dfobOLQ@OpenReview">Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</a>
                    <a class="i-star" onclick="toggleAppStar('QC4dfobOLQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QC4dfobOLQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-S2K5MyRjrL@OpenReview" style="display:none">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#S2K5MyRjrL@OpenReview">Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss</a>
                    <a class="i-star" onclick="toggleAppStar('S2K5MyRjrL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('S2K5MyRjrL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SnZ7SKykHh@OpenReview" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#SnZ7SKykHh@OpenReview">PokéChamp: an Expert-level Minimax Language Agent</a>
                    <a class="i-star" onclick="toggleAppStar('SnZ7SKykHh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SnZ7SKykHh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-T5IZ32ImAB@OpenReview" style="display:none">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#T5IZ32ImAB@OpenReview">Graph Diffusion for Robust Multi-Agent Coordination</a>
                    <a class="i-star" onclick="toggleAppStar('T5IZ32ImAB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('T5IZ32ImAB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-TmJvacopmV@OpenReview" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#TmJvacopmV@OpenReview">Discrepancy Minimization in Input-Sparsity Time</a>
                    <a class="i-star" onclick="toggleAppStar('TmJvacopmV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TmJvacopmV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-U354tbTjav@OpenReview" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#U354tbTjav@OpenReview">Return of the Latent Space COWBOYS: Re-thinking the use of VAEs for Bayesian Optimisation of Structured Spaces</a>
                    <a class="i-star" onclick="toggleAppStar('U354tbTjav@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('U354tbTjav@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-U64wEbM7NB@OpenReview" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#U64wEbM7NB@OpenReview">Trusted Multi-View Classification with Expert Knowledge Constraints</a>
                    <a class="i-star" onclick="toggleAppStar('U64wEbM7NB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('U64wEbM7NB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UFlyLkvyAE@OpenReview" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#UFlyLkvyAE@OpenReview">Graph Adaptive Autoregressive Moving Average Models</a>
                    <a class="i-star" onclick="toggleAppStar('UFlyLkvyAE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UFlyLkvyAE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-F0sinjQMnv@OpenReview" style="display:none">
                    <span class="i-index">#96</span>
                    <a class="i-title" href="#F0sinjQMnv@OpenReview">Identifying Causal Direction via Variational Bayesian Compression</a>
                    <a class="i-star" onclick="toggleAppStar('F0sinjQMnv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('F0sinjQMnv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EUH4VUCXay@OpenReview" style="display:none">
                    <span class="i-index">#97</span>
                    <a class="i-title" href="#EUH4VUCXay@OpenReview">am-ELO: A Stable Framework for Arena-based LLM Evaluation</a>
                    <a class="i-star" onclick="toggleAppStar('EUH4VUCXay@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EUH4VUCXay@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-BUONdewsBa@OpenReview" style="display:none">
                    <span class="i-index">#98</span>
                    <a class="i-title" href="#BUONdewsBa@OpenReview">Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('BUONdewsBa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BUONdewsBa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5liHhkgvAn@OpenReview" style="display:none">
                    <span class="i-index">#99</span>
                    <a class="i-title" href="#5liHhkgvAn@OpenReview">SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming</a>
                    <a class="i-star" onclick="toggleAppStar('5liHhkgvAn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5liHhkgvAn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0cEZyhHEks@OpenReview" style="display:none">
                    <span class="i-index">#100</span>
                    <a class="i-title" href="#0cEZyhHEks@OpenReview">Taming Knowledge Conflicts in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('0cEZyhHEks@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0cEZyhHEks@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pokj70ZAxJ@OpenReview" style="display:none">
                    <span class="i-index">#101</span>
                    <a class="i-title" href="#Pokj70ZAxJ@OpenReview">Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Pokj70ZAxJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pokj70ZAxJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LpE54NUnmO@OpenReview" style="display:none">
                    <span class="i-index">#102</span>
                    <a class="i-title" href="#LpE54NUnmO@OpenReview">G-Designer: Architecting Multi-agent Communication Topologies via Graph Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('LpE54NUnmO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LpE54NUnmO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KKwBo3u3IW@OpenReview" style="display:none">
                    <span class="i-index">#103</span>
                    <a class="i-title" href="#KKwBo3u3IW@OpenReview">Mastering Board Games by External and Internal Planning with Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('KKwBo3u3IW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KKwBo3u3IW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3pk0p4NGmQ@OpenReview" style="display:none">
                    <span class="i-index">#104</span>
                    <a class="i-title" href="#3pk0p4NGmQ@OpenReview">CVE-Bench: A Benchmark for AI Agents’ Ability to Exploit Real-World Web Application Vulnerabilities</a>
                    <a class="i-star" onclick="toggleAppStar('3pk0p4NGmQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3pk0p4NGmQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-13HPTmZKbM@OpenReview" style="display:none">
                    <span class="i-index">#105</span>
                    <a class="i-title" href="#13HPTmZKbM@OpenReview">Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting</a>
                    <a class="i-star" onclick="toggleAppStar('13HPTmZKbM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('13HPTmZKbM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9u5hPIcr6j@OpenReview" style="display:none">
                    <span class="i-index">#106</span>
                    <a class="i-title" href="#9u5hPIcr6j@OpenReview">LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression</a>
                    <a class="i-star" onclick="toggleAppStar('9u5hPIcr6j@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9u5hPIcr6j@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-V61nluxFlR@OpenReview" style="display:none">
                    <span class="i-index">#107</span>
                    <a class="i-title" href="#V61nluxFlR@OpenReview">Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('V61nluxFlR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('V61nluxFlR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-RL6d53a5jj@OpenReview" style="display:none">
                    <span class="i-index">#108</span>
                    <a class="i-title" href="#RL6d53a5jj@OpenReview">Probabilistic Factorial Experimental Design for Combinatorial Interventions</a>
                    <a class="i-star" onclick="toggleAppStar('RL6d53a5jj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('RL6d53a5jj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4Z04wVQ9FY@OpenReview" style="display:none">
                    <span class="i-index">#109</span>
                    <a class="i-title" href="#4Z04wVQ9FY@OpenReview">Linearization Turns Neural Operators into Function-Valued Gaussian Processes</a>
                    <a class="i-star" onclick="toggleAppStar('4Z04wVQ9FY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4Z04wVQ9FY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IYOksPHJKT@OpenReview" style="display:none">
                    <span class="i-index">#110</span>
                    <a class="i-title" href="#IYOksPHJKT@OpenReview">Catch Your Emotion: Sharpening Emotion Perception in Multimodal Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('IYOksPHJKT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IYOksPHJKT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mlmpf4Izrj@OpenReview" style="display:none">
                    <span class="i-index">#111</span>
                    <a class="i-title" href="#Mlmpf4Izrj@OpenReview">Efficiently Vectorized MCMC on Modern Accelerators</a>
                    <a class="i-star" onclick="toggleAppStar('Mlmpf4Izrj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mlmpf4Izrj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5EbiopWH6e@OpenReview" style="display:none">
                    <span class="i-index">#112</span>
                    <a class="i-title" href="#5EbiopWH6e@OpenReview">Implicit Language Models are RNNs: Balancing Parallelization and Expressivity</a>
                    <a class="i-star" onclick="toggleAppStar('5EbiopWH6e@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5EbiopWH6e@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qq5h78Eshy@OpenReview" style="display:none">
                    <span class="i-index">#113</span>
                    <a class="i-title" href="#Qq5h78Eshy@OpenReview">Rapid Overfitting of Multi-Pass SGD in Stochastic Convex Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('Qq5h78Eshy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qq5h78Eshy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WISfJyOA6M@OpenReview" style="display:none">
                    <span class="i-index">#114</span>
                    <a class="i-title" href="#WISfJyOA6M@OpenReview">PhySpec: Physically Consistent Spectral Reconstruction via Orthogonal Subspace Decomposition and Self-Supervised Meta-Auxiliary Learning</a>
                    <a class="i-star" onclick="toggleAppStar('WISfJyOA6M@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WISfJyOA6M@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WR0ahlhOoy@OpenReview" style="display:none">
                    <span class="i-index">#115</span>
                    <a class="i-title" href="#WR0ahlhOoy@OpenReview">Improving Zero-Shot Adversarial Robustness in Vision-Language Models by Closed-form Alignment of Adversarial Path Simplices</a>
                    <a class="i-star" onclick="toggleAppStar('WR0ahlhOoy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WR0ahlhOoy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UKR3HsAFkC@OpenReview" style="display:none">
                    <span class="i-index">#116</span>
                    <a class="i-title" href="#UKR3HsAFkC@OpenReview">Achieving Linear Speedup and Near-Optimal Complexity for Decentralized Optimization over Row-stochastic Networks</a>
                    <a class="i-star" onclick="toggleAppStar('UKR3HsAFkC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UKR3HsAFkC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WbP2OwMULq@OpenReview" style="display:none">
                    <span class="i-index">#117</span>
                    <a class="i-title" href="#WbP2OwMULq@OpenReview">HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('WbP2OwMULq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WbP2OwMULq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WxY61MmHYo@OpenReview" style="display:none">
                    <span class="i-index">#118</span>
                    <a class="i-title" href="#WxY61MmHYo@OpenReview">Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream</a>
                    <a class="i-star" onclick="toggleAppStar('WxY61MmHYo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WxY61MmHYo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XXFBqfwnUp@OpenReview" style="display:none">
                    <span class="i-index">#119</span>
                    <a class="i-title" href="#XXFBqfwnUp@OpenReview">Learning the RoPEs: Better 2D and 3D Position Encodings with STRING</a>
                    <a class="i-star" onclick="toggleAppStar('XXFBqfwnUp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XXFBqfwnUp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Y19ngWhN0b@OpenReview" style="display:none">
                    <span class="i-index">#120</span>
                    <a class="i-title" href="#Y19ngWhN0b@OpenReview">Weakly-Supervised Contrastive Learning for Imprecise Class Labels</a>
                    <a class="i-star" onclick="toggleAppStar('Y19ngWhN0b@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Y19ngWhN0b@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YucuAuXMpT@OpenReview" style="display:none">
                    <span class="i-index">#121</span>
                    <a class="i-title" href="#YucuAuXMpT@OpenReview">Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks</a>
                    <a class="i-star" onclick="toggleAppStar('YucuAuXMpT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YucuAuXMpT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yv416IYTFp@OpenReview" style="display:none">
                    <span class="i-index">#122</span>
                    <a class="i-title" href="#Yv416IYTFp@OpenReview">PASS: Private Attributes Protection with Stochastic Data Substitution</a>
                    <a class="i-star" onclick="toggleAppStar('Yv416IYTFp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yv416IYTFp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zm2M92TZyO@OpenReview" style="display:none">
                    <span class="i-index">#123</span>
                    <a class="i-title" href="#Zm2M92TZyO@OpenReview">Leveraging Diffusion Model as Pseudo-Anomalous Graph Generator for Graph-Level Anomaly Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Zm2M92TZyO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zm2M92TZyO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZrhGq664om@OpenReview" style="display:none">
                    <span class="i-index">#124</span>
                    <a class="i-title" href="#ZrhGq664om@OpenReview">Neural Collapse Beyond the Unconstrained Features Model: Landscape, Dynamics, and Generalization in the Mean-Field Regime</a>
                    <a class="i-star" onclick="toggleAppStar('ZrhGq664om@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZrhGq664om@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-a6Cagkpmgz@OpenReview" style="display:none">
                    <span class="i-index">#125</span>
                    <a class="i-title" href="#a6Cagkpmgz@OpenReview">Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints</a>
                    <a class="i-star" onclick="toggleAppStar('a6Cagkpmgz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('a6Cagkpmgz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-a7UM5c1CEa@OpenReview" style="display:none">
                    <span class="i-index">#126</span>
                    <a class="i-title" href="#a7UM5c1CEa@OpenReview">Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models</a>
                    <a class="i-star" onclick="toggleAppStar('a7UM5c1CEa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('a7UM5c1CEa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aFNq67ilos@OpenReview" style="display:none">
                    <span class="i-index">#127</span>
                    <a class="i-title" href="#aFNq67ilos@OpenReview">Training Dynamics of In-Context Learning in Linear Attention</a>
                    <a class="i-star" onclick="toggleAppStar('aFNq67ilos@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aFNq67ilos@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aJeLhLcsh0@OpenReview" style="display:none">
                    <span class="i-index">#128</span>
                    <a class="i-title" href="#aJeLhLcsh0@OpenReview">Multi-Turn Code Generation Through Single-Step Rewards</a>
                    <a class="i-star" onclick="toggleAppStar('aJeLhLcsh0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aJeLhLcsh0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aQUUUAcAw1@OpenReview" style="display:none">
                    <span class="i-index">#129</span>
                    <a class="i-title" href="#aQUUUAcAw1@OpenReview">Sparse-pivot: Dynamic correlation clustering for node insertions</a>
                    <a class="i-star" onclick="toggleAppStar('aQUUUAcAw1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aQUUUAcAw1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-afpc1MFMYU@OpenReview" style="display:none">
                    <span class="i-index">#130</span>
                    <a class="i-title" href="#afpc1MFMYU@OpenReview">Non-stationary Diffusion For Probabilistic Time Series Forecasting</a>
                    <a class="i-star" onclick="toggleAppStar('afpc1MFMYU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('afpc1MFMYU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bLcXkIasck@OpenReview" style="display:none">
                    <span class="i-index">#131</span>
                    <a class="i-title" href="#bLcXkIasck@OpenReview">Language Models May Verbatim Complete Text They Were Not Explicitly Trained On</a>
                    <a class="i-star" onclick="toggleAppStar('bLcXkIasck@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bLcXkIasck@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-beeNgQEfe2@OpenReview" style="display:none">
                    <span class="i-index">#132</span>
                    <a class="i-title" href="#beeNgQEfe2@OpenReview">Scaling Test-Time Compute Without Verification or RL is Suboptimal</a>
                    <a class="i-star" onclick="toggleAppStar('beeNgQEfe2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('beeNgQEfe2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bkauyuzBN4@OpenReview" style="display:none">
                    <span class="i-index">#133</span>
                    <a class="i-title" href="#bkauyuzBN4@OpenReview">Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting</a>
                    <a class="i-star" onclick="toggleAppStar('bkauyuzBN4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bkauyuzBN4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-c0dhw1du33@OpenReview" style="display:none">
                    <span class="i-index">#134</span>
                    <a class="i-title" href="#c0dhw1du33@OpenReview">Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations</a>
                    <a class="i-star" onclick="toggleAppStar('c0dhw1du33@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('c0dhw1du33@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cKaUC1PeJA@OpenReview" style="display:none">
                    <span class="i-index">#135</span>
                    <a class="i-title" href="#cKaUC1PeJA@OpenReview">Efficient and Separate Authentication Image Steganography Network</a>
                    <a class="i-star" onclick="toggleAppStar('cKaUC1PeJA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cKaUC1PeJA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-clJIQ4TKR0@OpenReview" style="display:none">
                    <span class="i-index">#136</span>
                    <a class="i-title" href="#clJIQ4TKR0@OpenReview">Investigating Non-Transitivity in LLM-as-a-Judge</a>
                    <a class="i-star" onclick="toggleAppStar('clJIQ4TKR0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('clJIQ4TKR0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dhRXGWJ027@OpenReview" style="display:none">
                    <span class="i-index">#137</span>
                    <a class="i-title" href="#dhRXGWJ027@OpenReview">Discovering Symbolic Cognitive Models from Human and Animal Behavior</a>
                    <a class="i-star" onclick="toggleAppStar('dhRXGWJ027@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dhRXGWJ027@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-e46xNZhwl8@OpenReview" style="display:none">
                    <span class="i-index">#138</span>
                    <a class="i-title" href="#e46xNZhwl8@OpenReview">Learning with Exact Invariances in Polynomial Time</a>
                    <a class="i-star" onclick="toggleAppStar('e46xNZhwl8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('e46xNZhwl8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-evb9dNxCN5@OpenReview" style="display:none">
                    <span class="i-index">#139</span>
                    <a class="i-title" href="#evb9dNxCN5@OpenReview">Where is the Truth? The Risk of Getting Confounded in a Continual World</a>
                    <a class="i-star" onclick="toggleAppStar('evb9dNxCN5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('evb9dNxCN5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fPOkujQBVb@OpenReview" style="display:none">
                    <span class="i-index">#140</span>
                    <a class="i-title" href="#fPOkujQBVb@OpenReview">Sharp Generalization for Nonparametric Regression by Over-Parameterized Neural Networks: A Distribution-Free Analysis in Spherical Covariate</a>
                    <a class="i-star" onclick="toggleAppStar('fPOkujQBVb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fPOkujQBVb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gcgzQSKR7y@OpenReview" style="display:none">
                    <span class="i-index">#141</span>
                    <a class="i-title" href="#gcgzQSKR7y@OpenReview">Stronger Neyman Regret Guarantees for Adaptive Experimental Design</a>
                    <a class="i-star" onclick="toggleAppStar('gcgzQSKR7y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gcgzQSKR7y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-glLqTK9En3@OpenReview" style="display:none">
                    <span class="i-index">#142</span>
                    <a class="i-title" href="#glLqTK9En3@OpenReview">Functional Alignment Can Mislead: Examining Model Stitching</a>
                    <a class="i-star" onclick="toggleAppStar('glLqTK9En3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('glLqTK9En3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-h2oNQOzbc5@OpenReview" style="display:none">
                    <span class="i-index">#143</span>
                    <a class="i-title" href="#h2oNQOzbc5@OpenReview">ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation</a>
                    <a class="i-star" onclick="toggleAppStar('h2oNQOzbc5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('h2oNQOzbc5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hRQyqtcjVv@OpenReview" style="display:none">
                    <span class="i-index">#144</span>
                    <a class="i-title" href="#hRQyqtcjVv@OpenReview">The Jailbreak Tax: How Useful are Your Jailbreak Outputs?</a>
                    <a class="i-star" onclick="toggleAppStar('hRQyqtcjVv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hRQyqtcjVv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hS2Ed5XYRq@OpenReview" style="display:none">
                    <span class="i-index">#145</span>
                    <a class="i-title" href="#hS2Ed5XYRq@OpenReview">MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models</a>
                    <a class="i-star" onclick="toggleAppStar('hS2Ed5XYRq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hS2Ed5XYRq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hrdLhNDAzp@OpenReview" style="display:none">
                    <span class="i-index">#146</span>
                    <a class="i-title" href="#hrdLhNDAzp@OpenReview">MCU: An Evaluation Framework for Open-Ended Game Agents</a>
                    <a class="i-star" onclick="toggleAppStar('hrdLhNDAzp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hrdLhNDAzp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hwTKGdM4TK@OpenReview" style="display:none">
                    <span class="i-index">#147</span>
                    <a class="i-title" href="#hwTKGdM4TK@OpenReview">Instance Correlation Graph-based Naive Bayes</a>
                    <a class="i-star" onclick="toggleAppStar('hwTKGdM4TK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hwTKGdM4TK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-iBpkzB5LEr@OpenReview" style="display:none">
                    <span class="i-index">#148</span>
                    <a class="i-title" href="#iBpkzB5LEr@OpenReview">Primal-Dual Neural Algorithmic Reasoning</a>
                    <a class="i-star" onclick="toggleAppStar('iBpkzB5LEr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('iBpkzB5LEr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-iFOXz5H2gB@OpenReview" style="display:none">
                    <span class="i-index">#149</span>
                    <a class="i-title" href="#iFOXz5H2gB@OpenReview">Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios</a>
                    <a class="i-star" onclick="toggleAppStar('iFOXz5H2gB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('iFOXz5H2gB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jEcQP3lGlq@OpenReview" style="display:none">
                    <span class="i-index">#150</span>
                    <a class="i-title" href="#jEcQP3lGlq@OpenReview">Elucidating the Design Space of Multimodal Protein Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('jEcQP3lGlq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jEcQP3lGlq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jJRkkPr474@OpenReview" style="display:none">
                    <span class="i-index">#151</span>
                    <a class="i-title" href="#jJRkkPr474@OpenReview">Geometric Hyena Networks for Large-scale Equivariant Learning</a>
                    <a class="i-star" onclick="toggleAppStar('jJRkkPr474@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jJRkkPr474@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jnPHZqcUdn@OpenReview" style="display:none">
                    <span class="i-index">#152</span>
                    <a class="i-title" href="#jnPHZqcUdn@OpenReview">scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data</a>
                    <a class="i-star" onclick="toggleAppStar('jnPHZqcUdn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jnPHZqcUdn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-js3gePctLu@OpenReview" style="display:none">
                    <span class="i-index">#153</span>
                    <a class="i-title" href="#js3gePctLu@OpenReview">Procurement Auctions via Approximately Optimal Submodular Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('js3gePctLu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('js3gePctLu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jvP1wbD0xh@OpenReview" style="display:none">
                    <span class="i-index">#154</span>
                    <a class="i-title" href="#jvP1wbD0xh@OpenReview">Better to Teach than to Give: Domain Generalized Semantic Segmentation via Agent Queries with Diffusion Model Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('jvP1wbD0xh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jvP1wbD0xh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kfYxyvCYQ4@OpenReview" style="display:none">
                    <span class="i-index">#155</span>
                    <a class="i-title" href="#kfYxyvCYQ4@OpenReview">Hyperspherical Normalization for Scalable Deep Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('kfYxyvCYQ4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kfYxyvCYQ4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kmg7hweySi@OpenReview" style="display:none">
                    <span class="i-index">#156</span>
                    <a class="i-title" href="#kmg7hweySi@OpenReview">Feature learning from non-Gaussian inputs: the case of Independent Component Analysis in high dimensions</a>
                    <a class="i-star" onclick="toggleAppStar('kmg7hweySi@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kmg7hweySi@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mEV0nvHcK3@OpenReview" style="display:none">
                    <span class="i-index">#157</span>
                    <a class="i-title" href="#mEV0nvHcK3@OpenReview">Towards Practical Defect-Focused Automated Code Review</a>
                    <a class="i-star" onclick="toggleAppStar('mEV0nvHcK3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mEV0nvHcK3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mruyFvKDKq@OpenReview" style="display:none">
                    <span class="i-index">#158</span>
                    <a class="i-title" href="#mruyFvKDKq@OpenReview">Invariant Deep Uplift Modeling for Incentive Assignment in Online Marketing via Probability of Necessity and Sufficiency</a>
                    <a class="i-star" onclick="toggleAppStar('mruyFvKDKq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mruyFvKDKq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mwSBIlNLdQ@OpenReview" style="display:none">
                    <span class="i-index">#159</span>
                    <a class="i-title" href="#mwSBIlNLdQ@OpenReview">Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures</a>
                    <a class="i-star" onclick="toggleAppStar('mwSBIlNLdQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mwSBIlNLdQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mzSwYvwYdC@OpenReview" style="display:none">
                    <span class="i-index">#160</span>
                    <a class="i-title" href="#mzSwYvwYdC@OpenReview">Independence Tests for Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('mzSwYvwYdC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mzSwYvwYdC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-nVD7KoU09V@OpenReview" style="display:none">
                    <span class="i-index">#161</span>
                    <a class="i-title" href="#nVD7KoU09V@OpenReview">Rethink GraphODE Generalization within Coupled Dynamical System</a>
                    <a class="i-star" onclick="toggleAppStar('nVD7KoU09V@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('nVD7KoU09V@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-oYyaVSqEFu@OpenReview" style="display:none">
                    <span class="i-index">#162</span>
                    <a class="i-title" href="#oYyaVSqEFu@OpenReview">When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network</a>
                    <a class="i-star" onclick="toggleAppStar('oYyaVSqEFu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oYyaVSqEFu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-oa7MYAO6h6@OpenReview" style="display:none">
                    <span class="i-index">#163</span>
                    <a class="i-title" href="#oa7MYAO6h6@OpenReview">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a>
                    <a class="i-star" onclick="toggleAppStar('oa7MYAO6h6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('oa7MYAO6h6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-otNB7BzsiR@OpenReview" style="display:none">
                    <span class="i-index">#164</span>
                    <a class="i-title" href="#otNB7BzsiR@OpenReview">Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('otNB7BzsiR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('otNB7BzsiR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pKaNgFzJBy@OpenReview" style="display:none">
                    <span class="i-index">#165</span>
                    <a class="i-title" href="#pKaNgFzJBy@OpenReview">On the Guidance of Flow Matching</a>
                    <a class="i-star" onclick="toggleAppStar('pKaNgFzJBy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pKaNgFzJBy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pWs925fKyK@OpenReview" style="display:none">
                    <span class="i-index">#166</span>
                    <a class="i-title" href="#pWs925fKyK@OpenReview">Bridging Layout and RTL: Knowledge Distillation based Timing Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('pWs925fKyK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pWs925fKyK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tNGdLEL4R0@OpenReview" style="display:none">
                    <span class="i-index">#167</span>
                    <a class="i-title" href="#tNGdLEL4R0@OpenReview">Scaling Trends in Language Model Robustness</a>
                    <a class="i-star" onclick="toggleAppStar('tNGdLEL4R0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tNGdLEL4R0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ps3aO9MHJv@OpenReview" style="display:none">
                    <span class="i-index">#168</span>
                    <a class="i-title" href="#ps3aO9MHJv@OpenReview">Prediction models that learn to avoid missing values</a>
                    <a class="i-star" onclick="toggleAppStar('ps3aO9MHJv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ps3aO9MHJv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pyIXyl4qFx@OpenReview" style="display:none">
                    <span class="i-index">#169</span>
                    <a class="i-title" href="#pyIXyl4qFx@OpenReview">G-Adaptivity: optimised graph-based mesh relocation for finite element methods</a>
                    <a class="i-star" onclick="toggleAppStar('pyIXyl4qFx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pyIXyl4qFx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qOgKMqv9T7@OpenReview" style="display:none">
                    <span class="i-index">#170</span>
                    <a class="i-title" href="#qOgKMqv9T7@OpenReview">TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation</a>
                    <a class="i-star" onclick="toggleAppStar('qOgKMqv9T7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qOgKMqv9T7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qtuxDy2qEB@OpenReview" style="display:none">
                    <span class="i-index">#171</span>
                    <a class="i-title" href="#qtuxDy2qEB@OpenReview">Parallel Simulation for Log-concave Sampling and Score-based Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('qtuxDy2qEB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qtuxDy2qEB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-r9HlTuCQfr@OpenReview" style="display:none">
                    <span class="i-index">#172</span>
                    <a class="i-title" href="#r9HlTuCQfr@OpenReview">Decision Making under the Exponential Family: Distributionally Robust Optimisation with Bayesian Ambiguity Sets</a>
                    <a class="i-star" onclick="toggleAppStar('r9HlTuCQfr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('r9HlTuCQfr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sDK6bSmHgM@OpenReview" style="display:none">
                    <span class="i-index">#173</span>
                    <a class="i-title" href="#sDK6bSmHgM@OpenReview">FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields</a>
                    <a class="i-star" onclick="toggleAppStar('sDK6bSmHgM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sDK6bSmHgM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sEBfiF8JBu@OpenReview" style="display:none">
                    <span class="i-index">#174</span>
                    <a class="i-title" href="#sEBfiF8JBu@OpenReview">PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling</a>
                    <a class="i-star" onclick="toggleAppStar('sEBfiF8JBu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sEBfiF8JBu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sQS0roNQZR@OpenReview" style="display:none">
                    <span class="i-index">#175</span>
                    <a class="i-title" href="#sQS0roNQZR@OpenReview">From Language Models over Tokens to Language Models over Characters</a>
                    <a class="i-star" onclick="toggleAppStar('sQS0roNQZR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sQS0roNQZR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-teUg2pMrF0@OpenReview" style="display:none">
                    <span class="i-index">#176</span>
                    <a class="i-title" href="#teUg2pMrF0@OpenReview">Large Language Model-driven Large Neighborhood Search for Large-Scale MILP Problems</a>
                    <a class="i-star" onclick="toggleAppStar('teUg2pMrF0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('teUg2pMrF0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tnyxtaSve5@OpenReview" style="display:none">
                    <span class="i-index">#177</span>
                    <a class="i-title" href="#tnyxtaSve5@OpenReview">Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning</a>
                    <a class="i-star" onclick="toggleAppStar('tnyxtaSve5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tnyxtaSve5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tqL8gJsuS5@OpenReview" style="display:none">
                    <span class="i-index">#178</span>
                    <a class="i-title" href="#tqL8gJsuS5@OpenReview">Efficient Source-free Unlearning via Energy-Guided Data Synthesis and Discrimination-Aware Multitask Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('tqL8gJsuS5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tqL8gJsuS5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-u3n5wuRGTa@OpenReview" style="display:none">
                    <span class="i-index">#179</span>
                    <a class="i-title" href="#u3n5wuRGTa@OpenReview">Discovering a Zero (Zero-Vector Class of Machine Learning)</a>
                    <a class="i-star" onclick="toggleAppStar('u3n5wuRGTa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('u3n5wuRGTa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-u6xeKVHS6K@OpenReview" style="display:none">
                    <span class="i-index">#180</span>
                    <a class="i-title" href="#u6xeKVHS6K@OpenReview">GMAIL: Generative Modality Alignment for generated Image Learning</a>
                    <a class="i-star" onclick="toggleAppStar('u6xeKVHS6K@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('u6xeKVHS6K@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uqCfoVXb67@OpenReview" style="display:none">
                    <span class="i-index">#181</span>
                    <a class="i-title" href="#uqCfoVXb67@OpenReview">UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control</a>
                    <a class="i-star" onclick="toggleAppStar('uqCfoVXb67@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uqCfoVXb67@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-v4DWXM93VV@OpenReview" style="display:none">
                    <span class="i-index">#182</span>
                    <a class="i-title" href="#v4DWXM93VV@OpenReview">Convergence of Mean-Field Langevin Stochastic Descent-Ascent for Distributional Minimax Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('v4DWXM93VV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('v4DWXM93VV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vOdz3zhSCj@OpenReview" style="display:none">
                    <span class="i-index">#183</span>
                    <a class="i-title" href="#vOdz3zhSCj@OpenReview">Neural Encoding and Decoding at Scale</a>
                    <a class="i-star" onclick="toggleAppStar('vOdz3zhSCj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vOdz3zhSCj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vhACnRfuYh@OpenReview" style="display:none">
                    <span class="i-index">#184</span>
                    <a class="i-title" href="#vhACnRfuYh@OpenReview">Latent Diffusion Planning for Imitation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('vhACnRfuYh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vhACnRfuYh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-w0xYx9CJhY@OpenReview" style="display:none">
                    <span class="i-index">#185</span>
                    <a class="i-title" href="#w0xYx9CJhY@OpenReview">Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('w0xYx9CJhY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('w0xYx9CJhY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wBJIO15pBV@OpenReview" style="display:none">
                    <span class="i-index">#186</span>
                    <a class="i-title" href="#wBJIO15pBV@OpenReview">Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion</a>
                    <a class="i-star" onclick="toggleAppStar('wBJIO15pBV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wBJIO15pBV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wXfuOj9C7L@OpenReview" style="display:none">
                    <span class="i-index">#187</span>
                    <a class="i-title" href="#wXfuOj9C7L@OpenReview">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a>
                    <a class="i-star" onclick="toggleAppStar('wXfuOj9C7L@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wXfuOj9C7L@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wbvshlfyB0@OpenReview" style="display:none">
                    <span class="i-index">#188</span>
                    <a class="i-title" href="#wbvshlfyB0@OpenReview">Nonparametric Teaching for Graph Property Learners</a>
                    <a class="i-star" onclick="toggleAppStar('wbvshlfyB0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wbvshlfyB0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wiQe95BPaB@OpenReview" style="display:none">
                    <span class="i-index">#189</span>
                    <a class="i-title" href="#wiQe95BPaB@OpenReview">FlashTP: Fused, Sparsity-Aware Tensor Product for Machine Learning Interatomic Potentials</a>
                    <a class="i-star" onclick="toggleAppStar('wiQe95BPaB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wiQe95BPaB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xVBfdltHST@OpenReview" style="display:none">
                    <span class="i-index">#190</span>
                    <a class="i-title" href="#xVBfdltHST@OpenReview">Relational Invariant Learning for Robust Solvation Free Energy Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('xVBfdltHST@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xVBfdltHST@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xtlixzbcfV@OpenReview" style="display:none">
                    <span class="i-index">#191</span>
                    <a class="i-title" href="#xtlixzbcfV@OpenReview">Novelty Detection in Reinforcement Learning with World Models</a>
                    <a class="i-star" onclick="toggleAppStar('xtlixzbcfV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xtlixzbcfV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xvLVYrYQ8a@OpenReview" style="display:none">
                    <span class="i-index">#192</span>
                    <a class="i-title" href="#xvLVYrYQ8a@OpenReview">Covered Forest: Fine-grained generalization analysis of graph neural networks</a>
                    <a class="i-star" onclick="toggleAppStar('xvLVYrYQ8a@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xvLVYrYQ8a@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-yXRixu0ONY@OpenReview" style="display:none">
                    <span class="i-index">#193</span>
                    <a class="i-title" href="#yXRixu0ONY@OpenReview">P(all-atom) Is Unlocking New Path For Protein Design</a>
                    <a class="i-star" onclick="toggleAppStar('yXRixu0ONY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('yXRixu0ONY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zIGIvysR1H@OpenReview" style="display:none">
                    <span class="i-index">#194</span>
                    <a class="i-title" href="#zIGIvysR1H@OpenReview">Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development</a>
                    <a class="i-star" onclick="toggleAppStar('zIGIvysR1H@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zIGIvysR1H@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zL6ljQvPzZ@OpenReview" style="display:none">
                    <span class="i-index">#195</span>
                    <a class="i-title" href="#zL6ljQvPzZ@OpenReview">Lightweight Protocols for Distributed Private Quantile Estimation</a>
                    <a class="i-star" onclick="toggleAppStar('zL6ljQvPzZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zL6ljQvPzZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zU4VCPHYRC@OpenReview" style="display:none">
                    <span class="i-index">#196</span>
                    <a class="i-title" href="#zU4VCPHYRC@OpenReview">On the Tension between Byzantine Robustness and No-Attack Accuracy in Distributed Learning</a>
                    <a class="i-star" onclick="toggleAppStar('zU4VCPHYRC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zU4VCPHYRC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zdOGBRQEbz@OpenReview" style="display:none">
                    <span class="i-index">#197</span>
                    <a class="i-title" href="#zdOGBRQEbz@OpenReview">From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('zdOGBRQEbz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zdOGBRQEbz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-42Au7FoD8F@OpenReview" style="display:none">
                    <span class="i-index">#198</span>
                    <a class="i-title" href="#42Au7FoD8F@OpenReview">Position: Deep Learning is Not So Mysterious or Different</a>
                    <a class="i-star" onclick="toggleAppStar('42Au7FoD8F@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('42Au7FoD8F@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ACzL62Jp4E@OpenReview" style="display:none">
                    <span class="i-index">#199</span>
                    <a class="i-title" href="#ACzL62Jp4E@OpenReview">Position: In-House Evaluation Is Not Enough. Towards Robust Third-Party Evaluation and Flaw Disclosure for General-Purpose AI</a>
                    <a class="i-star" onclick="toggleAppStar('ACzL62Jp4E@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ACzL62Jp4E@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-HuvAM5x2xG@OpenReview" style="display:none">
                    <span class="i-index">#200</span>
                    <a class="i-title" href="#HuvAM5x2xG@OpenReview">Position: Formal Mathematical Reasoning—A New Frontier in AI</a>
                    <a class="i-star" onclick="toggleAppStar('HuvAM5x2xG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HuvAM5x2xG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QMgCDPWL9Y@OpenReview" style="display:none">
                    <span class="i-index">#201</span>
                    <a class="i-title" href="#QMgCDPWL9Y@OpenReview">Position: General Intelligence Requires Reward-based Pretraining</a>
                    <a class="i-star" onclick="toggleAppStar('QMgCDPWL9Y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QMgCDPWL9Y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YhZ2PY2nZa@OpenReview" style="display:none">
                    <span class="i-index">#202</span>
                    <a class="i-title" href="#YhZ2PY2nZa@OpenReview">Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints</a>
                    <a class="i-star" onclick="toggleAppStar('YhZ2PY2nZa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YhZ2PY2nZa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-asQJx56NqB@OpenReview" style="display:none">
                    <span class="i-index">#203</span>
                    <a class="i-title" href="#asQJx56NqB@OpenReview">Position: We Can’t Understand AI Using our Existing Vocabulary</a>
                    <a class="i-star" onclick="toggleAppStar('asQJx56NqB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('asQJx56NqB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eI8KegpPyX@OpenReview" style="display:none">
                    <span class="i-index">#204</span>
                    <a class="i-title" href="#eI8KegpPyX@OpenReview">Position: The Categorization of Race in ML is a Flawed Premise</a>
                    <a class="i-star" onclick="toggleAppStar('eI8KegpPyX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eI8KegpPyX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eax2ixyeQL@OpenReview" style="display:none">
                    <span class="i-index">#205</span>
                    <a class="i-title" href="#eax2ixyeQL@OpenReview">Position: We Need An Algorithmic Understanding of Generative AI</a>
                    <a class="i-star" onclick="toggleAppStar('eax2ixyeQL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eax2ixyeQL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gwhPvu97Gm@OpenReview" style="display:none">
                    <span class="i-index">#206</span>
                    <a class="i-title" href="#gwhPvu97Gm@OpenReview">Position: Human Baselines in Model Evaluations Need Rigor and Transparency (With Recommendations &amp; Reporting Checklist)</a>
                    <a class="i-star" onclick="toggleAppStar('gwhPvu97Gm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gwhPvu97Gm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mzc1KPkIMJ@OpenReview" style="display:none">
                    <span class="i-index">#207</span>
                    <a class="i-title" href="#mzc1KPkIMJ@OpenReview">Position: Algebra Unveils Deep Learning - An Invitation to Neuroalgebraic Geometry</a>
                    <a class="i-star" onclick="toggleAppStar('mzc1KPkIMJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mzc1KPkIMJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tctWi7I5wd@OpenReview" style="display:none">
                    <span class="i-index">#208</span>
                    <a class="i-title" href="#tctWi7I5wd@OpenReview">Position: Rethinking LLM Bias Probing Using Lessons from the Social Sciences</a>
                    <a class="i-star" onclick="toggleAppStar('tctWi7I5wd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tctWi7I5wd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hfLqdquVt3@OpenReview" style="display:none">
                    <span class="i-index">#209</span>
                    <a class="i-title" href="#hfLqdquVt3@OpenReview">Do Multiple Instance Learning Models Transfer?</a>
                    <a class="i-star" onclick="toggleAppStar('hfLqdquVt3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hfLqdquVt3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-92oBV5HAGl@OpenReview" style="display:none">
                    <span class="i-index">#210</span>
                    <a class="i-title" href="#92oBV5HAGl@OpenReview">Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization</a>
                    <a class="i-star" onclick="toggleAppStar('92oBV5HAGl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('92oBV5HAGl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XrCbBdycDc@OpenReview" style="display:none">
                    <span class="i-index">#211</span>
                    <a class="i-title" href="#XrCbBdycDc@OpenReview">Monte Carlo Tree Diffusion for System 2 Planning</a>
                    <a class="i-star" onclick="toggleAppStar('XrCbBdycDc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XrCbBdycDc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qLfo1sef50@OpenReview" style="display:none">
                    <span class="i-index">#212</span>
                    <a class="i-title" href="#qLfo1sef50@OpenReview">Policy-labeled Preference Learning: Is Preference Enough for RLHF?</a>
                    <a class="i-star" onclick="toggleAppStar('qLfo1sef50@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qLfo1sef50@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-IfWKVF6LfY@OpenReview" style="display:none">
                    <span class="i-index">#213</span>
                    <a class="i-title" href="#IfWKVF6LfY@OpenReview">DPO Meets PPO: Reinforced Token Optimization for RLHF</a>
                    <a class="i-star" onclick="toggleAppStar('IfWKVF6LfY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('IfWKVF6LfY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4HQaMUYWAT@OpenReview" style="display:none">
                    <span class="i-index">#214</span>
                    <a class="i-title" href="#4HQaMUYWAT@OpenReview">An Analysis for Reasoning Bias of Language Models with Small Initialization</a>
                    <a class="i-star" onclick="toggleAppStar('4HQaMUYWAT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4HQaMUYWAT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5QAKPBVdFH@OpenReview" style="display:none">
                    <span class="i-index">#215</span>
                    <a class="i-title" href="#5QAKPBVdFH@OpenReview">Hide &amp; Seek: Transformer Symmetries Obscure Sharpness &amp; Riemannian Geometry Finds It</a>
                    <a class="i-star" onclick="toggleAppStar('5QAKPBVdFH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5QAKPBVdFH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GbJqQsIwJu@OpenReview" style="display:none">
                    <span class="i-index">#216</span>
                    <a class="i-title" href="#GbJqQsIwJu@OpenReview">Learning Parametric Distributions from Samples and Preferences</a>
                    <a class="i-star" onclick="toggleAppStar('GbJqQsIwJu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GbJqQsIwJu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hrp6jRIKdX@OpenReview" style="display:none">
                    <span class="i-index">#217</span>
                    <a class="i-title" href="#Hrp6jRIKdX@OpenReview">Towards a Mechanistic Explanation of Diffusion Model Generalization</a>
                    <a class="i-star" onclick="toggleAppStar('Hrp6jRIKdX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hrp6jRIKdX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ukjl86EsIk@OpenReview" style="display:none">
                    <span class="i-index">#218</span>
                    <a class="i-title" href="#Ukjl86EsIk@OpenReview">Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents</a>
                    <a class="i-star" onclick="toggleAppStar('Ukjl86EsIk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ukjl86EsIk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Z1qZoHa6ql@OpenReview" style="display:none">
                    <span class="i-index">#219</span>
                    <a class="i-title" href="#Z1qZoHa6ql@OpenReview">Counterfactual Graphical Models: Constraints and Inference</a>
                    <a class="i-star" onclick="toggleAppStar('Z1qZoHa6ql@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Z1qZoHa6ql@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-c16m2kUTLZ@OpenReview" style="display:none">
                    <span class="i-index">#220</span>
                    <a class="i-title" href="#c16m2kUTLZ@OpenReview">No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('c16m2kUTLZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('c16m2kUTLZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-d60cmFf89H@OpenReview" style="display:none">
                    <span class="i-index">#221</span>
                    <a class="i-title" href="#d60cmFf89H@OpenReview">TabFlex: Scaling Tabular Learning to Millions with Linear Attention</a>
                    <a class="i-star" onclick="toggleAppStar('d60cmFf89H@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('d60cmFf89H@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hYHczNrKoX@OpenReview" style="display:none">
                    <span class="i-index">#222</span>
                    <a class="i-title" href="#hYHczNrKoX@OpenReview">On the Benefits of Active Data Collection in Operator Learning</a>
                    <a class="i-star" onclick="toggleAppStar('hYHczNrKoX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hYHczNrKoX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mQeZEsdODh@OpenReview" style="display:none">
                    <span class="i-index">#223</span>
                    <a class="i-title" href="#mQeZEsdODh@OpenReview">Continual Reinforcement Learning by Planning with Online World Models</a>
                    <a class="i-star" onclick="toggleAppStar('mQeZEsdODh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mQeZEsdODh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-n1cqQK4hhC@OpenReview" style="display:none">
                    <span class="i-index">#224</span>
                    <a class="i-title" href="#n1cqQK4hhC@OpenReview">STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization</a>
                    <a class="i-star" onclick="toggleAppStar('n1cqQK4hhC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('n1cqQK4hhC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vES22INUKm@OpenReview" style="display:none">
                    <span class="i-index">#225</span>
                    <a class="i-title" href="#vES22INUKm@OpenReview">An Error Analysis of Flow Matching for Deep Generative Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('vES22INUKm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vES22INUKm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Typewriter, sans-serif;"></div></div></body></html>