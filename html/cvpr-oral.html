<html><head>
    <title>CVPR.2025 - Oral | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for CVPR.2025 - Oral, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body id="venue"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">CVPR.2025 - Oral</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 95
    </p>
    <div class="papers">
        <div id="Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="panel paper" keywords="scanline,pencils,radially,distorted,camera,geometric,resection,solver,measurements,eight">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted_CVPR_2025_paper.html" target="_blank" title="1/95"><span class="index notranslate">#1</span></a>
                <a id="title-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="title-link" href="/venue/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" target="_blank">Camera Resection from Known Line Pencils and a Radially Distorted Scanline</a>
                <a id="pdf-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF">494</sup>]</a>
                <a id="copy-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF">370</sup>]</a>
                <a id="rel-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Juan C. Dibene" target="_blank">Juan C. Dibene</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enrique Dunn" target="_blank">Enrique Dunn</a>
            </p>
            <p id="summary-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="summary">We present a marker-based geometric estimation framework for the absolute pose of a camera by analyzing the 1D observations in a single radially distorted pixel scanline.We leverage a pair of known co-planar pencils of lines, along with lens distortion parameters, to propose an ensemble of solvers exploring the space of estimation strategies applicable to our setup.First, we present a minimal algebraic solver requiring only six measurements and yielding eight solutions, which relies on the intersection of two conics defined by one of the pencils of lines.Then, we present a unique closed-form geometric solver from seven measurements.Finally, we present an homography-based formulation amenable to linear least-squares from eight or more measurements.Our geometric framework constitutes a theoretical analysis on the minimum geometric context necessary to solve in closed form for the absolute pose of a single camera from a single radially distorted scanline.</p>
            <p id="subjects-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" onclick="foldPdfKimi('Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="panel paper" keywords="hallucinations,tokens,mllms,outlier,causal,farsight,attention,decoding,mitigating,propagation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention_CVPR_2025_paper.html" target="_blank" title="2/95"><span class="index notranslate">#2</span></a>
                <a id="title-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="title-link" href="/venue/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" target="_blank">Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding</a>
                <a id="pdf-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF">450</sup>]</a>
                <a id="copy-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF">236</sup>]</a>
                <a id="rel-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Feilong Tang" target="_blank">Feilong Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengzhi Liu" target="_blank">Chengzhi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongxing Xu" target="_blank">Zhongxing Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Hu" target="_blank">Ming Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zile Huang" target="_blank">Zile Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haochen Xue" target="_blank">Haochen Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyang Chen" target="_blank">Ziyang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zelin Peng" target="_blank">Zelin Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiwei Yang" target="_blank">Zhiwei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sijin Zhou" target="_blank">Sijin Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxue Li" target="_blank">Wenxue Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulong Li" target="_blank">Yulong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxuan Song" target="_blank">Wenxuan Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyan Su" target="_blank">Shiyan Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Feng" target="_blank">Wei Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jionglong Su" target="_blank">Jionglong Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingquan Lin" target="_blank">Mingquan Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Peng" target="_blank">Yifan Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuelian Cheng" target="_blank">Xuelian Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Imran Razzak" target="_blank">Imran Razzak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongyuan Ge" target="_blank">Zongyuan Ge</a>
            </p>
            <p id="summary-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="summary">Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.</p>
            <p id="subjects-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" onclick="foldPdfKimi('Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="panel paper" keywords="uniap,parallelism,intra,parallel,layer,inter,automatic,mixed,integer,quadratic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer_CVPR_2025_paper.html" target="_blank" title="3/95"><span class="index notranslate">#3</span></a>
                <a id="title-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="title-link" href="/venue/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" target="_blank">UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming</a>
                <a id="pdf-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF">165</sup>]</a>
                <a id="copy-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF">109</sup>]</a>
                <a id="rel-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Lin" target="_blank">Hao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ke Wu" target="_blank">Ke Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Li" target="_blank">Jie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Li" target="_blank">Jun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wu-Jun Li" target="_blank">Wu-Jun Li</a>
            </p>
            <p id="summary-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="summary">Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 3.80<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mo" id="MathJax-Span-3" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">\times</script> in throughput and reduces strategy optimization time by up to 107<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-5"><span class="mo" id="MathJax-Span-6" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-2">\times</script> across five Transformer-based models.</p>
            <p id="subjects-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" onclick="foldPdfKimi('Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="panel paper" keywords="remote,sensing,simfeatup,segearth,segmentation,vocabulary,images,ovss,semantic,patch">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.html" target="_blank" title="4/95"><span class="index notranslate">#4</span></a>
                <a id="title-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="title-link" href="/venue/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" target="_blank">SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images</a>
                <a id="pdf-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF">300</sup>]</a>
                <a id="copy-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF">135</sup>]</a>
                <a id="rel-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiyu Li" target="_blank">Kaiyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruixun Liu" target="_blank">Ruixun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyong Cao" target="_blank">Xiangyong Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xueru Bai" target="_blank">Xueru Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Zhou" target="_blank">Feng Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deyu Meng" target="_blank">Deyu Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhi Wang" target="_blank">Zhi Wang</a>
            </p>
            <p id="summary-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="summary">Current remote sensing semantic segmentation methods are mostly built on the close-set assumption, meaning that the model can only recognize pre-defined categories that exist in the training set. However, in practical Earth observation, there are countless unseen categories, and manual annotation is impractical. To address this challenge, we first attempt to introduce training-free open-vocabulary semantic segmentation (OVSS) into the remote sensing context. However, due to the sensitivity of remote sensing images to low-resolution features, distorted target shapes and ill-fitting boundaries are exhibited in the prediction mask. To tackle these issues, we propose a simple and universal upsampler, i.e. SimFeatUp, to restore lost spatial information of deep features. Specifically, SimFeatUp only needs to learn from a few unlabeled images, and can upsample arbitrary remote sensing image features. Furthermore, based on the observation of the abnormal response of patch tokens to the [CLS] token in CLIP, we propose to execute a simple subtraction operation to alleviate the global bias in patch tokens. Extensive experiments are conducted on 17 remote sensing datasets of 4 tasks, including semantic segmentation, building extraction, road detection, and flood detection. Our method achieves an average of 5.8\%, 8.2\%, 4.0\%, and 15.3\% improvement over state-of-the-art methods on the 4 tasks.</p>
            <p id="subjects-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" onclick="foldPdfKimi('Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="panel paper" keywords="vggn,vggt,point,hundreds,grounded,visual,geometry,forward,camera,depth">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html" target="_blank" title="5/95"><span class="index notranslate">#5</span></a>
                <a id="title-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="title-link" href="/venue/Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" target="_blank">VGGT: Visual Geometry Grounded Transformer</a>
                <a id="pdf-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF">293</sup>]</a>
                <a id="copy-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF">159</sup>]</a>
                <a id="rel-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyuan Wang" target="_blank">Jianyuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minghao Chen" target="_blank">Minghao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikita Karaev" target="_blank">Nikita Karaev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Rupprecht" target="_blank">Christian Rupprecht</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Novotny" target="_blank">David Novotny</a>
            </p>
            <p id="summary-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="summary">We present VGGN, a feed-forward neural network that infers directly all key 3D attributes of a scene, such as camera poses, point maps, depth maps, and 3D point tracks, from few or hundreds of its views. Unlike recent alternatives, VGGN does not need to use visual geometry optimization techniques to refine the results in post-processing, obtaining all quantities of interest directly. This approach is simple and more efficient, reconstructing hundreds of images in seconds. We train VGGN on a large number of publicly available datasets with 3D annotations and demonstrate its ability to achieve state-of-the-art results in multiple 3D tasks, including camera pose estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. This is a step forward in 3D computer vision, where models have been typically constrained to and specialized for single tasks. We extensively evaluate our method on unseen datasets to demonstrate its superior performance. We will release the code and trained model.</p>
            <p id="subjects-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" onclick="foldPdfKimi('Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="panel paper" keywords="biomechanically,skeleton,reconstructing,humans,pose,estimation,accurate,pseudo,benchmarks,realistic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton_CVPR_2025_paper.html" target="_blank" title="6/95"><span class="index notranslate">#6</span></a>
                <a id="title-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="title-link" href="/venue/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" target="_blank">Reconstructing Humans with a Biomechanically Accurate Skeleton</a>
                <a id="pdf-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF">213</sup>]</a>
                <a id="copy-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF">88</sup>]</a>
                <a id="rel-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Xia" target="_blank">Yan Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaowei Zhou" target="_blank">Xiaowei Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Etienne Vouga" target="_blank">Etienne Vouga</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixing Huang" target="_blank">Qixing Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgios Pavlakos" target="_blank">Georgios Pavlakos</a>
            </p>
            <p id="summary-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="summary">In this paper, we introduce a method for reconstructing humans in 3D from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to generate pseudo ground truth data and implement a training procedure that iteratively refines these pseudo labels for improved accuracy. Compared to state-of-the-art methods in 3D human pose estimation, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. This result highlights the benefits of using a biomechanical skeleton with realistic degrees of freedom for robust pose estimation. Additionally, we show that previous models frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom leading to more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We will make all code, models and data publicly available upon publication.</p>
            <p id="subjects-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" onclick="foldPdfKimi('Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="panel paper" keywords="mesh,craftsman,refiner,craftsman3d,native,interactive,mumber,geometry,roughs,fidelity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive_CVPR_2025_paper.html" target="_blank" title="7/95"><span class="index notranslate">#7</span></a>
                <a id="title-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="title-link" href="/venue/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" target="_blank">CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner</a>
                <a id="pdf-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF">171</sup>]</a>
                <a id="copy-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF">71</sup>]</a>
                <a id="rel-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weiyu Li" target="_blank">Weiyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiarui Liu" target="_blank">Jiarui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyu Yan" target="_blank">Hongyu Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Chen" target="_blank">Rui Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixun Liang" target="_blank">Yixun Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuelin Chen" target="_blank">Xuelin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Tan" target="_blank">Ping Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoxiao Long" target="_blank">Xiaoxiao Long</a>
            </p>
            <p id="summary-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="summary">We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, self-occlusion, irregular mesh topologies, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling softwares. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we first introduce a robust data preprocessing pipeline that utilizes visibility check and winding mumber to maximize the use of existing 3D data. Leveraging this data, we employ a 3D-native DiT model that directly models the distribution of 3D data in latent space, generating coarse geometries with regular mesh topology in seconds. Subsequently, a normal-based geometry refiner enhances local surface details, which can be applied automatically or interactively with user input. Extensive experiments demonstrate that our method achieves high efficacy in producing superior quality 3D assets compared to existing methods.</p>
            <p id="subjects-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" onclick="foldPdfKimi('Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="panel paper" keywords="dictionary,shape,dnf,unconditional,motion,generative,fidelity,deformable,achived,shapes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields_CVPR_2025_paper.html" target="_blank" title="8/95"><span class="index notranslate">#8</span></a>
                <a id="title-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="title-link" href="/venue/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" target="_blank">DNF: Unconditional 4D Generation with Dictionary-based Neural Fields</a>
                <a id="pdf-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF">149</sup>]</a>
                <a id="copy-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF">65</sup>]</a>
                <a id="rel-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyi Zhang" target="_blank">Xinyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naiqi Li" target="_blank">Naiqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angela Dai" target="_blank">Angela Dai</a>
            </p>
            <p id="summary-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="summary">While remarkable success has been achived through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields.Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression -- combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations.</p>
            <p id="subjects-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="panel paper" keywords="photos,raw,256p,optional,photo,accepts,reflections,images,system,consumer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kee_Removing_Reflections_from_RAW_Photos_CVPR_2025_paper.html" target="_blank" title="9/95"><span class="index notranslate">#9</span></a>
                <a id="title-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="title-link" href="/venue/Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" target="_blank">Removing Reflections from RAW Photos</a>
                <a id="pdf-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kee_Removing_Reflections_from_RAW_Photos_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF">220</sup>]</a>
                <a id="copy-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF">94</sup>]</a>
                <a id="rel-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Kee" target="_blank">Eric Kee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Pikielny" target="_blank">Adam Pikielny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Blackburn-Matzen" target="_blank">Kevin Blackburn-Matzen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Levoy" target="_blank">Marc Levoy</a>
            </p>
            <p id="summary-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="summary">We describe a system to remove real-world reflections from images for consumer photography. Our system operates on linear (RAW) photos, and accepts an optional contextual photo looking in the opposite direction (e.g., the "selfie" camera on a mobile device). This optional photo helps disambiguate what should be considered the reflection. The system is trained solely on synthetic mixtures of real-world RAW images, which we combine using a reflection simulation that is photometrically and geometrically accurate. Our system comprises a base model that accepts the captured photo and optional context photo as input, and runs at 256p, followed by an up-sampling model that transforms 256p images to full resolution. The system can produce images for review at 1K in 4.5 to 6.5 seconds on a MacBook or iPhone 14 Pro. We test on RAW photos that were captured in the field and embody typical consumer photos, and show that our RAW-image simulation yields SOTA performance.</p>
            <p id="subjects-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" onclick="foldPdfKimi('Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="panel paper" keywords="sam,segmentation,annotation,anything,preference,medical,segment,still,prompts,prompting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised_CVPR_2025_paper.html" target="_blank" title="10/95"><span class="index notranslate">#10</span></a>
                <a id="title-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="title-link" href="/venue/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" target="_blank">Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation</a>
                <a id="pdf-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF">238</sup>]</a>
                <a id="copy-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF">99</sup>]</a>
                <a id="rel-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aishik Konwer" target="_blank">Aishik Konwer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijian Yang" target="_blank">Zhijian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erhan Bas" target="_blank">Erhan Bas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cao Xiao" target="_blank">Cao Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Prasanna" target="_blank">Prateek Prasanna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Parminder Bhatia" target="_blank">Parminder Bhatia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taha Kass-Hout" target="_blank">Taha Kass-Hout</a>
            </p>
            <p id="summary-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="summary">Foundational models such as the Segment Anything Model<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mtext&gt;&amp;#xA0;&lt;/mtext&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7" style="width: 0.315em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.982em, 1000em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-8"><span class="mtext" id="MathJax-Span-9" style="font-family: MathJax_Main;">&nbsp;</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext></math></span></span><script type="math/tex" id="MathJax-Element-3">~</script>(SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks. However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts. Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth. To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering. We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process. State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios.</p>
            <p id="subjects-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" onclick="foldPdfKimi('Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="designdiffusion,text,generation,textual,visual,design,style,image,quality,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="11/95"><span class="index notranslate">#11</span></a>
                <a id="title-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" target="_blank">DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models</a>
                <a id="pdf-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF">293</sup>]</a>
                <a id="copy-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF">100</sup>]</a>
                <a id="rel-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhendong Wang" target="_blank">Zhendong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianmin Bao" target="_blank">Jianmin Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuyang Gu" target="_blank">Shuyang Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Chen" target="_blank">Dong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wengang Zhou" target="_blank">Wengang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Houqiang Li" target="_blank">Houqiang Li</a>
            </p>
            <p id="summary-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="summary">In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation.</p>
            <p id="subjects-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="panel paper" keywords="video,audio,gated,retrieval,text,avigate,representation,attention,guided,textual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.html" target="_blank" title="12/95"><span class="index notranslate">#12</span></a>
                <a id="title-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="title-link" href="/venue/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" target="_blank">Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval</a>
                <a id="pdf-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF">142</sup>]</a>
                <a id="copy-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF">65</sup>]</a>
                <a id="rel-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Boseung Jeong" target="_blank">Boseung Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jicheol Park" target="_blank">Jicheol Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sungyeon Kim" target="_blank">Sungyeon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suha Kwak" target="_blank">Suha Kwak</a>
            </p>
            <p id="summary-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="summary">Video-text retrieval, the task of retrieving videos based on a textual query or vice versa, is of paramount importance for video understanding and multimodal information retrieval. Recent methods in this area rely primarily on visual and textual features and often ignore audio, although it helps enhance overall comprehension of video content.Moreover, traditional models that incorporate audio blindly utilize the audio input regardless of whether it is useful or not, resulting in suboptimal video representation. To address these limitations, we propose a novel video-text retrieval framework, Audio-guided VIdeo representation learning with GATEd attention (AVIGATE), that effectively leverages audio cues through a gated attention mechanism that selectively filters out uninformative audio signals.In addition, we propose an adaptive margin-based contrastive loss to deal with the inherently unclear positive-negative relationship between video and text, which facilitates learning better video-text alignment.Our extensive experiments demonstrate that AVIGATE achieves state-of-the-art performance on all the public benchmarks.</p>
            <p id="subjects-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" onclick="foldPdfKimi('Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="panel paper" keywords="randar,decoder,token,generation,orders,autoregressive,visual,random,order,generatng">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders_CVPR_2025_paper.html" target="_blank" title="13/95"><span class="index notranslate">#13</span></a>
                <a id="title-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="title-link" href="/venue/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" target="_blank">RandAR: Decoder-only Autoregressive Visual Generation in Random Orders</a>
                <a id="pdf-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF">127</sup>]</a>
                <a id="copy-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF">70</sup>]</a>
                <a id="rel-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqi Pang" target="_blank">Ziqi Pang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyuan Zhang" target="_blank">Tianyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fujun Luan" target="_blank">Fujun Luan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunze Man" target="_blank">Yunze Man</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tan" target="_blank">Hao Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=William T. Freeman" target="_blank">William T. Freeman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Xiong Wang" target="_blank">Yu-Xiong Wang</a>
            </p>
            <p id="summary-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="summary">We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generatng images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enabling random order is to insert a "position instruction token" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports in-painting, outpainting and resolution extrapolation in a zero-shot manner.We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios.</p>
            <p id="subjects-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" onclick="foldPdfKimi('Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="panel paper" keywords="videos,camera,casual,dynamic,scenes,parallax,megasam,accurate,robust,motion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.html" target="_blank" title="14/95"><span class="index notranslate">#14</span></a>
                <a id="title-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="title-link" href="/venue/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" target="_blank">MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos</a>
                <a id="pdf-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF">123</sup>]</a>
                <a id="copy-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF">58</sup>]</a>
                <a id="rel-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengqi Li" target="_blank">Zhengqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Tucker" target="_blank">Richard Tucker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Forrester Cole" target="_blank">Forrester Cole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianqian Wang" target="_blank">Qianqian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linyi Jin" target="_blank">Linyi Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vickie Ye" target="_blank">Vickie Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angjoo Kanazawa" target="_blank">Angjoo Kanazawa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aleksander Holynski" target="_blank">Aleksander Holynski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Snavely" target="_blank">Noah Snavely</a>
            </p>
            <p id="summary-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="summary">We present a system that allows for accurate, fast, and robust estimation of camera parameters and depth maps from casual monocular videos of dynamic scenes. Most conventional structure from motion and monocular SLAM techniques assume input videos that feature predominantly static scenes with large amounts of parallax. Such methods tend to produce erroneous estimates in the absence of these conditions. Recent neural network based approaches attempt to overcome these challenges; however, such methods are either computationally expensive or brittle when run on dynamic videos with uncontrolled camera motion or unknown field of view. We demonstrate the surprising effectiveness of the deep visual SLAM framework, and with careful modifications to its training and inference schemes, this system can scale to real-world videos of complex dynamic scenes with unconstrained camera paths, including videos with little camera parallax. Extensive experiments on both synthetic and real videos demonstrate that our system is significantly more accurate and robust at camera pose and depth estimation when compared with prior and concurrent work, with faster or comparable running times.</p>
            <p id="subjects-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" onclick="foldPdfKimi('Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="panel paper" keywords="stereo,shot,zero,foundation,matching,foundationstereo,tuning,vision,stereoanything,depth">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wen_FoundationStereo_Zero-Shot_Stereo_Matching_CVPR_2025_paper.html" target="_blank" title="15/95"><span class="index notranslate">#15</span></a>
                <a id="title-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="title-link" href="/venue/Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" target="_blank">FoundationStereo: Zero-Shot Stereo Matching</a>
                <a id="pdf-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wen_FoundationStereo_Zero-Shot_Stereo_Matching_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF">133</sup>]</a>
                <a id="copy-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF">90</sup>]</a>
                <a id="rel-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Wen" target="_blank">Bowen Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Trepte" target="_blank">Matthew Trepte</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joseph Aribido" target="_blank">Joseph Aribido</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Kautz" target="_blank">Jan Kautz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Orazio Gallo" target="_blank">Orazio Gallo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stan Birchfield" target="_blank">Stan Birchfield</a>
            </p>
            <p id="summary-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="summary">Tremendous progress has been made in deep stereo matching to excel on benchmark datasets through per-domain fine-tuning. However, achieving strong zero-shot generalization — a hallmark of foundation models in other computer vision tasks — remains challenging for stereo matching. We introduce StereoAnything, a foundation model for stereo depth estimation designed to achieve strong zero-shot generalization. To this end, we first construct a large-scale (1M stereo pairs) synthetic training dataset featuring large diversity and high photorealism, followed by an automatic self-curation pipeline to remove ambiguous samples. We then design a number of network architecture components to enhance scalability, including a side-tuning feature backbone that adapts rich monocular priors from vision foundation models to mitigate the sim-to-real gap, and long-range context reasoning for effective cost volume filtering. Together, these components lead to strong robustness and accuracy across domains, establishing a new standard in zero-shot stereo depth estimation.</p>
            <p id="subjects-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" onclick="foldPdfKimi('Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="panel paper" keywords="motion,video,prompts,trajectories,sparse,generation,temporally,conditioning,control,prompting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories_CVPR_2025_paper.html" target="_blank" title="16/95"><span class="index notranslate">#16</span></a>
                <a id="title-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="title-link" href="/venue/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" target="_blank">Motion Prompting: Controlling Video Generation with Motion Trajectories</a>
                <a id="pdf-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF">149</sup>]</a>
                <a id="copy-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF">55</sup>]</a>
                <a id="rel-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Geng" target="_blank">Daniel Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Herrmann" target="_blank">Charles Herrmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junhwa Hur" target="_blank">Junhwa Hur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Forrester Cole" target="_blank">Forrester Cole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Serena Zhang" target="_blank">Serena Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tobias Pfaff" target="_blank">Tobias Pfaff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tatiana Lopez-Guevara" target="_blank">Tatiana Lopez-Guevara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yusuf Aytar" target="_blank">Yusuf Aytar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Rubinstein" target="_blank">Michael Rubinstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Sun" target="_blank">Chen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Wang" target="_blank">Oliver Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Owens" target="_blank">Andrew Owens</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deqing Sun" target="_blank">Deqing Sun</a>
            </p>
            <p id="summary-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="summary">Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse _or_ dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as _motion prompts_. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term _motion prompt expansion_. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance.</p>
            <p id="subjects-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" onclick="foldPdfKimi('Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="panel paper" keywords="tokenization,textok,512,dit,image,fid,256,generation,text,tokens">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zha_Language-Guided_Image_Tokenization_for_Generation_CVPR_2025_paper.html" target="_blank" title="17/95"><span class="index notranslate">#17</span></a>
                <a id="title-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="title-link" href="/venue/Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" target="_blank">Language-Guided Image Tokenization for Generation</a>
                <a id="pdf-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zha_Language-Guided_Image_Tokenization_for_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF">206</sup>]</a>
                <a id="copy-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF">84</sup>]</a>
                <a id="rel-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiwen Zha" target="_blank">Kaiwen Zha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lijun Yu" target="_blank">Lijun Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alireza Fathi" target="_blank">Alireza Fathi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David A. Ross" target="_blank">David A. Ross</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cordelia Schmid" target="_blank">Cordelia Schmid</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dina Katabi" target="_blank">Dina Katabi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiuye Gu" target="_blank">Xiuye Gu</a>
            </p>
            <p id="summary-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="summary">Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide high-level semantics. By conditioning the tokenization process on descriptive text captions, TexTok allows the tokenization process to focusing on encoding fine-grained visual details into latent tokens, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2\% and 48.1\% on ImageNet 256<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-10" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="mo" id="MathJax-Span-12" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-4">\times</script>256 and 512<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-13" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-14"><span class="mo" id="MathJax-Span-15" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-5">\times</script>512 benchmarks respectively, across varying number of tokens. These tokenization improvements consistently translate to 16.3\% and 34.3\% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve 93.5<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mo" id="MathJax-Span-18" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-6">\times</script> inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization.</p>
            <p id="subjects-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="panel paper" keywords="tokens,multimodal,timestep,diffusion,comprehension,mllms,timesteps,visual,generation,language">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens_CVPR_2025_paper.html" target="_blank" title="18/95"><span class="index notranslate">#18</span></a>
                <a id="title-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="title-link" href="/venue/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" target="_blank">Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens</a>
                <a id="pdf-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF">167</sup>]</a>
                <a id="copy-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF">73</sup>]</a>
                <a id="rel-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaihang Pan" target="_blank">Kaihang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wang Lin" target="_blank">Wang Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongqi Yue" target="_blank">Zhongqi Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tenglong Ao" target="_blank">Tenglong Ao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liyu Jia" target="_blank">Liyu Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Zhao" target="_blank">Wei Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juncheng Li" target="_blank">Juncheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siliang Tang" target="_blank">Siliang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwang Zhang" target="_blank">Hanwang Zhang</a>
            </p>
            <p id="summary-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="summary">Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation by combining LLM and diffusion models, the state-of-the-art in each task, respectively. Existing approaches rely on spatial visual tokens, where image patches are encoded and arranged according to a spatial order (e.g., raster scan). However, we show that spatial tokens lack the recursive structure inherent to languages, hence form an impossible language for LLM to master. In this paper, we build a proper visual language by leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our proposed tokens recursively compensate for the progressive attribute loss in noisy images as timesteps increase, enabling the diffusion model to reconstruct the original image at any timestep. This approach allows us to effectively integrate the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. Extensive experiments show that we achieve a new SOTA for multimodal comprehension and generation simultaneously compared with other MLLMs.</p>
            <p id="subjects-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" onclick="foldPdfKimi('Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="panel paper" keywords="object,centric,temporally,temporal,consistency,slots,videos,representations,contrasting,consistent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots_CVPR_2025_paper.html" target="_blank" title="19/95"><span class="index notranslate">#19</span></a>
                <a id="title-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="title-link" href="/venue/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" target="_blank">Temporally Consistent Object-Centric Learning by Contrasting Slots</a>
                <a id="pdf-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF">93</sup>]</a>
                <a id="copy-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF">40</sup>]</a>
                <a id="rel-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Manasyan" target="_blank">Anna Manasyan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maximilian Seitzer" target="_blank">Maximilian Seitzer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Filip Radovic" target="_blank">Filip Radovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georg Martius" target="_blank">Georg Martius</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrii Zadaianchuk" target="_blank">Andrii Zadaianchuk</a>
            </p>
            <p id="summary-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="summary">Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.</p>
            <p id="subjects-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" onclick="foldPdfKimi('Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="panel paper" keywords="brain,video,stimuli,motion,reanimating,activity,dynamic,decoded,reanimation,understanding">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli_CVPR_2025_paper.html" target="_blank" title="20/95"><span class="index notranslate">#20</span></a>
                <a id="title-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="title-link" href="/venue/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" target="_blank">Reanimating Images using Neural Representations of Dynamic Stimuli</a>
                <a id="pdf-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF">95</sup>]</a>
                <a id="copy-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF">40</sup>]</a>
                <a id="rel-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Yeung" target="_blank">Jacob Yeung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew F. Luo" target="_blank">Andrew F. Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Sarch" target="_blank">Gabriel Sarch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Margaret M. Henderson" target="_blank">Margaret M. Henderson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deva Ramanan" target="_blank">Deva Ramanan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael J. Tarr" target="_blank">Michael J. Tarr</a>
            </p>
            <p id="summary-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="summary">While computer vision models have made incredible strides in static image recognition, they still do not match human performance in tasks that require the understanding of complex, dynamic motion. This is notably true for real-world scenarios where embodied agents face complex and motion-rich environments. Our approach leverages state-of-the-art video diffusion models to decouple static image representation from motion generation, enabling us to utilize fMRI brain activity for a deeper understanding of human responses to dynamic visual stimuli. Conversely, we also demonstrate that information about the brain's representation of motion can enhance the prediction of optical flow in artificial systems. Our novel approach leads to four main findings: (1) Visual motion, represented as fine-grained, object-level resolution optical flow, can be decoded from brain activity generated by participants viewing video stimuli; (2) Video encoders outperform image-based models in predicting video-driven brain activity; (3) Brain-decoded motion signals enable realistic video reanimation based only on the initial frame of the video; and (4) We extend prior work to achieve full video decoding from video-driven brain activity. This framework advances our understanding of how the brain represents spatial and temporal information in dynamic visual scenes. Our findings demonstrate the potential of combining brain imaging with video diffusion models for developing more robust and biologically-inspired computer vision systems.</p>
            <p id="subjects-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" onclick="foldPdfKimi('Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="panel paper" keywords="task,unidd,universal,diffusion,distillation,dataset,driven,methods,specific,imagenet">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.html" target="_blank" title="21/95"><span class="index notranslate">#21</span></a>
                <a id="title-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" target="_blank">Towards Universal Dataset Distillation via Task-Driven Diffusion</a>
                <a id="pdf-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF">108</sup>]</a>
                <a id="copy-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF">44</sup>]</a>
                <a id="rel-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ding Qi" target="_blank">Ding Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Li" target="_blank">Jian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyao Gao" target="_blank">Junyao Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuguang Dou" target="_blank">Shuguang Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Tai" target="_blank">Ying Tai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianlong Hu" target="_blank">Jianlong Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zhao" target="_blank">Bo Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yabiao Wang" target="_blank">Yabiao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengjie Wang" target="_blank">Chengjie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cairong Zhao" target="_blank">Cairong Zhao</a>
            </p>
            <p id="summary-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="summary">Dataset distillation (DD) condenses key information from large-scale datasets into smaller synthetic datasets, reducing storage and computational costs for training networks. However, recent research has primarily focused on image classification tasks, with limited expansion to detection and segmentation. Two key challenges remain: (i) Task Optimization Heterogeneity, where existing methods focus on class-level information and fail to address the diverse needs of detection and segmentation and (ii) Inflexible Image Generation, where current generation methods rely on global updates for single-class targets and lack localized optimization for specific object regions.To address these challenges, we propose a universal dataset distillation framework, named UniDD, a task-driven diffusion model for diverse DD tasks, as illustrated in Fig.1. Our approach operates in two stages: Universal Task Knowledge Mining, which captures task-relevant information through task-specific proxy model training, and Universal Task-Driven Diffusion, where these proxies guide the diffusion process to generate task-specific synthetic images.Extensive experiments across ImageNet-1K, Pascal VOC, and MS COCO demonstrate that UniDD consistently outperforms state-of-the-art methods. In particular, on ImageNet-1K with IPC-10, UniDD surpasses previous diffusion-based methods by 6.1\%, while also reducing deployment costs.</p>
            <p id="subjects-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="panel paper" keywords="lvlms,position,bias,sofa,reasoning,image,middle,beginning,vision,struggle">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models_CVPR_2025_paper.html" target="_blank" title="22/95"><span class="index notranslate">#22</span></a>
                <a id="title-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="title-link" href="/venue/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" target="_blank">Identifying and Mitigating Position Bias of Multi-image Vision-Language Models</a>
                <a id="pdf-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF">134</sup>]</a>
                <a id="copy-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF">53</sup>]</a>
                <a id="rel-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Tian" target="_blank">Xinyu Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shu Zou" target="_blank">Shu Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyuan Yang" target="_blank">Zhaoyuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Zhang" target="_blank">Jing Zhang</a>
            </p>
            <p id="summary-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="summary">The evolution of Large Vision-Language Models (LVLMs) has progressed from single-image understanding to multi-image reasoning. Despite this advancement, our findings indicate that LVLMs struggle to robustly utilize information across multiple images, with predictions significantly affected by the alteration of image positions. To further explore this issue, we introduce Position-wise Question Answering (PQA), a meticulously designed task to quantify reasoning capabilities at each position. Our analysis reveals a pronounced position bias in LVLMs: open-source models excel in reasoning with images positioned later but underperform with those in the middle or at the beginning, while proprietary models like GPT-4o show improved comprehension for images at the beginning and end but struggle with those in the middle. Motivated by these insights, we propose SoFt Attention (SoFA), a simple, training-free approach that mitigates this bias by employing linear interpolation between inter-image causal attention and bidirectional counterparts. Experimental results demonstrate that SoFA effectively reduces position bias and significantly enhances the reasoning performance of existing LVLMs.</p>
            <p id="subjects-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" onclick="foldPdfKimi('Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="panel paper" keywords="vlms,molmo,pixmo,open,proprietary,textbf,weights,vision,datasets,dataset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.html" target="_blank" title="23/95"><span class="index notranslate">#23</span></a>
                <a id="title-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="title-link" href="/venue/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" target="_blank">Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</a>
                <a id="pdf-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF">126</sup>]</a>
                <a id="copy-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF">60</sup>]</a>
                <a id="rel-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matt Deitke" target="_blank">Matt Deitke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Clark" target="_blank">Christopher Clark</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangho Lee" target="_blank">Sangho Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohun Tripathi" target="_blank">Rohun Tripathi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Yang" target="_blank">Yue Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jae Sung Park" target="_blank">Jae Sung Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammadreza Salehi" target="_blank">Mohammadreza Salehi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Muennighoff" target="_blank">Niklas Muennighoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyle Lo" target="_blank">Kyle Lo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Soldaini" target="_blank">Luca Soldaini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiasen Lu" target="_blank">Jiasen Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taira Anderson" target="_blank">Taira Anderson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erin Bransom" target="_blank">Erin Bransom</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kiana Ehsani" target="_blank">Kiana Ehsani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huong Ngo" target="_blank">Huong Ngo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=YenSung Chen" target="_blank">YenSung Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ajay Patel" target="_blank">Ajay Patel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Yatskar" target="_blank">Mark Yatskar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chris Callison-Burch" target="_blank">Chris Callison-Burch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Head" target="_blank">Andrew Head</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rose Hendrix" target="_blank">Rose Hendrix</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Favyen Bastani" target="_blank">Favyen Bastani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eli VanderBilt" target="_blank">Eli VanderBilt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Lambert" target="_blank">Nathan Lambert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yvonne Chou" target="_blank">Yvonne Chou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arnavi Chheda" target="_blank">Arnavi Chheda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jenna Sparks" target="_blank">Jenna Sparks</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sam Skjonsberg" target="_blank">Sam Skjonsberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Schmitz" target="_blank">Michael Schmitz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Sarnat" target="_blank">Aaron Sarnat</a><span style="display:none">,
                <a class="author notranslate" href="https://www.google.com/search?q=Byron Bischoff" target="_blank">Byron Bischoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pete Walsh" target="_blank">Pete Walsh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chris Newell" target="_blank">Chris Newell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Piper Wolters" target="_blank">Piper Wolters</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tanmay Gupta" target="_blank">Tanmay Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuo-Hao Zeng" target="_blank">Kuo-Hao Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jon Borchardt" target="_blank">Jon Borchardt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dirk Groeneveld" target="_blank">Dirk Groeneveld</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Crystal Nam" target="_blank">Crystal Nam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sophie Lebrecht" target="_blank">Sophie Lebrecht</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caitlin Wittlif" target="_blank">Caitlin Wittlif</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carissa Schoenick" target="_blank">Carissa Schoenick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oscar Michel" target="_blank">Oscar Michel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ranjay Krishna" target="_blank">Ranjay Krishna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Weihs" target="_blank">Luca Weihs</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah A. Smith" target="_blank">Noah A. Smith</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hannaneh Hajishirzi" target="_blank">Hannaneh Hajishirzi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ross Girshick" target="_blank">Ross Girshick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Farhadi" target="_blank">Ali Farhadi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aniruddha Kembhavi" target="_blank">Aniruddha Kembhavi</a></span>
                <a class="notranslate" onclick="toggleAuthorList(this, 'et al. (20 additional authors not shown)')">et al. (20 additional authors not shown)</a>
            </p>
            <p id="summary-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="summary">Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present \textbf{Molmo}, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets, including a dataset of highly detailed image captions for pre-training called \textbf{PixMo}, a free-form image Q\&amp;A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code will all be released.</p>
            <p id="subjects-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" onclick="foldPdfKimi('Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="panel paper" keywords="panaf,chimpanzee,behaviour,fgbg,camera,wildlife,backgrounds,video,dataset,impact">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife_CVPR_2025_paper.html" target="_blank" title="24/95"><span class="index notranslate">#24</span></a>
                <a id="title-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="title-link" href="/venue/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" target="_blank">The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition</a>
                <a id="pdf-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF">61</sup>]</a>
                <a id="copy-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF">23</sup>]</a>
                <a id="rel-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Otto Brookes" target="_blank">Otto Brookes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maksim Kukushkin" target="_blank">Maksim Kukushkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Majid Mirmehdi" target="_blank">Majid Mirmehdi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Colleen Stephens" target="_blank">Colleen Stephens</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paula Dieguez" target="_blank">Paula Dieguez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thurston C. Hicks" target="_blank">Thurston C. Hicks</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sorrel Jones" target="_blank">Sorrel Jones</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Lee" target="_blank">Kevin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maureen S. McCarthy" target="_blank">Maureen S. McCarthy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amelia Meier" target="_blank">Amelia Meier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emmanuelle Normand" target="_blank">Emmanuelle Normand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erin G. Wessling" target="_blank">Erin G. Wessling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roman M. Wittig" target="_blank">Roman M. Wittig</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Langergraber" target="_blank">Kevin Langergraber</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Klaus Zuberbühler" target="_blank">Klaus Zuberbühler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lukas Boesch" target="_blank">Lukas Boesch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Schmid" target="_blank">Thomas Schmid</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mimi Arandjelovic" target="_blank">Mimi Arandjelovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hjalmar Kühl" target="_blank">Hjalmar Kühl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tilo Burghardt" target="_blank">Tilo Burghardt</a>
            </p>
            <p id="summary-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="summary">Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42\% mAP for convolutional and +3.75\% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos). The full dataset, baseline models, and weights will be available at `anonymous'.</p>
            <p id="subjects-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" onclick="foldPdfKimi('Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="panel paper" keywords="lasers,passive,mhz,camera,pulsed,opportunistic,light,ambient,laser,scene">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Nousias_Opportunistic_Single-Photon_Time_of_Flight_CVPR_2025_paper.html" target="_blank" title="25/95"><span class="index notranslate">#25</span></a>
                <a id="title-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="title-link" href="/venue/Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" target="_blank">Opportunistic Single-Photon Time of Flight</a>
                <a id="pdf-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Nousias_Opportunistic_Single-Photon_Time_of_Flight_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF">68</sup>]</a>
                <a id="copy-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF">30</sup>]</a>
                <a id="rel-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sotiris Nousias" target="_blank">Sotiris Nousias</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mian Wei" target="_blank">Mian Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Howard Xiao" target="_blank">Howard Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maxx Wu" target="_blank">Maxx Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shahmeer Athar" target="_blank">Shahmeer Athar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin J. Wang" target="_blank">Kevin J. Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anagh Malik" target="_blank">Anagh Malik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David A. Barmherzig" target="_blank">David A. Barmherzig</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David B. Lindell" target="_blank">David B. Lindell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyros N. Kutulakos" target="_blank">Kyros N. Kutulakos</a>
            </p>
            <p id="summary-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="summary">Scattered light from pulsed lasers is increasingly part of our ambient illumination, as many devices rely on them for active 3D sensing. In this work, we ask: can these “ambient” light signals be detected and leveraged for passive 3D vision? We show that pulsed lasers, despite being weak and fluctuating at MHz to GHz frequencies, leave a distinctive sinc comb pattern in the temporal frequency domain of incident flux that is specific to each laser and invariant to the scene. This enables their passive detection and analysis with a free-running SPAD camera, even when they are unknown, asynchronous, out of sight, and emitting concurrently. We show how to synchronize with such lasers computationally, characterize their pulse emissions, separate their contributions, and—if many are present—localize them in 3D and recover a depth map of the camera’s field of view. We use our camera prototype to demonstrate (1) a first-of-its-kind visualization of asynchronously propagating light pulses from multiple lasers through the same scene, (2) passive estimation of a laser’s MHz-scale pulse repetition frequency with mHz precision, and (3) mm-scale 3D imaging over room-scale distances by passively harvesting photons from two or more out-of-view lasers.</p>
            <p id="subjects-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" onclick="foldPdfKimi('Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
    <div id="Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="panel paper" keywords="rendering,renderer,diffusion,inverse,video,world,buffers,videos,real,lighting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion_CVPR_2025_paper.html" target="_blank" title="26/95"><span class="index notranslate">#26</span></a>
                <a id="title-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" target="_blank">Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models</a>
                <a id="pdf-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF">99</sup>]</a>
                <a id="copy-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF">38</sup>]</a>
                <a id="rel-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruofan Liang" target="_blank">Ruofan Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zan Gojcic" target="_blank">Zan Gojcic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Ling" target="_blank">Huan Ling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Munkberg" target="_blank">Jacob Munkberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jon Hasselgren" target="_blank">Jon Hasselgren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chih-Hao Lin" target="_blank">Chih-Hao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Gao" target="_blank">Jun Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Keller" target="_blank">Alexander Keller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nandita Vijaykumar" target="_blank">Nandita Vijaykumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanja Fidler" target="_blank">Sanja Fidler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zian Wang" target="_blank">Zian Wang</a>
            </p>
            <p id="summary-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="summary">Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations--explicit 3D geometry, high-quality material properties, and lighting conditions--that are often impractical to obtain in real-world scenarios. Therefore, we introduce Diffusion Renderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Specifically, we first train a video diffusion model for inverse rendering on synthetic data, which generalizes well to real-world videos and allows us to auto-label diverse real-world videos. We then co-train our rendering model using both synthetic and auto-labeled real-world data. Experiments demonstrate that Diffusion Renderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input—including relighting, material editing, and realistic object insertion.</p>
            <p id="subjects-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="panel paper" keywords="light,rendering,propagating,indirect,captured,inverse,radiance,neural,relighting,arriving">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.html" target="_blank" title="27/95"><span class="index notranslate">#27</span></a>
                <a id="title-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="title-link" href="/venue/Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" target="_blank">Neural Inverse Rendering from Propagating Light</a>
                <a id="pdf-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF">74</sup>]</a>
                <a id="copy-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF">28</sup>]</a>
                <a id="rel-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anagh Malik" target="_blank">Anagh Malik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Attal" target="_blank">Benjamin Attal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Xie" target="_blank">Andrew Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew O'Toole" target="_blank">Matthew O'Toole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David B. Lindell" target="_blank">David B. Lindell</a>
            </p>
            <p id="summary-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="summary">We present the first system for physically based, neural inverse rendering from multi-viewpoint videos of propagating light. Our approach relies on a time-resolved extension of neural radiance caching --- a technique that accelerates inverse rendering by storing infinite-bounce radiance arriving at any point from any direction. The resulting model accurately accounts for direct and indirect light transport effects and, when applied to captured measurements from a flash lidar system, enables state-of-the-art 3D reconstruction in the presence of strong indirect light. Further, we demonstrate view synthesis of propagating light, automatic decomposition of captured measurements into direct and indirect components, as well as novel capabilities such as multi-view transient relighting of captured scenes.</p>
            <p id="subjects-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" onclick="foldPdfKimi('Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="panel paper" keywords="interleaved,generation,opening,intjudge,multimodal,ended,judge,benchmark,text,judging">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation_CVPR_2025_paper.html" target="_blank" title="28/95"><span class="index notranslate">#28</span></a>
                <a id="title-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="title-link" href="/venue/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" target="_blank">OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation</a>
                <a id="pdf-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF">63</sup>]</a>
                <a id="copy-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF">30</sup>]</a>
                <a id="rel-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Zhou" target="_blank">Pengfei Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaopeng Peng" target="_blank">Xiaopeng Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Song" target="_blank">Jiajun Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuanhao Li" target="_blank">Chuanhao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaopan Xu" target="_blank">Zhaopan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Yang" target="_blank">Yue Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyao Guo" target="_blank">Ziyao Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhang" target="_blank">Hao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqi Lin" target="_blank">Yuqi Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yefei He" target="_blank">Yefei He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lirui Zhao" target="_blank">Lirui Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuo Liu" target="_blank">Shuo Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianhua Li" target="_blank">Tianhua Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Xie" target="_blank">Yuxuan Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojun Chang" target="_blank">Xiaojun Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Qiao" target="_blank">Yu Qiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Shao" target="_blank">Wenqi Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaipeng Zhang" target="_blank">Kaipeng Zhang</a>
            </p>
            <p id="summary-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="summary">Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce OpenING, a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The benchmark, code and judge models will be released.</p>
            <p id="subjects-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="panel paper" keywords="custany,customization,object,customizing,idc,anything,general,text,dataset,shot">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kong_CustAny_Customizing_Anything_from_A_Single_Example_CVPR_2025_paper.html" target="_blank" title="29/95"><span class="index notranslate">#29</span></a>
                <a id="title-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="title-link" href="/venue/Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" target="_blank">CustAny: Customizing Anything from A Single Example</a>
                <a id="pdf-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kong_CustAny_Customizing_Anything_from_A_Single_Example_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF">100</sup>]</a>
                <a id="copy-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF">30</sup>]</a>
                <a id="rel-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lingjie Kong" target="_blank">Lingjie Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Wu" target="_blank">Kai Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengming Xu" target="_blank">Chengming Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaobin Hu" target="_blank">Xiaobin Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhui Han" target="_blank">Wenhui Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinlong Peng" target="_blank">Jinlong Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Donghao Luo" target="_blank">Donghao Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengtian Li" target="_blank">Mengtian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangning Zhang" target="_blank">Jiangning Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengjie Wang" target="_blank">Chengjie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Fu" target="_blank">Yanwei Fu</a>
            </p>
            <p id="summary-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="summary">Recent advances in diffusion-based text-to-image models have simplified creating high-fidelity images, but preserving the identity (ID) of specific elements, like a personal dog, is still challenging.Object customization, using reference images and textual descriptions, is key to addressing this issue. Current object customization methods are either object-specific, requiring extensive fine-tuning, or object-agnostic, offering zero-shot customization but limited to specialized domains. The primary issue of promoting zero-shot object customization from specific domains to the general domain is to establish a large-scale general ID dataset for model pre-training, which is time-consuming and labor-intensive. In this paper, we propose a novel pipeline to construct a large dataset of general objects and build the Multi-Category ID-Consistent (MC-IDC) dataset, featuring 315k text-image samples across 10k categories. With the help of MC-IDC, we introduce Customizing Anything (CustAny), a zero-shot framework that maintains ID fidelity and supports flexible text editing for general objects. CustAny features three key components: a general ID extraction module, a dual-level ID injection module, and an ID-aware decoupling module, allowing it to customize any object from a single reference image and text prompt. Experiments demonstrate that CustAny outperforms existing methods in both general object customization and specialized domains like human customization and virtual try-on. Our contributions include a large-scale dataset, the CustAny framework and novel ID processing to advance this field.</p>
            <p id="subjects-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" onclick="foldPdfKimi('Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="panel paper" keywords="intrinsics,rose,temporal,object,blooming,birth,death,lifespan,reflectance,animation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Birth_and_Death_of_a_Rose_CVPR_2025_paper.html" target="_blank" title="30/95"><span class="index notranslate">#30</span></a>
                <a id="title-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="title-link" href="/venue/Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" target="_blank">Birth and Death of a Rose</a>
                <a id="pdf-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Geng_Birth_and_Death_of_a_Rose_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF">164</sup>]</a>
                <a id="copy-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF">67</sup>]</a>
                <a id="rel-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Geng" target="_blank">Chen Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunzhi Zhang" target="_blank">Yunzhi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangzhe Wu" target="_blank">Shangzhe Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>
            </p>
            <p id="summary-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="summary">We study the problem of generating temporal object intrinsics—temporally evolving sequences of object geometry, reflectance, and texture, such as a blooming rose—from pre-trained 2D foundation models. Unlike conventional 3D modeling and animation techniques that require extensive manual effort and expertise, we introduce a method that generates such assets with signals distilled from pretrained 2D diffusion models. To ensure the temporal consistency of object intrinsics, we propose Neural Templates for temporal-state-guided distillation, derived automatically from image features from self-supervised learning. Our method can generate high-quality temporal object intrinsics for several natural phenomena and enable the sampling and controllable rendering of these dynamic objects from any viewpoint, under any environmental lighting conditions, at any time of their lifespan.</p>
            <p id="subjects-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" onclick="foldPdfKimi('Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="panel paper" keywords="motion,control,video,noise,warped,temporal,controllable,diffusion,warping,gaussianity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise_CVPR_2025_paper.html" target="_blank" title="31/95"><span class="index notranslate">#31</span></a>
                <a id="title-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="title-link" href="/venue/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" target="_blank">Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</a>
                <a id="pdf-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF">81</sup>]</a>
                <a id="copy-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF">27</sup>]</a>
                <a id="rel-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Burgert" target="_blank">Ryan Burgert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuancheng Xu" target="_blank">Yuancheng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Xian" target="_blank">Wenqi Xian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Pilarski" target="_blank">Oliver Pilarski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Clausen" target="_blank">Pascal Clausen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingming He" target="_blank">Mingming He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Ma" target="_blank">Li Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yitong Deng" target="_blank">Yitong Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingxiao Li" target="_blank">Lingxiao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohsen Mousavi" target="_blank">Mohsen Mousavi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Ryoo" target="_blank">Michael Ryoo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Debevec" target="_blank">Paul Debevec</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ning Yu" target="_blank">Ning Yu</a>
            </p>
            <p id="summary-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="summary">Generative modeling aims to transform chaotic noise into structured outputs that align with training data distributions. In this work, we enhance video diffusion generative models by introducing motion control as a structured component within latent space sampling. Specifically, we propose a novel real-time noise warping method that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, enabling fine-grained motion control independent of model architecture and guidance type. We fine-tune modern video diffusion base models and provide a unified paradigm for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. By leveraging a real-time noise-warping algorithm that preserves spatial Gaussianity while efficiently maintaining temporal consistency, we enable flexible and diverse motion control applications with minimal trade-offs in pixel quality and temporal coherence. Extensive experiments and user studies demonstrate the advantages of our method in terms of visual quality, motion controllability, and temporal consistency, making it a robust and scalable solution for motion-controllable video synthesis.</p>
            <p id="subjects-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" onclick="foldPdfKimi('Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="watermarks,watermark,forgery,attacks,latent,watermarked,semantic,watermarking,attackers,unrelated">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="32/95"><span class="index notranslate">#32</span></a>
                <a id="title-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" target="_blank">Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models</a>
                <a id="pdf-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF">62</sup>]</a>
                <a id="copy-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF">36</sup>]</a>
                <a id="rel-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Müller" target="_blank">Andreas Müller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Denis Lukovnikov" target="_blank">Denis Lukovnikov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Thietke" target="_blank">Jonas Thietke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Asja Fischer" target="_blank">Asja Fischer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erwin Quiring" target="_blank">Erwin Quiring</a>
            </p>
            <p id="summary-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="summary">Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions.</p>
            <p id="subjects-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="panel paper" keywords="pointmaps,state,online,reconstruction,raymap,video,perception,persistent,stateful,accepting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Continuous_3D_Perception_Model_with_Persistent_State_CVPR_2025_paper.html" target="_blank" title="33/95"><span class="index notranslate">#33</span></a>
                <a id="title-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="title-link" href="/venue/Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" target="_blank">Continuous 3D Perception Model with Persistent State</a>
                <a id="pdf-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Continuous_3D_Perception_Model_with_Persistent_State_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF">73</sup>]</a>
                <a id="copy-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF">32</sup>]</a>
                <a id="rel-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qianqian Wang" target="_blank">Qianqian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Zhang" target="_blank">Yifei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aleksander Holynski" target="_blank">Aleksander Holynski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexei A. Efros" target="_blank">Alexei A. Efros</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angjoo Kanazawa" target="_blank">Angjoo Kanazawa</a>
            </p>
            <p id="summary-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="summary">We propose a novel unified framework capable of solving a broad range of 3D tasks. At the core of our approach is an online stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, our method leverages the evolving state to generate metric-scale pointmaps for each input in an online manner. These pointmaps reside within a common coordinate system, accumulating into a coherent 3D scene reconstruction. Our model captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen structures beyond the coverage of the input images through a raymap probe. Our method is simple yet highly flexible, naturally accepting varying lengths of image sequences and working seamlessly with both video streams and unordered photo collections. We evaluate our method on various 3D/4D tasks including monocular/video depth estimation, camera estimation, multi-view reconstruction, and achieve competitive or state-of-the-art performance. Additionally, we showcase intriguing behaviors enabled by our state representation.</p>
            <p id="subjects-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="panel paper" keywords="editing,anyedit,quality,instruction,image,instructions,mastering,unified,comprehensive,diversity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea_CVPR_2025_paper.html" target="_blank" title="34/95"><span class="index notranslate">#34</span></a>
                <a id="title-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="title-link" href="/venue/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" target="_blank">AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea</a>
                <a id="pdf-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF">106</sup>]</a>
                <a id="copy-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF">41</sup>]</a>
                <a id="rel-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qifan Yu" target="_blank">Qifan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Chow" target="_blank">Wei Chow</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongqi Yue" target="_blank">Zhongqi Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaihang Pan" target="_blank">Kaihang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Wu" target="_blank">Yang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyang Wan" target="_blank">Xiaoyang Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juncheng Li" target="_blank">Juncheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siliang Tang" target="_blank">Siliang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwang Zhang" target="_blank">Hanwang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueting Zhuang" target="_blank">Yueting Zhuang</a>
            </p>
            <p id="summary-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="summary">Instruction-based image editing aims to modify specific image elements with natural language instructions. However, current models in this domain often struggle to accurately execute complex user instructions, as they are trained on low-quality data with limited editing types. We present AnyEdit, a comprehensive multi-modal instruction editing dataset, comprising 2.5 million high-quality editing pairs spanning over 20 editing types and five domains. We ensure the diversity and quality of the AnyEdit collection through three aspects: initial data diversity, adaptive editing process, and automated selection of editing results. Using the dataset, we further train a novel AnyEdit Stable Diffusion with task-aware routing and learnable task embedding for unified image editing. Comprehensive experiments on three benchmark datasets show that AnyEdit consistently boosts the performance of diffusion-based editing models. This presents prospects for developing instruction-driven image editing models that support human creativity. The code is available in \url{https://anonymous.4open.science/r/AnyEdit-C53B}.</p>
            <p id="subjects-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" onclick="foldPdfKimi('Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="panel paper" keywords="libragrad,gradient,imbalances,universally,faithfulness,fullgrad,attribution,attributions,transformer,struggle">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions_CVPR_2025_paper.html" target="_blank" title="35/95"><span class="index notranslate">#35</span></a>
                <a id="title-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="title-link" href="/venue/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" target="_blank">LibraGrad: Balancing Gradient Flow for Universally Better Vision Transformer Attributions</a>
                <a id="pdf-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF">48</sup>]</a>
                <a id="copy-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF">29</sup>]</a>
                <a id="rel-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Faridoun Mehri" target="_blank">Faridoun Mehri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahdieh Soleymani Baghshah" target="_blank">Mahdieh Soleymani Baghshah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Taher Pilehvar" target="_blank">Mohammad Taher Pilehvar</a>
            </p>
            <p id="summary-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="summary">Why do gradient-based explanations struggle with Transformers, and how can we improve them? We identify gradient flow imbalances in Transformers that violate FullGrad-completeness, a critical property for attribution faithfulness that CNNs naturally possess. To address this issue, we introduce LibraGrad—a theoretically grounded post-hoc approach that corrects gradient imbalances through pruning and scaling of backward paths, without changing the forward pass or adding computational overhead. We evaluate LibraGrad using three metric families: Faithfulness, which quantifies prediction changes under perturbations of the most and least relevant features; Completeness Error, which measures attribution conservation relative to model outputs; and Segmentation AP, which assesses alignment with human perception. Extensive experiments across 8 architectures, 4 model sizes, and 4 datasets show that LibraGrad universally enhances gradient-based methods, outperforming existing white-box methods—including Transformer-specific approaches—across all metrics. We demonstrate superior qualitative results through two complementary evaluations: precise text-prompted region highlighting on CLIP models and accurate class discrimination between co-occurring animals on ImageNet-finetuned models—two settings on which existing methods often struggle. LibraGrad is effective even on the attention-free MLP-Mixer architecture, indicating potential for extension to other modern architectures. Our code is freely available.</p>
            <p id="subjects-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" onclick="foldPdfKimi('Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="panel paper" keywords="splatting,scooping,3dgs,sss,student,new,mixture,spiked,unnormalized,component">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_3D_Student_Splatting_and_Scooping_CVPR_2025_paper.html" target="_blank" title="36/95"><span class="index notranslate">#36</span></a>
                <a id="title-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="title-link" href="/venue/Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" target="_blank">3D Student Splatting and Scooping</a>
                <a id="pdf-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_3D_Student_Splatting_and_Scooping_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF">99</sup>]</a>
                <a id="copy-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF">40</sup>]</a>
                <a id="rel-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jialin Zhu" target="_blank">Jialin Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangbei Yue" target="_blank">Jiangbei Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feixiang He" target="_blank">Feixiang He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Wang" target="_blank">He Wang</a>
            </p>
            <p id="summary-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="summary">Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student's t distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as 82%.</p>
            <p id="subjects-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" onclick="foldPdfKimi('Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="panel paper" keywords="annealing,daps,inverse,diffusion,decoupled,sampling,noise,improving,complicated,posterior">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing_CVPR_2025_paper.html" target="_blank" title="37/95"><span class="index notranslate">#37</span></a>
                <a id="title-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" target="_blank">Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing</a>
                <a id="pdf-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF">80</sup>]</a>
                <a id="copy-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF">31</sup>]</a>
                <a id="rel-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bingliang Zhang" target="_blank">Bingliang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenda Chu" target="_blank">Wenda Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julius Berner" target="_blank">Julius Berner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenlin Meng" target="_blank">Chenlin Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anima Anandkumar" target="_blank">Anima Anandkumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Song" target="_blank">Yang Song</a>
            </p>
            <p id="summary-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="summary">Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems.</p>
            <p id="subjects-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="panel paper" keywords="features,noise,diffusion,cleandift,semantic,wide,downstream,remedied,tasks,images">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Stracke_CleanDIFT_Diffusion_Features_without_Noise_CVPR_2025_paper.html" target="_blank" title="38/95"><span class="index notranslate">#38</span></a>
                <a id="title-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="title-link" href="/venue/Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" target="_blank">CleanDIFT: Diffusion Features without Noise</a>
                <a id="pdf-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Stracke_CleanDIFT_Diffusion_Features_without_Noise_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF">101</sup>]</a>
                <a id="copy-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF">33</sup>]</a>
                <a id="rel-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nick Stracke" target="_blank">Nick Stracke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Andreas Baumann" target="_blank">Stefan Andreas Baumann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kolja Bauer" target="_blank">Kolja Bauer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Frank Fundel" target="_blank">Frank Fundel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Björn Ommer" target="_blank">Björn Ommer</a>
            </p>
            <p id="summary-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="summary">Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.</p>
            <p id="subjects-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" onclick="foldPdfKimi('Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="panel paper" keywords="portrait,avatars,cap4d,avatar,morphable,view,reconstruction,reference,animate,multi">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion_CVPR_2025_paper.html" target="_blank" title="39/95"><span class="index notranslate">#39</span></a>
                <a id="title-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" target="_blank">CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</a>
                <a id="pdf-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF">52</sup>]</a>
                <a id="copy-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF">19</sup>]</a>
                <a id="rel-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Taubner" target="_blank">Felix Taubner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruihang Zhang" target="_blank">Ruihang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mathieu Tuli" target="_blank">Mathieu Tuli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David B. Lindell" target="_blank">David B. Lindell</a>
            </p>
            <p id="summary-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="summary">Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints — for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques.</p>
            <p id="subjects-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="panel paper" keywords="sea,icediff,ice,forecasting,arctic,25km,resolution,concentration,finer,utilized">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with_CVPR_2025_paper.html" target="_blank" title="40/95"><span class="index notranslate">#40</span></a>
                <a id="title-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="title-link" href="/venue/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" target="_blank">IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior</a>
                <a id="pdf-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF">54</sup>]</a>
                <a id="copy-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF">25</sup>]</a>
                <a id="rel-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Xu" target="_blank">Jingyi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Tu" target="_blank">Siwei Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weidong Yang" target="_blank">Weidong Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Fei" target="_blank">Ben Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhao Li" target="_blank">Shuhao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keyi Liu" target="_blank">Keyi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yeqi Luo" target="_blank">Yeqi Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lipeng Ma" target="_blank">Lipeng Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Bai" target="_blank">Lei Bai</a>
            </p>
            <p id="summary-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="summary">Variation of Arctic sea ice has significant impacts on polar ecosystems, transporting routes, coastal communities, and global climate. Tracing the change of sea ice at a finer scale is paramount for both operational applications and scientific studies. Recent pan-Arctic sea ice forecasting methods that leverage advances in artificial intelligence have made promising progress over numerical models. However, forecasting sea ice at higher resolutions is still under-explored. To bridge the gap, we propose a two-module cooperative deep learning framework, IceDiff, to forecast sea ice concentration at finer scales. IceDiff first leverages a vision transformer to generate coarse yet superior forecasting results over previous methods at a regular 25km grid. This high-quality sea ice forecasting can be utilized as reliable guidance for the next module. Subsequently, an unconditional diffusion model pre-trained on low-resolution sea ice concentration maps is utilized for sampling down-scaled sea ice forecasting via a zero-shot guided sampling strategy and a patch-based method. For the first time, IceDiff demonstrates sea ice forecasting with a 6.25km resolution. IceDiff extends the boundary of existing sea ice forecasting models and more importantly, its capability to generate high-resolution sea ice concentration data is vital for pragmatic usages and research.</p>
            <p id="subjects-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" onclick="foldPdfKimi('Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="panel paper" keywords="alignment,video,fsar,team,matching,temporal,action,shot,units,flexibility">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.html" target="_blank" title="41/95"><span class="index notranslate">#41</span></a>
                <a id="title-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="title-link" href="/venue/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" target="_blank">Temporal Alignment-Free Video Matching for Few-shot Action Recognition</a>
                <a id="pdf-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF">69</sup>]</a>
                <a id="copy-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF">25</sup>]</a>
                <a id="rel-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=SuBeen Lee" target="_blank">SuBeen Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=WonJun Moon" target="_blank">WonJun Moon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyun Seok Seong" target="_blank">Hyun Seok Seong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jae-Pil Heo" target="_blank">Jae-Pil Heo</a>
            </p>
            <p id="summary-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="summary">Few-Shot Action Recognition (FSAR) aims to train a model with only a few labeled video instances.A key challenge in FSAR is handling divergent narrative trajectories for precise video matching.While the frame- and tuple-level alignment approaches have been promising, their methods heavily rely on pre-defined and length-dependent alignment units (e.g., frames or tuples), which limits flexibility for actions of varying lengths and speeds. In this work, we introduce a novel TEmporal Alignment-free Matching (TEAM) approach, which eliminates the need for temporal units in action representation and brute-force alignment during matching.Specifically, TEAM represents each video with a fixed set of pattern tokens that capture globally discriminative clues within the video instance regardless of action length or speed, ensuring its flexibility.Furthermore, TEAM is inherently efficient, using token-wise comparisons to measure similarity between videos, unlike existing methods that rely on pairwise comparisons for temporal alignment. Additionally, we propose an adaptation process that identifies and removes common information across novel classes, establishing clear boundaries even between novel categories. Extensive experiments demonstrate the effectiveness of TEAM.</p>
            <p id="subjects-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" onclick="foldPdfKimi('Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="panel paper" keywords="cyclic,symmetry,algorithms,gromov,wasserstein,gradient,registration,problem,iteration,matching">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry_CVPR_2025_paper.html" target="_blank" title="42/95"><span class="index notranslate">#42</span></a>
                <a id="title-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="title-link" href="/venue/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" target="_blank">Gromov-Wasserstein Problem with Cyclic Symmetry</a>
                <a id="pdf-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF">33</sup>]</a>
                <a id="copy-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF">12</sup>]</a>
                <a id="rel-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shoichiro Takeda" target="_blank">Shoichiro Takeda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yasunori Akagi" target="_blank">Yasunori Akagi</a>
            </p>
            <p id="summary-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="summary">We propose novel fast algorithms for the Gromov–Wasserstein problem (GW) using cyclic symmetry of input data. Such GW with cyclic symmetry naturally appears as an object matching task underlying various real-world computer vision applications, e.g., image registration, point cloud registration, stereo matching, and 3D reconstruction. Gradient-based algorithms have been used to solve GW, and our main idea is to use the following remarkable and non-trivial property: By setting the initial solution to have cyclic symmetry, all intermediate solutions and matrices appearing in the gradient-based algorithms have the same cyclic symmetry until convergence. Based on this property, our gradient-based algorithms restrict the solution space to have cyclic symmetry and update only one of the symmetric parts of solutions and matrices at each iteration, which results in fast computation. Furthermore, the original gradient-based algorithms and ours must solve the Optimal Transport problem (OT) at each iteration, but only in ours does this problem exhibit cyclic symmetry. This cyclic OT can be solved efficiently, and as a result, the total computational time of our algorithms is dramatically faster than the original ones. Experiments showed the effectiveness of our algorithms in synthetic and real-world data with strict and approximate cyclic symmetry, respectively.</p>
            <p id="subjects-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" onclick="foldPdfKimi('Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="panel paper" keywords="bitwise,infinity,autoregressive,1024,vocabulary,sd3,image,refactors,visual,unleashes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis_CVPR_2025_paper.html" target="_blank" title="43/95"><span class="index notranslate">#43</span></a>
                <a id="title-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="title-link" href="/venue/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" target="_blank">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</a>
                <a id="pdf-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF">62</sup>]</a>
                <a id="copy-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF">27</sup>]</a>
                <a id="rel-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Han" target="_blank">Jian Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinlai Liu" target="_blank">Jinlai Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Jiang" target="_blank">Yi Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Yan" target="_blank">Bin Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqi Zhang" target="_blank">Yuqi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zehuan Yuan" target="_blank">Zehuan Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingyue Peng" target="_blank">Bingyue Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaobing Liu" target="_blank">Xiaobing Liu</a>
            </p>
            <p id="summary-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="summary">We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity refactors visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary classifier and bitwise self-correction mechanism. By theoretically expanding the tokenizer vocabulary size to infinity in Transformer, our method significantly unleashes powerful scaling capabilities to infinity compared to vanilla VAR. Extensive experiments indicate Infinity outperforms AutoRegressive Text-to-Image models by large margins, matches or surpasses leading diffusion models. Without extra optimization, Infinity generates a 1024<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="mo" id="MathJax-Span-21" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-7">\times</script>1024 image in 0.8s, 2.6<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-22" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-23"><span class="mo" id="MathJax-Span-24" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-8">\times</script> faster than SD3-Medium, making it the fastest Text-to-Image model. Models and codes will be released to promote the further exploration of Infinity for visual generation.</p>
            <p id="subjects-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" onclick="foldPdfKimi('Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="panel paper" keywords="dust3r,view,views,reconstruction,reference,pointmaps,mast3r,stage,scene,sparse">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds_CVPR_2025_paper.html" target="_blank" title="44/95"><span class="index notranslate">#44</span></a>
                <a id="title-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="title-link" href="/venue/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" target="_blank">MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds</a>
                <a id="pdf-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF">59</sup>]</a>
                <a id="copy-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF">22</sup>]</a>
                <a id="rel-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenggang Tang" target="_blank">Zhenggang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchen Fan" target="_blank">Yuchen Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dilin Wang" target="_blank">Dilin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyu Xu" target="_blank">Hongyu Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rakesh Ranjan" target="_blank">Rakesh Ranjan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Schwing" target="_blank">Alexander Schwing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhicheng Yan" target="_blank">Zhicheng Yan</a>
            </p>
            <p id="summary-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="summary">Recent sparse view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released.</p>
            <p id="subjects-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" onclick="foldPdfKimi('Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="panel paper" keywords="eval,100k,alignment,quality,text,visual,evaluating,vision,content,human">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content_CVPR_2025_paper.html" target="_blank" title="45/95"><span class="index notranslate">#45</span></a>
                <a id="title-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" target="_blank">Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content</a>
                <a id="pdf-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF">48</sup>]</a>
                <a id="copy-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF">18</sup>]</a>
                <a id="rel-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zicheng Zhang" target="_blank">Zicheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tengchuan Kou" target="_blank">Tengchuan Kou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shushi Wang" target="_blank">Shushi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunyi Li" target="_blank">Chunyi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Sun" target="_blank">Wei Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Wang" target="_blank">Wei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Li" target="_blank">Xiaoyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongyu Wang" target="_blank">Zongyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuezhi Cao" target="_blank">Xuezhi Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiongkuo Min" target="_blank">Xiongkuo Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohong Liu" target="_blank">Xiaohong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangtao Zhai" target="_blank">Guangtao Zhai</a>
            </p>
            <p id="summary-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="summary">Evaluating text-to-vision content hinges on two crucial aspects: **visual quality** and **alignment**. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to **Scaling Law**, increasing the number of human-labeled instances follows a predictable pattern that enhances the performance of evaluation models.Therefore, we introduce a comprehensive dataset designed to **E**valuate **V**isual quality and **A**lignment **L**evel for text-to-vision content (**Q-EVAL-100K**), featuring the largest collection of human-labeled Mean Opinion Scores (MOS) for the mentioned two aspects.The **Q-EVAL-100K** dataset encompasses both text-to-image and text-to-video models, with 960K human annotations specifically focused on visual quality and alignment for 100K instances (60K images and 40K videos). Leveraging this dataset with context prompt, we propose **Q-Eval-Score**, a unified model capable of evaluating both visual quality and alignment with special improvements for handling long-text prompt alignment.Experimental results indicate that the proposed **Q-Eval-Score** achieves superior performance on both visual quality and alignment, with strong generalization capabilities across other benchmarks. These findings highlight the significant value of the **Q-EVAL-100K** dataset. **The data and code will be released** to help promote the generation models.</p>
            <p id="subjects-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="panel paper" keywords="geometry,moge,supervision,monocular,unlocking,map,global,accurate,estimation,open">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html" target="_blank" title="46/95"><span class="index notranslate">#46</span></a>
                <a id="title-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="title-link" href="/venue/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" target="_blank">MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision</a>
                <a id="pdf-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF">51</sup>]</a>
                <a id="copy-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF">21</sup>]</a>
                <a id="rel-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruicheng Wang" target="_blank">Ruicheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sicheng Xu" target="_blank">Sicheng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cassie Dai" target="_blank">Cassie Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfeng Xiang" target="_blank">Jianfeng Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Deng" target="_blank">Yu Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Tong" target="_blank">Xin Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaolong Yang" target="_blank">Jiaolong Yang</a>
            </p>
            <p id="summary-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="summary">We present MoGe, a powerful model for recovering 3D geometry from monocular open-domain images. Given a single image, our model directly predicts a 3D point map of the captured scene with an affine-invariant representation, which is agnostic to true global scale and shift. This new representation precludes ambiguous supervision in training and facilitates effective geometry learning. Furthermore, we propose a set of novel global and local geometry supervision techniques that empower the model to learn high-quality geometry. These include a robust, optimal, and efficient point cloud alignment solver for accurate global shape learning, and a multi-scale local geometry loss promoting precise local geometry supervision. We train our model on a large, mixed dataset and demonstrate its strong generalizability and high accuracy. In our comprehensive evaluation on diverse unseen datasets, our model significantly outperforms state-of-the-art methods across all tasks, including monocular estimation of 3D point map, depth map, and camera field of view.</p>
            <p id="subjects-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" onclick="foldPdfKimi('Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="panel paper" keywords="gaussians,flight,radiance,dynamic,depth,underappreciated,indirectly,reconstruction,scene,constrained">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly_CVPR_2025_paper.html" target="_blank" title="47/95"><span class="index notranslate">#47</span></a>
                <a id="title-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="title-link" href="/venue/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" target="_blank">Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields</a>
                <a id="pdf-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF">39</sup>]</a>
                <a id="copy-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF">18</sup>]</a>
                <a id="rel-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Runfeng Li" target="_blank">Runfeng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mikhail Okunev" target="_blank">Mikhail Okunev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Guo" target="_blank">Zixuan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anh Ha Duong" target="_blank">Anh Ha Duong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Richardt" target="_blank">Christian Richardt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew O'Toole" target="_blank">Matthew O'Toole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Tompkin" target="_blank">James Tompkin</a>
            </p>
            <p id="summary-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="summary">We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight cameras using raw sensor samples that is as accurate as past methods and is 100<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-25" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-26"><span class="mo" id="MathJax-Span-27" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-9">\times</script> faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. Recent 3D Gaussian splatting methods often depend on multi-view data to produce satisfactory results and are brittle in their optimizations otherwise.In time-of-flight radiance field reconstruction, the property of interest---depth---is not directly optimized, causing additional challenges.We describe how these problems have a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussians.Then, we incorporate two heuristics into our optimization to improve the accuracy of scene geometry for under-constrained time-of-flight Gaussians.Experimental results show that our approach produces accurate reconstructions under constrained sensing conditions, including for fast motions like swinging baseball bats.</p>
            <p id="subjects-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" onclick="foldPdfKimi('Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="panel paper" keywords="ctta,pruning,sensitivity,domain,channels,guided,channel,test,adaptive,object">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning_CVPR_2025_paper.html" target="_blank" title="48/95"><span class="index notranslate">#48</span></a>
                <a id="title-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="title-link" href="/venue/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" target="_blank">Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning</a>
                <a id="pdf-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF">76</sup>]</a>
                <a id="copy-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF">34</sup>]</a>
                <a id="rel-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kunyu Wang" target="_blank">Kunyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xueyang Fu" target="_blank">Xueyang Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Lu" target="_blank">Xin Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengjie Ge" target="_blank">Chengjie Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengzhi Cao" target="_blank">Chengzhi Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Zhai" target="_blank">Wei Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng-Jun Zha" target="_blank">Zheng-Jun Zha</a>
            </p>
            <p id="summary-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="summary">Continual test-time adaptive object detection (CTTA-OD) aims to online adapt a source pre-trained detector to ever-changing environments during inference under continuous domain shifts. Most existing CTTA-OD methods prioritize effectiveness while overlooking computational efficiency, which is crucial for resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD method via pruning. Our motivation stems from the observation that not all learned source features are beneficial; certain domain-sensitive feature channels can adversely affect target domain performance. Inspired by this, we introduce a sensitivity-guided channel pruning strategy that quantifies each channel based on its sensitivity to domain discrepancies at both image and instance levels. We apply weighted sparsity regularization to selectively suppress and prune these sensitive channels, focusing adaptation efforts on invariant ones. Additionally, we introduce a stochastic channel reactivation mechanism to restore pruned channels, enabling recovery of potentially useful features and mitigating the risks of early pruning. Extensive experiments on three benchmarks show that our method achieves superior adaptation performance while reducing computational overhead by 12% in FLOPs compared to the recent SOTA method.</p>
            <p id="subjects-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="panel paper" keywords="reasoning,video,lvlms,videoqa,videoespresso,frame,multimodal,annotations,pairs,cot">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html" target="_blank" title="49/95"><span class="index notranslate">#49</span></a>
                <a id="title-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="title-link" href="/venue/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" target="_blank">VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</a>
                <a id="pdf-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF">60</sup>]</a>
                <a id="copy-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF">28</sup>]</a>
                <a id="rel-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Songhao Han" target="_blank">Songhao Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Huang" target="_blank">Wei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hairong Shi" target="_blank">Hairong Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Zhuo" target="_blank">Le Zhuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiu Su" target="_blank">Xiu Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shifeng Zhang" target="_blank">Shifeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Zhou" target="_blank">Xu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojuan Qi" target="_blank">Xiaojuan Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Liao" target="_blank">Yue Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Si Liu" target="_blank">Si Liu</a>
            </p>
            <p id="summary-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="summary">The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities.</p>
            <p id="subjects-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" onclick="foldPdfKimi('Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="panel paper" keywords="rainy,stacking,rain,deraining,world,video,real,filter,temporal,space">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video_CVPR_2025_paper.html" target="_blank" title="50/95"><span class="index notranslate">#50</span></a>
                <a id="title-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="title-link" href="/venue/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" target="_blank">Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining</a>
                <a id="pdf-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF">52</sup>]</a>
                <a id="copy-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF">16</sup>]</a>
                <a id="rel-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shangquan Sun" target="_blank">Shangquan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Ren" target="_blank">Wenqi Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juxiang Zhou" target="_blank">Juxiang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shu Wang" target="_blank">Shu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianhou Gan" target="_blank">Jianhou Gan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaochun Cao" target="_blank">Xiaochun Cao</a>
            </p>
            <p id="summary-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="summary">Significant progress has been made in video restoration under rainy conditions over the past decade, largely propelled by advancements in deep learning. Nevertheless, existing methods that depend on paired data struggle to generalize effectively to real-world scenarios, primarily due to the disparity between synthetic and authentic rain effects. To address these limitations, we propose a dual-branch spatio-temporal state-space model to enhance rain streak removal in video sequences. Specifically, we design spatial and temporal state-space model layers to extract spatial features and incorporate temporal dependencies across frames, respectively. To improve multi-frame feature fusion, we derive a dynamic stacking filter, which adaptively approximates statistical filters for superior pixel-wise feature refinement. Moreover, we integrate a median stacking loss to enable semi-supervised learning by generating pseudo-clean patches based on the sparsity prior of rain. To further explore the capacity of deraining models in supporting other vision-based tasks in rainy environments, we introduce a novel real-world benchmark focused on object detection and tracking in rainy conditions. Our method is extensively evaluated across multiple benchmarks containing numerous synthetic and real-world rainy videos, consistently demonstrating its superiority in quantitative metrics, visual quality, efficiency, and its utility for downstream tasks. Our code will be made publicly available.</p>
            <p id="subjects-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" onclick="foldPdfKimi('Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="panel paper" keywords="pdfactor,manipulation,tri,action,robotic,latent,diffusion,perspective,accuracy,representation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.html" target="_blank" title="51/95"><span class="index notranslate">#51</span></a>
                <a id="title-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="title-link" href="/venue/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" target="_blank">PDFactor: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation</a>
                <a id="pdf-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF">51</sup>]</a>
                <a id="copy-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Tian" target="_blank">Jingyi Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Wang" target="_blank">Le Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanping Zhou" target="_blank">Sanping Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sen Wang" target="_blank">Sen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Li" target="_blank">Jiayi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haowen Sun" target="_blank">Haowen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Tang" target="_blank">Wei Tang</a>
            </p>
            <p id="summary-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="summary">Robotic manipulation based on visual observations and natural language instructions is a long-standing challenge in robotics. Yet prevailing approaches model action distribution by adopting explicit or implicit representations, which often struggle to achieve a trade-off between accuracy and efficiency. In response, we propose PDFactor, a novel framework that models action distribution with a hybrid triplane representation. In particular, PDFactor decomposes 3D point cloud into three orthogonal feature planes and leverages a tri-perspective view transformer to produce dense cubic features as a latent diffusion field aligned with observation space representing 6-DoF action probability distribution at an arbitrary location. We employ a small denoising network conceptually as both a parameterized loss function measuring the quality of the learned latent features and an action gradient decoder to sample actions from the latent diffusion field during inference. This design enables our PDFactor to benefit from spatial awareness of explicit representation and arbitrary resolution of implicit representation, rendering it with manipulation accuracy, inference efficiency, and model scalability. Experiments demonstrate that PDFactor outperforms state-of-the-art approaches across a diverse range of manipulation tasks in RLBench simulation. Moreover, PDFactor can effectively learn multi-task policies from a limited number of human demonstrations, achieving promising accuracy in a variety of real-world manipulation tasks.</p>
            <p id="subjects-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" onclick="foldPdfKimi('Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="panel paper" keywords="stereo,stereo4d,internet,videos,reconstructions,dust3r,scenes,motion,world,supervising">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo_CVPR_2025_paper.html" target="_blank" title="52/95"><span class="index notranslate">#52</span></a>
                <a id="title-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="title-link" href="/venue/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" target="_blank">Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos</a>
                <a id="pdf-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF">35</sup>]</a>
                <a id="copy-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF">12</sup>]</a>
                <a id="rel-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linyi Jin" target="_blank">Linyi Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Tucker" target="_blank">Richard Tucker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengqi Li" target="_blank">Zhengqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Fouhey" target="_blank">David Fouhey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Snavely" target="_blank">Noah Snavely</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aleksander Holynski" target="_blank">Aleksander Holynski</a>
            </p>
            <p id="summary-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="summary">Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3r to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes.</p>
            <p id="subjects-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" onclick="foldPdfKimi('Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="panel paper" keywords="skills,hsi,tokenhsi,task,unified,policy,scene,tokenization,tasks,human">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization_CVPR_2025_paper.html" target="_blank" title="53/95"><span class="index notranslate">#53</span></a>
                <a id="title-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="title-link" href="/venue/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" target="_blank">TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</a>
                <a id="pdf-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF">33</sup>]</a>
                <a id="copy-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF">11</sup>]</a>
                <a id="rel-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Pan" target="_blank">Liang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeshi Yang" target="_blank">Zeshi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyang Dou" target="_blank">Zhiyang Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjia Wang" target="_blank">Wenjia Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Buzhen Huang" target="_blank">Buzhen Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Dai" target="_blank">Bo Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taku Komura" target="_blank">Taku Komura</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingbo Wang" target="_blank">Jingbo Wang</a>
            </p>
            <p id="summary-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="summary">Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks.</p>
            <p id="subjects-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" onclick="foldPdfKimi('Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="panel paper" keywords="seal,video,long,representation,videos,lvbench,redundancy,mantic,moviechat,tasks">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.html" target="_blank" title="54/95"><span class="index notranslate">#54</span></a>
                <a id="title-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="title-link" href="/venue/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" target="_blank">SEAL: Semantic Attention Learning for Long Video Representation</a>
                <a id="pdf-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF">69</sup>]</a>
                <a id="copy-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF">21</sup>]</a>
                <a id="rel-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lan Wang" target="_blank">Lan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujia Chen" target="_blank">Yujia Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Du Tran" target="_blank">Du Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vishnu Naresh Boddeti" target="_blank">Vishnu Naresh Boddeti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen-Sheng Chu" target="_blank">Wen-Sheng Chu</a>
            </p>
            <p id="summary-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="summary">Long video understanding presents challenges due to the inherent high computational complexity and redundant temporal information. An effective representation for long videos must process such redundancy efficiently while preserving essential contents for downstream tasks. This paper introduces **SE**mantic **A**ttention **L**earning (SEAL), a novel unified representation for long videos. To reduce computational complexity, long videos are decomposed into three distinct types of semantic entities: scenes, objects, and actions, allowing models to operate on a handful of entities rather than a large number of frames or pixels. To further address redundancy, we propose an attention learning module that balances token relevance with diversity formulated as a subset selection optimization problem. Our representation is versatile, enabling applications across various long video understanding tasks. Extensive experiments show that SEAL significantly outperforms state-of-the-art methods in video question answering and temporal grounding tasks and benchmarks including LVBench, MovieChat-1K, and Ego4D.</p>
            <p id="subjects-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" onclick="foldPdfKimi('Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="panel paper" keywords="spiking,xnor,similarity,spikes,spike,transformers,trains,calculation,rethinking,dot">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking_CVPR_2025_paper.html" target="_blank" title="55/95"><span class="index notranslate">#55</span></a>
                <a id="title-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="title-link" href="/venue/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" target="_blank">Rethinking Spiking Self-Attention Mechanism: Implementing a-XNOR Similarity Calculation in Spiking Transformers</a>
                <a id="pdf-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF">34</sup>]</a>
                <a id="copy-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF">19</sup>]</a>
                <a id="rel-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yichen Xiao" target="_blank">Yichen Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Wang" target="_blank">Shuai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dehao Zhang" target="_blank">Dehao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjie Wei" target="_blank">Wenjie Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yimeng Shan" target="_blank">Yimeng Shan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoli Liu" target="_blank">Xiaoli Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulin Jiang" target="_blank">Yulin Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Malu Zhang" target="_blank">Malu Zhang</a>
            </p>
            <p id="summary-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="summary">Transformers significantly raise the performance limits across various tasks, spurring research into integrating them into spiking neural networks. However, a notable performance gap remains between existing spiking Transformers and their artificial neural network counterparts. Here, we first analyze the reason for this gap and identify that the dot product ineffectively calculates similarity between spiking Queries (Q) and Keys (K). To address this challenge, we introduce an innovative α-XNOR similarity calculation method tailored for spike trains. α-XNOR similarity redefines the correlation of non-spike pairs as a specific value α, effectively overcoming the limitations of dot-product similarity caused by numerous non-spiking events. Additionally, considering the sparse nature of spike trains where spikes carry more information than non-spikes, the α-XNOR similarity correspondingly highlights the distinct importance of spikes over non-spikes. Extensive experiments demonstrate that our α-XNOR similarity significantly improves performance across different spiking Transformer architectures in various static and neuromorphic datasets. This is the first attempt to develop a spiking self-attention paradigm tailored for the binary characteristics of spike trains.</p>
            <p id="subjects-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" onclick="foldPdfKimi('Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="panel paper" keywords="radar,tacodepth,depth,camera,estimation,stage,fusion,efficient,dense,intermediate">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.html" target="_blank" title="56/95"><span class="index notranslate">#56</span></a>
                <a id="title-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="title-link" href="/venue/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" target="_blank">TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion</a>
                <a id="pdf-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF">57</sup>]</a>
                <a id="copy-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF">17</sup>]</a>
                <a id="rel-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiran Wang" target="_blank">Yiran Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Li" target="_blank">Jiaqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaoyi Hong" target="_blank">Chaoyi Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruibo Li" target="_blank">Ruibo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liusheng Sun" target="_blank">Liusheng Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Song" target="_blank">Xiao Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Wang" target="_blank">Zhe Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiguo Cao" target="_blank">Zhiguo Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guosheng Lin" target="_blank">Guosheng Lin</a>
            </p>
            <p id="summary-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="summary">Radar-Camera depth estimation aims to predict dense and accurate metric depth by fusing input images and Radar data. Model efficiency is crucial for this task in pursuit of real-time processing on autonomous vehicles and robotic platforms. However, due to the sparsity of Radar returns, the prevailing methods adopt multi-stage frameworks with intermediate quasi-dense depth, which are time-consuming and not robust. To address these challenges, we propose TacoDepth, an efficient and accurate Radar-Camera depth estimation model with one-stage fusion. Specifically, the graph-based Radar structure extractor and the pyramid-based Radar fusion module are designed to capture and integrate the graph structures of Radar point clouds, delivering superior model efficiency and robustness without relying on the intermediate depth results. Moreover, TacoDepth can be flexible for different inference modes, providing a better balance of speed and accuracy. Extensive experiments are conducted to demonstrate the efficacy of our method. Compared with the previous state-of-the-art approach, TacoDepth improves depth accuracy and processing speed by 12.8% and 91.8%. Our work provides a new perspective on efficient Radar-Camera depth estimation.</p>
            <p id="subjects-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" onclick="foldPdfKimi('Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="panel paper" keywords="3dgs,splatting,3dgut,cameras,secondary,rasterization,rendering,tracing,unscented,distorted">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting_CVPR_2025_paper.html" target="_blank" title="57/95"><span class="index notranslate">#57</span></a>
                <a id="title-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="title-link" href="/venue/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" target="_blank">3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting</a>
                <a id="pdf-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF">35</sup>]</a>
                <a id="copy-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Wu" target="_blank">Qi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Janick Martinez Esturo" target="_blank">Janick Martinez Esturo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ashkan Mirzaei" target="_blank">Ashkan Mirzaei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Moënne-Loccoz" target="_blank">Nicolas Moënne-Loccoz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zan Gojcic" target="_blank">Zan Gojcic</a>
            </p>
            <p id="summary-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="summary">3D Gaussian Splatting (3DGS) has shown great potential for efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing volumetric particles instead, however, this comes at the cost of significantly slower rendering speeds. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation.</p>
            <p id="subjects-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" onclick="foldPdfKimi('Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="panel paper" keywords="human,hmr,mega,mesh,masked,meshes,single,generative,autoencoder,recovery">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery_CVPR_2025_paper.html" target="_blank" title="58/95"><span class="index notranslate">#58</span></a>
                <a id="title-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="title-link" href="/venue/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" target="_blank">MEGA: Masked Generative Autoencoder for Human Mesh Recovery</a>
                <a id="pdf-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF">51</sup>]</a>
                <a id="copy-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF">21</sup>]</a>
                <a id="rel-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guénolé Fiche" target="_blank">Guénolé Fiche</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Leglaive" target="_blank">Simon Leglaive</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xavier Alameda-Pineda" target="_blank">Xavier Alameda-Pineda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francesc Moreno-Noguer" target="_blank">Francesc Moreno-Noguer</a>
            </p>
            <p id="summary-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="summary">Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous problem, as an infinite set of 3D interpretations can explain the 2D observation equally well. Nevertheless, most HMR methods overlook this issue and make a single prediction without accounting for this ambiguity. A few approaches generate a distribution of human meshes, enabling the sampling of multiple predictions; however, none of them is competitive with the latest single-output model when making a single prediction. This work proposes a new approach based on masked generative modeling. By tokenizing the human pose and shape, we formulate the HMR task as generating a sequence of discrete tokens conditioned on an input image. We introduce MEGA, a MaskEd Generative Autoencoder trained to recover human meshes from images and partial human mesh token sequences. Given an image, our flexible generation scheme allows us to predict a single human mesh in deterministic mode or to generate multiple human meshes in stochastic mode. Experiments on in-the-wild benchmarks show that MEGA achieves state-of-the-art performance in deterministic and stochastic modes, outperforming single-output and multi-output approaches. Code and trained models will be released upon acceptance.</p>
            <p id="subjects-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" onclick="foldPdfKimi('Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="panel paper" keywords="simplicity,bias,tasks,activation,architectures,gelus,relu,suboptimal,functions,cases">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal_CVPR_2025_paper.html" target="_blank" title="59/95"><span class="index notranslate">#59</span></a>
                <a id="title-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="title-link" href="/venue/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" target="_blank">Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild</a>
                <a id="pdf-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF">48</sup>]</a>
                <a id="copy-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF">19</sup>]</a>
                <a id="rel-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Damien Teney" target="_blank">Damien Teney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangze Jiang" target="_blank">Liangze Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florin Gogianu" target="_blank">Florin Gogianu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ehsan Abbasnejad" target="_blank">Ehsan Abbasnejad</a>
            </p>
            <p id="summary-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="summary">Common choices of architecture give neural networks a preference for fitting data with simple functions. This simplicity bias is known as key to their success. This paper explores the limits of this assumption. Building on recent work that showed that activation functions are the origin of the simplicity bias (Teney, 2024), we introduce a method to meta-learn activation functions to modulate this bias.**Findings.** We discover multiple tasks where the assumption of simplicity is inadequate, and standard ReLU architectures are therefore suboptimal. In these cases, we find activation functions that perform better by inducing a prior of higher complexity. Interestingly, these cases correspond to domains where neural networks have historically struggled: tabular data, regression tasks, cases of shortcut learning, and algorithmic grokking tasks. In comparison, the simplicity bias proves adequate on image tasks, where learned activations are nearly identical to ReLUs and GeLUs.**Implications.** (1) Contrary to common belief, the simplicity bias is not universally useful. There exist real tasks where it is suboptimal. (2) The suitability of ReLU models for image classification is not accidental. (3) The success of ML ultimately depends on the adequacy between data and architectures, and there may be benefits for architectures tailored to specific distributions of tasks.</p>
            <p id="subjects-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" onclick="foldPdfKimi('Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="panel paper" keywords="cell,topological,topology,pathology,layouts,topocellgen,tissue,generating,histopathology,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model_CVPR_2025_paper.html" target="_blank" title="60/95"><span class="index notranslate">#60</span></a>
                <a id="title-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="title-link" href="/venue/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" target="_blank">TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model</a>
                <a id="pdf-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF">34</sup>]</a>
                <a id="copy-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF">13</sup>]</a>
                <a id="rel-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Meilong Xu" target="_blank">Meilong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saumya Gupta" target="_blank">Saumya Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoling Hu" target="_blank">Xiaoling Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Li" target="_blank">Chen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shahira Abousamra" target="_blank">Shahira Abousamra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dimitris Samaras" target="_blank">Dimitris Samaras</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Prasanna" target="_blank">Prateek Prasanna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Chen" target="_blank">Chao Chen</a>
            </p>
            <p id="summary-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="summary">Accurately modeling multi-class cell topology is crucial in digital pathology, as it provides critical insights into tissue structure and pathology. The synthetic generation of cell topology enables realistic simulations of complex tissue environments, enhances downstream tasks by augmenting training data, aligns more closely with pathologists' domain knowledge, and offers new opportunities for controlling and generalizing the tumor microenvironment. In this paper, we propose a novel approach that integrates topological constraints into a diffusion model to improve the generation of realistic, contextually accurate cell topologies. Our method refines the simulation of cell distributions and interactions, increasing the precision and interpretability of results in downstream tasks such as cell detection and classification. To assess the topological fidelity of generated layouts, we introduce a new metric, Topological Fréchet Distance (TopoFD), which overcomes the limitations of traditional metrics like FID in evaluating topological structure. Experimental results demonstrate the effectiveness of our approach in generating multi-class cell layouts that capture intricate topological relationships.</p>
            <p id="subjects-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" onclick="foldPdfKimi('Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="panel paper" keywords="quantization,calibration,data,diverse,diversity,dfq,collapse,4open,84e6,mixer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Enhancing_Diversity_for_Data-free_Quantization_CVPR_2025_paper.html" target="_blank" title="61/95"><span class="index notranslate">#61</span></a>
                <a id="title-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="title-link" href="/venue/Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" target="_blank">Enhancing Diversity for Data-free Quantization</a>
                <a id="pdf-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Enhancing_Diversity_for_Data-free_Quantization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF">35</sup>]</a>
                <a id="copy-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhao" target="_blank">Kai Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Zhuang" target="_blank">Zhihao Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miao Zhang" target="_blank">Miao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenjuan Guo" target="_blank">Chenjuan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Shu" target="_blank">Yang Shu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Yang" target="_blank">Bin Yang</a>
            </p>
            <p id="summary-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="summary">Model quantization is an effective way to compress deep neural networks and accelerate the inference time on edge devices. Existing quantization methods usually require original data for calibration during the compressing process, which may be inaccessible due to privacy issues. A common way is to generate calibration data to mimic the origin data. However, the generators in these methods have the mode collapse problem, making them unable to synthesize diverse data. To solve this problem, we leverage the information from the full-precision model and enhance both inter-class and intra-class diversity for generating better calibration data, by devising a multi-layer features mixer and normalization flow based attention. Besides, novel regulation losses are proposed to make the generator produce diverse data with more patterns from the perspective of activated feature values and for the quantized model to learn better clip ranges adaptive to our diverse calibration data. Extensive experiments show that our method achieves state-of-the-art quantization results for both Transformer and CNN architectures. In addition, we visualize the generated data to verify that our strategies can effectively handle the mode collapse issue. Our codes are available at https://anonymous.4open.science/r/DFQ-84E6 and will be publicly available.</p>
            <p id="subjects-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" onclick="foldPdfKimi('Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bar_Navigation_World_Models@CVPR2025@CVF" class="panel paper" keywords="nwm,navigation,video,egocentric,planning,cdit,visual,unlabeled,world,agents">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bar_Navigation_World_Models_CVPR_2025_paper.html" target="_blank" title="62/95"><span class="index notranslate">#62</span></a>
                <a id="title-Bar_Navigation_World_Models@CVPR2025@CVF" class="title-link" href="/venue/Bar_Navigation_World_Models@CVPR2025@CVF" target="_blank">Navigation World Models</a>
                <a id="pdf-Bar_Navigation_World_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bar_Navigation_World_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bar_Navigation_World_Models@CVPR2025@CVF">80</sup>]</a>
                <a id="copy-Bar_Navigation_World_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bar_Navigation_World_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bar_Navigation_World_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bar_Navigation_World_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bar_Navigation_World_Models@CVPR2025@CVF">41</sup>]</a>
                <a id="rel-Bar_Navigation_World_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bar_Navigation_World_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bar_Navigation_World_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Amir Bar" target="_blank">Amir Bar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaoyue Zhou" target="_blank">Gaoyue Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danny Tran" target="_blank">Danny Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Trevor Darrell" target="_blank">Trevor Darrell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yann LeCun" target="_blank">Yann LeCun</a>
            </p>
            <p id="summary-Bar_Navigation_World_Models@CVPR2025@CVF" class="summary">Navigation is a fundamental skill of agents with visual-motor capabilities. We propose a Navigation World Model (NWM), a controllable video generation model that predicts the future visual observation given the past observations and navigation actions. NWM is a Conditional Diffusion Transformer (CDiT) trained on the video footage of robots as well as unlabeled egocentric video data. We scale the model up to 1B parameters and train it over human and robot agents data from numerous environments and embodiments. Our model scales favorably on known and unknown environments and can leverage unlabeled egocentric video data. NWM exhibits improved navigation planning skills either by planning from scratch or by ranking proposals from an external navigation policy. Compared to existing supervised navigation models which are ``hard coded'', NWM can incorporate new constraints when planning trajectories. NWM learns visual priors that enable it to imagine navigation trajectories based on just a single input image.</p>
            <p id="subjects-Bar_Navigation_World_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Bar_Navigation_World_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bar_Navigation_World_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bar_Navigation_World_Models@CVPR2025@CVF" onclick="foldPdfKimi('Bar_Navigation_World_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="cat4d,video,view,monocular,anything,multi,diffusion,synthesis,reconstruction,scene">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="63/95"><span class="index notranslate">#63</span></a>
                <a id="title-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" target="_blank">CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models</a>
                <a id="pdf-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF">39</sup>]</a>
                <a id="copy-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF">11</sup>]</a>
                <a id="rel-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rundi Wu" target="_blank">Rundi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiqi Gao" target="_blank">Ruiqi Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Poole" target="_blank">Ben Poole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Trevithick" target="_blank">Alex Trevithick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changxi Zheng" target="_blank">Changxi Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan T. Barron" target="_blank">Jonathan T. Barron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aleksander Holynski" target="_blank">Aleksander Holynski</a>
            </p>
            <p id="summary-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="summary">We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos.</p>
            <p id="subjects-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="panel paper" keywords="fluidnexus,fluid,video,reconstruction,view,videos,prediction,novel,simulation,single">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video_CVPR_2025_paper.html" target="_blank" title="64/95"><span class="index notranslate">#64</span></a>
                <a id="title-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="title-link" href="/venue/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" target="_blank">FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video</a>
                <a id="pdf-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF">43</sup>]</a>
                <a id="copy-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF">11</sup>]</a>
                <a id="rel-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Gao" target="_blank">Yue Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong-Xing Yu" target="_blank">Hong-Xing Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zhu" target="_blank">Bo Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>
            </p>
            <p id="summary-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="summary">We study reconstructing and predicting 3D fluid appearance and velocity from a single video. Current methods require multi-view videos for fluid reconstruction. We present FluidNexus, a novel framework that bridges video generation and physics simulation to tackle this task. Our key insight is to synthesize multiple novel-view videos as references for reconstruction. FluidNexus consists of two key components: (1) a novel-view video synthesizer that combines frame-wise view synthesis with video diffusion refinement for generating realistic videos, and (2) a physics-integrated particle representation coupling differentiable simulation and rendering to simultaneously facilitate 3D fluid reconstruction and prediction. To evaluate our approach, we collect two new real-world fluid datasets featuring textured backgrounds and object interactions. Our method enables dynamic novel view synthesis, future prediction, and interaction simulation from a single fluid video. we will release code and datasets.</p>
            <p id="subjects-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" onclick="foldPdfKimi('Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="panel paper" keywords="video,mllms,vst,compression,visual,long,understanding,token,instruction,customizes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.html" target="_blank" title="65/95"><span class="index notranslate">#65</span></a>
                <a id="title-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="title-link" href="/venue/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" target="_blank">Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding</a>
                <a id="pdf-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF">44</sup>]</a>
                <a id="copy-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF">18</sup>]</a>
                <a id="rel-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Shu" target="_blank">Yan Shu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Liu" target="_blank">Zheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peitian Zhang" target="_blank">Peitian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minghao Qin" target="_blank">Minghao Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Zhou" target="_blank">Junjie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyang Liang" target="_blank">Zhengyang Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiejun Huang" target="_blank">Tiejun Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zhao" target="_blank">Bo Zhao</a>
            </p>
            <p id="summary-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="summary">Long video understanding poses a significant challenge for current Multi-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained by their limited context lengths and the substantial costs while processing long videos. Although several existing methods attempt to reduce visual tokens, their strategies encounter severe bottleneck, restricting MLLMs' ability to perceive fine-grained visual details. In this work, we propose Video-XL, a novel approach that leverages MLLMs' inherent key-value (KV) sparsification capacity to condense the visual input. Specifically, we introduce a new special token, the Visual Summarization Token (VST), for each interval of the video, which summarizes the visual information within the interval as its associated KV. The VST module is trained by instruction fine-tuning, where two optimizing strategies are offered. 1. Curriculum learning, where VST learns to make small (easy) and large compression (hard) progressively. 2. Composite data curation, which integrates single-image, multi-image, and synthetic data to overcome the scarcity of long-video instruction data. The compression quality is further improved by dynamic compression, which customizes compression granularity based on the information density of different video intervals. Video-XL's effectiveness is verified from three aspects. First, it achieves a superior long-video understanding capability, outperforming state-of-the-art models of comparable sizes across multiple popular benchmarks. Second, it effectively preserves video information, with minimal compression loss even at 16x compression ratio. Third, it realizes outstanding cost-effectiveness, enabling high-quality processing of thousands of frames on a single A100 GPU.</p>
            <p id="subjects-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" onclick="foldPdfKimi('Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="panel paper" keywords="grc,reflectance,weather,adverse,collaboration,lidar,geometry,segmentation,augmentation,capitalizing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse_CVPR_2025_paper.html" target="_blank" title="66/95"><span class="index notranslate">#66</span></a>
                <a id="title-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="title-link" href="/venue/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" target="_blank">Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather</a>
                <a id="pdf-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF">33</sup>]</a>
                <a id="copy-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF">10</sup>]</a>
                <a id="rel-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Longyu Yang" target="_blank">Longyu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Hu" target="_blank">Ping Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangbo Yuan" target="_blank">Shangbo Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Zhang" target="_blank">Lu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Liu" target="_blank">Jun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengtao Shen" target="_blank">Hengtao Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofeng Zhu" target="_blank">Xiaofeng Zhu</a>
            </p>
            <p id="summary-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="summary">Existing LiDAR semantic segmentation models often suffer from decreased accuracy when exposed to adverse weather conditions. Recent methods addressing this issue focus on enhancing training data through weather simulation or universal augmentation techniques. However, few works have studied the negative impacts caused by the heterogeneous domain shifts in the geometric structure and reflectance intensity of point clouds. In this paper, we delve into this challenge and address it with a novel Geometry-Reflectance Collaboration (GRC) framework that explicitly separates feature extraction for geometry and reflectance. Specifically, GRC employs a dual-branch architecture designed to process geometric and reflectance features independently initially, thereby capitalizing on their distinct characteristic. Then, GRC adopts a robust multi-level feature collaboration module to suppress redundant and unreliable information from both branches. Consequently, without complex simulation or augmentation, our method effectively extracts intrinsic information about the scene while suppressing interference, thus achieving better robustness and generalization in adverse weather conditions. We demonstrate the effectiveness of GRC through comprehensive experiments on challenging benchmarks, showing that our method outperforms previous approaches and establishes new state-of-the-art results.</p>
            <p id="subjects-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" onclick="foldPdfKimi('Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="panel paper" keywords="t2i,minority,prompt,generation,samplers,text,pretrained,diffusion,optimization,instances">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization_CVPR_2025_paper.html" target="_blank" title="67/95"><span class="index notranslate">#67</span></a>
                <a id="title-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="title-link" href="/venue/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" target="_blank">Minority-Focused Text-to-Image Generation via Prompt Optimization</a>
                <a id="pdf-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF">59</sup>]</a>
                <a id="copy-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF">27</sup>]</a>
                <a id="rel-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Soobin Um" target="_blank">Soobin Um</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jong Chul Ye" target="_blank">Jong Chul Ye</a>
            </p>
            <p id="summary-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="summary">We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of *text-conditional* data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for producing high-quality generations. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that can encourage the emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes the generation of minority features by incorporating a carefully-crafted likelihood objective. Our comprehensive experiments, conducted across various types of T2I models, demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers.</p>
            <p id="subjects-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" onclick="foldPdfKimi('Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="panel paper" keywords="egolm,egocentric,motion,wearable,language,motions,sensors,modal,devices,understanding">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions_CVPR_2025_paper.html" target="_blank" title="68/95"><span class="index notranslate">#68</span></a>
                <a id="title-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="title-link" href="/venue/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" target="_blank">EgoLM: Multi-Modal Language Model of Egocentric Motions</a>
                <a id="pdf-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF">37</sup>]</a>
                <a id="copy-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF">19</sup>]</a>
                <a id="rel-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fangzhou Hong" target="_blank">Fangzhou Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vladimir Guzov" target="_blank">Vladimir Guzov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyo Jin Kim" target="_blank">Hyo Jin Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuting Ye" target="_blank">Yuting Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Newcombe" target="_blank">Richard Newcombe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Liu" target="_blank">Ziwei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingni Ma" target="_blank">Lingni Ma</a>
            </p>
            <p id="summary-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="summary">As wearable devices become more prevalent, understanding the user's motion is crucial for improving contextual AI systems. We introduce EgoLM, a versatile framework designed for egocentric motion understanding using multi-modal data. EgoLM integrates the rich contextual information from egocentric videos and motion sensors afforded by wearable devices. It also combines dense supervision signals from motion and language, leveraging the vast knowledge encoded in pre-trained large language models (LLMs). EgoLM models the joint distribution of egocentric motions and natural language using LLMs, conditioned on observations from egocentric videos and motion sensors. It unifies a range of motion understanding tasks, including motion narration from video or motion data, as well as motion generation from text or sparse sensor data. Unique to wearable devices, it also enables a novel task to generate text descriptions from sparse sensors. Through extensive experiments, we validate the effectiveness of EgoLM in addressing the challenges of under-constrained egocentric motion learning, and demonstrate its capability as a generalist model through a variety of applications.</p>
            <p id="subjects-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" onclick="foldPdfKimi('Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="panel paper" keywords="tokenized,loop,traffic,tuning,cat,closed,fine,simulation,covariate,102m">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models_CVPR_2025_paper.html" target="_blank" title="69/95"><span class="index notranslate">#69</span></a>
                <a id="title-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" target="_blank">Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models</a>
                <a id="pdf-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF">26</sup>]</a>
                <a id="copy-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhejun Zhang" target="_blank">Zhejun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Karkus" target="_blank">Peter Karkus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maximilian Igl" target="_blank">Maximilian Igl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Ding" target="_blank">Wenhao Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxiao Chen" target="_blank">Yuxiao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boris Ivanovic" target="_blank">Boris Ivanovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Pavone" target="_blank">Marco Pavone</a>
            </p>
            <p id="summary-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="summary">Traffic simulation aims to learn a policy for traffic agents that, when unrolled in closed-loop, faithfully recovers the joint distribution of trajectories observed in the real world. Inspired by large language models, tokenized multi-agent policies have recently become the state-of-the-art in traffic simulation. However, they are typically trained through open-loop behavior cloning, and thus suffer from covariate shift when executed in closed-loop during simulation. In this work, we present Closest Among Top-K (CAT-K) rollouts, a simple yet effective closed-loop fine-tuning strategy to mitigate covariate shift. CAT-K fine-tuning only requires existing trajectory data, without reinforcement learning or generative adversarial imitation. Concretely, CAT-K fine-tuning enables a small 7M-parameter tokenized traffic simulation policy to outperform a 102M-parameter model from the same model family, achieving the top spot on the Waymo Sim Agent Challenge leaderboard at the time of submission. Our code will be made publicly available.</p>
            <p id="subjects-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="panel paper" keywords="lora,knowledge,specialized,mllms,lorasculpt,harmonizing,sculpting,multimodal,downstream,harmful">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in_CVPR_2025_paper.html" target="_blank" title="70/95"><span class="index notranslate">#70</span></a>
                <a id="title-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="title-link" href="/venue/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" target="_blank">LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models</a>
                <a id="pdf-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF">61</sup>]</a>
                <a id="copy-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF">34</sup>]</a>
                <a id="rel-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Liang" target="_blank">Jian Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenke Huang" target="_blank">Wenke Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guancheng Wan" target="_blank">Guancheng Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qu Yang" target="_blank">Qu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mang Ye" target="_blank">Mang Ye</a>
            </p>
            <p id="summary-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="summary">While Multimodal Large Language Models (MLLMs) excel at generalizing across modalities and tasks, effectively adapting them to specific downstream tasks while simultaneously retaining both general and specialized knowledge remains challenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently acquire specialized knowledge in MLLMs, it introduces substantial harmful redundancy during visual instruction tuning, which exacerbates the forgetting of general knowledge and degrades downstream task performance.To address this issue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby harmonizing general and specialized knowledge.Specifically, under theoretical guarantees, we introduce sparse updates into LoRA to discard redundant parameters effectively. Furthermore, we propose a Conflict Mitigation Regularizer to refine the update trajectory of LoRA, mitigating knowledge conflicts with the pretrained weights.Extensive experimental results demonstrate that even at very high degree of sparsity (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x2264;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-28" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-29"><span class="mo" id="MathJax-Span-30" style="font-family: MathJax_Main;">≤</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>≤</mo></math></span></span><script type="math/tex" id="MathJax-Element-10">\le</script> 5\%), our method simultaneously enhances generalization and downstream task performance. This confirms that our approach effectively mitigates the catastrophic forgetting issue and further promotes knowledge harmonization in MLLMs.</p>
            <p id="subjects-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" onclick="foldPdfKimi('Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="latent,tokenizers,dit,gfid,generation,diffusion,epochs,rfid,reconstruction,dilemma">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="71/95"><span class="index notranslate">#71</span></a>
                <a id="title-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" target="_blank">Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</a>
                <a id="pdf-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF">56</sup>]</a>
                <a id="copy-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF">29</sup>]</a>
                <a id="rel-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingfeng Yao" target="_blank">Jingfeng Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Yang" target="_blank">Bin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinggang Wang" target="_blank">Xinggang Wang</a>
            </p>
            <p id="summary-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="summary">Latent diffusion models (LDM) with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: increasing the per-token feature dimension in visual tokenizers improves reconstruction quality but requires substantially larger diffusion models and extended training time to maintain generation performance. This results in prohibitively high computational costs, making high-dimensional tokenizers impractical. In this paper, we argue that this limitation stems from the inherent difficulty of learning unconstrained high-dimensional latent spaces and address this limitation by aligning the latent space with pre-trained vision foundation models. Our VA-VAE (Vision foundation model Aligned Variational AutoEncoder) expands the Pareto frontier of visual tokenizers, enabling 2.7 times faster Diffusion Transformers (DiT) convergence in high-dimensional latent space. To further validate our approach, we optimize a DiT baseline, referred to as LightningDiT, achieving superior performance on class conditional generation with only 6% of the original training epochs. The integrated system demonstrates the effectiveness of VA-VAE, achieving 0.28 rFID and 1.73 gFID on ImageNet-256 generation in 400 epochs—outperforming the original DiT's 0.71 rFID and 2.27 gFID in 1400 epochs, without more complex designs. To our knowledge, this marks the first latent diffusion system to achieve both superior generation and reconstruction without increasing training costs. Our codes and weights will be open source.</p>
            <p id="subjects-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="panel paper" keywords="modality,rgb,modalities,segmentation,multimodal,fusion,symmetrical,cross,efficient,semantic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic_CVPR_2025_paper.html" target="_blank" title="72/95"><span class="index notranslate">#72</span></a>
                <a id="title-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="title-link" href="/venue/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" target="_blank">Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation</a>
                <a id="pdf-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF">49</sup>]</a>
                <a id="copy-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF">21</sup>]</a>
                <a id="rel-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxin Cai" target="_blank">Jiaxin Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingze Su" target="_blank">Jingze Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Li" target="_blank">Qi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjie Yang" target="_blank">Wenjie Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shu Wang" target="_blank">Shu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiesong Zhao" target="_blank">Tiesong Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengfeng He" target="_blank">Shengfeng He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxi Liu" target="_blank">Wenxi Liu</a>
            </p>
            <p id="summary-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="summary">Multimodal semantic segmentation is a critical challenge in computer vision, with early methods suffering from high computational costs and limited transferability due to full fine-tuning of RGB-based pre-trained parameters. Recent studies, while leveraging additional modalities as supplementary prompts to RGB, still predominantly rely on RGB, which restricts the full potential of other modalities. To address these issues, we propose a novel symmetric parameter-efficient fine-tuning framework for multimodal segmentation, featuring with a modality-aware prompting and adaptation scheme, to simultaneously adapt the capabilities of a powerful pre-trained model to both RGB and X modalities. Furthermore, prevalent approaches use the global cross-modality correlations of attention mechanism for modality fusion, which inadvertently introduces noise across modalities. To mitigate this noise, we propose a dynamic sparse cross-modality fusion module to facilitate effective and efficient cross-modality fusion. To further strengthen the above two modules, we propose a training strategy that leverages accurately predicted dual-modality results to self-teach the single-modality outcomes. In comprehensive experiments, we demonstrate that our method outperforms previous state-of-the-art approaches across six multimodal segmentation scenarios with minimal computation cost.</p>
            <p id="subjects-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" onclick="foldPdfKimi('Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="panel paper" keywords="sam,segmentation,esc,vocabulary,mask,net,open,pascal,vlf,semantic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html" target="_blank" title="73/95"><span class="index notranslate">#73</span></a>
                <a id="title-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="title-link" href="/venue/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" target="_blank">Effective SAM Combination for Open-Vocabulary Semantic Segmentation</a>
                <a id="pdf-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF">77</sup>]</a>
                <a id="copy-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF">29</sup>]</a>
                <a id="rel-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Minhyeok Lee" target="_blank">Minhyeok Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suhwan Cho" target="_blank">Suhwan Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jungho Lee" target="_blank">Jungho Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sunghun Yang" target="_blank">Sunghun Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heeseung Choi" target="_blank">Heeseung Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ig-Jae Kim" target="_blank">Ig-Jae Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangyoun Lee" target="_blank">Sangyoun Lee</a>
            </p>
            <p id="summary-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="summary">Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAM’s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. Additionally, a Vision-Language Fusion (VLF) module enhances the final mask prediction through image and text guidance. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions.</p>
            <p id="subjects-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" onclick="foldPdfKimi('Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="panel paper" keywords="pixel,ppa,processor,tracking,descriptors,point,feature,descriptor,sensor,processors">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays_CVPR_2025_paper.html" target="_blank" title="74/95"><span class="index notranslate">#74</span></a>
                <a id="title-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="title-link" href="/venue/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" target="_blank">Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays</a>
                <a id="pdf-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF">36</sup>]</a>
                <a id="copy-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF">13</sup>]</a>
                <a id="rel-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Laurie Bose" target="_blank">Laurie Bose</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianing Chen" target="_blank">Jianing Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Piotr Dudek" target="_blank">Piotr Dudek</a>
            </p>
            <p id="summary-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="summary">This paper presents a novel approach for joint point-feature detection and tracking, specifically designed for Pixel Processor Array sensors (PPA). Instead of standard pixels, PPA sensors consists of thousands of "pixel-processors", enabling massive parallel computation of visual data at the point of light capture. Our approach performs all computation within these pixel-processors, meaning no raw image data need ever leave the sensor. Instead, sensor output can be reduced to merely the locations of tracked features, and the descriptors of newly initialized features, minimizing data transfer between sensor and external processing. To achieve this we store feature descriptors inside every pixel-processor, adjusting the layout of these descriptors every frame. The PPA's architecture enables us to compute the response of every stored descriptor in parallel. This "response map" is utilized for both detection and tracking of point-features across the pixel-processor array. This approach is very fast, our implementation upon the SCAMP-7 PPA prototype runs at over 3000 FPS (Frames Per Second), tracking point-features reliably even under violent motion. This is the first work performing point-feature detection and tracking entirely "in-pixel".</p>
            <p id="subjects-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" onclick="foldPdfKimi('Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="panel paper" keywords="murre,sfm,view,reconstruction,depth,multi,mvs,scenes,monocular,scene">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation_CVPR_2025_paper.html" target="_blank" title="75/95"><span class="index notranslate">#75</span></a>
                <a id="title-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="title-link" href="/venue/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" target="_blank">Multi-view Reconstruction via SfM-guided Monocular Depth Estimation</a>
                <a id="pdf-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF">49</sup>]</a>
                <a id="copy-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF">16</sup>]</a>
                <a id="rel-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyu Guo" target="_blank">Haoyu Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Zhu" target="_blank">He Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sida Peng" target="_blank">Sida Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haotong Lin" target="_blank">Haotong Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunzhi Yan" target="_blank">Yunzhi Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Xie" target="_blank">Tao Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenguan Wang" target="_blank">Wenguan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaowei Zhou" target="_blank">Xiaowei Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hujun Bao" target="_blank">Hujun Bao</a>
            </p>
            <p id="summary-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="summary">This paper aims to reconstruct the scene geometry from multi-view images with strong robustness and high quality. Previous learning-based methods incorporate neural networks into the multi-view stereo matching and have shown impressive reconstruction results. However, due to the reliance on matching across input images, they typically suffer from high GPU memory consumption and tend to fail in sparse view scenarios. To overcome this problem, we develop a new pipeline, named Murre, for multi-view geometry reconstruction of 3D scenes based on SfM-guided monocular depth estimation. For input images, Murre first recover the SfM point cloud that captures the global scene structure, and then use it to guide a conditional diffusion model to produce multi-view metric depth maps for the final TSDF fusion. By predicting the depth map from a single image, Murre bypasses the multi-view matching step and naturally resolves the issues of previous MVS-based methods. In addition, the diffusion-based model can easily leverage the powerful priors of 2D foundation models, achieving good generalization ability across diverse real-world scenes. To obtain multi-view consistent depth maps, our key design is providing effective guidance on the diffusion model through the SfM point cloud, which is a condensed form of multi-view information, highlighting the scene's salient structure, and can be readily transformed into point maps to drive the image-space estimation process. We evaluate the reconstruction quality of Murre in various types of real-world datasets including indoor, streetscapes, and aerial scenes, surpassing state-of-the-art MVS-based and implicit neural reconstruction-based methods. The code will be released for reproducibility.</p>
            <p id="subjects-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" onclick="foldPdfKimi('Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="panel paper" keywords="cpg,adv,portrait,facial,customized,generation,portraits,adversarial,attacks,attack">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks_CVPR_2025_paper.html" target="_blank" title="76/95"><span class="index notranslate">#76</span></a>
                <a id="title-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="title-link" href="/venue/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" target="_blank">Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks</a>
                <a id="pdf-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF">30</sup>]</a>
                <a id="copy-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junying Wang" target="_blank">Junying Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyuan Zhang" target="_blank">Hongyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Yuan" target="_blank">Yuan Yuan</a>
            </p>
            <p id="summary-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="summary">Recent personalized portrait generation methods, taking a facial image and a textual prompt as inputs, have attracted substantial attention. Although these methods generate high-fidelity portraits, they fail to prevent the generated portraits from being tracked and misused by malicious face recognition systems. To address this, this paper proposes a Customized Portrait Generation framework with facial Adversarial attacks (Adv-CPG). Specifically, to achieve facial privacy protection, we devise a lightweight local ID encryptor and an encryption enhancer. They implement progressive double-layer encryption protection by directly injecting the target identity and adding additional identity guidance, respectively. Furthermore, to accomplish fine-grained and customized portrait generation, we develop a multi-modal image customizer capable of generating controllable fine-grained facial features. To the best of our knowledge, Adv-CPG is the first study that introduces facial adversarial attacks into customized portrait generation. Extensive experiments demonstrate the superiority of Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is 28.1% and 2.86% higher compared to the SOTA noise-based attack methods and unconstrained attack methods, respectively.</p>
            <p id="subjects-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="panel paper" keywords="global,skew,geometric,shapes,distribution,scenarios,domain,federated,guided,handling">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning_CVPR_2025_paper.html" target="_blank" title="77/95"><span class="index notranslate">#77</span></a>
                <a id="title-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="title-link" href="/venue/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" target="_blank">Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning</a>
                <a id="pdf-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF">31</sup>]</a>
                <a id="copy-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF">12</sup>]</a>
                <a id="rel-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yanbiao Ma" target="_blank">Yanbiao Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Dai" target="_blank">Wei Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenke Huang" target="_blank">Wenke Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Chen" target="_blank">Jiayi Chen</a>
            </p>
            <p id="summary-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="summary">Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence.</p>
            <p id="subjects-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" onclick="foldPdfKimi('Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="panel paper" keywords="dreamrelation,prompts,relation,customized,generation,relations,object,image,confusion,customization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation_CVPR_2025_paper.html" target="_blank" title="78/95"><span class="index notranslate">#78</span></a>
                <a id="title-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="title-link" href="/venue/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" target="_blank">DreamRelation: Bridging Customization and Relation Generation</a>
                <a id="pdf-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF">45</sup>]</a>
                <a id="copy-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF">17</sup>]</a>
                <a id="rel-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qingyu Shi" target="_blank">Qingyu Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Qi" target="_blank">Lu Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianzong Wu" target="_blank">Jianzong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinbin Bai" target="_blank">Jinbin Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingbo Wang" target="_blank">Jingbo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhai Tong" target="_blank">Yunhai Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangtai Li" target="_blank">Xiangtai Li</a>
            </p>
            <p id="summary-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="summary">Customized image generation is essential for delivering personalized content based on user-provided prompts, enabling large-scale text-to-image diffusion models to better align with individual needs. However, existing models often neglect the relationships between customized objects in generated images. In contrast, this work addresses this gap by focusing on relation-aware customized image generation, which seeks to preserve the identities from image prompts while maintaining the predicate relations specified in text prompts. Specifically, we introduce DreamRelation, a framework that disentangles identity and relation learning using a carefully curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges—generating accurate and natural relations, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features from the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on our proposed benchmarks demonstrate the superiority of DreamRelation in generating precise relations while preserving object identities across a diverse set of objects and relations. The source code and trained models will be made available to the public.</p>
            <p id="subjects-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="panel paper" keywords="m2f2,det,forgery,face,deepfake,forgeries,detection,interpretability,modal,forged">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face_CVPR_2025_paper.html" target="_blank" title="79/95"><span class="index notranslate">#79</span></a>
                <a id="title-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="title-link" href="/venue/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" target="_blank">Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector</a>
                <a id="pdf-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF">56</sup>]</a>
                <a id="copy-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Guo" target="_blank">Xiao Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiufeng Song" target="_blank">Xiufeng Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Zhang" target="_blank">Yue Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohong Liu" target="_blank">Xiaohong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoming Liu" target="_blank">Xiaoming Liu</a>
            </p>
            <p id="summary-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="summary">Deepfake detection is a long-established research topic crucial for combating the spread of malicious misinformation. Unlike previous methods that provide either binary classification results or textual explanations for deepfake detection, we propose a novel method that delivers both simultaneously. Our method harnesses the multi-modal learning power of the pre-trained CLIP and the unprecedented interpretability of large language models (LLMs) to enhance both the generalization and interpretability of deepfake detection. Specifically, we introduce a multi-modal face forgery detector (M2F2-Det) that employs specially designed face forgery prompt learning, integrating zero-shot learning capabilities of the pre-trained CLIP to improve generalization to unseen forgeries.Also, M2F2-Det incorporates the LLM to provide detailed explanations for detection decisions, offering strong interpretability by bridging the gap between natural language and the subtle nuances of facial forgery detection. Empirically, we evaluate M2F2-Det for both detection and sentence generation tasks, on both of which M2F2-Det achieves state-of-the-art performance, showing its effectiveness in detecting and explaining diverse and unseen forgeries. Code and models will be released upon publication.</p>
            <p id="subjects-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" onclick="foldPdfKimi('Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="panel paper" keywords="stereo,rgbd,depth,imaging,cues,igev,binocular,encoding,information,focus">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and_CVPR_2025_paper.html" target="_blank" title="80/95"><span class="index notranslate">#80</span></a>
                <a id="title-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="title-link" href="/venue/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" target="_blank">Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues</a>
                <a id="pdf-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF">18</sup>]</a>
                <a id="copy-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhui Liu" target="_blank">Yuhui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangxun Ou" target="_blank">Liangxun Ou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Fu" target="_blank">Qiang Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hadi Amata" target="_blank">Hadi Amata</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wolfgang Heidrich" target="_blank">Wolfgang Heidrich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Peng" target="_blank">Yifan Peng</a>
            </p>
            <p id="summary-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="summary">Extracting high-fidelity RGBD information from two-dimensional (2D) images is essential for various visual computing applications. Stereo imaging, as a reliable passive imaging technique for obtaining three-dimensional (3D) scene information, has benefited greatly from deep learning advancements. However, existing stereo depth estimation algorithms struggle to perceive high-frequency information and resolve high-resolution depth maps in realistic camera settings with large depth variations. These algorithms commonly neglect the hardware parameter configuration, limiting the potential for achieving optimal solutions solely through software-based design strategies.This work presents a hardware-software co-designed RGBD imaging framework that leverages both stereo and focus cues to reconstruct texture-rich color images along with detailed depth maps over a wide depth range. A pair of rank-2 parameterized diffractive optical elements (DOEs) is employed to encode perpendicular complementary information optically during stereo acquisitions. Additionally, we employ an IGEV-UNet-fused neural network tailored to the proposed rank-2 encoding for stereo matching and image reconstruction. Through prototyping a stereo camera with customized DOEs, our deep stereo imaging paradigm has demonstrated superior performance over existing monocular and stereo imaging systems in both image PSNR by 2.96 dB gain and depth accuracy in high-frequency details across distances from 0.67 to 8 meters.</p>
            <p id="subjects-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" onclick="foldPdfKimi('Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="panel paper" keywords="spatial,robospatial,robotics,centric,understanding,scans,ego,robots,world,images">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.html" target="_blank" title="81/95"><span class="index notranslate">#81</span></a>
                <a id="title-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="title-link" href="/venue/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" target="_blank">RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</a>
                <a id="pdf-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF">54</sup>]</a>
                <a id="copy-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chan Hee Song" target="_blank">Chan Hee Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Valts Blukis" target="_blank">Valts Blukis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Tremblay" target="_blank">Jonathan Tremblay</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephen Tyree" target="_blank">Stephen Tyree</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Su" target="_blank">Yu Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stan Birchfield" target="_blank">Stan Birchfield</a>
            </p>
            <p id="summary-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="summary">Spatial understanding is a crucial capability for robots to make grounded decisions based on their environment. This foundational skill enables robots not only to perceive their surroundings but also to reason about and interact meaningfully within the world. In modern robotics, these capabilities are taken on by visual language models, and they face significant challenges when applied to spatial reasoning context due to their training data sources. These sources utilize general-purpose image datasets, and they often lack sophisticated spatial scene understanding capabilities. For example, the datasets do not address reference frame comprehension — spatial relationships require clear contextual understanding, whether from a ego-centric, object-centric, or world-centric perspective, which allow for effective real-world interaction. To address this issue, we introduce RoboSpatial, a large-scale spatial understanding dataset consisting of real indoor and tabletop scenes captured as 3D scans and ego-centric images, annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M annotated spatial relationships, with paired 2D egocentric images and 3D scans to make it both 2D and 3D ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation.</p>
            <p id="subjects-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" onclick="foldPdfKimi('Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="panel paper" keywords="unpaired,ego,exo,view,rosetta,viewpoint,stone,video,rst,translator">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.html" target="_blank" title="82/95"><span class="index notranslate">#82</span></a>
                <a id="title-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="title-link" href="/venue/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" target="_blank">Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning</a>
                <a id="pdf-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF">26</sup>]</a>
                <a id="copy-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mi Luo" target="_blank">Mi Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihui Xue" target="_blank">Zihui Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Dimakis" target="_blank">Alex Dimakis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kristen Grauman" target="_blank">Kristen Grauman</a>
            </p>
            <p id="summary-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="summary">Egocentric and exocentric perspectives of human action differ significantly, yet overcoming this extreme viewpoint gap is critical for applications in augmented reality and robotics. We propose ViewpointRosetta, an approach that unlocks large-scale unpaired ego and exo video data to learn clip-level viewpoint-invariant video representations. Our framework introduces (1) a diffusion-based Rosetta Stone Translator (RST), which, leveraging a moderate amount of synchronized multi-view videos, serves as a translator in feature space to decipher the alignments between unpaired ego and exo data, and (2) a dual encoder that aligns unpaired data representations through contrastive learning with RST-based synthetic feature augmentation and soft alignment. To evaluate the learned features in a standardized setting, we construct a new cross-view benchmark using Ego-Exo4D, covering cross-view retrieval, action recognition, and skill assessment. Our framework demonstrates superior cross-view understanding compared to previous view-invariant learning and egocentric video representation learning approaches, and opens the door to bringing vast amounts of traditional third-person video to bear on the more nascent first-person setting.</p>
            <p id="subjects-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" onclick="foldPdfKimi('Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="panel paper" keywords="ard,fid,trajectory,historical,imagenet,256,autoregressive,distillation,transformer,susceptible">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Autoregressive_Distillation_of_Diffusion_Transformers_CVPR_2025_paper.html" target="_blank" title="83/95"><span class="index notranslate">#83</span></a>
                <a id="title-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="title-link" href="/venue/Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" target="_blank">Autoregressive Distillation of Diffusion Transformers</a>
                <a id="pdf-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Autoregressive_Distillation_of_Diffusion_Transformers_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF">63</sup>]</a>
                <a id="copy-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF">28</sup>]</a>
                <a id="rel-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yeongmin Kim" target="_blank">Yeongmin Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sotiris Anagnostidis" target="_blank">Sotiris Anagnostidis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuming Du" target="_blank">Yuming Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edgar Schönfeld" target="_blank">Edgar Schönfeld</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Kohler" target="_blank">Jonas Kohler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Markos Georgopoulos" target="_blank">Markos Georgopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albert Pumarola" target="_blank">Albert Pumarola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Thabet" target="_blank">Ali Thabet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Artsiom Sanakoyeu" target="_blank">Artsiom Sanakoyeu</a>
            </p>
            <p id="summary-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="summary">Diffusion models with transformer architectures have demonstrated promising capabilities in generating high-fidelity images and scalability for high resolution. However, iterative sampling process required for synthesis is very resource-intensive. A line of work has focused on distilling solutions to probability flow ODEs into few-step student models. Nevertheless, existing methods have been limited by their reliance on the most recent denoised samples as input, rendering them susceptible to exposure bias. To address this limitation, we propose AutoRegressive Distillation (ARD), a novel approach that leverages the historical trajectory of the ODE to predict future steps. ARD offers two key benefits: 1) it mitigates exposure bias by utilizing a predicted historical trajectory that is less susceptible to accumulated errors, and 2) it leverages the previous history of the ODE trajectory as a more effective source of coarse-grained information. ARD modifies the teacher transformer architecture by adding token-wise time embedding to mark each input from the trajectory history and employs a block-wise causal attention mask for training. Furthermore, incorporating historical inputs only in lower transformer layers enhances performance and efficiency. We validate the effectiveness of ARD in a class-conditioned generation on ImageNet and T2I synthesis. Our model achieves a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-31" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-32"><span class="mn" id="MathJax-Span-33" style="font-family: MathJax_Main;">5</span><span class="mo" id="MathJax-Span-34" style="font-family: MathJax_Main;">×</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo>×</mo></math></span></span><script type="math/tex" id="MathJax-Element-11">5\times</script> reduction in FID degradation compared to the baseline methods while requiring only 1.1\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of 1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available 1024p text-to-image distilled models in prompt adherence score with a minimal drop in FID compared to the teacher.</p>
            <p id="subjects-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" onclick="foldPdfKimi('Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="panel paper" keywords="mllms,spatial,remember,visual,multimodal,intelligence,subhuman,think,awareness,models">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember_CVPR_2025_paper.html" target="_blank" title="84/95"><span class="index notranslate">#84</span></a>
                <a id="title-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="title-link" href="/venue/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" target="_blank">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a>
                <a id="pdf-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF">79</sup>]</a>
                <a id="copy-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF">26</sup>]</a>
                <a id="rel-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jihan Yang" target="_blank">Jihan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shusheng Yang" target="_blank">Shusheng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anjali W. Gupta" target="_blank">Anjali W. Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rilyn Han" target="_blank">Rilyn Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Fei-Fei" target="_blank">Li Fei-Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saining Xie" target="_blank">Saining Xie</a>
            </p>
            <p id="summary-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="summary">Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also "think in space" from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive—though subhuman—visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance awareness.</p>
            <p id="subjects-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" onclick="foldPdfKimi('Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="panel paper" keywords="vps,manhattan,globustvp,world,convex,sdp,relaxation,vanishing,association,optimality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World_CVPR_2025_paper.html" target="_blank" title="85/95"><span class="index notranslate">#85</span></a>
                <a id="title-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="title-link" href="/venue/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" target="_blank">Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World</a>
                <a id="pdf-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF">30</sup>]</a>
                <a id="copy-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF">21</sup>]</a>
                <a id="rel-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bangyan Liao" target="_blank">Bangyan Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenjun Zhao" target="_blank">Zhenjun Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoang Li" target="_blank">Haoang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhou" target="_blank">Yi Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingping Zeng" target="_blank">Yingping Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Li" target="_blank">Hao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peidong Liu" target="_blank">Peidong Liu</a>
            </p>
            <p id="summary-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="summary">Determining the vanishing points (VPs) in a Manhattan world, as a fundamental task in many 3D vision applications, consists of jointly inferring the line-VP association and locating each VP. Existing methods are, however, either sub-optimal solvers or pursuing global optimality at a significant cost of computing time. In contrast to prior works, we introduce convex relaxation techniques to solve this task for the first time. Specifically, we employ a “soft” association scheme, realized via a truncated multi-selection error, that allows for joint estimation of VPs’ locations and line-VP associations. This approach leads to a primal problem that can be reformulated into a quadratically constrained quadratic programming (QCQP) problem, which is then relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP problem efficiently, we present a globally optimal outlier-robust iterative solver (called GlobustVP), which independently searches for one VP and its associated lines in each iteration, treating other lines as outliers. After each independent update of all VPs, the mutual orthogonality between the three VPs in a Manhattan world is reinforced via local refinement. Extensive experiments on both synthetic and real-world data demonstrate that GlobustVP achieves a favorable balance between efficiency, robustness, and global optimality compared to previous works. We will release our code for further study.</p>
            <p id="subjects-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" onclick="foldPdfKimi('Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="panel paper" keywords="scene,flow,wild,robotap,shot,foundation,prediction,scenes,casually,monocular">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild_CVPR_2025_paper.html" target="_blank" title="86/95"><span class="index notranslate">#86</span></a>
                <a id="title-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="title-link" href="/venue/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" target="_blank">Zero-Shot Monocular Scene Flow Estimation in the Wild</a>
                <a id="pdf-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF">52</sup>]</a>
                <a id="copy-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF">19</sup>]</a>
                <a id="rel-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiqing Liang" target="_blank">Yiqing Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhishek Badki" target="_blank">Abhishek Badki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Su" target="_blank">Hang Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Tompkin" target="_blank">James Tompkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Orazio Gallo" target="_blank">Orazio Gallo</a>
            </p>
            <p id="summary-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="summary">Foundation models have shown generalization across datasets for many low-level vision tasks, like depth estimation, but no such model exists for scene flow.Even though scene flow has wide potential use, it is not used in practice because current predictive models do not generalize well.We solve three challenges to fix this problem.First, we create a method that jointly estimates geometry and motion for accurate prediction.Second, we alleviate scene flow data scarcity with a data recipe that affords us 1M annotated training samples across diverse synthetic scenes.Third, we evaluate different parameterizations for scene flow prediction and identify a natural and effective parameterization.Our resulting model outperforms existing methods as well baselines built on foundation models in term of 3D end-point error, and shows zero-shot generalization to the casually captured videos from DAVIS and the robotic manipulation scenes from RoboTAP.Overall, this makes scene flow prediction significantly more practical for in-the-wild use.</p>
            <p id="subjects-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" onclick="foldPdfKimi('Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="panel paper" keywords="overlock,convnet,top,dds,contmix,context,biomimetic,overview,look,dynamic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels_CVPR_2025_paper.html" target="_blank" title="87/95"><span class="index notranslate">#87</span></a>
                <a id="title-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="title-link" href="/venue/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" target="_blank">OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels</a>
                <a id="pdf-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF">46</sup>]</a>
                <a id="copy-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF">20</sup>]</a>
                <a id="rel-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Lou" target="_blank">Meng Lou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhou Yu" target="_blank">Yizhou Yu</a>
            </p>
            <p id="summary-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="summary">In the human vision system, top-down attention plays a crucial role in perception, wherein the brain initially performs an overall but rough scene analysis to extract salient cues (i.e., overview first), followed by a finer-grained examination to make more accurate judgments (i.e., look closely next). However, recent efforts in ConvNet designs primarily focused on increasing kernel size to obtain a larger receptive field without considering this crucial biomimetic mechanism to further improve performance. To this end, we propose a novel pure ConvNet vision backbone, termed OverLoCK, which is carefully devised from both the architecture and mixer perspectives. Specifically, we introduce a biomimetic Deep-stage Decomposition Strategy (DDS) that fuses semantically meaningful context representations into middle and deep layers by providing dynamic top-down context guidance at both feature and kernel weight levels. To fully unleash the power of top-down context guidance, we further propose a novel **Cont**ext-**Mix**ing Dynamic Convolution (ContMix) that effectively models long-range dependencies while preserving inherent local inductive biases even when the input resolution increases. These properties are absent in previous convolutions. With the support from both DDS and ContMix, our OverLoCK exhibits notable performance improvement over existing methods. For instance, OverLoCK-T achieves a Top-1 accuracy of 84.2\%, significantly surpassing ConvNeXt-B while only using around one-third of the FLOPs/parameters. On object detection with Cascade Mask R-CNN, our OverLoCK-S surpasses MogaNet-B by a significant 1\% in AP<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-35" style="width: 0.471em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.37em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-36"><span class="msubsup" id="MathJax-Span-37"><span style="display: inline-block; position: relative; width: 0.367em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-38"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="mi" id="MathJax-Span-39" style="font-size: 70.7%; font-family: MathJax_Math-italic;">b</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mi>b</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-12">^b</script>. On semantic segmentation with UperNet, our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7\% in mIoU.</p>
            <p id="subjects-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" onclick="foldPdfKimi('Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="panel paper" keywords="grove,vocabulary,reward,skill,open,physical,vlm,learning,generalized,pose2clip">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill_CVPR_2025_paper.html" target="_blank" title="88/95"><span class="index notranslate">#88</span></a>
                <a id="title-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="title-link" href="/venue/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" target="_blank">GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill</a>
                <a id="pdf-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF">36</sup>]</a>
                <a id="copy-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF">20</sup>]</a>
                <a id="rel-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jieming Cui" target="_blank">Jieming Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tengyu Liu" target="_blank">Tengyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyu Meng" target="_blank">Ziyu Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiale Yu" target="_blank">Jiale Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Song" target="_blank">Ran Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Zhang" target="_blank">Wei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixin Zhu" target="_blank">Yixin Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Huang" target="_blank">Siyuan Huang</a>
            </p>
            <p id="summary-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="summary">Learning open-vocabulary physical skills for simulated agents remains challenging due to the limitations of reinforcement learning approaches: manually designed rewards lack scalability, while demonstration-based methods struggle to cover arbitrary tasks. We propose GROVE, a generalized reward framework for open-vocabulary physical skill learning without manual reward design or task-specific demonstrations. GROVE uniquely combines Large Language Models (LLMs) for generating precise constraints with Vision Language Models (VLMs) for semantic evaluation. Through an iterative reward design process, VLM-based feedback guides the refinement of LLM-generated constraints, significantly enhancing the reliability of our method. Central to our approach is Pose2CLIP, a lightweight pose-to-semantic feature mapper that significantly enhances the quality and efficiency of VLM evaluation. Extensive experiments demonstrate GROVE's versatility across diverse tasks and learning paradigms. Our approach achieves 22.2% higher naturalness and 25.7% better task completion score while training 8.4 times faster than previous open-vocabulary methods, establishing a new foundation for scalable physical skill acquisition.</p>
            <p id="subjects-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" onclick="foldPdfKimi('Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="panel paper" keywords="equivariance,ldm,ldms,shift,aliasing,alias,latent,redesign,diffusion,equivariant">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion_CVPR_2025_paper.html" target="_blank" title="89/95"><span class="index notranslate">#89</span></a>
                <a id="title-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" target="_blank">Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space</a>
                <a id="pdf-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF">36</sup>]</a>
                <a id="copy-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF">20</sup>]</a>
                <a id="rel-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Zhou" target="_blank">Yifan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeqi Xiao" target="_blank">Zeqi Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Yang" target="_blank">Shuai Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingang Pan" target="_blank">Xingang Pan</a>
            </p>
            <p id="summary-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="summary">Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation.</p>
            <p id="subjects-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="panel paper" keywords="dpo,policy,hallucination,hallucinations,opa,responses,aligns,vision,preference,expert">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.html" target="_blank" title="90/95"><span class="index notranslate">#90</span></a>
                <a id="title-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="title-link" href="/venue/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" target="_blank">Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key</a>
                <a id="pdf-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF">45</sup>]</a>
                <a id="copy-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF">17</sup>]</a>
                <a id="rel-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihe Yang" target="_blank">Zhihe Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xufang Luo" target="_blank">Xufang Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongqi Han" target="_blank">Dongqi Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunjian Xu" target="_blank">Yunjian Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongsheng Li" target="_blank">Dongsheng Li</a>
            </p>
            <p id="summary-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="summary">Hallucination remains a major challenge for Large Vision-Language Models (LVLMs). Direct Preference Optimization (DPO) has gained increasing attention as a simple solution to hallucination issues. It directly learns from constructed preference pairs that reflect the severity of hallucinations in responses to the same prompt and image. Nonetheless, different data construction methods in existing works bring notable performance variations. We identify a crucial factor here: outcomes are largely contingent on whether the constructed data aligns on-policy w.r.t the initial (reference) policy of DPO. Theoretical analysis suggests that learning from off-policy data is impeded by the presence of KL-divergence between the updated policy and the reference policy. From the perspective of dataset distribution, we systematically summarize the inherent flaws in existing algorithms that employ DPO to address hallucination issues. To alleviate the problems, we propose On-Policy Alignment (OPA)-DPO framework, which uniquely leverages expert feedback to correct hallucinated responses and aligns both the original and expert-revised responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B: 13.26\% on the AMBER benchmark and 5.39\% on the Object-Hal benchmark, compared to the previous SOTA algorithm trained with 16k samples.</p>
            <p id="subjects-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" onclick="foldPdfKimi('Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="panel paper" keywords="gea,generalist,embodied,multimodal,agents,lessons,diverse,online,mllms,mllm">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons_CVPR_2025_paper.html" target="_blank" title="91/95"><span class="index notranslate">#91</span></a>
                <a id="title-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="title-link" href="/venue/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" target="_blank">From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons</a>
                <a id="pdf-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF">44</sup>]</a>
                <a id="copy-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF">22</sup>]</a>
                <a id="rel-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Szot" target="_blank">Andrew Szot</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bogdan Mazoure" target="_blank">Bogdan Mazoure</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Omar Attia" target="_blank">Omar Attia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aleksei Timofeev" target="_blank">Aleksei Timofeev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harsh Agrawal" target="_blank">Harsh Agrawal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Devon Hjelm" target="_blank">Devon Hjelm</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Gan" target="_blank">Zhe Gan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zsolt Kira" target="_blank">Zsolt Kira</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Toshev" target="_blank">Alexander Toshev</a>
            </p>
            <p id="summary-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="summary">We examine the capability of Multimodal Large Language Models (MLLMs) to tackle diverse domains that extend beyond the traditional language and vision tasks these models are typically trained on. Specifically, our focus lies in areas such as Embodied AI, Games, UI Control, and Planning. To this end, we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA). GEA is a single unified model capable of grounding itself across these varied domains through a multi-embodiment action tokenizer. GEA is trained with supervised learning on a large dataset of embodied experiences and with online RL in interactive simulators. We explore the data and algorithmic choices necessary to develop such a model. Our findings reveal the importance of training with cross-domain data and online RL for building generalist agents. The final GEA model achieves strong generalization performance to unseen tasks across diverse benchmarks compared to other generalist models and benchmark-specific approaches.</p>
            <p id="subjects-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" onclick="foldPdfKimi('Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="panel paper" keywords="degradation,dornet,depth,rgb,oriented,regularized,unknown,resolution,super,real">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth_CVPR_2025_paper.html" target="_blank" title="92/95"><span class="index notranslate">#92</span></a>
                <a id="title-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="title-link" href="/venue/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" target="_blank">DORNet: A Degradation Oriented and Regularized Network for Blind Depth Super-Resolution</a>
                <a id="pdf-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF">42</sup>]</a>
                <a id="copy-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF">17</sup>]</a>
                <a id="rel-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengxue Wang" target="_blank">Zhengxue Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqiang Yan" target="_blank">Zhiqiang Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinshan Pan" target="_blank">Jinshan Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangwei Gao" target="_blank">Guangwei Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Yang" target="_blank">Jian Yang</a>
            </p>
            <p id="summary-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="summary">Recent RGB-guided depth super-resolution methods have achieved impressive performance under the assumption of fixed and known degradation (e.g., bicubic downsampling). However, in real-world scenarios, captured depth data often suffer from unconventional and unknown degradation due to sensor limitations and complex imaging environments (e.g., low reflective surfaces, varying illumination). Consequently, the performance of these methods significantly declines when real-world degradation deviate from their assumptions. In this paper, we propose the Degradation Oriented and Regularized Network (DORNet), a novel framework designed to adaptively address unknown degradation in real-world scenes through implicit degradation representations. Our approach begins with the development of a self-supervised degradation learning strategy, which models the degradation representations of low-resolution depth data using routing selection-based degradation regularization. To facilitate effective RGB-D fusion, we further introduce a degradation-oriented feature transformation module that selectively propagates RGB content into the depth data based on the learned degradation priors. Extensive experimental results on both real and synthetic datasets demonstrate the superiority of our DORNet in handling unknown degradations, outperforming existing methods.</p>
            <p id="subjects-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" onclick="foldPdfKimi('Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="panel paper" keywords="warping,illusions,viewed,generative,pyramid,lookingglass,anamorphoses,laplacian,anamorphosis,anagrams">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping_CVPR_2025_paper.html" target="_blank" title="93/95"><span class="index notranslate">#93</span></a>
                <a id="title-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="title-link" href="/venue/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" target="_blank">LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping</a>
                <a id="pdf-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF">45</sup>]</a>
                <a id="copy-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF">24</sup>]</a>
                <a id="rel-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Chang" target="_blank">Pascal Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergio Sancho" target="_blank">Sergio Sancho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingwei Tang" target="_blank">Jingwei Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Markus Gross" target="_blank">Markus Gross</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vinicius Azevedo" target="_blank">Vinicius Azevedo</a>
            </p>
            <p id="summary-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="summary">Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce _Laplacian Pyramid Warping_, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams [Geng et al. 2024] to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions.</p>
            <p id="subjects-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" onclick="foldPdfKimi('Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="panel paper" keywords="difffno,ode,wfno,fourier,super,operator,resolution,neural,diffusion,details">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator_CVPR_2025_paper.html" target="_blank" title="94/95"><span class="index notranslate">#94</span></a>
                <a id="title-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="title-link" href="/venue/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" target="_blank">DiffFNO: Diffusion Fourier Neural Operator</a>
                <a id="pdf-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF">82</sup>]</a>
                <a id="copy-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF">35</sup>]</a>
                <a id="rel-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyi Liu" target="_blank">Xiaoyi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tang" target="_blank">Hao Tang</a>
            </p>
            <p id="summary-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="summary">We introduce DiffFNO, a novel framework for arbitrary-scale super-resolution that incorporates a Weighted Fourier Neural Operator (WFNO) enhanced by a diffusion process. DiffFNO's adaptive mode weighting mechanism in the Fourier domain effectively captures critical frequency components, significantly improving the reconstruction of high-frequency image details that are essential for super-resolution tasks.Additionally, we propose a Gated Fusion Mechanism to efficiently integrate features from the WFNO and an attention-based neural operator, enhancing the network's capability to capture both global and local image details. To further improve efficiency, DiffFNO employs a deterministic ODE sampling strategy called the Adaptive Time-step ODE Solver (AT-ODE), which accelerates inference by dynamically adjusting step sizes while preserving output quality.Extensive experiments demonstrate that DiffFNO achieves state-of-the-art results, outperforming existing methods across various scaling factors, including those beyond the training distribution, by a margin of 2–4 dB in PSNR. Our approach sets a new standard in super-resolution, delivering both superior accuracy and computational efficiency.</p>
            <p id="subjects-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" onclick="foldPdfKimi('Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="difix3d,difix,reconstruction,underconstrained,artifacts,rendered,novel,step,diffusion,single">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="95/95"><span class="index notranslate">#95</span></a>
                <a id="title-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" target="_blank">DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</a>
                <a id="pdf-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF">85</sup>]</a>
                <a id="copy-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF">39</sup>]</a>
                <a id="rel-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jay Zhangjie Wu" target="_blank">Jay Zhangjie Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Zhang" target="_blank">Yuxuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haithem Turki" target="_blank">Haithem Turki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanchi Ren" target="_blank">Xuanchi Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Gao" target="_blank">Jun Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mike Zheng Shou" target="_blank">Mike Zheng Shou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanja Fidler" target="_blank">Sanja Fidler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zan Gojcic" target="_blank">Zan Gojcic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Ling" target="_blank">Huan Ling</a>
            </p>
            <p id="summary-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="summary">Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation.Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2x improvement in FID score over baselines while maintaining 3D consistency.</p>
            <p id="subjects-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Oral" target="_blank">CVPR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF">Camera Resection from Known Line Pencils and a Radially Distorted Scanline</a>
                    <a class="i-star" onclick="toggleAppStar('Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF">Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF">UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming</a>
                    <a class="i-star" onclick="toggleAppStar('Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF">SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images</a>
                    <a class="i-star" onclick="toggleAppStar('Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF">VGGT: Visual Geometry Grounded Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF">Reconstructing Humans with a Biomechanically Accurate Skeleton</a>
                    <a class="i-star" onclick="toggleAppStar('Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF">CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner</a>
                    <a class="i-star" onclick="toggleAppStar('Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF">DNF: Unconditional 4D Generation with Dictionary-based Neural Fields</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF">Removing Reflections from RAW Photos</a>
                    <a class="i-star" onclick="toggleAppStar('Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF">Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF">DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF">Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF">RandAR: Decoder-only Autoregressive Visual Generation in Random Orders</a>
                    <a class="i-star" onclick="toggleAppStar('Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF">MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF">FoundationStereo: Zero-Shot Stereo Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF">Motion Prompting: Controlling Video Generation with Motion Trajectories</a>
                    <a class="i-star" onclick="toggleAppStar('Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF">Language-Guided Image Tokenization for Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF">Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens</a>
                    <a class="i-star" onclick="toggleAppStar('Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF">Temporally Consistent Object-Centric Learning by Contrasting Slots</a>
                    <a class="i-star" onclick="toggleAppStar('Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF">Reanimating Images using Neural Representations of Dynamic Stimuli</a>
                    <a class="i-star" onclick="toggleAppStar('Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF">Towards Universal Dataset Distillation via Task-Driven Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF">Identifying and Mitigating Position Bias of Multi-image Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF">Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF">The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF">Opportunistic Single-Photon Time of Flight</a>
                    <a class="i-star" onclick="toggleAppStar('Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF">Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF">Neural Inverse Rendering from Propagating Light</a>
                    <a class="i-star" onclick="toggleAppStar('Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF">OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF">CustAny: Customizing Anything from A Single Example</a>
                    <a class="i-star" onclick="toggleAppStar('Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF">Birth and Death of a Rose</a>
                    <a class="i-star" onclick="toggleAppStar('Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF">Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</a>
                    <a class="i-star" onclick="toggleAppStar('Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF">Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF">Continuous 3D Perception Model with Persistent State</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF">AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF">LibraGrad: Balancing Gradient Flow for Universally Better Vision Transformer Attributions</a>
                    <a class="i-star" onclick="toggleAppStar('Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF">3D Student Splatting and Scooping</a>
                    <a class="i-star" onclick="toggleAppStar('Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF">Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF">CleanDIFT: Diffusion Features without Noise</a>
                    <a class="i-star" onclick="toggleAppStar('Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF">CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF">IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF">Temporal Alignment-Free Video Matching for Few-shot Action Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF">Gromov-Wasserstein Problem with Cyclic Symmetry</a>
                    <a class="i-star" onclick="toggleAppStar('Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF">MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF">Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF">MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF">Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF">Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF">VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</a>
                    <a class="i-star" onclick="toggleAppStar('Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF">Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining</a>
                    <a class="i-star" onclick="toggleAppStar('Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF">PDFactor: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation</a>
                    <a class="i-star" onclick="toggleAppStar('Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF">Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF">TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</a>
                    <a class="i-star" onclick="toggleAppStar('Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF">SEAL: Semantic Attention Learning for Long Video Representation</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF">Rethinking Spiking Self-Attention Mechanism: Implementing a-XNOR Similarity Calculation in Spiking Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF">TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF">3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF">MEGA: Masked Generative Autoencoder for Human Mesh Recovery</a>
                    <a class="i-star" onclick="toggleAppStar('Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF">Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild</a>
                    <a class="i-star" onclick="toggleAppStar('Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF">TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF">Enhancing Diversity for Data-free Quantization</a>
                    <a class="i-star" onclick="toggleAppStar('Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bar_Navigation_World_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#Bar_Navigation_World_Models@CVPR2025@CVF">Navigation World Models</a>
                    <a class="i-star" onclick="toggleAppStar('Bar_Navigation_World_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bar_Navigation_World_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF">CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF">FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video</a>
                    <a class="i-star" onclick="toggleAppStar('Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF">Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF">Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF">Minority-Focused Text-to-Image Generation via Prompt Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF">EgoLM: Multi-Modal Language Model of Egocentric Motions</a>
                    <a class="i-star" onclick="toggleAppStar('Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF">Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF">LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF">Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF">Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF">Effective SAM Combination for Open-Vocabulary Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF">Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays</a>
                    <a class="i-star" onclick="toggleAppStar('Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF">Multi-view Reconstruction via SfM-guided Monocular Depth Estimation</a>
                    <a class="i-star" onclick="toggleAppStar('Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF">Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF">Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF">DreamRelation: Bridging Customization and Relation Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF">Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector</a>
                    <a class="i-star" onclick="toggleAppStar('Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF">Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF">RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</a>
                    <a class="i-star" onclick="toggleAppStar('Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF">Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF">Autoregressive Distillation of Diffusion Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF">Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World</a>
                    <a class="i-star" onclick="toggleAppStar('Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF">Zero-Shot Monocular Scene Flow Estimation in the Wild</a>
                    <a class="i-star" onclick="toggleAppStar('Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF">OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels</a>
                    <a class="i-star" onclick="toggleAppStar('Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF">GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill</a>
                    <a class="i-star" onclick="toggleAppStar('Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF">Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF">Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF">From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons</a>
                    <a class="i-star" onclick="toggleAppStar('Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF">DORNet: A Degradation Oriented and Regularized Network for Blind Depth Super-Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF">LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping</a>
                    <a class="i-star" onclick="toggleAppStar('Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF">DiffFNO: Diffusion Fourier Neural Operator</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF">DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Math-italic, sans-serif;"></div></div></body></html>