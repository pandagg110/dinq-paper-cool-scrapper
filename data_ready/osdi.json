{
  "source_html": "html\\osdi.html",
  "paper_count": 53,
  "conference": "osdi",
  "year": 2025,
  "status": "normal",
  "papers": [
    {
      "paper_id": "zhang-tony@osdi25@USENIX",
      "index": 1,
      "title": "Basilisk: Using Provenance Invariants to Automate Proofs of Undecidable Protocols",
      "authors": [
        "Tony Nuda Zhang",
        "Keshav Singh",
        "Tej Chajed",
        "Manos Kapritsos",
        "Bryan Parno"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "provenance",
        "invariants",
        "basilisk",
        "inductive",
        "developer",
        "protocols",
        "protocol",
        "host",
        "proofs",
        "safety"
      ],
      "summary": "Distributed protocols are challenging to design correctly. One promising approach to improve their reliability uses formal verification to prove that a protocol satisfies a desired safety property. These proofs require finding an inductive invariant that holds initially, implies safety, and is inductive. Devising an inductive invariant is a difficult task that prior work has either automated by requiring the protocol to be expressed in a decidable but restrictive fragment of logic, or required the developer to find by a painful search process. In this work we aim to automatically find inductive invariants without restricting the logic. We do so using two key insights. Our first insight is that many of the complex inter-host properties that prior work required the developer to provide can instead be derived using Provenance Invariants, a class of invariants that relate a local variable in a host to its provenance, i.e., the protocol step that caused it to have its current value. By tracing the provenance of one host variable back to another host, we can derive an invariant relating the two hosts' states. Second, we develop an algorithm called Atomic Sharding to derive Provenance Invariants automatically by statically analyzing the protocol's steps. We implement these ideas in a tool called Basilisk and apply it to 16 distributed protocols. Basilisk automatically finds a set of invariants and proves their inductiveness, with little or no developer assistance. In all cases, these generated invariants are sufficient for us to prove safety without needing to identify any new inductive invariants.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/zhang-tony"
        ],
        "venue": [
          "/venue/zhang-tony@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-zhang-tony.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/zhang-tony"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 9
      },
      "raw_excerpt": "Basilisk: Using Provenance Invariants to Automate Proofs of Undecidable Protocols [PDF 3 ] [Copy] [Kimi 9 ] [REL] Authors : Tony Nuda Zhang , Keshav Singh , Tej Chajed , Manos Kapritsos , Bryan Parno Distributed protocols are challenging to design correctly. One promising approach to improve their reliability uses formal verification to prove that a protocol satisfies a desired safety property. These proofs require finding an inductive invariant that holds initially, implies safety, and is inductive. Devising an inductive invariant is a difficult task that prior work has either automated by requiring the protocol to be expressed in a decidable but restrictive fragment of logic, or required the developer to find by a painful search process. In this work we aim to automatically find inductive invariants without restricting the logic. We do so using two key insights. Our first insight is that many of the complex inter-host properties that prior work required the developer to provide can instead be derived using Provenance Invariants, a class of invariants that relate a local variable in a host to its provenance, i.e., the protocol step that caused it to have its current value. By tracing the provenance of one host variable back to another host, we can derive an invariant relating the two hosts' states. Second, we develop an algorithm called Atomic Sharding to derive Provenance Invariants automatically by statically analyzing the protocol's steps. We implement these ideas in a tool called Basilisk and apply it to 16 distributed protocols. Basilisk automatically finds a set of invariants and proves their inductiveness, with little or no developer assistance. In all cases, these generated invariants are sufficient for us to prove safety without needing to identify any new inductive invariants. Subject : OSDI.2025"
    },
    {
      "paper_id": "lou@osdi25@USENIX",
      "index": 2,
      "title": "Deriving Semantic Checkers from Tests to Detect Silent Failures in Production Distributed Systems",
      "authors": [
        "Chang Lou",
        "Dimas Shidqi Parikesit",
        "Yujin Huang",
        "Zhewen Yang",
        "Senapati Diwangkara",
        "Yuzhuo Jing",
        "Achmad Imam Kistijantoro",
        "Ding Yuan",
        "Suman Nath",
        "Peng Huang"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "checkers",
        "t2c",
        "failures",
        "silent",
        "detect",
        "distributed",
        "test",
        "runtime",
        "silently",
        "systems"
      ],
      "summary": "Production distributed systems provide rich features, but various defects can cause a system to silently violate its semantics without explicit errors. Such failures cause serious consequences. Yet, they are extremely challenging to detect, as it requires deep domain knowledge and substantial manual efforts to write good checkers. In this paper, we explore a novel approach that directly derives semantic checkers from system test code. We first present a large-scale study on existing system test cases. Guided by the study findings, we develop T2C, a framework that uses static and dynamic analysis to transform and generalize a test into a runtime checker. We apply T2C on four large, popular distributed systems and successfully derive tens to hundreds of checkers. These checkers detect 15 out of 20 real-world silent failures we reproduce and incur small runtime overhead.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/lou"
        ],
        "venue": [
          "/venue/lou@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-lou.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/lou"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Deriving Semantic Checkers from Tests to Detect Silent Failures in Production Distributed Systems [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Chang Lou , Dimas Shidqi Parikesit , Yujin Huang , Zhewen Yang , Senapati Diwangkara , Yuzhuo Jing , Achmad Imam Kistijantoro , Ding Yuan , Suman Nath , Peng Huang Production distributed systems provide rich features, but various defects can cause a system to silently violate its semantics without explicit errors. Such failures cause serious consequences. Yet, they are extremely challenging to detect, as it requires deep domain knowledge and substantial manual efforts to write good checkers. In this paper, we explore a novel approach that directly derives semantic checkers from system test code. We first present a large-scale study on existing system test cases. Guided by the study findings, we develop T2C, a framework that uses static and dynamic analysis to transform and generalize a test into a runtime checker. We apply T2C on four large, popular distributed systems and successfully derive tens to hundreds of checkers. These checkers detect 15 out of 20 real-world silent failures we reproduce and incur small runtime overhead. Subject : OSDI.2025"
    },
    {
      "paper_id": "frank@osdi25@USENIX",
      "index": 3,
      "title": "Picsou: Enabling Replicated State Machines to Communicate Efficiently",
      "authors": [
        "Reginald Frank",
        "Micah Murray",
        "Chawinphat Tankuranand",
        "Junseo Yoo",
        "Ethan Xu",
        "Natacha Crooks",
        "Suyash Gupta",
        "Manos Kapritsos"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "picsou",
        "communicate",
        "quacks",
        "c3b",
        "rsms",
        "replicated",
        "machines",
        "resends",
        "tolerant",
        "fault"
      ],
      "summary": "Replicated state machines (RSMs) cannot communicate effectively today as there is no formal framework or efficient protocol to do so. To address this issue, we introduce a new primitive, Cross-Cluster Consistent Broadcast (C3B) and present Picsou, a practical C3B implementation. Picsou draws inspiration from networking and TCP to allow two RSMs to communicate with constant metadata overhead in the failure-free case and a minimal number of message resends in the case of failures. Picsou is flexible and allows both crash fault tolerant and Byzantine fault tolerant protocols to communicate. At the heart of Picsou's good performance and generality is the concept of Quacks (quorum acknowledgments). Quacks allow nodes in each RSM to precisely determine when messages have definitely been received, or likely lost. Our results are promising: we obtain up to 24× better performance than prior solutions on microbenchmarks and applications, ranging from disaster recovery to data reconciliation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/frank"
        ],
        "venue": [
          "/venue/frank@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-frank.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/frank"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Picsou: Enabling Replicated State Machines to Communicate Efficiently [PDF ] [Copy] [Kimi ] [REL] Authors : Reginald Frank , Micah Murray , Chawinphat Tankuranand , Junseo Yoo , Ethan Xu , Natacha Crooks , Suyash Gupta , Manos Kapritsos Replicated state machines (RSMs) cannot communicate effectively today as there is no formal framework or efficient protocol to do so. To address this issue, we introduce a new primitive, Cross-Cluster Consistent Broadcast (C3B) and present Picsou, a practical C3B implementation. Picsou draws inspiration from networking and TCP to allow two RSMs to communicate with constant metadata overhead in the failure-free case and a minimal number of message resends in the case of failures. Picsou is flexible and allows both crash fault tolerant and Byzantine fault tolerant protocols to communicate. At the heart of Picsou's good performance and generality is the concept of Quacks (quorum acknowledgments). Quacks allow nodes in each RSM to precisely determine when messages have definitely been received, or likely lost. Our results are promising: we obtain up to 24× better performance than prior solutions on microbenchmarks and applications, ranging from disaster recovery to data reconciliation. Subject : OSDI.2025"
    },
    {
      "paper_id": "wang-xiaoyang@osdi25@USENIX",
      "index": 4,
      "title": "FineMem: Breaking the Allocation Overhead vs. Memory Waste Dilemma in Fine-Grained Disaggregated Memory Management",
      "authors": [
        "Xiaoyang Wang",
        "Yongkun Li",
        "Kan Wu",
        "Wenzhe Zhu",
        "Yuqi Li",
        "Yinlong Xu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "memory",
        "rdma",
        "finemem",
        "allocation",
        "remote",
        "waste",
        "grained",
        "node",
        "management",
        "allocations"
      ],
      "summary": "RDMA-enabled memory disaggregation has emerged as an attractive approach to reducing memory costs in modern data centers. While RDMA enables efficient remote read/write operations, it presents challenges in remote memory (de)allocation. Consequently, existing systems adopt coarse-grained allocations (in GBs), leading to memory waste. We introduce FineMem, an RDMA-connected remote memory management system that enables high-performance, fine-grained memory allocation. FineMem addresses latency and scalability challenges related to fine-grained allocations. It removes RDMA memory region (MR) registration costs from allocation paths through per-compute node MR pre-registration, while ensuring remote memory isolation using RDMA memory windows and a trusted allocation service on each compute node. It employs a lock-free, one-sided RDMA-based protocol to allocate memory chunks (e.g., 4KB, 2MB) without involving the memory node's CPU and maintains metadata consistency during compute node failures via logging. We show that FineMem reduces remote memory allocation latency by as much as 95% compared to state-of-the-art remote memory management systems. It enables memory malloc systems, key-value stores systems, and swap systems running on FineMem to achieve low memory waste with minimal overhead.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-xiaoyang"
        ],
        "venue": [
          "/venue/wang-xiaoyang@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-wang-xiaoyang.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-xiaoyang"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "FineMem: Breaking the Allocation Overhead vs. Memory Waste Dilemma in Fine-Grained Disaggregated Memory Management [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Xiaoyang Wang , Yongkun Li , Kan Wu , Wenzhe Zhu , Yuqi Li , Yinlong Xu RDMA-enabled memory disaggregation has emerged as an attractive approach to reducing memory costs in modern data centers. While RDMA enables efficient remote read/write operations, it presents challenges in remote memory (de)allocation. Consequently, existing systems adopt coarse-grained allocations (in GBs), leading to memory waste. We introduce FineMem, an RDMA-connected remote memory management system that enables high-performance, fine-grained memory allocation. FineMem addresses latency and scalability challenges related to fine-grained allocations. It removes RDMA memory region (MR) registration costs from allocation paths through per-compute node MR pre-registration, while ensuring remote memory isolation using RDMA memory windows and a trusted allocation service on each compute node. It employs a lock-free, one-sided RDMA-based protocol to allocate memory chunks (e.g., 4KB, 2MB) without involving the memory node's CPU and maintains metadata consistency during compute node failures via logging. We show that FineMem reduces remote memory allocation latency by as much as 95% compared to state-of-the-art remote memory management systems. It enables memory malloc systems, key-value stores systems, and swap systems running on FineMem to achieve low memory waste with minimal overhead. Subject : OSDI.2025"
    },
    {
      "paper_id": "wang-yun@osdi25@USENIX",
      "index": 5,
      "title": "To PRI or Not To PRI, That's the question",
      "authors": [
        "Yun Wang",
        "Liang Chen",
        "Jie Ji",
        "Xianting Tian",
        "Ben Luo",
        "Zhixiang Wei",
        "Zhibai Huang",
        "Kailiang Xu",
        "Kaihuan Peng",
        "Kaijie Guo",
        "Ning Luo",
        "Guangjian Wang",
        "Shengdong Dai",
        "Yibin Shen",
        "Jiesheng Wu",
        "Zhengwei Qi"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "passthrough",
        "pri",
        "vio",
        "page",
        "device",
        "iopfs",
        "devices",
        "memory",
        "iopa",
        "native"
      ],
      "summary": "SR-IOV and I/O device passthrough enable network and storage devices to be shared among multiple tenants with high density using virtual functions (VFs), achieving near-native performance. However, passthrough does not support page faults, requiring the hypervisor to statically pin the VM-allocated memory. This approach is unacceptable for cloud service providers (CSPs) that rely on oversubscription to enhance memory utilization and reduce costs. The Page Request Interface (PRI) was designed to support device-side I/O page faults (IOPFs) through collaboration among devices, Input-Output Memory Management Units (IOMMU), and the OS. But PRI has not seen broad adoption in devices like NICs and storage. We propose VIO, a novel dynamic I/O device passthrough approach that achieves near-native performance and is hardware-independent. By leveraging a shadow available queue, VIO can dynamically and transparently switch devices between VIO and passthrough modes based on I/O operations per second (IOPS) pressure, balancing resource utilization and performance. Each DMA request is probed via IOPA-snooping in the virtio data plane to eliminate IOPFs, while device interrupts are directly passed through to the VM guest, enabling performance close to passthrough. VIO is extensively tested and deployed by a leading global CSP across 300K VMs, supporting both legacy and new instances while reclaiming up to the equivalent of 30K VM memory daily without compromising user Service Level Objectives (SLOs). As the scale grows, the benefits continue to increase.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-yun"
        ],
        "venue": [
          "/venue/wang-yun@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-wang-yun.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-yun"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "To PRI or Not To PRI, That's the question [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yun Wang , Liang Chen , Jie Ji , Xianting Tian , Ben Luo , Zhixiang Wei , Zhibai Huang , Kailiang Xu , Kaihuan Peng , Kaijie Guo , Ning Luo , Guangjian Wang , Shengdong Dai , Yibin Shen , Jiesheng Wu , Zhengwei Qi SR-IOV and I/O device passthrough enable network and storage devices to be shared among multiple tenants with high density using virtual functions (VFs), achieving near-native performance. However, passthrough does not support page faults, requiring the hypervisor to statically pin the VM-allocated memory. This approach is unacceptable for cloud service providers (CSPs) that rely on oversubscription to enhance memory utilization and reduce costs. The Page Request Interface (PRI) was designed to support device-side I/O page faults (IOPFs) through collaboration among devices, Input-Output Memory Management Units (IOMMU), and the OS. But PRI has not seen broad adoption in devices like NICs and storage. We propose VIO, a novel dynamic I/O device passthrough approach that achieves near-native performance and is hardware-independent. By leveraging a shadow available queue, VIO can dynamically and transparently switch devices between VIO and passthrough modes based on I/O operations per second (IOPS) pressure, balancing resource utilization and performance. Each DMA request is probed via IOPA-snooping in the virtio data plane to eliminate IOPFs, while device interrupts are directly passed through to the VM guest, enabling performance close to passthrough. VIO is extensively tested and deployed by a leading global CSP across 300K VMs, supporting both legacy and new instances while reclaiming up to the equivalent of 30K VM memory daily without compromising user Service Level Objectives (SLOs). As the scale grows, the benefits continue to increase. Subject : OSDI.2025"
    },
    {
      "paper_id": "ren@osdi25@USENIX",
      "index": 6,
      "title": "Enabling Efficient GPU Communication over Multiple NICs with FuseLink",
      "authors": [
        "Zhenghang Ren",
        "Yuxuan Li",
        "Zilong Wang",
        "Xinyang Huang",
        "Wenxue Li",
        "Kaiqiang Xu",
        "Xudong Liao",
        "Yijun Sun",
        "Bowen Liu",
        "Han Tian",
        "Junxue Zhang",
        "Mingfei Wang",
        "Zhizhen Zhong",
        "Guyue Liu",
        "Ying Zhang",
        "Kai Chen"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "nics",
        "fuselink",
        "server",
        "gpu",
        "nccl",
        "communication",
        "gpus",
        "inter",
        "traffic",
        "imbalanced"
      ],
      "summary": "Machine learning (ML) clusters stack multiple network interface cards (NICs) within each server to improve inter-server GPU communication bandwidth. However, existing systems fall short in fully utilizing NICs because of static GPU-NIC bindings. This leads to bottlenecks at hot-spot NICs when handling imbalanced communication in ML tasks. For example, large language model serving instances may have different communication demands across NICs; expert-parallel training tasks have imbalanced all-to-all traffic; and the embedding transmission volumes during recommendation model training vary across GPUs. To fully utilize all NICs, we propose FuseLink to enable efficient GPU communication over multiple NICs. FuseLink extends inter-server network by integrating high-speed intra-server connections, and leverages GPUs to efficiently relay traffic to idle NICs. We implement FuseLink and integrate it into NCCL, so that ML applications can benefit from FuseLink seamlessly without code modifications. Compared to NCCL, we demonstrate that FuseLink achieves up to 212GBps bandwidth between two inter-server GPUs and accelerates ML tasks with dynamic traffic patterns. Specifically, it reduces the latencies of first-token generation in LLM model servings by 1.04-2.73×, improves the training throughput of mixture-of-experts model by up to 1.3×, and accelerates deep learning recommendation model training by up to 1.2×.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/ren"
        ],
        "venue": [
          "/venue/ren@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-ren.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/ren"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Enabling Efficient GPU Communication over Multiple NICs with FuseLink [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Zhenghang Ren , Yuxuan Li , Zilong Wang , Xinyang Huang , Wenxue Li , Kaiqiang Xu , Xudong Liao , Yijun Sun , Bowen Liu , Han Tian , Junxue Zhang , Mingfei Wang , Zhizhen Zhong , Guyue Liu , Ying Zhang , Kai Chen Machine learning (ML) clusters stack multiple network interface cards (NICs) within each server to improve inter-server GPU communication bandwidth. However, existing systems fall short in fully utilizing NICs because of static GPU-NIC bindings. This leads to bottlenecks at hot-spot NICs when handling imbalanced communication in ML tasks. For example, large language model serving instances may have different communication demands across NICs; expert-parallel training tasks have imbalanced all-to-all traffic; and the embedding transmission volumes during recommendation model training vary across GPUs. To fully utilize all NICs, we propose FuseLink to enable efficient GPU communication over multiple NICs. FuseLink extends inter-server network by integrating high-speed intra-server connections, and leverages GPUs to efficiently relay traffic to idle NICs. We implement FuseLink and integrate it into NCCL, so that ML applications can benefit from FuseLink seamlessly without code modifications. Compared to NCCL, we demonstrate that FuseLink achieves up to 212GBps bandwidth between two inter-server GPUs and accelerates ML tasks with dynamic traffic patterns. Specifically, it reduces the latencies of first-token generation in LLM model servings by 1.04-2.73×, improves the training throughput of mixture-of-experts model by up to 1.3×, and accelerates deep learning recommendation model training by up to 1.2×. Subject : OSDI.2025"
    },
    {
      "paper_id": "huang-yibo@osdi25@USENIX",
      "index": 7,
      "title": "Tigon: A Distributed Database for a CXL Pod",
      "authors": [
        "Yibo Huang",
        "Haowei Chen",
        "Newton Ni",
        "Yan Sun",
        "Vijay Chidambaram",
        "Dixin Tang",
        "Emmett Witchel"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "cxl",
        "tigon",
        "distributed",
        "memory",
        "databases",
        "database",
        "accesses",
        "host",
        "concurrent",
        "pod"
      ],
      "summary": "Building efficient distributed transactional databases remains a challenging problem despite decades of research. Existing distributed databases synchronize cross-host concurrent data accesses over a network, which requires numerous message exchanges and introduces performance overhead. We describe Tigon, the first distributed in-memory database that synchronizes cross-host concurrent data accesses using atomic operations on CXL memory. Using CXL memory is more efficient than network-based approaches, however, Tigon must address the limitations of CXL memory. The limitations are CXL’s higher latency and lower bandwidth relative to local DRAM, and its limited hardware support for cross-host cache coherence. For TPC-C and a variant of YCSB, Tigon achieves up to 2.5× higher throughput compared with two optimized shared-nothing databases that use CXL memory as a transport and up to 18.5× higher throughput compared with an RDMA-based distributed database.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/huang-yibo"
        ],
        "venue": [
          "/venue/huang-yibo@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-huang-yibo.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/huang-yibo"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Tigon: A Distributed Database for a CXL Pod [PDF ] [Copy] [Kimi ] [REL] Authors : Yibo Huang , Haowei Chen , Newton Ni , Yan Sun , Vijay Chidambaram , Dixin Tang , Emmett Witchel Building efficient distributed transactional databases remains a challenging problem despite decades of research. Existing distributed databases synchronize cross-host concurrent data accesses over a network, which requires numerous message exchanges and introduces performance overhead. We describe Tigon, the first distributed in-memory database that synchronizes cross-host concurrent data accesses using atomic operations on CXL memory. Using CXL memory is more efficient than network-based approaches, however, Tigon must address the limitations of CXL memory. The limitations are CXL’s higher latency and lower bandwidth relative to local DRAM, and its limited hardware support for cross-host cache coherence. For TPC-C and a variant of YCSB, Tigon achieves up to 2.5× higher throughput compared with two optimized shared-nothing databases that use CXL memory as a transport and up to 18.5× higher throughput compared with an RDMA-based distributed database. Subject : OSDI.2025"
    },
    {
      "paper_id": "shen-weihai@osdi25@USENIX",
      "index": 8,
      "title": "Mako: Speculative Distributed Transactions with Geo-Replication",
      "authors": [
        "Weihai Shen",
        "Yang Cui",
        "Siddhartha Sen",
        "Sebastian Angel",
        "Shuai Mu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "mako",
        "transactions",
        "replication",
        "geo",
        "shards",
        "speculatively",
        "replicated",
        "distributed",
        "transaction",
        "66m"
      ],
      "summary": "This paper introduces Mako, a highly available, high-throughput, and horizontally scalable transactional key-value store. Mako performs strongly consistent geo-replication to maintain availability despite entire datacenter failures, uses multi-core machines for fast serializable transaction processing, and shards data to scale out. To achieve these properties, especially to overcome the overheads of distributed transactions in geo-replicated settings, Mako decouples transaction execution and replication. This enables Mako to run transactions speculatively and very fast, and replicate transactions in the background to make them fault-tolerant. The key innovation in Mako is the use of two-phase commit (2PC) speculatively to allow distributed transactions to proceed without having to wait for their decisions to be replicated, while also preventing unbounded cascading aborts if shards fail prior to the end of replication. Our experimental evaluation on Azure shows that Mako processes 3.66M TPC-C transactions per second when data is split across 10 shards, each of which runs with 24 threads. This is an 8.6× higher throughput than state-of-the-art systems optimized for geo-replication.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/shen-weihai"
        ],
        "venue": [
          "/venue/shen-weihai@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-shen-weihai.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/shen-weihai"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Mako: Speculative Distributed Transactions with Geo-Replication [PDF ] [Copy] [Kimi ] [REL] Authors : Weihai Shen , Yang Cui , Siddhartha Sen , Sebastian Angel , Shuai Mu This paper introduces Mako, a highly available, high-throughput, and horizontally scalable transactional key-value store. Mako performs strongly consistent geo-replication to maintain availability despite entire datacenter failures, uses multi-core machines for fast serializable transaction processing, and shards data to scale out. To achieve these properties, especially to overcome the overheads of distributed transactions in geo-replicated settings, Mako decouples transaction execution and replication. This enables Mako to run transactions speculatively and very fast, and replicate transactions in the background to make them fault-tolerant. The key innovation in Mako is the use of two-phase commit (2PC) speculatively to allow distributed transactions to proceed without having to wait for their decisions to be replicated, while also preventing unbounded cascading aborts if shards fail prior to the end of replication. Our experimental evaluation on Azure shows that Mako processes 3.66M TPC-C transactions per second when data is split across 10 shards, each of which runs with 24 threads. This is an 8.6× higher throughput than state-of-the-art systems optimized for geo-replication. Subject : OSDI.2025"
    },
    {
      "paper_id": "mohoney@osdi25@USENIX",
      "index": 9,
      "title": "Quake: Adaptive Indexing for Vector Search",
      "authors": [
        "Jason Mohoney",
        "Devesh Sarda",
        "Mengze Tang",
        "Shihabur Rahman Chowdhury",
        "Anil Pacaci",
        "Ihab F. Ilyas",
        "Theodoros Rekatsinas",
        "Shivaram Venkataraman"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "quake",
        "query",
        "latency",
        "vector",
        "search",
        "workloads",
        "indexing",
        "recall",
        "access",
        "workload"
      ],
      "summary": "Vector search, the task of finding the k-nearest neighbors of a query vector against a database of high-dimensional vectors, underpins many machine learning applications, including retrieval-augmented generation, recommendation systems, and information retrieval. However, existing approximate nearest neighbor (ANN) methods perform poorly under dynamic and skewed workloads where data distributions evolve. We introduce Quake, an adaptive indexing system that maintains low latency and high recall in such environments. Quake employs a multi-level partitioning scheme that adjusts to updates and changing access patterns, guided by a cost model that predicts query latency based on partition sizes and access frequencies. Quake also dynamically sets query execution parameters to meet recall targets using a novel recall estimation model. Furthermore, Quake utilizes NUMA-aware intra-query parallelism for improved memory bandwidth utilization during search. To evaluate Quake, we prepare a Wikipedia vector search workload and develop a workload generator to create vector search workloads with configurable access patterns. Our evaluation shows that on dynamic workloads, Quake achieves query latency reductions of 1.5–38× and update latency reductions of 4.5–126× compared to state-of-the-art indexes such as SVS, DiskANN, HNSW, and SCANN.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/mohoney"
        ],
        "venue": [
          "/venue/mohoney@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-mohoney.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/mohoney"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Quake: Adaptive Indexing for Vector Search [PDF ] [Copy] [Kimi ] [REL] Authors : Jason Mohoney , Devesh Sarda , Mengze Tang , Shihabur Rahman Chowdhury , Anil Pacaci , Ihab F. Ilyas , Theodoros Rekatsinas , Shivaram Venkataraman Vector search, the task of finding the k-nearest neighbors of a query vector against a database of high-dimensional vectors, underpins many machine learning applications, including retrieval-augmented generation, recommendation systems, and information retrieval. However, existing approximate nearest neighbor (ANN) methods perform poorly under dynamic and skewed workloads where data distributions evolve. We introduce Quake, an adaptive indexing system that maintains low latency and high recall in such environments. Quake employs a multi-level partitioning scheme that adjusts to updates and changing access patterns, guided by a cost model that predicts query latency based on partition sizes and access frequencies. Quake also dynamically sets query execution parameters to meet recall targets using a novel recall estimation model. Furthermore, Quake utilizes NUMA-aware intra-query parallelism for improved memory bandwidth utilization during search. To evaluate Quake, we prepare a Wikipedia vector search workload and develop a workload generator to create vector search workloads with configurable access patterns. Our evaluation shows that on dynamic workloads, Quake achieves query latency reductions of 1.5–38× and update latency reductions of 4.5–126× compared to state-of-the-art indexes such as SVS, DiskANN, HNSW, and SCANN. Subject : OSDI.2025"
    },
    {
      "paper_id": "guo@osdi25@USENIX",
      "index": 10,
      "title": "Achieving Low-Latency Graph-Based Vector Search via Aligning Best-First Search Algorithm with SSD",
      "authors": [
        "Hao Guo",
        "Youyou Lu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "search",
        "latency",
        "ssd",
        "pipeann",
        "aligning",
        "vamana",
        "diskann",
        "graph",
        "best",
        "disk"
      ],
      "summary": "We propose PipeANN, an on-disk graph-based approximate nearest neighbor search (ANNS) system, which significantly bridges the latency gap with in-memory ones. We achieve this by aligning the best-first search algorithm with SSD characteristics, avoiding strict compute-I/O order across search steps. Experiments show that PipeANN has 1.14×--2.02× search latency compared to in-memory Vamana, and 35.0% of the latency of on-disk DiskANN in billion-scale datasets, without sacrificing search accuracy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/guo"
        ],
        "venue": [
          "/venue/guo@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-guo.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/guo"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Achieving Low-Latency Graph-Based Vector Search via Aligning Best-First Search Algorithm with SSD [PDF ] [Copy] [Kimi ] [REL] Authors : Hao Guo , Youyou Lu We propose PipeANN, an on-disk graph-based approximate nearest neighbor search (ANNS) system, which significantly bridges the latency gap with in-memory ones. We achieve this by aligning the best-first search algorithm with SSD characteristics, avoiding strict compute-I/O order across search steps. Experiments show that PipeANN has 1.14×--2.02× search latency compared to in-memory Vamana, and 35.0% of the latency of on-disk DiskANN in billion-scale datasets, without sacrificing search accuracy. Subject : OSDI.2025"
    },
    {
      "paper_id": "lyerly@osdi25@USENIX",
      "index": 11,
      "title": "Skybridge: Bounded Staleness for Distributed Caches",
      "authors": [
        "Robert Lyerly",
        "Scott Pruett",
        "Kevin Doherty",
        "Greg Rogers",
        "Nathan Bronson",
        "John Hugg"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "skybridge",
        "staleness",
        "replication",
        "meta",
        "caches",
        "writes",
        "eventual",
        "consistency",
        "annoyances",
        "bounded"
      ],
      "summary": "Meta Platforms Inc. is a social media company whose products require high availability and low latency. Meta’s services run in multiple geographic locations around the world and use asynchronous replication to keep the numerous cached copies of the datastore in sync. This setup reduces consistency in order to meet availability and latency requirements. Eventual consistency due to asynchronous replication causes issues for Meta’s services, ranging from minor annoyances to product-breaking bugs. Therefore, we ask: can we put meaningful bounds on how long it takes writes to be visible while maintaining the scalability afforded by eventual consistency? In this work we present Skybridge, an out-of-band replication stream for providing bounded staleness for distributed caches. Skybridge takes advantage of the fact that Meta’s systems already have a reliable delivery stream and instead focuses on real-time delivery of updates. Skybridge is complementary to the main replication pipeline and avoids correlated failures while being lightweight. We show that Skybridge helps provide 2-second bounded staleness for 99.99998% of writes, while the main replication pipeline only achieves this 99.993% of the time. Skybridge is able to achieve this while only being 0.54% the size of cache deployments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/lyerly"
        ],
        "venue": [
          "/venue/lyerly@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-lyerly.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/lyerly"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Skybridge: Bounded Staleness for Distributed Caches [PDF ] [Copy] [Kimi ] [REL] Authors : Robert Lyerly , Scott Pruett , Kevin Doherty , Greg Rogers , Nathan Bronson , John Hugg Meta Platforms Inc. is a social media company whose products require high availability and low latency. Meta’s services run in multiple geographic locations around the world and use asynchronous replication to keep the numerous cached copies of the datastore in sync. This setup reduces consistency in order to meet availability and latency requirements. Eventual consistency due to asynchronous replication causes issues for Meta’s services, ranging from minor annoyances to product-breaking bugs. Therefore, we ask: can we put meaningful bounds on how long it takes writes to be visible while maintaining the scalability afforded by eventual consistency? In this work we present Skybridge, an out-of-band replication stream for providing bounded staleness for distributed caches. Skybridge takes advantage of the fact that Meta’s systems already have a reliable delivery stream and instead focuses on real-time delivery of updates. Skybridge is complementary to the main replication pipeline and avoids correlated failures while being lightweight. We show that Skybridge helps provide 2-second bounded staleness for 99.99998% of writes, while the main replication pipeline only achieves this 99.993% of the time. Skybridge is able to achieve this while only being 0.54% the size of cache deployments. Subject : OSDI.2025"
    },
    {
      "paper_id": "guan@osdi25@USENIX",
      "index": 12,
      "title": "KPerfIR: Towards a Open and Compiler-centric Ecosystem for GPU Kernel Performance Tooling on Modern AI Workloads",
      "authors": [
        "Yue Guan",
        "Yuanwei Fang",
        "Keren Zhou",
        "Corbin Robeck",
        "Manman Ren",
        "Zhongkai Yu",
        "Yufei Ding",
        "Adnan Aziz"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "compiler",
        "kperfir",
        "centric",
        "compilers",
        "modern",
        "workloads",
        "profiling",
        "gpu",
        "tooling",
        "gpus"
      ],
      "summary": "In this work, we propose KPerfIR, a novel multi-level compiler-centric infrastructure designed to enable the development of customizable, extendable, and portable performance tools tailored for modern artificial intelligence (AI) workloads on modern GPUs. Our approach integrates profiling capabilities directly into the compiler workflow, allowing profiling functionalities to be implemented as compiler passes, offering a programmable and reusable framework for performance analysis. This design bridges the gap between compilers and profilers, enabling fine-grained insights into complex optimization challenges, such as overlapping the execution of fine-grained function units on GPUs. KPerfIR is integrated into the Triton infrastructure to highlight the power of a compiler-centric approach for advancing performance analysis and optimization in the ever-evolving landscape of AI compilers. Our evaluation shows that our tool incurs low overhead (8.2%), provides accurate measurements (2% relative error), and delivers actionable insights into complicated GPU intra-kernel events.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/guan"
        ],
        "venue": [
          "/venue/guan@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-guan.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/guan"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "KPerfIR: Towards a Open and Compiler-centric Ecosystem for GPU Kernel Performance Tooling on Modern AI Workloads [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Yue Guan , Yuanwei Fang , Keren Zhou , Corbin Robeck , Manman Ren , Zhongkai Yu , Yufei Ding , Adnan Aziz In this work, we propose KPerfIR, a novel multi-level compiler-centric infrastructure designed to enable the development of customizable, extendable, and portable performance tools tailored for modern artificial intelligence (AI) workloads on modern GPUs. Our approach integrates profiling capabilities directly into the compiler workflow, allowing profiling functionalities to be implemented as compiler passes, offering a programmable and reusable framework for performance analysis. This design bridges the gap between compilers and profilers, enabling fine-grained insights into complex optimization challenges, such as overlapping the execution of fine-grained function units on GPUs. KPerfIR is integrated into the Triton infrastructure to highlight the power of a compiler-centric approach for advancing performance analysis and optimization in the ever-evolving landscape of AI compilers. Our evaluation shows that our tool incurs low overhead (8.2%), provides accurate measurements (2% relative error), and delivers actionable insights into complicated GPU intra-kernel events. Subject : OSDI.2025"
    },
    {
      "paper_id": "wu-mengdi@osdi25@USENIX",
      "index": 13,
      "title": "Mirage: A Multi-Level Superoptimizer for Tensor Programs",
      "authors": [
        "Mengdi Wu",
        "Xinhao Cheng",
        "Shengyu Liu",
        "Chunan Shi",
        "Jianan Ji",
        "Man Kit Ao",
        "Praveen Velliengiri",
        "Xupeng Miao",
        "Oded Padon",
        "Zhihao Jia"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "mirage",
        "superoptimizer",
        "programs",
        "µgraphs",
        "tensor",
        "thread",
        "transformations",
        "introduces",
        "optimized",
        "µgraph"
      ],
      "summary": "We introduce Mirage, the first multi-level superoptimizer for tensor programs. A key idea in Mirage is µGraphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy. µGraphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels. To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee. To ensure that the optimized µGraph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees. Our evaluation shows that Mirage significantly outperforms existing approaches even for DNNs that are widely used and heavily optimized. Mirage is publicly available at https://github.com/mirage-project/mirage.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/wu-mengdi"
        ],
        "venue": [
          "/venue/wu-mengdi@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-wu-mengdi.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/wu-mengdi"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Mirage: A Multi-Level Superoptimizer for Tensor Programs [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Mengdi Wu , Xinhao Cheng , Shengyu Liu , Chunan Shi , Jianan Ji , Man Kit Ao , Praveen Velliengiri , Xupeng Miao , Oded Padon , Zhihao Jia We introduce Mirage, the first multi-level superoptimizer for tensor programs. A key idea in Mirage is µGraphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy. µGraphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels. To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee. To ensure that the optimized µGraph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees. Our evaluation shows that Mirage significantly outperforms existing approaches even for DNNs that are widely used and heavily optimized. Mirage is publicly available at https://github.com/mirage-project/mirage. Subject : OSDI.2025"
    },
    {
      "paper_id": "dong@osdi25@USENIX",
      "index": 14,
      "title": "QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach",
      "authors": [
        "Shouyang Dong",
        "Yuanbo Wen",
        "Jun Bi",
        "Di Huang",
        "Jiaming Guo",
        "Jianxing Xu",
        "Ruibai Xu",
        "Xinkai Song",
        "Yifan Hao",
        "Ling Li",
        "Xuehai Zhou",
        "Tianshi Chen",
        "Qi Guo",
        "Yunji Chen"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "qimeng",
        "xpiler",
        "programs",
        "symbolic",
        "dls",
        "tensor",
        "program",
        "synthesis",
        "transcompiling",
        "transcompilation"
      ],
      "summary": "Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering “Write Once, Run Anywhere” of tensor programs an open question. We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/dong"
        ],
        "venue": [
          "/venue/dong@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-dong.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/dong"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Shouyang Dong , Yuanbo Wen , Jun Bi , Di Huang , Jiaming Guo , Jianxing Xu , Ruibai Xu , Xinkai Song , Yifan Hao , Ling Li , Xuehai Zhou , Tianshi Chen , Qi Guo , Yunji Chen Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering “Write Once, Run Anywhere” of tensor programs an open question. We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average. Subject : OSDI.2025"
    },
    {
      "paper_id": "he@osdi25@USENIX",
      "index": 15,
      "title": "WaferLLM: Large Language Model Inference at Wafer Scale",
      "authors": [
        "Congjie He",
        "Yeqi Huang",
        "Pei Mu",
        "Ziming Miao",
        "Jilong Xue",
        "Lingxiao Ma",
        "Fan Yang",
        "Luo Mai"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "waferllm",
        "wafer",
        "scale",
        "gemv",
        "llm",
        "accelerators",
        "inference",
        "chip",
        "a100",
        "sglang"
      ],
      "summary": "Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully. We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as \"Plummer\") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators. Evaluations show that WaferLLM achieves up to 200× higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606× faster and 16× more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20× speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/he"
        ],
        "venue": [
          "/venue/he@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-he.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/he"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "WaferLLM: Large Language Model Inference at Wafer Scale [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Congjie He , Yeqi Huang , Pei Mu , Ziming Miao , Jilong Xue , Lingxiao Ma , Fan Yang , Luo Mai Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully. We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as \"Plummer\") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators. Evaluations show that WaferLLM achieves up to 200× higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606× faster and 16× more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20× speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM. Subject : OSDI.2025"
    },
    {
      "paper_id": "zhang-dingyan@osdi25@USENIX",
      "index": 16,
      "title": "BlitzScale: Fast and Live Large Model Autoscaling with O(1) Host Caching",
      "authors": [
        "Dingyan Zhang",
        "Haotian Wang",
        "Yang Liu",
        "Xingda Wei",
        "Yizhou Shan",
        "Rong Chen",
        "Haibo Chen"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "autoscaling",
        "caching",
        "live",
        "scaling",
        "blitzscale",
        "instances",
        "cache",
        "loading",
        "host",
        "parameters"
      ],
      "summary": "Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. In this paper, we first show that the data plane can be made fast with no or O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable to host cache and is underutilized, and (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction for inference from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled ones without waiting for the parameters to be fully loaded.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/zhang-dingyan"
        ],
        "venue": [
          "/venue/zhang-dingyan@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-zhang-dingyan.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/zhang-dingyan"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "BlitzScale: Fast and Live Large Model Autoscaling with O(1) Host Caching [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Dingyan Zhang , Haotian Wang , Yang Liu , Xingda Wei , Yizhou Shan , Rong Chen , Haibo Chen Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. In this paper, we first show that the data plane can be made fast with no or O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable to host cache and is underutilized, and (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction for inference from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled ones without waiting for the parameters to be fully loaded. Subject : OSDI.2025"
    },
    {
      "paper_id": "jeong@osdi25@USENIX",
      "index": 17,
      "title": "Bayesian Code Diffusion for Efficient Automatic Deep Learning Program Optimization",
      "authors": [
        "Isu Jeong",
        "Seulki Lee"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "program",
        "bayesian",
        "code",
        "optimization",
        "deep",
        "diffusion",
        "learning",
        "cpu",
        "ansor",
        "execution"
      ],
      "summary": "We introduce Bayesian code diffusion, a new deep learning program optimization strategy devised to accelerate the auto-tuning process of deep learning compilers. By using the concepts of prior and posterior distributions in the Bayesian framework and reformulating them to the context of deep learning program optimization, the proposed approach efficiently searches for optimal program code in a significantly reduced search space through an iterative diffusion of program code. To further enhance the efficiency of program optimization, we propose pre-training and fine-tuning of the cost model, which improves both the model's predictive accuracy and training efficiency. We implement Bayesian code diffusion in Ansor and evaluate its performance on a wide range of deep learning models on both CPUs and GPUs. Existing approaches struggle to reliably generate high-performing deep learning programs, i.e., achieving low program execution latency, across various configurations, including diverse deep learning model architectures and hardware platforms (CPU and GPU). In contrast, Bayesian code diffusion reduces the end-to-end compilation (optimization) time required to generate the equivalent program execution latency on various setups, e.g., achieving up to 3.31× optimization speedup. This substantial improvement demonstrates that Bayesian code diffusion performs efficient and principled deep learning program optimization across a wide range of deep learning models, operators, and hardware (CPU and GPU).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/jeong"
        ],
        "venue": [
          "/venue/jeong@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-jeong.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/jeong"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Bayesian Code Diffusion for Efficient Automatic Deep Learning Program Optimization [PDF ] [Copy] [Kimi ] [REL] Authors : Isu Jeong , Seulki Lee We introduce Bayesian code diffusion, a new deep learning program optimization strategy devised to accelerate the auto-tuning process of deep learning compilers. By using the concepts of prior and posterior distributions in the Bayesian framework and reformulating them to the context of deep learning program optimization, the proposed approach efficiently searches for optimal program code in a significantly reduced search space through an iterative diffusion of program code. To further enhance the efficiency of program optimization, we propose pre-training and fine-tuning of the cost model, which improves both the model's predictive accuracy and training efficiency. We implement Bayesian code diffusion in Ansor and evaluate its performance on a wide range of deep learning models on both CPUs and GPUs. Existing approaches struggle to reliably generate high-performing deep learning programs, i.e., achieving low program execution latency, across various configurations, including diverse deep learning model architectures and hardware platforms (CPU and GPU). In contrast, Bayesian code diffusion reduces the end-to-end compilation (optimization) time required to generate the equivalent program execution latency on various setups, e.g., achieving up to 3.31× optimization speedup. This substantial improvement demonstrates that Bayesian code diffusion performs efficient and principled deep learning program optimization across a wide range of deep learning models, operators, and hardware (CPU and GPU). Subject : OSDI.2025"
    },
    {
      "paper_id": "jiang@osdi25@USENIX",
      "index": 18,
      "title": "Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks",
      "authors": [
        "Yuxuan Jiang",
        "Ziming Zhou",
        "Boyu Xu",
        "Beijie Liu",
        "Runhui Xu",
        "Peng Huang"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "silent",
        "traincheck",
        "errors",
        "training",
        "proactive",
        "catching",
        "checks",
        "detect",
        "invariants",
        "diagnose"
      ],
      "summary": "Training deep learning (DL) models is a complex process, making it prone to silent errors that are challenging to detect and diagnose. This paper presents TRAINCHECK, a framework that takes a proactive checking approach to address silent training errors. TRAINCHECK automatically infers invariants tailored for DL training. It uses these invariants to proactively detect silent errors during the training process while providing debugging help. To evaluate TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root causes. TRAINCHECK successfully detects 18 errors within a single training iteration. It also uncovers 6 unknown bugs in popular training libraries that lead to silent errors.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/jiang"
        ],
        "venue": [
          "/venue/jiang@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-jiang.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/jiang"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Yuxuan Jiang , Ziming Zhou , Boyu Xu , Beijie Liu , Runhui Xu , Peng Huang Training deep learning (DL) models is a complex process, making it prone to silent errors that are challenging to detect and diagnose. This paper presents TRAINCHECK, a framework that takes a proactive checking approach to address silent training errors. TRAINCHECK automatically infers invariants tailored for DL training. It uses these invariants to proactively detect silent errors during the training process while providing debugging help. To evaluate TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root causes. TRAINCHECK successfully detects 18 errors within a single training iteration. It also uncovers 6 unknown bugs in popular training libraries that lead to silent errors. Subject : OSDI.2025"
    },
    {
      "paper_id": "huang-songlin@osdi25@USENIX",
      "index": 19,
      "title": "Neutrino: Fine-grained GPU Kernel Profiling via Programmable Probing",
      "authors": [
        "Songlin Huang",
        "Chenshu Wu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "neutrino",
        "gpu",
        "profiling",
        "kernel",
        "fine",
        "grained",
        "programmable",
        "probing",
        "gpus",
        "dmat"
      ],
      "summary": "As GPUs play an increasingly important role in computer systems in the scaling laws era, understanding fine-grained GPU runtime behavior is more crucial than ever. However, existing GPU kernel profilers, typically kernel-exclusive or hardware-dependent, often fail to capture fine-grained measurements. This paper presents NEUTRINO, a programmable interface for GPU kernel profiling that leverages assembly-layer probing to achieve instruction-level fine granularity, profiling versatility across time and value domains, and hardware independence. To better visualize the rich details captured by NEUTRINO, we introduce the Densified Memory Access Timeline (DMAT), a novel representation that offers new insights into GPU runtime behavior. We implement NEUTRINO in Linux for both NVIDIA and AMD GPUs and conduct extensive evaluations and analyses. The results demonstrate NEUTRINO’s superior capabilities in GPU kernel profiling with low overhead. We envision NEUTRINO as a valuable tool for the community and have open-sourced it to facilitate future research at https://github.com/open-neutrino/neutrino.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/huang-songlin"
        ],
        "venue": [
          "/venue/huang-songlin@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-huang-songlin.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/huang-songlin"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Neutrino: Fine-grained GPU Kernel Profiling via Programmable Probing [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Songlin Huang , Chenshu Wu As GPUs play an increasingly important role in computer systems in the scaling laws era, understanding fine-grained GPU runtime behavior is more crucial than ever. However, existing GPU kernel profilers, typically kernel-exclusive or hardware-dependent, often fail to capture fine-grained measurements. This paper presents NEUTRINO, a programmable interface for GPU kernel profiling that leverages assembly-layer probing to achieve instruction-level fine granularity, profiling versatility across time and value domains, and hardware independence. To better visualize the rich details captured by NEUTRINO, we introduce the Densified Memory Access Timeline (DMAT), a novel representation that offers new insights into GPU runtime behavior. We implement NEUTRINO in Linux for both NVIDIA and AMD GPUs and conduct extensive evaluations and analyses. The results demonstrate NEUTRINO’s superior capabilities in GPU kernel profiling with low overhead. We envision NEUTRINO as a valuable tool for the community and have open-sourced it to facilitate future research at https://github.com/open-neutrino/neutrino. Subject : OSDI.2025"
    },
    {
      "paper_id": "park-sujin@osdi25@USENIX",
      "index": 20,
      "title": "Principles and Methodologies for Serial Performance Optimization",
      "authors": [
        "Sujin Park",
        "Mingyu Guan",
        "Xiang Cheng",
        "Taesoo Kim"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "sysgpt",
        "optimization",
        "methodologies",
        "osdi",
        "performance",
        "principles",
        "serial",
        "sosp",
        "gpt",
        "precomputing"
      ],
      "summary": "Throughout the history of computer science, optimizing existing systems to achieve higher performance has been a longstanding aspiration. While the primary emphasis of this endeavor lies in reducing latency and increasing throughput, these two are closely intertwined, and answering the how question has remained a challenge, often relying on intuition and experience. This paper introduces a systematic approach to optimizing sequential tasks, which are fundamental for overall performance. We define three principles—task removal, replacement, and reordering—and distill them into eight actionable methodologies: batching, caching, precomputing, deferring, relaxation, contextualization, hardware specialization, and layering. Our review of OSDI and SOSP papers over the past decade shows that these techniques, when taken together, comprehensively account for the observed sequential optimization strategies. To illustrate the framework’s practical value, we present two case studies: one on file and storage systems, and another analyzing kernel synchronization to uncover missed optimization opportunities. Furthermore, we introduce SysGPT, a fine-tuned GPT model trained on curated literature analysis, which offer context-aware performance suggestions. SysGPT’s outputs are more specific and feasible than GPT-4’s, aligning with core strategies from recent research without direct exposure, demonstrating its utility as an optimization assistant.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/park-sujin"
        ],
        "venue": [
          "/venue/park-sujin@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-park-sujin.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/park-sujin"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Principles and Methodologies for Serial Performance Optimization [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Sujin Park , Mingyu Guan , Xiang Cheng , Taesoo Kim Throughout the history of computer science, optimizing existing systems to achieve higher performance has been a longstanding aspiration. While the primary emphasis of this endeavor lies in reducing latency and increasing throughput, these two are closely intertwined, and answering the how question has remained a challenge, often relying on intuition and experience. This paper introduces a systematic approach to optimizing sequential tasks, which are fundamental for overall performance. We define three principles—task removal, replacement, and reordering—and distill them into eight actionable methodologies: batching, caching, precomputing, deferring, relaxation, contextualization, hardware specialization, and layering. Our review of OSDI and SOSP papers over the past decade shows that these techniques, when taken together, comprehensively account for the observed sequential optimization strategies. To illustrate the framework’s practical value, we present two case studies: one on file and storage systems, and another analyzing kernel synchronization to uncover missed optimization opportunities. Furthermore, we introduce SysGPT, a fine-tuned GPT model trained on curated literature analysis, which offer context-aware performance suggestions. SysGPT’s outputs are more specific and feasible than GPT-4’s, aligning with core strategies from recent research without direct exposure, demonstrating its utility as an optimization assistant. Subject : OSDI.2025"
    },
    {
      "paper_id": "wang-weitao@osdi25@USENIX",
      "index": 21,
      "title": "Söze: One Network Telemetry Is All You Need for Per-flow Weighted Bandwidth Allocation at Scale",
      "authors": [
        "Weitao Wang",
        "T. S. Eugene Ng"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "söze",
        "bandwidth",
        "telemetry",
        "weighted",
        "allocation",
        "flow",
        "per",
        "ethernet",
        "agile",
        "commodity"
      ],
      "summary": "Weighted bandwidth allocation is a powerful abstraction that has a wide range of use cases in modern data center networks. However, realizing highly agile and precise weighted bandwidth allocation for large-scale cloud environments is fundamentally challenging. In this paper, we propose Söze, a lightweight decentralized weighted bandwidth allocation system that leverages simple network telemetry features of commodity Ethernet switches. Given the flow weights, Söze can effectively use the telemetry information to compute and enforce the weighted bandwidth allocations without per-flow, topology, or routing knowledge. We demonstrate the effectiveness of Söze through simulations and testbed experiments, improving TPC-H jobs completion time by up to 0.59× and 0.79× on average.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-weitao"
        ],
        "venue": [
          "/venue/wang-weitao@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-wang-weitao.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-weitao"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Söze: One Network Telemetry Is All You Need for Per-flow Weighted Bandwidth Allocation at Scale [PDF ] [Copy] [Kimi ] [REL] Authors : Weitao Wang , T. S. Eugene Ng Weighted bandwidth allocation is a powerful abstraction that has a wide range of use cases in modern data center networks. However, realizing highly agile and precise weighted bandwidth allocation for large-scale cloud environments is fundamentally challenging. In this paper, we propose Söze, a lightweight decentralized weighted bandwidth allocation system that leverages simple network telemetry features of commodity Ethernet switches. Given the flow weights, Söze can effectively use the telemetry information to compute and enforce the weighted bandwidth allocations without per-flow, topology, or routing knowledge. We demonstrate the effectiveness of Söze through simulations and testbed experiments, improving TPC-H jobs completion time by up to 0.59× and 0.79× on average. Subject : OSDI.2025"
    },
    {
      "paper_id": "xu@osdi25@USENIX",
      "index": 22,
      "title": "Decouple and Decompose: Scaling Resource Allocation with DeDe",
      "authors": [
        "Zhiying Xu",
        "Minlan Yu",
        "Francis Y. Yan"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "dede",
        "resource",
        "allocation",
        "demand",
        "decouple",
        "decompose",
        "allocations",
        "optimization",
        "outpaced",
        "tenants"
      ],
      "summary": "Efficient resource allocation is essential in cloud systems to facilitate resource sharing among tenants. However, the growing scale of these optimization problems have outpaced commercial solvers commonly employed in production. To accelerate resource allocation, prior approaches either customize solutions for narrow domains or impose workload-specific assumptions. In this work, we revisit real-world resource allocation problems and uncover a common underlying structure: the vast majority of these problems are inherently separable, i.e., they optimize the aggregate utility of individual resource and demand allocations, under separate constraints for each resource and each demand. Building on this observation, we develop DeDe, a scalable and theoretically rooted optimization framework for large-scale resource allocation. At the core of DeDe is a decouple-and-decompose approach: it decouples entangled resource and demand constraints and thereby decomposes the overall optimization into alternating per-resource and per-demand subproblems that can be solved efficiently and in parallel. We have implemented and released DeDe as a Python package with a familiar modeling interface. Our experiments on three representative resource allocation tasks — cluster scheduling, traffic engineering, and load balancing — demonstrate that DeDe delivers significant speedups while generating higher-quality allocations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/xu"
        ],
        "venue": [
          "/venue/xu@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-xu.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/xu"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Decouple and Decompose: Scaling Resource Allocation with DeDe [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Zhiying Xu , Minlan Yu , Francis Y. Yan Efficient resource allocation is essential in cloud systems to facilitate resource sharing among tenants. However, the growing scale of these optimization problems have outpaced commercial solvers commonly employed in production. To accelerate resource allocation, prior approaches either customize solutions for narrow domains or impose workload-specific assumptions. In this work, we revisit real-world resource allocation problems and uncover a common underlying structure: the vast majority of these problems are inherently separable, i.e., they optimize the aggregate utility of individual resource and demand allocations, under separate constraints for each resource and each demand. Building on this observation, we develop DeDe, a scalable and theoretically rooted optimization framework for large-scale resource allocation. At the core of DeDe is a decouple-and-decompose approach: it decouples entangled resource and demand constraints and thereby decomposes the overall optimization into alternating per-resource and per-demand subproblems that can be solved efficiently and in parallel. We have implemented and released DeDe as a Python package with a familiar modeling interface. Our experiments on three representative resource allocation tasks — cluster scheduling, traffic engineering, and load balancing — demonstrate that DeDe delivers significant speedups while generating higher-quality allocations. Subject : OSDI.2025"
    },
    {
      "paper_id": "tao@osdi25@USENIX",
      "index": 23,
      "title": "Quantum Virtual Machines",
      "authors": [
        "Runzhou Tao",
        "Hongzheng Zhu",
        "Jason Nieh",
        "Jianan Yao",
        "Ronghui Gu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "hyperq",
        "quantum",
        "programs",
        "machines",
        "virtual",
        "computer",
        "computing",
        "ibm",
        "hardware",
        "executed"
      ],
      "summary": "Cloud computing services offer time on quantum computers, but users are forced to each use the entire quantum computer to run their programs as there is no way to multiplex a quantum computer among multiple programs at the same time. We present HyperQ, a system that introduces virtual machines for quantum computers to provide fault isolation, better resource utilization, and lower latency for quantum cloud computing. A quantum virtual machine is defined in terms of quantum computer hardware, specifically its quantum gates and qubits arranged in a hardware-specific topology. HyperQ enables quantum virtual machines to be simultaneously executed together on a quantum computer by multiplexing them in time and space on the hardware and ensuring that they are isolated from one another. HyperQ works with existing quantum programs and compiler frameworks; programs are simply compiled to run in virtual machines without the programs or compilers needing to know what else might be executed at the same time. We have implemented HyperQ for the IBM quantum computing service, the largest quantum computing fleet in the world. Our experimental results running quantum programs in virtual machines using the IBM service demonstrate that HyperQ can increase utilization and throughput while reducing program latency, by up to an order of magnitude, without sacrificing, and in some cases improving, fidelity in the results of quantum program execution.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/tao"
        ],
        "venue": [
          "/venue/tao@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-tao.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/tao"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Quantum Virtual Machines [PDF ] [Copy] [Kimi ] [REL] Authors : Runzhou Tao , Hongzheng Zhu , Jason Nieh , Jianan Yao , Ronghui Gu Cloud computing services offer time on quantum computers, but users are forced to each use the entire quantum computer to run their programs as there is no way to multiplex a quantum computer among multiple programs at the same time. We present HyperQ, a system that introduces virtual machines for quantum computers to provide fault isolation, better resource utilization, and lower latency for quantum cloud computing. A quantum virtual machine is defined in terms of quantum computer hardware, specifically its quantum gates and qubits arranged in a hardware-specific topology. HyperQ enables quantum virtual machines to be simultaneously executed together on a quantum computer by multiplexing them in time and space on the hardware and ensuring that they are isolated from one another. HyperQ works with existing quantum programs and compiler frameworks; programs are simply compiled to run in virtual machines without the programs or compilers needing to know what else might be executed at the same time. We have implemented HyperQ for the IBM quantum computing service, the largest quantum computing fleet in the world. Our experimental results running quantum programs in virtual machines using the IBM service demonstrate that HyperQ can increase utilization and throughput while reducing program latency, by up to an order of magnitude, without sacrificing, and in some cases improving, fidelity in the results of quantum program execution. Subject : OSDI.2025"
    },
    {
      "paper_id": "giortamis@osdi25@USENIX",
      "index": 24,
      "title": "QOS: Quantum Operating System",
      "authors": [
        "Emmanouil Giortamis",
        "Francisco Romão",
        "Nathaniel Tornow",
        "Pramod Bhatotia"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "qos",
        "quantum",
        "fidelity",
        "utilization",
        "tradeoffs",
        "hardware",
        "resource",
        "operating",
        "system",
        "face"
      ],
      "summary": "Quantum computers face challenges due to hardware constraints, noise errors, and heterogeneity, and face fundamental design tradeoffs between key performance metrics such as quantum fidelity and system utilization. This substantially complicates managing quantum resources to scale the size and number of quantum algorithms that can be executed reliably in a given time. We introduce QOS, a modular quantum operating system that holistically addresses the challenges of quantum resource management by systematically exploring key design tradeoffs across the stack.QOS exposes a hardware-agnostic API for transparent quantum job execution, mitigates hardware errors, and systematically multi-programs and schedules the jobs across space and time to achieve high quantum fidelity in a resource-efficient manner. QOS's modular design enables synergistic cross- and intra-layer optimizations, while introducing new concepts such as compatibility-based multi-programming and effective utilization. We evaluate QOS on real quantum devices hosted by IBM, using 7000 real quantum runs of more than 70.000 benchmark instances. We show that the QOS achieves 2.6--456.5× higher fidelity, increases resource utilization by up to 9.6×, and reduces waiting times by up to 5× while sacrificing only 1--3% fidelity, on average, compared to the baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/giortamis"
        ],
        "venue": [
          "/venue/giortamis@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-giortamis.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/giortamis"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "QOS: Quantum Operating System [PDF ] [Copy] [Kimi ] [REL] Authors : Emmanouil Giortamis , Francisco Romão , Nathaniel Tornow , Pramod Bhatotia Quantum computers face challenges due to hardware constraints, noise errors, and heterogeneity, and face fundamental design tradeoffs between key performance metrics such as quantum fidelity and system utilization. This substantially complicates managing quantum resources to scale the size and number of quantum algorithms that can be executed reliably in a given time. We introduce QOS, a modular quantum operating system that holistically addresses the challenges of quantum resource management by systematically exploring key design tradeoffs across the stack.QOS exposes a hardware-agnostic API for transparent quantum job execution, mitigates hardware errors, and systematically multi-programs and schedules the jobs across space and time to achieve high quantum fidelity in a resource-efficient manner. QOS's modular design enables synergistic cross- and intra-layer optimizations, while introducing new concepts such as compatibility-based multi-programming and effective utilization. We evaluate QOS on real quantum devices hosted by IBM, using 7000 real quantum runs of more than 70.000 benchmark instances. We show that the QOS achieves 2.6--456.5× higher fidelity, increases resource utilization by up to 9.6×, and reduces waiting times by up to 5× while sacrificing only 1--3% fidelity, on average, compared to the baselines. Subject : OSDI.2025"
    },
    {
      "paper_id": "sun@osdi25@USENIX",
      "index": 25,
      "title": "Scalio: Scaling up DPU-based JBOF Key-value Store with NVMe-oF Target Offload",
      "authors": [
        "Xun Sun",
        "Mingxing Zhang",
        "Yingdi Shan",
        "Kang Chen",
        "Jinlei Jiang",
        "Yongwei Wu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "dpu",
        "scalio",
        "jbof",
        "ssd",
        "nvme",
        "offload",
        "rdma",
        "disaggregated",
        "store",
        "key"
      ],
      "summary": "The rapid growth of data-intensive applications has created a demand for high-density storage systems. Data-Processing-Unit-based (DPU-based) Just a Bunch of Flash (JBOF) solutions provide an energy-efficient and cost-effective architecture to meet this need. However, existing JBOF solutions struggle with scalability when handling an increasing number of attached SSDs, due to their heavy reliance on the DPU's CPU for SSD I/O operations. In this paper, we introduce Scalio, a scalable disaggregated key-value store designed to address the limitations of current DPU-based JBOF systems. Scalio offloads as many SSD I/O operations as possible to the DPU's network I/O capabilities, including traditional RDMA verbs and a recent hardware optimization, NVMe over Fabrics Target Offload. Additionally, Scalio incorporates a two-layer design with compact in-memory data structures to handle hot read traffic and manage bursty writes. One of the key challenges in this design is ensuring consistency between the DRAM states in the DPU and the SSD states, which, unlike CPU L1/L2 caches, are not automatically synchronized through hardware cache coherence protocols. To address this, Scalio introduces an RDMA-based cache consistency protocol that guarantees linearizability across the system, despite the disaggregated nature of the architecture. Our experiments show that Scalio significantly improves both scalability and throughput, achieving up to 3.3× higher throughput compared to existing systems, especially in high-density SSD configurations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/sun"
        ],
        "venue": [
          "/venue/sun@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-sun.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/sun"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Scalio: Scaling up DPU-based JBOF Key-value Store with NVMe-oF Target Offload [PDF ] [Copy] [Kimi ] [REL] Authors : Xun Sun , Mingxing Zhang , Yingdi Shan , Kang Chen , Jinlei Jiang , Yongwei Wu The rapid growth of data-intensive applications has created a demand for high-density storage systems. Data-Processing-Unit-based (DPU-based) Just a Bunch of Flash (JBOF) solutions provide an energy-efficient and cost-effective architecture to meet this need. However, existing JBOF solutions struggle with scalability when handling an increasing number of attached SSDs, due to their heavy reliance on the DPU's CPU for SSD I/O operations. In this paper, we introduce Scalio, a scalable disaggregated key-value store designed to address the limitations of current DPU-based JBOF systems. Scalio offloads as many SSD I/O operations as possible to the DPU's network I/O capabilities, including traditional RDMA verbs and a recent hardware optimization, NVMe over Fabrics Target Offload. Additionally, Scalio incorporates a two-layer design with compact in-memory data structures to handle hot read traffic and manage bursty writes. One of the key challenges in this design is ensuring consistency between the DRAM states in the DPU and the SSD states, which, unlike CPU L1/L2 caches, are not automatically synchronized through hardware cache coherence protocols. To address this, Scalio introduces an RDMA-based cache consistency protocol that guarantees linearizability across the system, despite the disaggregated nature of the architecture. Our experiments show that Scalio significantly improves both scalability and throughput, achieving up to 3.3× higher throughput compared to existing systems, especially in high-density SSD configurations. Subject : OSDI.2025"
    },
    {
      "paper_id": "bhat@osdi25@USENIX",
      "index": 26,
      "title": "Low End-to-End Latency atop a Speculative Shared Log with Fix-Ante Ordering",
      "authors": [
        "Shreesha G. Bhat",
        "Tony Hong",
        "Xuhao Luo",
        "Jiyu Hu",
        "Aishwarya Ganesan",
        "Ramnatthan Alagappan"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "ante",
        "ordering",
        "fix",
        "shards",
        "shared",
        "belfast",
        "records",
        "e2e",
        "end",
        "speclog"
      ],
      "summary": "Today’s shared logs incur expensive coordination to globally order records across storage shards before they can deliver records to applications. This makes them unsuitable for many modern applications that must process ingested data as early as possible and realize low end-to-end (e2e) latencies. We propose SpecLog, a new shared log abstraction that delivers records by speculating the global order, allowing the application’s computation and shared-log coordination to be overlapped, thus reducing e2e latency. To enable accurate speculations, we introduce fix-ante ordering, a novel ordering mechanism that predetermines the global order and makes the shards adhere to the predetermined order. With fix-ante ordering, shards, except in rare cases, can accurately predict where their records will sit in the total order before global coordination. We build Belfast, an implementation of the SpecLog abstraction and fix-ante ordering. Our experiments show that Belfast offers lower e2e latencies than current shared logs while preserving their elasticity, flexibility, and scalability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/bhat"
        ],
        "venue": [
          "/venue/bhat@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-bhat.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/bhat"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Low End-to-End Latency atop a Speculative Shared Log with Fix-Ante Ordering [PDF ] [Copy] [Kimi ] [REL] Authors : Shreesha G. Bhat , Tony Hong , Xuhao Luo , Jiyu Hu , Aishwarya Ganesan , Ramnatthan Alagappan Today’s shared logs incur expensive coordination to globally order records across storage shards before they can deliver records to applications. This makes them unsuitable for many modern applications that must process ingested data as early as possible and realize low end-to-end (e2e) latencies. We propose SpecLog, a new shared log abstraction that delivers records by speculating the global order, allowing the application’s computation and shared-log coordination to be overlapped, thus reducing e2e latency. To enable accurate speculations, we introduce fix-ante ordering, a novel ordering mechanism that predetermines the global order and makes the shards adhere to the predetermined order. With fix-ante ordering, shards, except in rare cases, can accurately predict where their records will sit in the total order before global coordination. We build Belfast, an implementation of the SpecLog abstraction and fix-ante ordering. Our experiments show that Belfast offers lower e2e latencies than current shared logs while preserving their elasticity, flexibility, and scalability. Subject : OSDI.2025"
    },
    {
      "paper_id": "lin-jinkun@osdi25@USENIX",
      "index": 27,
      "title": "Understanding Stragglers in Large Model Training Using What-if Analysis",
      "authors": [
        "Jinkun Lin",
        "Ziheng Jiang",
        "Zuquan Song",
        "Sida Zhao",
        "Menghan Yu",
        "Zhanghan Wang",
        "Chenyuan Wang",
        "Zuocheng Shi",
        "Xiang Shi",
        "Wei Jia",
        "Zherui Liu",
        "Shuguang Wang",
        "Haibin Lin",
        "Xin Liu",
        "Aurojit Panda",
        "Jinyang Li"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "stragglers",
        "bytedance",
        "training",
        "llm",
        "stalled",
        "straggler",
        "often",
        "trivially",
        "simulates",
        "jobs"
      ],
      "summary": "Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/lin-jinkun"
        ],
        "venue": [
          "/venue/lin-jinkun@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-lin-jinkun.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/lin-jinkun"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Understanding Stragglers in Large Model Training Using What-if Analysis [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Jinkun Lin , Ziheng Jiang , Zuquan Song , Sida Zhao , Menghan Yu , Zhanghan Wang , Chenyuan Wang , Zuocheng Shi , Xiang Shi , Wei Jia , Zherui Liu , Shuguang Wang , Haibin Lin , Xin Liu , Aurojit Panda , Jinyang Li Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers? Subject : OSDI.2025"
    },
    {
      "paper_id": "chai-xiaohu@osdi25@USENIX",
      "index": 28,
      "title": "Fork in the Road: Reflections and Optimizations for Cold Start Latency in Production Serverless Systems",
      "authors": [
        "Xiaohu Chai",
        "Tianyu Zhou",
        "Keyang Hu",
        "Jianfeng Tan",
        "Tiwei Bie",
        "Anqi Shen",
        "Dawei Shen",
        "Qi Xing",
        "Shun Song",
        "Tongkai Yang",
        "Le Gao",
        "Feng Yu",
        "Zhengyu He",
        "Dong Du",
        "Yubin Xia",
        "Kang Chen",
        "Yu Chen"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "latency",
        "serverless",
        "start",
        "cold",
        "afaas",
        "optimizations",
        "deployments",
        "ant",
        "fork",
        "execution"
      ],
      "summary": "Serverless computing has seen widespread adoption in public cloud environments. However, it continues to suffer from long cold start latency, which remains a key performance bottleneck. We have conducted an in-depth investigation of existing cold start optimizations and evaluated their effectiveness in large-scale industrial deployments. Our study reveals several common limitations in prior research: (1) reliance on simplified assumptions that overlook the complexities of large-scale systems; (2) a narrow focus on optimizing isolated components of the cold start process, while ignoring end-to-end workflow interactions; and (3) insufficient attention to the challenges introduced by concurrent execution environments. As a result, despite incorporating prior techniques, cold start latency on the Ant Group serverless platform remains in the range of hundreds of milliseconds to several seconds. This paper identifies three previously overlooked sources of latency: (1) control path latency, stemming from interactions within the serverless runtime; (2) resource contention latency, arising under high concurrency and sustained execution; and (3) user code initialization latency, which reflects the trade-off between resource efficiency and startup performance. To address these challenges, we propose a suite of novel techniques that overcome key limitations in existing approaches. These techniques are designed to be both adaptable to real-world workloads and scalable to large deployments. Our system, AFaaS (short for Ant FaaS), reduces cold start latency to the millisecond level. AFaaS has been deployed in production for over 18 months and has consistently demonstrated stable performance at scale.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/chai-xiaohu"
        ],
        "venue": [
          "/venue/chai-xiaohu@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-chai-xiaohu.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/chai-xiaohu"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Fork in the Road: Reflections and Optimizations for Cold Start Latency in Production Serverless Systems [PDF ] [Copy] [Kimi ] [REL] Authors : Xiaohu Chai , Tianyu Zhou , Keyang Hu , Jianfeng Tan , Tiwei Bie , Anqi Shen , Dawei Shen , Qi Xing , Shun Song , Tongkai Yang , Le Gao , Feng Yu , Zhengyu He , Dong Du , Yubin Xia , Kang Chen , Yu Chen Serverless computing has seen widespread adoption in public cloud environments. However, it continues to suffer from long cold start latency, which remains a key performance bottleneck. We have conducted an in-depth investigation of existing cold start optimizations and evaluated their effectiveness in large-scale industrial deployments. Our study reveals several common limitations in prior research: (1) reliance on simplified assumptions that overlook the complexities of large-scale systems; (2) a narrow focus on optimizing isolated components of the cold start process, while ignoring end-to-end workflow interactions; and (3) insufficient attention to the challenges introduced by concurrent execution environments. As a result, despite incorporating prior techniques, cold start latency on the Ant Group serverless platform remains in the range of hundreds of milliseconds to several seconds. This paper identifies three previously overlooked sources of latency: (1) control path latency, stemming from interactions within the serverless runtime; (2) resource contention latency, arising under high concurrency and sustained execution; and (3) user code initialization latency, which reflects the trade-off between resource efficiency and startup performance. To address these challenges, we propose a suite of novel techniques that overcome key limitations in existing approaches. These techniques are designed to be both adaptable to real-world workloads and scalable to large deployments. Our system, AFaaS (short for Ant FaaS), reduces cold start latency to the millisecond level. AFaaS has been deployed in production for over 18 months and has consistently demonstrated stable performance at scale. Subject : OSDI.2025"
    },
    {
      "paper_id": "domingo@osdi25@USENIX",
      "index": 29,
      "title": "Kamino: Efficient VM Allocation at Scale with Latency-Driven Cache-Aware Scheduling",
      "authors": [
        "David Domingo",
        "Hugo Barbalho",
        "Marco Molinaro",
        "Kuan Liu",
        "Abhisek Pan",
        "David Dion",
        "Thomas Moscibroda",
        "Sudarsun Kannan",
        "Ishai Menache"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "kamino",
        "cache",
        "latency",
        "request",
        "allocation",
        "latencies",
        "scheduling",
        "aware",
        "caching",
        "requests"
      ],
      "summary": "In virtual machine (VM) allocation systems, caching repetitive and similar VM allocation requests and associated resolution rules is crucial for reducing computational costs and meeting strict latency requirements. While modern allocation systems distribute requests among multiple allocator agents and use caching to improve performance, current schedulers often neglect the cache state and latency considerations when assigning each new request to an agent. Due to the high variance in costs of cache hits and misses and the associated processing overheads of updating the caches, simple load-balancing and cache-aware mechanisms result in high latencies. We introduce Kamino, a high-performance, latency-driven and cache-aware request scheduling system aimed at minimizing end-to-end latencies. Kamino employs a novel scheduling algorithm grounded in theory which uses partial indicators from the cache state to assign each new request to the agent with the lowest estimated latency. Evaluation of Kamino using a high-fidelity simulator on large-scale production workloads shows a 42% reduction in average request latencies. Our deployment of Kamino in the control plane of a large public cloud confirms these improvements, with a 33% decrease in cache miss rates and 17% reduction in memory usage.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/domingo"
        ],
        "venue": [
          "/venue/domingo@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-domingo.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/domingo"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Kamino: Efficient VM Allocation at Scale with Latency-Driven Cache-Aware Scheduling [PDF ] [Copy] [Kimi ] [REL] Authors : David Domingo , Hugo Barbalho , Marco Molinaro , Kuan Liu , Abhisek Pan , David Dion , Thomas Moscibroda , Sudarsun Kannan , Ishai Menache In virtual machine (VM) allocation systems, caching repetitive and similar VM allocation requests and associated resolution rules is crucial for reducing computational costs and meeting strict latency requirements. While modern allocation systems distribute requests among multiple allocator agents and use caching to improve performance, current schedulers often neglect the cache state and latency considerations when assigning each new request to an agent. Due to the high variance in costs of cache hits and misses and the associated processing overheads of updating the caches, simple load-balancing and cache-aware mechanisms result in high latencies. We introduce Kamino, a high-performance, latency-driven and cache-aware request scheduling system aimed at minimizing end-to-end latencies. Kamino employs a novel scheduling algorithm grounded in theory which uses partial indicators from the cache state to assign each new request to the agent with the lowest estimated latency. Evaluation of Kamino using a high-fidelity simulator on large-scale production workloads shows a 42% reduction in average request latencies. Our deployment of Kamino in the control plane of a large public cloud confirms these improvements, with a 33% decrease in cache miss rates and 17% reduction in memory usage. Subject : OSDI.2025"
    },
    {
      "paper_id": "wang-zhuang@osdi25@USENIX",
      "index": 30,
      "title": "ZEN: Empowering Distributed Training with Sparsity-driven Data Synchronization",
      "authors": [
        "Zhuang Wang",
        "Zhaozhuo Xu",
        "Jingyi Xi",
        "Yuke Wang",
        "Anshumali Shrivastava",
        "T. S. Eugene Ng"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "zen",
        "sparsity",
        "synchronization",
        "tensors",
        "sparse",
        "speedup",
        "training",
        "empowering",
        "communication",
        "09x"
      ],
      "summary": "Distributed training is the de facto standard to scale up the training of deep learning models with multiple GPUs. Its performance bottleneck lies in communications for gradient synchronization. Although high tensor sparsity is widely observed, the optimal communication scheme to fully leverage sparsity is still missing. This paper aims to bridge this gap. We first analyze the characteristics of sparse tensors in popular models to understand the fundamentals of sparsity. We then systematically explore the design space of communication schemes for sparse tensors and find the optimal ones. These findings give a new understanding and inspire us to develop a holistic gradient synchronization system for sparse tensors called ZEN. We demonstrate that ZEN can achieve up to 5.09x speedup in communication time and up to 2.48x speedup in training throughput compared to the state-of-the-art methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-zhuang"
        ],
        "venue": [
          "/venue/wang-zhuang@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-wang-zhuang.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-zhuang"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "ZEN: Empowering Distributed Training with Sparsity-driven Data Synchronization [PDF ] [Copy] [Kimi ] [REL] Authors : Zhuang Wang , Zhaozhuo Xu , Jingyi Xi , Yuke Wang , Anshumali Shrivastava , T. S. Eugene Ng Distributed training is the de facto standard to scale up the training of deep learning models with multiple GPUs. Its performance bottleneck lies in communications for gradient synchronization. Although high tensor sparsity is widely observed, the optimal communication scheme to fully leverage sparsity is still missing. This paper aims to bridge this gap. We first analyze the characteristics of sparse tensors in popular models to understand the fundamentals of sparsity. We then systematically explore the design space of communication schemes for sparse tensors and find the optimal ones. These findings give a new understanding and inspire us to develop a holistic gradient synchronization system for sparse tensors called ZEN. We demonstrate that ZEN can achieve up to 5.09x speedup in communication time and up to 2.48x speedup in training throughput compared to the state-of-the-art methods. Subject : OSDI.2025"
    },
    {
      "paper_id": "zheng-yusheng@osdi25@USENIX",
      "index": 31,
      "title": "Extending Applications Safely and Efficiently",
      "authors": [
        "Yusheng Zheng",
        "Tong Yu",
        "Yiwei Yang",
        "Yanpeng Hu",
        "Xiaozheng Lai",
        "Dan Williams",
        "Andi Quinn"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "bpftime",
        "eim",
        "extension",
        "ebpf",
        "mpk",
        "userspace",
        "safely",
        "hardware",
        "resources",
        "berkeley"
      ],
      "summary": "This paper presents the Extension Interface Model (EIM) and bpftime, which together enable safer and more efficient extension of userspace applications than the current state-of-the-art. EIM is a new model that treats each required feature of an extension as a resource, including concrete hardware resources (e.g., memory) and abstract ones (e.g., the ability to invoke a function from the extended application). An extension manager, i.e., the person who manages a deployment, uses EIM to specify only the resources an extension needs to perform its task. bpftime is a new extension framework that enforces an EIM specification. Compared to prior systems, bpftime is efficient because it uses extended Berkeley Packet Filter (eBPF)-style verification, hardware-supported isolation features (e.g., Intel MPK), and dynamic binary rewriting. Moreover, bpftime is easy to adopt into existing workflows since it is compatible with the current eBPF ecosystem. We demonstrate the usefulness of EIM and bpftime across 6 use cases that improve security, monitor and enhance performance, and explore configuration trade-offs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/zheng-yusheng"
        ],
        "venue": [
          "/venue/zheng-yusheng@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-zheng-yusheng.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/zheng-yusheng"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Extending Applications Safely and Efficiently [PDF ] [Copy] [Kimi ] [REL] Authors : Yusheng Zheng , Tong Yu , Yiwei Yang , Yanpeng Hu , Xiaozheng Lai , Dan Williams , Andi Quinn This paper presents the Extension Interface Model (EIM) and bpftime, which together enable safer and more efficient extension of userspace applications than the current state-of-the-art. EIM is a new model that treats each required feature of an extension as a resource, including concrete hardware resources (e.g., memory) and abstract ones (e.g., the ability to invoke a function from the extended application). An extension manager, i.e., the person who manages a deployment, uses EIM to specify only the resources an extension needs to perform its task. bpftime is a new extension framework that enforces an EIM specification. Compared to prior systems, bpftime is efficient because it uses extended Berkeley Packet Filter (eBPF)-style verification, hardware-supported isolation features (e.g., Intel MPK), and dynamic binary rewriting. Moreover, bpftime is easy to adopt into existing workflows since it is compatible with the current eBPF ecosystem. We demonstrate the usefulness of EIM and bpftime across 6 use cases that improve security, monitor and enhance performance, and explore configuration trade-offs. Subject : OSDI.2025"
    },
    {
      "paper_id": "li@osdi25@USENIX",
      "index": 32,
      "title": "Tintin: A Unified Hardware Performance Profiling Infrastructure to Uncover and Manage Uncertainty",
      "authors": [
        "Ao Li",
        "Marion Sudvarg",
        "Zihan Li",
        "Sanjoy Baruah",
        "Chris Gill",
        "Ning Zhang"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "profiling",
        "tintin",
        "hardware",
        "uncertainty",
        "infrastructure",
        "events",
        "introduces",
        "epx",
        "attribution",
        "multiplexing"
      ],
      "summary": "Hardware performance counters (HPCs) enable the measurement of microarchitectural events, which are crucial for tracking and predicting program behavior. High-fidelity measurement and precise attribution are essential for accurate profiling. However, existing profiling tools have fundamental challenges in both aspects. In measurement, numerous events compete for limited hardware monitoring resources; while for attribution, applications have diverse requirements, but systems provide limited support. Existing tools mitigate the former limitation through event multiplexing, but this approach introduces non-trivial errors. The latter limitation, however, remains largely unaddressed. This paper introduces Tintin, an HPC profiling infrastructure with a modular three-component design that addresses both challenges. Tintin introduces mechanisms to mitigate multiplexing errors by characterizing uncertainty at runtime, scheduling events to minimize it, and reporting uncertainty to applications. It also proposes the Event Profiling Context (ePX) as a new OS primitive to unify diverse profiling requirements. Tintin is evaluated using benchmarks as well as real-world resource orchestration, performance debugging, and intrusion detection systems, to demonstrate its ability to improve hardware profiling with low runtime overhead.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/li"
        ],
        "venue": [
          "/venue/li@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-li.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/li"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Tintin: A Unified Hardware Performance Profiling Infrastructure to Uncover and Manage Uncertainty [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Ao Li , Marion Sudvarg , Zihan Li , Sanjoy Baruah , Chris Gill , Ning Zhang Hardware performance counters (HPCs) enable the measurement of microarchitectural events, which are crucial for tracking and predicting program behavior. High-fidelity measurement and precise attribution are essential for accurate profiling. However, existing profiling tools have fundamental challenges in both aspects. In measurement, numerous events compete for limited hardware monitoring resources; while for attribution, applications have diverse requirements, but systems provide limited support. Existing tools mitigate the former limitation through event multiplexing, but this approach introduces non-trivial errors. The latter limitation, however, remains largely unaddressed. This paper introduces Tintin, an HPC profiling infrastructure with a modular three-component design that addresses both challenges. Tintin introduces mechanisms to mitigate multiplexing errors by characterizing uncertainty at runtime, scheduling events to minimize it, and reporting uncertainty to applications. It also proposes the Event Profiling Context (ePX) as a new OS primitive to unify diverse profiling requirements. Tintin is evaluated using benchmarks as well as real-world resource orchestration, performance debugging, and intrusion detection systems, to demonstrate its ability to improve hardware profiling with low runtime overhead. Subject : OSDI.2025"
    },
    {
      "paper_id": "schuermann@osdi25@USENIX",
      "index": 33,
      "title": "Building Bridges: Safe Interactions with Foreign Languages through Omniglot",
      "authors": [
        "Leon Schuermann",
        "Jack Toubes",
        "Tyler Potyondy",
        "Pat Pannuto",
        "Mae Milano",
        "Amit Levy"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "omniglot",
        "foreign",
        "languages",
        "safe",
        "libraries",
        "soundness",
        "safety",
        "guarantees",
        "interactions",
        "vulnerabilities"
      ],
      "summary": "Memory- and type-safe languages promise to eliminate entire classes of systems vulnerabilities by construction. In practice, though, even clean-slate systems often need to incorporate libraries written in other languages with fewer safety guarantees. Because these interactions threaten the soundness of safe languages, they can reintroduce the exact vulnerabilities that safe languages prevent in the first place. This paper presents Omniglot: the first framework to efficiently uphold safety and soundness of Rust in the presence of unmodified and untrusted foreign libraries. Omniglot facilitates interactions with foreign code by integrating with a memory isolation primitive and validation infrastructure, and avoids expensive operations such as copying or serialization. We implement Omniglot for two systems: we use it to integrate kernel components in a highly-constrained embedded operating system kernel, as well as to interface with conventional Linux userspace libraries. Omniglot performs comparably to approaches that deliver weaker guarantees and significantly better than those with similar safety guarantees.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/schuermann"
        ],
        "venue": [
          "/venue/schuermann@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-schuermann.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/schuermann"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Building Bridges: Safe Interactions with Foreign Languages through Omniglot [PDF ] [Copy] [Kimi ] [REL] Authors : Leon Schuermann , Jack Toubes , Tyler Potyondy , Pat Pannuto , Mae Milano , Amit Levy Memory- and type-safe languages promise to eliminate entire classes of systems vulnerabilities by construction. In practice, though, even clean-slate systems often need to incorporate libraries written in other languages with fewer safety guarantees. Because these interactions threaten the soundness of safe languages, they can reintroduce the exact vulnerabilities that safe languages prevent in the first place. This paper presents Omniglot: the first framework to efficiently uphold safety and soundness of Rust in the presence of unmodified and untrusted foreign libraries. Omniglot facilitates interactions with foreign code by integrating with a memory isolation primitive and validation infrastructure, and avoids expensive operations such as copying or serialization. We implement Omniglot for two systems: we use it to integrate kernel components in a highly-constrained embedded operating system kernel, as well as to interface with conventional Linux userspace libraries. Omniglot performs comparably to approaches that deliver weaker guarantees and significantly better than those with similar safety guarantees. Subject : OSDI.2025"
    },
    {
      "paper_id": "zhang-tianren@osdi25@USENIX",
      "index": 34,
      "title": "KRR: Efficient and Scalable Kernel Record Replay",
      "authors": [
        "Tianren Zhang",
        "Sishuai Gong",
        "Pedro Fonseca"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "krr",
        "record",
        "replay",
        "kernel",
        "bugs",
        "execution",
        "rocksdb",
        "recording",
        "slowdown",
        "failures"
      ],
      "summary": "Modern kernels are large, complex, and plagued with bugs. Unfortunately, their large size and complexity make kernel failures very challenging for developers to diagnose since failures encountered in deployment are often notoriously difficult to reproduce. Although record-replay techniques provide the powerful ability to accurately record a failed execution and deterministically replay it, enabling advanced manual and automated analysis techniques, they are inefficient and do not scale with modern I/O-intensive, concurrent workloads. This paper introduces KRR, a kernel record-replay framework that provides a highly efficient execution recording mechanism by narrowing the scope of the record and replay boundary to the kernel. Unlike previous record-replay whole-stack approaches, KRR adopts a split-recorder design that employs the guest and the host to jointly record the kernel execution. Our evaluation demonstrates that KRR scales efficiently up to 8 cores, across a range of different workloads, including kernel compilation, RocksDB, and Nginx. When recording 8-core VMs that run RocksDB and kernel compilation, KRR incurs only a 1.52× ~ 2.79× slowdown compared to native execution, while traditional whole-VM RR suffers from 8.97× ~ 29.94× slowdown. We validate that KRR is practical and has a broad recording scope by reproducing 17 bugs across different Linux versions, including 6 non-deterministic bugs and 5 high-risk CVEs; KRR was able to record and reproduce all but one non-deterministic bug.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/zhang-tianren"
        ],
        "venue": [
          "/venue/zhang-tianren@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-zhang-tianren.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/zhang-tianren"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "KRR: Efficient and Scalable Kernel Record Replay [PDF ] [Copy] [Kimi ] [REL] Authors : Tianren Zhang , Sishuai Gong , Pedro Fonseca Modern kernels are large, complex, and plagued with bugs. Unfortunately, their large size and complexity make kernel failures very challenging for developers to diagnose since failures encountered in deployment are often notoriously difficult to reproduce. Although record-replay techniques provide the powerful ability to accurately record a failed execution and deterministically replay it, enabling advanced manual and automated analysis techniques, they are inefficient and do not scale with modern I/O-intensive, concurrent workloads. This paper introduces KRR, a kernel record-replay framework that provides a highly efficient execution recording mechanism by narrowing the scope of the record and replay boundary to the kernel. Unlike previous record-replay whole-stack approaches, KRR adopts a split-recorder design that employs the guest and the host to jointly record the kernel execution. Our evaluation demonstrates that KRR scales efficiently up to 8 cores, across a range of different workloads, including kernel compilation, RocksDB, and Nginx. When recording 8-core VMs that run RocksDB and kernel compilation, KRR incurs only a 1.52× ~ 2.79× slowdown compared to native execution, while traditional whole-VM RR suffers from 8.97× ~ 29.94× slowdown. We validate that KRR is practical and has a broad recording scope by reproducing 17 bugs across different Linux versions, including 6 non-deterministic bugs and 5 high-risk CVEs; KRR was able to record and reproduce all but one non-deterministic bug. Subject : OSDI.2025"
    },
    {
      "paper_id": "yedidia@osdi25@USENIX",
      "index": 35,
      "title": "Deterministic Client: Enforcing Determinism on Untrusted Machine Code",
      "authors": [
        "Zachary Yedidia",
        "Geoffrey Ramseyer",
        "David Mazières"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "decl",
        "code",
        "deterministic",
        "sandboxed",
        "isolation",
        "trusted",
        "determinism",
        "untrusted",
        "client",
        "enforcing"
      ],
      "summary": "This paper presents Deterministic Client (DeCl), a software-based sandboxing system for enforcing deterministic behavior on untrusted machine code, either x86-64 or Arm64. DeCl adapts techniques from Software Fault Isolation (SFI) traditionally used to guarantee memory isolation to instead enforce the stronger property of determinism. By using a simple and efficient machine code verifier that can guarantee that a program behaves deterministically, DeCl does not rely on a trusted compiler/interpreter for correctness. This allows the use of LLVM without compromising the size of the trusted code base. We also describe how to implement two efficient metering mechanisms that enforce deterministic preemption of sandboxed programs, and how DeCl can be implemented in combination with traditional software-based isolation, by making the sandboxed code position-oblivious. DeCl is able to combine and improve upon the benefits of both interpreters and JIT compilers at once, with low CPU overhead, fast startup time, and strong security via a small trusted code base. We evaluate DeCl's effectiveness on general-purpose CPU benchmarks, as well as in an application-specific context by integrating with the Groundhog smart contract engine, and using DeCl for zero-knowledge-proof verification.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/yedidia"
        ],
        "venue": [
          "/venue/yedidia@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-yedidia.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/yedidia"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Deterministic Client: Enforcing Determinism on Untrusted Machine Code [PDF ] [Copy] [Kimi ] [REL] Authors : Zachary Yedidia , Geoffrey Ramseyer , David Mazières This paper presents Deterministic Client (DeCl), a software-based sandboxing system for enforcing deterministic behavior on untrusted machine code, either x86-64 or Arm64. DeCl adapts techniques from Software Fault Isolation (SFI) traditionally used to guarantee memory isolation to instead enforce the stronger property of determinism. By using a simple and efficient machine code verifier that can guarantee that a program behaves deterministically, DeCl does not rely on a trusted compiler/interpreter for correctness. This allows the use of LLVM without compromising the size of the trusted code base. We also describe how to implement two efficient metering mechanisms that enforce deterministic preemption of sandboxed programs, and how DeCl can be implemented in combination with traditional software-based isolation, by making the sandboxed code position-oblivious. DeCl is able to combine and improve upon the benefits of both interpreters and JIT compilers at once, with low CPU overhead, fast startup time, and strong security via a small trusted code base. We evaluate DeCl's effectiveness on general-purpose CPU benchmarks, as well as in an application-specific context by integrating with the Groundhog smart contract engine, and using DeCl for zero-knowledge-proof verification. Subject : OSDI.2025"
    },
    {
      "paper_id": "pismenny@osdi25@USENIX",
      "index": 36,
      "title": "Disentangling the Dual Role of NIC Receive Rings",
      "authors": [
        "Boris Pismenny",
        "Adam Morrison",
        "Dan Tsafrir"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "nic",
        "rxbisect",
        "rings",
        "shring",
        "buffers",
        "packet",
        "cores",
        "bottleneck",
        "receive",
        "core"
      ],
      "summary": "CPUs parallelize packet processing across cores via per-core receive (Rx) rings, which are typically sized to absorb bursts with >=1Ki entries by default. The combined I/O working set (packet buffers pointed to by all Rx rings) easily exceeds the LLC capacity, thus degrading performance due to high memory bandwidth pressure. Recent work has reduced the I/O working set size by sharing Rx rings among cores with the \"shRing\" system. But this approach suffers from a bottleneck under imbalanced loads, which are common. We contend that the bottleneck stems from an unnecessary entanglement of two orthogonal producer-consumer structures: (1) memory allocation, where the core produces empty buffers that the NIC consumes to store packets; and (2) packet delivery, where the NIC produces incoming packets that the core consumes. We propose rxBisect, a new CPU-NIC interface that decouples these structures. RxBisect replaces each Rx ring with two separate rings corresponding to the two structures, allowing memory allocation to be performed independently of packet reception. RxBisect can thus pass empty buffers efficiently between cores upon imbalance, thereby eliminating the aforementioned bottleneck. We implement rxBisect with software emulation and find that it improves throughput by up to 20% and 37% relative to the state-of-the-art (shRing) and state-of-the-practice (per-core Rx rings).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/pismenny"
        ],
        "venue": [
          "/venue/pismenny@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-pismenny.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/pismenny"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Disentangling the Dual Role of NIC Receive Rings [PDF ] [Copy] [Kimi ] [REL] Authors : Boris Pismenny , Adam Morrison , Dan Tsafrir CPUs parallelize packet processing across cores via per-core receive (Rx) rings, which are typically sized to absorb bursts with >=1Ki entries by default. The combined I/O working set (packet buffers pointed to by all Rx rings) easily exceeds the LLC capacity, thus degrading performance due to high memory bandwidth pressure. Recent work has reduced the I/O working set size by sharing Rx rings among cores with the \"shRing\" system. But this approach suffers from a bottleneck under imbalanced loads, which are common. We contend that the bottleneck stems from an unnecessary entanglement of two orthogonal producer-consumer structures: (1) memory allocation, where the core produces empty buffers that the NIC consumes to store packets; and (2) packet delivery, where the NIC produces incoming packets that the core consumes. We propose rxBisect, a new CPU-NIC interface that decouples these structures. RxBisect replaces each Rx ring with two separate rings corresponding to the two structures, allowing memory allocation to be performed independently of packet reception. RxBisect can thus pass empty buffers efficiently between cores upon imbalance, thereby eliminating the aforementioned bottleneck. We implement rxBisect with software emulation and find that it improves throughput by up to 20% and 37% relative to the state-of-the-art (shRing) and state-of-the-practice (per-core Rx rings). Subject : OSDI.2025"
    },
    {
      "paper_id": "shen-weihang@osdi25@USENIX",
      "index": 37,
      "title": "XSched: Preemptive Scheduling for Diverse XPUs",
      "authors": [
        "Weihang Shen",
        "Mingcong Han",
        "Jialong Liu",
        "Rong Chen",
        "Haibo Chen"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "xpus",
        "xsched",
        "scheduling",
        "preemptive",
        "multitasking",
        "diverse",
        "flexible",
        "xqueue",
        "hardware",
        "abstraction"
      ],
      "summary": "XPUs, such as GPUs, NPUs, ASICs, and FPGAs, lack flexible scheduling capabilities, failing to meet rich application requirements (e.g., priority and fairness) in multitasking environments. This paper presents XSched, a scheduling framework that enables preemptive scheduling on diverse XPUs with flexible policies. XSched provides unified interfaces for scheduling XPU tasks through a preemptible command queue abstraction (XQueue). The key challenge in implementing the abstraction is adapting to XPUs with diverse and evolving hardware capabilities and software stacks. XSched proposes a multi-level hardware model that enables mature, advanced XPUs to achieve optimal scheduling performance while maintaining compatibility with emerging, wimpy XPUs. To demonstrate the generalizability of XSched, we adapted it to ten XPUs of different types, brands, and generations across seven software platforms and implemented two hardware-agnostic scheduling policies. We further evaluated XSched through three case studies of multitasking workloads on XPUs. XSched effectively achieves various scheduling objectives using its efficient and flexible preemption mechanisms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/shen-weihang"
        ],
        "venue": [
          "/venue/shen-weihang@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-shen-weihang.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/shen-weihang"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "XSched: Preemptive Scheduling for Diverse XPUs [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Weihang Shen , Mingcong Han , Jialong Liu , Rong Chen , Haibo Chen XPUs, such as GPUs, NPUs, ASICs, and FPGAs, lack flexible scheduling capabilities, failing to meet rich application requirements (e.g., priority and fairness) in multitasking environments. This paper presents XSched, a scheduling framework that enables preemptive scheduling on diverse XPUs with flexible policies. XSched provides unified interfaces for scheduling XPU tasks through a preemptible command queue abstraction (XQueue). The key challenge in implementing the abstraction is adapting to XPUs with diverse and evolving hardware capabilities and software stacks. XSched proposes a multi-level hardware model that enables mature, advanced XPUs to achieve optimal scheduling performance while maintaining compatibility with emerging, wimpy XPUs. To demonstrate the generalizability of XSched, we adapted it to ten XPUs of different types, brands, and generations across seven software platforms and implemented two hardware-agnostic scheduling policies. We further evaluated XSched through three case studies of multitasking workloads on XPUs. XSched effectively achieves various scheduling objectives using its efficient and flexible preemption mechanisms. Subject : OSDI.2025"
    },
    {
      "paper_id": "wu-yuanpei@osdi25@USENIX",
      "index": 38,
      "title": "OS Rendering Service Made Parallel with Out-of-Order Execution and In-Order Commit",
      "authors": [
        "Yuanpei Wu",
        "Dong Du",
        "Chao Xu",
        "Yubin Xia",
        "Yang Yu",
        "Ming Fu",
        "Binyu Zang",
        "Haibo Chen"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "rendering",
        "spars",
        "order",
        "service",
        "drawing",
        "commit",
        "dependency",
        "execution",
        "frame",
        "openharmony"
      ],
      "summary": "Rendering service is an indispensable OS service on smart-device OSes like Android, iOS and OpenHarmony. However, the recent shift towards highly scalable display scenarios, such as foldable and multiple screens, has notably amplified the rendering workload, leading to low frame rates that degrade user experience. Yet, rendering services predominantly follow a sequential model, which is notoriously hard to parallelize due to the complex state dependency, drawing order dependency, and interface dependency. This paper observes that a significant portion of the rendering procedure is potentially parallelizable through proper state pre-untangling and drawing order post-preserving. To this end, this paper introduces Spars, a scalable parallelized OS rendering service inspired by the out-of-order execution with in-order commit in computer architecture. Spars revolutionizes the rendering procedure by initially generating self-contained rendering tasks through in-order preparation, executing such tasks in an out-of-order manner to maximize multi-core parallelism, and subsequently committing the tasks in-order to enforce drawing order dependencies. Evaluation results on state-of-the-art single-screen, dual-fold, and tri-fold smartphones (Mate 70, X5, XT) as well as one-chip-multiple-screen configurations show an average frame rate improvement of 1.76×–1.91×. Moreover, Spars is able to decrease the device power consumption by 3.0% or increase the budget of graphics primitives by 2.31× for more appealing visual effects with the same stable frame rate.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/wu-yuanpei"
        ],
        "venue": [
          "/venue/wu-yuanpei@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-wu-yuanpei.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/wu-yuanpei"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "OS Rendering Service Made Parallel with Out-of-Order Execution and In-Order Commit [PDF ] [Copy] [Kimi ] [REL] Authors : Yuanpei Wu , Dong Du , Chao Xu , Yubin Xia , Yang Yu , Ming Fu , Binyu Zang , Haibo Chen Rendering service is an indispensable OS service on smart-device OSes like Android, iOS and OpenHarmony. However, the recent shift towards highly scalable display scenarios, such as foldable and multiple screens, has notably amplified the rendering workload, leading to low frame rates that degrade user experience. Yet, rendering services predominantly follow a sequential model, which is notoriously hard to parallelize due to the complex state dependency, drawing order dependency, and interface dependency. This paper observes that a significant portion of the rendering procedure is potentially parallelizable through proper state pre-untangling and drawing order post-preserving. To this end, this paper introduces Spars, a scalable parallelized OS rendering service inspired by the out-of-order execution with in-order commit in computer architecture. Spars revolutionizes the rendering procedure by initially generating self-contained rendering tasks through in-order preparation, executing such tasks in an out-of-order manner to maximize multi-core parallelism, and subsequently committing the tasks in-order to enforce drawing order dependencies. Evaluation results on state-of-the-art single-screen, dual-fold, and tri-fold smartphones (Mate 70, X5, XT) as well as one-chip-multiple-screen configurations show an average frame rate improvement of 1.76×–1.91×. Moreover, Spars is able to decrease the device power consumption by 3.0% or increase the budget of graphics primitives by 2.31× for more appealing visual effects with the same stable frame rate. Subject : OSDI.2025"
    },
    {
      "paper_id": "chai-siyuan@osdi25@USENIX",
      "index": 39,
      "title": "EMT: An OS Framework for New Memory Translation Architectures",
      "authors": [
        "Siyuan Chai",
        "Jiyuan Zhang",
        "Jongyul Kim",
        "Alan Wang",
        "Fan Chung",
        "Jovan Stojkovic",
        "Weiwei Jia",
        "Dimitrios Skarlatos",
        "Josep Torrellas",
        "Tianyin Xu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "emt",
        "memory",
        "translation",
        "hardware",
        "linux",
        "oses",
        "schemes",
        "extensibility",
        "architectures",
        "empower"
      ],
      "summary": "With terabyte-scale memory capacity and memory-intensive workloads, memory translation has become a major performance bottleneck. Many novel hardware schemes are developed to speed up memory translation, but few are experimented with commodity OSes. A main reason is that memory management in major OSes, like Linux, does not have the extensibility to empower emerging hardware schemes. We develop EMT, a pragmatic framework atop Linux to empower different hardware schemes of memory translation such as radix tree and hash table. EMT provides an architecture neutral interface that 1) supports diverse memory translation architectures, 2) enables hardware-specific optimizations, 3) accommodates modern hardware and OS complexity, and 4) has negligible overhead over hardwired implementations. We port Linux’s memory management onto EMT and show that EMT enables extensibility without sacrificing performance. We use EMT to implement OS support for ECPT and FPT, two recent experimental translation schemes for fast translation; EMT enables us to understand the OS perspective of these architectures and further optimize their designs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/chai-siyuan"
        ],
        "venue": [
          "/venue/chai-siyuan@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-chai-siyuan.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/chai-siyuan"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "EMT: An OS Framework for New Memory Translation Architectures [PDF ] [Copy] [Kimi ] [REL] Authors : Siyuan Chai , Jiyuan Zhang , Jongyul Kim , Alan Wang , Fan Chung , Jovan Stojkovic , Weiwei Jia , Dimitrios Skarlatos , Josep Torrellas , Tianyin Xu With terabyte-scale memory capacity and memory-intensive workloads, memory translation has become a major performance bottleneck. Many novel hardware schemes are developed to speed up memory translation, but few are experimented with commodity OSes. A main reason is that memory management in major OSes, like Linux, does not have the extensibility to empower emerging hardware schemes. We develop EMT, a pragmatic framework atop Linux to empower different hardware schemes of memory translation such as radix tree and hash table. EMT provides an architecture neutral interface that 1) supports diverse memory translation architectures, 2) enables hardware-specific optimizations, 3) accommodates modern hardware and OS complexity, and 4) has negligible overhead over hardwired implementations. We port Linux’s memory management onto EMT and show that EMT enables extensibility without sacrificing performance. We use EMT to implement OS support for ECPT and FPT, two recent experimental translation schemes for fast translation; EMT enables us to understand the OS perspective of these architectures and further optimize their designs. Subject : OSDI.2025"
    },
    {
      "paper_id": "liu@osdi25@USENIX",
      "index": 40,
      "title": "Tiered Memory Management Beyond Hotness",
      "authors": [
        "Jinshu Liu",
        "Hamid Hadian",
        "Hanchen Xu",
        "Huaicheng Li"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "hotness",
        "aol",
        "tiering",
        "tiered",
        "memory",
        "soar",
        "alto",
        "latency",
        "access",
        "offcore"
      ],
      "summary": "Tiered memory systems often rely on access frequency (''hotness'') to guide data placement. However, hot data is not always performance-critical, limiting the effectiveness of hotness-based policies. We introduce amortized offcore latency (AOL), a novel metric that precisely captures the true performance impact of memory accesses by accounting for memory access latency and memory-level parallelism (MLP). Leveraging AOL, we present two powerful tiering mechanisms: SOAR, a profile-guided allocation policy that places objects based on their performance contribution, and ALTO, a lightweight page migration regulation policy to eliminate unnecessary migrations. SOAR and ALTO outperform four state-of-the-art tiering designs across a diverse set of workloads by up to 12.4×, while underperforming in a few cases by no more than 3%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/liu"
        ],
        "venue": [
          "/venue/liu@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-liu.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/liu"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Tiered Memory Management Beyond Hotness [PDF ] [Copy] [Kimi ] [REL] Authors : Jinshu Liu , Hamid Hadian , Hanchen Xu , Huaicheng Li Tiered memory systems often rely on access frequency (''hotness'') to guide data placement. However, hot data is not always performance-critical, limiting the effectiveness of hotness-based policies. We introduce amortized offcore latency (AOL), a novel metric that precisely captures the true performance impact of memory accesses by accounting for memory access latency and memory-level parallelism (MLP). Leveraging AOL, we present two powerful tiering mechanisms: SOAR, a profile-guided allocation policy that places objects based on their performance contribution, and ALTO, a lightweight page migration regulation policy to eliminate unnecessary migrations. SOAR and ALTO outperform four state-of-the-art tiering designs across a diverse set of workloads by up to 12.4×, while underperforming in a few cases by no more than 3%. Subject : OSDI.2025"
    },
    {
      "paper_id": "zhu-kan@osdi25@USENIX",
      "index": 41,
      "title": "NanoFlow: Towards Optimal Large Language Model Serving Throughput",
      "authors": [
        "Kan Zhu",
        "Yufei Gao",
        "Yilong Zhao",
        "Liangyu Zhao",
        "Gefei Zuo",
        "Yile Gu",
        "Dedong Xie",
        "Tian Tang",
        "Qinyu Xu",
        "Zihao Ye",
        "Keisuke Kamahori",
        "Chien-Yu Lin",
        "Ziren Wang",
        "Stephanie Wang",
        "Arvind Krishnamurthy",
        "Baris Kasikci"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "nanoflow",
        "serving",
        "throughput",
        "llm",
        "memory",
        "end",
        "batches",
        "llama",
        "operations",
        "device"
      ],
      "summary": "Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems’ performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving—compute, memory, networking—are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8×7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91× throughput boost compared to state-of-the-art serving systems achieving 50% to 72 % of optimal throughput across popular models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/zhu-kan"
        ],
        "venue": [
          "/venue/zhu-kan@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-zhu-kan.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/zhu-kan"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "NanoFlow: Towards Optimal Large Language Model Serving Throughput [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Kan Zhu , Yufei Gao , Yilong Zhao , Liangyu Zhao , Gefei Zuo , Yile Gu , Dedong Xie , Tian Tang , Qinyu Xu , Zihao Ye , Keisuke Kamahori , Chien-Yu Lin , Ziren Wang , Stephanie Wang , Arvind Krishnamurthy , Baris Kasikci Large Language Models (LLMs) have resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput has emerged as a key metric that determines serving systems’ performance. Due to large model sizes and memory-intensive self-attention, LLM serving has been commonly assumed to be memory-bound. Through a detailed analysis, we show that despite having memory-intensive components, end-to-end LLM serving is compute bound for most common workloads and LLMs. Alas, most existing serving engines fall short from optimal compute utilization, because the heterogeneous operations that comprise LLM serving—compute, memory, networking—are executed sequentially within a device. We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of heterogeneous resources within a single device. NanoFlow splits inputs into smaller nano-batches and duplicates operations to operate on each portion independently, enabling overlapping. NanoFlow automatically identifies the number, size, ordering, and GPU resource allocation of nano-batches to minimize the execution time, while considering the interference of concurrent operations. We evaluate NanoFlow's end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8×7B, LLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91× throughput boost compared to state-of-the-art serving systems achieving 50% to 72 % of optimal throughput across popular models. Subject : OSDI.2025"
    },
    {
      "paper_id": "cheng@osdi25@USENIX",
      "index": 42,
      "title": "PipeThreader: Software-Defined Pipelining for Efficient DNN Execution",
      "authors": [
        "Yu Cheng",
        "Lei Wang",
        "Yining Shi",
        "Yuqing Xia",
        "Lingxiao Ma",
        "Jilong Xue",
        "Yang Wang",
        "Zhiwen Mo",
        "Feiyang Chen",
        "Fan Yang",
        "Mao Yang",
        "Zhi Yang"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "pipethreader",
        "dnn",
        "pipelining",
        "scheduling",
        "hardware",
        "tensorcores",
        "abstraction",
        "mamba2",
        "software",
        "specialized"
      ],
      "summary": "To effectively utilize heterogeneous specialized hardware units in modern GPUs, such as TensorCores and Tensor Memory Accelerators, this paper introduces PipeThreader, a new DNN compiler. PipeThreader proposes shifting scheduling functionality from hardware to software so as to enable more efficient and sophisticated computation pipelining with minimal manual effort. This is achieved through sTask-graph, a new DNN computation abstraction, a hierarchical hardware abstraction that captures the capabilities of specialized units, and new scheduling primitives. As a result, PipeThreader can discover efficient pipeline scheduling for well-studied DNN architectures like FlashAttention, achieving comparable or even superior performance. Additionally, it can uncover novel pipeline schemes for emerging models like Mamba2, delivering significantly better performance compared to state-of-the-art hand-crafted implementations. The code is open-sourced at https://github.com/tile-ai/tilelang.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/cheng"
        ],
        "venue": [
          "/venue/cheng@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-cheng.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/cheng"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "PipeThreader: Software-Defined Pipelining for Efficient DNN Execution [PDF ] [Copy] [Kimi ] [REL] Authors : Yu Cheng , Lei Wang , Yining Shi , Yuqing Xia , Lingxiao Ma , Jilong Xue , Yang Wang , Zhiwen Mo , Feiyang Chen , Fan Yang , Mao Yang , Zhi Yang To effectively utilize heterogeneous specialized hardware units in modern GPUs, such as TensorCores and Tensor Memory Accelerators, this paper introduces PipeThreader, a new DNN compiler. PipeThreader proposes shifting scheduling functionality from hardware to software so as to enable more efficient and sophisticated computation pipelining with minimal manual effort. This is achieved through sTask-graph, a new DNN computation abstraction, a hierarchical hardware abstraction that captures the capabilities of specialized units, and new scheduling primitives. As a result, PipeThreader can discover efficient pipeline scheduling for well-studied DNN architectures like FlashAttention, achieving comparable or even superior performance. Additionally, it can uncover novel pipeline schemes for emerging models like Mamba2, delivering significantly better performance compared to state-of-the-art hand-crafted implementations. The code is open-sourced at https://github.com/tile-ai/tilelang. Subject : OSDI.2025"
    },
    {
      "paper_id": "wang-zheng@osdi25@USENIX",
      "index": 43,
      "title": "WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training",
      "authors": [
        "Zheng Wang",
        "Anna Cai",
        "Xinfeng Xie",
        "Zaifeng Pan",
        "Yue Guan",
        "Weiwei Chu",
        "Jie Wang",
        "Shikai Li",
        "Jianyu Huang",
        "Chris Cai",
        "Yuchen Hao",
        "Yufei Ding"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "wlb",
        "parallelism",
        "llm",
        "workload",
        "imbalance",
        "training",
        "ork",
        "alanced",
        "oad",
        "arge"
      ],
      "summary": "In this work, we present WLB-LLM, a W ork L oad- B alanced 4D Parallelism for L arge L anguage M odel Training. We first thoroughly analyze the workload imbalance issue in LLM training and identify two primary sources of imbalance at the pipeline parallelism and context parallelism levels. Then, to address the imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a workload-aware variable-length document packing method to balance the computation and communication workload across micro-batches. Additionally, at the context parallelism level, WLB-LLM introduces a novel fine-grained per-document sharding strategy, ensuring each worker within a context parallelism group has an identical workload. Comprehensive experiments under different model scales demonstrate that WLB-LLM significantly mitigates the workload imbalance during 4D parallelism LLM training and achieves an average speedup of 1.23× when applying WLB-LLM in our internal LLM training framework.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-zheng"
        ],
        "venue": [
          "/venue/wang-zheng@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-wang-zheng.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/wang-zheng"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Zheng Wang , Anna Cai , Xinfeng Xie , Zaifeng Pan , Yue Guan , Weiwei Chu , Jie Wang , Shikai Li , Jianyu Huang , Chris Cai , Yuchen Hao , Yufei Ding In this work, we present WLB-LLM, a W ork L oad- B alanced 4D Parallelism for L arge L anguage M odel Training. We first thoroughly analyze the workload imbalance issue in LLM training and identify two primary sources of imbalance at the pipeline parallelism and context parallelism levels. Then, to address the imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a workload-aware variable-length document packing method to balance the computation and communication workload across micro-batches. Additionally, at the context parallelism level, WLB-LLM introduces a novel fine-grained per-document sharding strategy, ensuring each worker within a context parallelism group has an identical workload. Comprehensive experiments under different model scales demonstrate that WLB-LLM significantly mitigates the workload imbalance during 4D parallelism LLM training and achieves an average speedup of 1.23× when applying WLB-LLM in our internal LLM training framework. Subject : OSDI.2025"
    },
    {
      "paper_id": "park-yeonhong@osdi25@USENIX",
      "index": 44,
      "title": "DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization",
      "authors": [
        "Yeonhong Park",
        "Jake Hyun",
        "Hojoon Kim",
        "Jae W. Lee"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "decdec",
        "bit",
        "quantization",
        "channels",
        "residuals",
        "portion",
        "salient",
        "gpu",
        "activation",
        "fetches"
      ],
      "summary": "Quantization of Large Language Models (LLMs) has recently gained popularity, particularly for on-device settings with limited hardware resources. While efficient, quantization inevitably degrades model quality, especially in aggressive low-bit settings such as 3-bit and 4-bit precision. In this paper, we propose DecDEC, an inference scheme that improves the quality of low-bit LLMs while preserving the key benefits of quantization: GPU memory savings and latency reduction. DecDEC stores the residual matrix—the difference between full-precision and quantized weights—in CPU, and dynamically fetches the residuals for only a small portion of the weights. This portion corresponds to the salient channels, marked by activation outliers, with the fetched residuals helping to correct quantization errors in these channels. Salient channels are identified dynamically at each decoding step by analyzing the input activations---this enables adaptation to the dynamic nature of activation distribution, thus maximizing the effectiveness of error compensation. We demonstrate the effectiveness of DecDEC by augmenting state-of-the-art quantization methods. For example, DecDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model from 10.15 to 9.12—outperforming its 3.5-bit counterpart—while adding less than 0.0003% to GPU memory usage and incurring only a 1.7% inference slowdown on NVIDIA RTX 4050 Mobile.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/park-yeonhong"
        ],
        "venue": [
          "/venue/park-yeonhong@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-park-yeonhong.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/park-yeonhong"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization [PDF ] [Copy] [Kimi ] [REL] Authors : Yeonhong Park , Jake Hyun , Hojoon Kim , Jae W. Lee Quantization of Large Language Models (LLMs) has recently gained popularity, particularly for on-device settings with limited hardware resources. While efficient, quantization inevitably degrades model quality, especially in aggressive low-bit settings such as 3-bit and 4-bit precision. In this paper, we propose DecDEC, an inference scheme that improves the quality of low-bit LLMs while preserving the key benefits of quantization: GPU memory savings and latency reduction. DecDEC stores the residual matrix—the difference between full-precision and quantized weights—in CPU, and dynamically fetches the residuals for only a small portion of the weights. This portion corresponds to the salient channels, marked by activation outliers, with the fetched residuals helping to correct quantization errors in these channels. Salient channels are identified dynamically at each decoding step by analyzing the input activations---this enables adaptation to the dynamic nature of activation distribution, thus maximizing the effectiveness of error compensation. We demonstrate the effectiveness of DecDEC by augmenting state-of-the-art quantization methods. For example, DecDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model from 10.15 to 9.12—outperforming its 3.5-bit counterpart—while adding less than 0.0003% to GPU memory usage and incurring only a 1.7% inference slowdown on NVIDIA RTX 4050 Mobile. Subject : OSDI.2025"
    },
    {
      "paper_id": "gao@osdi25@USENIX",
      "index": 45,
      "title": "Stripeless Data Placement for Erasure-Coded In-Memory Storage",
      "authors": [
        "Jian Gao",
        "Jiwu Shu",
        "Bin Yan",
        "Yuhao Zhang",
        "Keji Huang"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "erasure",
        "nos",
        "coding",
        "storage",
        "stripeless",
        "stripes",
        "overheads",
        "nostor",
        "memory",
        "sbibd"
      ],
      "summary": "Erasure coding plays a crucial role in distributed storage systems to provide fault tolerance at a low storage cost. Conventional erasure coding schemes are based on stripes. However, placing data into stripes can incur non-negligible performance overheads that will manifest in emerging fast in-memory storage systems, making conventional erasure coding schemes suboptimal in such scenarios. Aiming to eliminate such overheads, we present Nos, a stripeless erasure coding scheme. It lets each node in the storage system independently replicate data to other nodes and encode received data replica into parities with XOR. Thus, Nos avoids the overheads caused by stripes. To enable failure recovery, Nos uses a combinatoric structure called symmetric balanced incomplete block design (SBIBD) to decide primary-to-backup node affinities during replication. Atop Nos, we further build Nostor, a distributed in-memory key-value store. Evaluations demonstrate that Nostor achieves 1.61x and 2.60x throughputs with similar or lower latencies than stripe-based erasure coding baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/gao"
        ],
        "venue": [
          "/venue/gao@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-gao.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/gao"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Stripeless Data Placement for Erasure-Coded In-Memory Storage [PDF ] [Copy] [Kimi ] [REL] Authors : Jian Gao , Jiwu Shu , Bin Yan , Yuhao Zhang , Keji Huang Erasure coding plays a crucial role in distributed storage systems to provide fault tolerance at a low storage cost. Conventional erasure coding schemes are based on stripes. However, placing data into stripes can incur non-negligible performance overheads that will manifest in emerging fast in-memory storage systems, making conventional erasure coding schemes suboptimal in such scenarios. Aiming to eliminate such overheads, we present Nos, a stripeless erasure coding scheme. It lets each node in the storage system independently replicate data to other nodes and encode received data replica into parities with XOR. Thus, Nos avoids the overheads caused by stripes. To enable failure recovery, Nos uses a combinatoric structure called symmetric balanced incomplete block design (SBIBD) to decide primary-to-backup node affinities during replication. Atop Nos, we further build Nostor, a distributed in-memory key-value store. Evaluations demonstrate that Nostor achieves 1.61x and 2.60x throughputs with similar or lower latencies than stripe-based erasure coding baselines. Subject : OSDI.2025"
    },
    {
      "paper_id": "leblanc@osdi25@USENIX",
      "index": 46,
      "title": "PoWER Never Corrupts: Tool-Agnostic Verification of Crash Consistency and Corruption Detection",
      "authors": [
        "Hayley LeBlanc",
        "Jacob R. Lorch",
        "Chris Hawblitzel",
        "Cheng Huang",
        "Yiheng Tao",
        "Nickolai Zeldovich",
        "Vijay Chidambaram"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "corruption",
        "verification",
        "storage",
        "crash",
        "capybarakv",
        "systems",
        "preconditions",
        "corrupts",
        "resilient",
        "constructs"
      ],
      "summary": "Storage systems must maintain integrity even after rare and difficult-to-test-for conditions like power losses and media errors. Formal verification presents a promising avenue to ensure storage systems are resilient, but current approaches involve significant complexity and rely on verification constructs or forms of logic beyond what most verifiers natively support. In this paper, we present two new verification techniques that rely only on standard constructs provided by most verification tools such as Hoare logic, ghost variables, and quantifiers. First, we introduce PoWER (Preconditions on Writes Enforcing Recoverability), a novel approach to verifying crash consistency that encodes its requirements in the preconditions of storage API methods. Second, we present a new model of media corruption for provable corruption detection on any type of storage device. To demonstrate the power of these new techniques, we use them to build two verified storage systems using two different verification frameworks. We build and verify the key-value (KV) store CapybaraKV using Verus and the notary server CapybaraNS using Dafny. Both systems are built for persistent memory (PM), which we target due to new challenges it presents to building resilient storage systems. We develop new techniques to address these challenges, including the corruption-detecting Boolean, a new primitive for atomic checksum updates. Both systems verify in under a minute, and CapybaraKV achieves performance competitive with similar unverified PM KV stores.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/leblanc"
        ],
        "venue": [
          "/venue/leblanc@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-leblanc.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/leblanc"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "PoWER Never Corrupts: Tool-Agnostic Verification of Crash Consistency and Corruption Detection [PDF ] [Copy] [Kimi ] [REL] Authors : Hayley LeBlanc , Jacob R. Lorch , Chris Hawblitzel , Cheng Huang , Yiheng Tao , Nickolai Zeldovich , Vijay Chidambaram Storage systems must maintain integrity even after rare and difficult-to-test-for conditions like power losses and media errors. Formal verification presents a promising avenue to ensure storage systems are resilient, but current approaches involve significant complexity and rely on verification constructs or forms of logic beyond what most verifiers natively support. In this paper, we present two new verification techniques that rely only on standard constructs provided by most verification tools such as Hoare logic, ghost variables, and quantifiers. First, we introduce PoWER (Preconditions on Writes Enforcing Recoverability), a novel approach to verifying crash consistency that encodes its requirements in the preconditions of storage API methods. Second, we present a new model of media corruption for provable corruption detection on any type of storage device. To demonstrate the power of these new techniques, we use them to build two verified storage systems using two different verification frameworks. We build and verify the key-value (KV) store CapybaraKV using Verus and the notary server CapybaraNS using Dafny. Both systems are built for persistent memory (PM), which we target due to new challenges it presents to building resilient storage systems. We develop new techniques to address these challenges, including the corruption-detecting Boolean, a new primitive for atomic checksum updates. Both systems verify in under a minute, and CapybaraKV achieves performance competitive with similar unverified PM KV stores. Subject : OSDI.2025"
    },
    {
      "paper_id": "pan@osdi25@USENIX",
      "index": 47,
      "title": "Fast and Synchronous Crash Consistency with Metadata Write-Once File System",
      "authors": [
        "Yanqi Pan",
        "Wen Xia",
        "Yifeng Zhang",
        "Xiangyu Zou",
        "Hao Huang",
        "Zhenhua Li",
        "Chentao Wu"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "file",
        "wofs",
        "crash",
        "metadata",
        "synchronous",
        "write",
        "wolves",
        "consistency",
        "package",
        "system"
      ],
      "summary": "Low-latency persistent memory (PM) encourages file systems to pursue synchronous crash consistency. However, existing crash consistency approaches, such as journaling and log structure file system, incur many small, random, and ordered metadata I/Os, failing to exploit PM I/O potential. We propose a new file system model called metadata write-once file system (WOFS) to achieve fast and synchronous crash consistency. The key idea is to generate specific metadata for each file operation as a checksum-protected package and write it once with a single ordering point. The package is then managed to provide file abstractions through a package translation layer without extra writes. Using an array of techniques to generate, organize, and recover from packages, WOFS can provide practical, efficient, and reliable file system services. We implement WOLVES as a WOFS prototype in Linux kernel. Experiments using benchmarks and applications suggest that WOLVES can recover from crashes, improve operation throughput, and potentially reach PM I/O bandwidth limits.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/pan"
        ],
        "venue": [
          "/venue/pan@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-pan.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/pan"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Fast and Synchronous Crash Consistency with Metadata Write-Once File System [PDF ] [Copy] [Kimi ] [REL] Authors : Yanqi Pan , Wen Xia , Yifeng Zhang , Xiangyu Zou , Hao Huang , Zhenhua Li , Chentao Wu Low-latency persistent memory (PM) encourages file systems to pursue synchronous crash consistency. However, existing crash consistency approaches, such as journaling and log structure file system, incur many small, random, and ordered metadata I/Os, failing to exploit PM I/O potential. We propose a new file system model called metadata write-once file system (WOFS) to achieve fast and synchronous crash consistency. The key idea is to generate specific metadata for each file operation as a checksum-protected package and write it once with a single ordering point. The package is then managed to provide file abstractions through a package translation layer without extra writes. Using an array of techniques to generate, organize, and recover from packages, WOFS can provide practical, efficient, and reliable file system services. We implement WOLVES as a WOFS prototype in Linux kernel. Experiments using benchmarks and applications suggest that WOLVES can recover from crashes, improve operation throughput, and potentially reach PM I/O bandwidth limits. Subject : OSDI.2025"
    },
    {
      "paper_id": "cui@osdi25@USENIX",
      "index": 48,
      "title": "Decentralized, Epoch-based F2FS Journaling with Fine-grained Crash Recovery",
      "authors": [
        "Yaotian Cui",
        "Zhiqi Wang",
        "Renhai Chen",
        "Zili Shao"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "f2fs",
        "f2fsj",
        "journaling",
        "crash",
        "journal",
        "checkpointing",
        "update",
        "filesystem",
        "recovery",
        "metadata"
      ],
      "summary": "F2FS, a log-structured filesystem, has gained widespread adoption in Android systems. However, F2FS relies on coarse-grained checkpointing for crash recovery. When triggered, this mechanism significantly degrades system performance by blocking file writes. Additionally, F2FS’s checkpointing approach may not fully recover file data and metadata to a consistent state after a crash. Given these limitations, it is crucial to design a new journaling mechanism for F2FS that provides fine-grained crash recovery. While journaling methods are well-studied for in-place-update filesystems (such as JBD2 for EXT4), directly applying these state-of-the-art techniques to F2FS - an out-of-place-update filesystem - does not yield similar benefits. In this paper, we propose a novel journaling technique, called F2FSJ, for F2FS with ordered journal mode. Catering to the out-of-place update features of F2FS, F2FSJ incorporate several innovative designs. First, in F2FSJ, only metadata changes are journaled and committed after data flushing, by which I/O and storage overheads can be mitigated. Second, we propose a decentralized journal design by embedding journal logs into inodes, which significantly reduces lock contention and interference when recording metadata changes. Third, we propose an epoch-based approach with a novel data/control-plane decoupling mechanism, which eliminates waiting times during journal period transfers. Finally, for journal apply, we propose a fast-forward-to-latest approach to consolidate multiple small updates into one update for reducing small writes. We have implemented a fully functional prototype of F2FSJ and conducted extensive experiments. Our experimental results demonstrate that F2FSJ can effectively reduce the checkpointing time by up to 4.9x and reduce the latency by up to 35% compared with F2FS. F2FSJ is open-sourced for public access.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/cui"
        ],
        "venue": [
          "/venue/cui@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-cui.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/cui"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Decentralized, Epoch-based F2FS Journaling with Fine-grained Crash Recovery [PDF ] [Copy] [Kimi ] [REL] Authors : Yaotian Cui , Zhiqi Wang , Renhai Chen , Zili Shao F2FS, a log-structured filesystem, has gained widespread adoption in Android systems. However, F2FS relies on coarse-grained checkpointing for crash recovery. When triggered, this mechanism significantly degrades system performance by blocking file writes. Additionally, F2FS’s checkpointing approach may not fully recover file data and metadata to a consistent state after a crash. Given these limitations, it is crucial to design a new journaling mechanism for F2FS that provides fine-grained crash recovery. While journaling methods are well-studied for in-place-update filesystems (such as JBD2 for EXT4), directly applying these state-of-the-art techniques to F2FS - an out-of-place-update filesystem - does not yield similar benefits. In this paper, we propose a novel journaling technique, called F2FSJ, for F2FS with ordered journal mode. Catering to the out-of-place update features of F2FS, F2FSJ incorporate several innovative designs. First, in F2FSJ, only metadata changes are journaled and committed after data flushing, by which I/O and storage overheads can be mitigated. Second, we propose a decentralized journal design by embedding journal logs into inodes, which significantly reduces lock contention and interference when recording metadata changes. Third, we propose an epoch-based approach with a novel data/control-plane decoupling mechanism, which eliminates waiting times during journal period transfers. Finally, for journal apply, we propose a fast-forward-to-latest approach to consolidate multiple small updates into one update for reducing small writes. We have implemented a fully functional prototype of F2FSJ and conducted extensive experiments. Our experimental results demonstrate that F2FSJ can effectively reduce the checkpointing time by up to 4.9x and reduce the latency by up to 35% compared with F2FS. F2FSJ is open-sourced for public access. Subject : OSDI.2025"
    },
    {
      "paper_id": "athlur@osdi25@USENIX",
      "index": 49,
      "title": "Okapi: Decoupling Data Striping and Redundancy Grouping in Cluster File Systems",
      "authors": [
        "Sanjith Athlur",
        "Timothy Kim",
        "Saurabh Kadekodi",
        "Francisco Maturana",
        "Xavier Ramos",
        "Arif Merchant",
        "K. V. Rashmi",
        "Gregory R. Ganger"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "okapi",
        "striping",
        "decoupling",
        "redundancy",
        "file",
        "grouping",
        "configured",
        "data",
        "reliability",
        "goals"
      ],
      "summary": "The Okapi cluster file system decouples how data is spread across disks (data striping) for IO efficiency from how data is erasure coded together (redundancy grouping) for durability. Existing systems couple these two mechanisms’ configurations, inducing significant inefficiencies. Decoupling allows grouping to be configured based on reliability and space efficiency goals, while simultaneously allowing striping to be configured based on performance goals. Decoupling also allows redundancy scheme changes from one EC scheme to another (e.g., to react to data temperature or disk failure rate changes) to occur without having to re-write data. Evaluation of an Okapi prototype shows that decoupling can be accomplished with <1% increase in metadata size and file manager memory, and minimal file creation and degraded read resource increase. Experiments demonstrate that decoupling can improve read throughput by 80% and reduce seeks per second by up to 70%, without yielding any data reliability, and reduce the overhead of redundancy transitions by up to 70%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/athlur"
        ],
        "venue": [
          "/venue/athlur@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-athlur.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/athlur"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Okapi: Decoupling Data Striping and Redundancy Grouping in Cluster File Systems [PDF ] [Copy] [Kimi ] [REL] Authors : Sanjith Athlur , Timothy Kim , Saurabh Kadekodi , Francisco Maturana , Xavier Ramos , Arif Merchant , K. V. Rashmi , Gregory R. Ganger The Okapi cluster file system decouples how data is spread across disks (data striping) for IO efficiency from how data is erasure coded together (redundancy grouping) for durability. Existing systems couple these two mechanisms’ configurations, inducing significant inefficiencies. Decoupling allows grouping to be configured based on reliability and space efficiency goals, while simultaneously allowing striping to be configured based on performance goals. Decoupling also allows redundancy scheme changes from one EC scheme to another (e.g., to react to data temperature or disk failure rate changes) to occur without having to re-write data. Evaluation of an Okapi prototype shows that decoupling can be accomplished with <1% increase in metadata size and file manager memory, and minimal file creation and degraded read resource increase. Experiments demonstrate that decoupling can improve read throughput by 80% and reduce seeks per second by up to 70%, without yielding any data reliability, and reduce the overhead of redundancy transitions by up to 70%. Subject : OSDI.2025"
    },
    {
      "paper_id": "zhu-jinhao@osdi25@USENIX",
      "index": 50,
      "title": "Compass: Encrypted Semantic Search with High Accuracy",
      "authors": [
        "Jinhao Zhu",
        "Liana Patel",
        "Matei Zaharia",
        "Raluca Ada Popa"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "compass",
        "encrypted",
        "search",
        "semantic",
        "neighbor",
        "prefetch",
        "oram",
        "accuracy",
        "plaintext",
        "hides"
      ],
      "summary": "We present Compass, a semantic search system for encrypted data that achieves high accuracy, matching state-of-the-art plaintext search quality, while ensuring the privacy of data, queries, and results, even if the server is compromised. Compass contributes a novel way to traverse a state-of-the-art graph-based semantic search index and a white-box co-design with Oblivious RAM, a cryptographic primitive that hides access patterns, to enable efficient search over encrypted embeddings. With our techniques, Directional Neighbor Filtering, Speculative Neighbor Prefetch, and Graph-Traversal Tailored ORAM, Compass achieves user-perceived latencies within or around a second and is orders of magnitude faster than baselines under various network conditions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/zhu-jinhao"
        ],
        "venue": [
          "/venue/zhu-jinhao@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-zhu-jinhao.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/zhu-jinhao"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Compass: Encrypted Semantic Search with High Accuracy [PDF ] [Copy] [Kimi ] [REL] Authors : Jinhao Zhu , Liana Patel , Matei Zaharia , Raluca Ada Popa We present Compass, a semantic search system for encrypted data that achieves high accuracy, matching state-of-the-art plaintext search quality, while ensuring the privacy of data, queries, and results, even if the server is compromised. Compass contributes a novel way to traverse a state-of-the-art graph-based semantic search index and a white-box co-design with Oblivious RAM, a cryptographic primitive that hides access patterns, to enable efficient search over encrypted embeddings. With our techniques, Directional Neighbor Filtering, Speculative Neighbor Prefetch, and Graph-Traversal Tailored ORAM, Compass achieves user-perceived latencies within or around a second and is orders of magnitude faster than baselines under various network conditions. Subject : OSDI.2025"
    },
    {
      "paper_id": "soleimani@osdi25@USENIX",
      "index": 51,
      "title": "Weave: Efficient and Expressive Oblivious Analytics at Scale",
      "authors": [
        "Mahdi Soleimani",
        "Grace Jia",
        "Anurag Khandelwal"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "analytics",
        "weave",
        "oblivious",
        "overheads",
        "jobs",
        "expressive",
        "functionality",
        "obliviousness",
        "hardware",
        "enclaves"
      ],
      "summary": "Many distributed analytics applications that are offloaded to the cloud operate on sensitive data. Even when the computations for such analytics workloads are confined to trusted hardware enclaves and all stored data and network communications are encrypted, several studies have shown that they are still vulnerable to access pattern attacks. Prior efforts towards preventing access pattern leakage often incur network and compute overheads that are logarithmic in dataset size, while also limiting the functionality of supported analytics jobs. We present Weave, an efficient, expressive, and secure analytics platform that scales to large datasets. Weaveemploys a combination of noise injection and hardware memory isolation via enclave page caches to reduce the network and compute overheads for oblivious analytics to a constant factor. Weave also employs several optimizations and extensions that exploit dataset and workload-specific properties to ensure performance at scale without compromising on functionality. Our evaluations show that Weave reduces the end-to-end execution time for a wide range of analytics jobs on large real-world datasets by 4--10× compared to prior state-of-the-art while providing strong obliviousness guarantees.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/soleimani"
        ],
        "venue": [
          "/venue/soleimani@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-soleimani.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/soleimani"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Weave: Efficient and Expressive Oblivious Analytics at Scale [PDF ] [Copy] [Kimi ] [REL] Authors : Mahdi Soleimani , Grace Jia , Anurag Khandelwal Many distributed analytics applications that are offloaded to the cloud operate on sensitive data. Even when the computations for such analytics workloads are confined to trusted hardware enclaves and all stored data and network communications are encrypted, several studies have shown that they are still vulnerable to access pattern attacks. Prior efforts towards preventing access pattern leakage often incur network and compute overheads that are logarithmic in dataset size, while also limiting the functionality of supported analytics jobs. We present Weave, an efficient, expressive, and secure analytics platform that scales to large datasets. Weaveemploys a combination of noise injection and hardware memory isolation via enclave page caches to reduce the network and compute overheads for oblivious analytics to a constant factor. Weave also employs several optimizations and extensions that exploit dataset and workload-specific properties to ensure performance at scale without compromising on functionality. Our evaluations show that Weave reduces the end-to-end execution time for a wide range of analytics jobs on large real-world datasets by 4--10× compared to prior state-of-the-art while providing strong obliviousness guarantees. Subject : OSDI.2025"
    },
    {
      "paper_id": "adam@osdi25@USENIX",
      "index": 52,
      "title": "Paralegal: Practical Static Analysis for Privacy Bugs",
      "authors": [
        "Justus Adam",
        "Carolyn Zech",
        "Livia Zhu",
        "Sreshtaa Rajesh",
        "Nathan Harbison",
        "Mithi Jethwa",
        "Will Crichton",
        "Shriram Krishnamurthi",
        "Malte Schwarzkopf"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "paralegal",
        "privacy",
        "bugs",
        "pdg",
        "markers",
        "code",
        "rust",
        "developers",
        "program",
        "engineers"
      ],
      "summary": "Finding privacy bugs in software today usually requires onerous manual audits. Code analysis tools could help, but existing tools aren’t sufficiently practical and ergonomic to be used. Paralegal is a static analysis tool to find privacy bugs in Rust programs. Key to Paralegal’s practicality is its distribution of work between the program analyzer, privacy engineers, and application developers. Privacy engineers express a high-level privacy policy over markers, which application developers then apply to source code entities. Paralegal extracts a Program Dependence Graph (PDG) from the program, leveraging Rust’s ownership type system to model the behavior of library code. Paralegal augments the PDG with the developers’ markers and checks privacy policies against the marked PDG. In an evaluation on eight real-world applications, Paralegal found real privacy bugs, including two previously unknown ones. Paralegal supports a broader range of policies than information flow control (IFC) and CodeQL, a widely-used code analysis engine. Paralegal is fast enough to deploy interactively, and its markers are easy to maintain as code evolves.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/adam"
        ],
        "venue": [
          "/venue/adam@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-adam.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/adam"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Paralegal: Practical Static Analysis for Privacy Bugs [PDF ] [Copy] [Kimi ] [REL] Authors : Justus Adam , Carolyn Zech , Livia Zhu , Sreshtaa Rajesh , Nathan Harbison , Mithi Jethwa , Will Crichton , Shriram Krishnamurthi , Malte Schwarzkopf Finding privacy bugs in software today usually requires onerous manual audits. Code analysis tools could help, but existing tools aren’t sufficiently practical and ergonomic to be used. Paralegal is a static analysis tool to find privacy bugs in Rust programs. Key to Paralegal’s practicality is its distribution of work between the program analyzer, privacy engineers, and application developers. Privacy engineers express a high-level privacy policy over markers, which application developers then apply to source code entities. Paralegal extracts a Program Dependence Graph (PDG) from the program, leveraging Rust’s ownership type system to model the behavior of library code. Paralegal augments the PDG with the developers’ markers and checks privacy policies against the marked PDG. In an evaluation on eight real-world applications, Paralegal found real privacy bugs, including two previously unknown ones. Paralegal supports a broader range of policies than information flow control (IFC) and CodeQL, a widely-used code analysis engine. Paralegal is fast enough to deploy interactively, and its markers are easy to maintain as code evolves. Subject : OSDI.2025"
    },
    {
      "paper_id": "miemietz@osdi25@USENIX",
      "index": 53,
      "title": "MettEagle: Costs and Benefits of Implementing Containers on Microkernels",
      "authors": [
        "Till Miemietz",
        "Viktor Reusch",
        "Matthias Hille",
        "Lars Wrenger",
        "Jana Eisoldt",
        "Jan Klötzke",
        "Max Kurze",
        "Adam Lackorzynski",
        "Michael Roitzsch",
        "Hermann Härtig"
      ],
      "subjects": [
        "OSDI.2025"
      ],
      "keywords": [
        "containers",
        "microkernels",
        "l4re",
        "container",
        "metteagle",
        "microkernel",
        "security",
        "authority",
        "linux",
        "implementing"
      ],
      "summary": "Today, many applications are hosted by cloud providers. In order to isolate the workloads of different clients, cloud enterprises mostly rely on containers rather than standard processes, since the latter are able to exercise a lot of ambient authority. Containers counter this deficiency by sandboxing processes. To this end, they use dedicated security mechanisms such as seccomp-bpf. However, these mechanisms add complexity to the kernel and increase its attack surface, thus prompting new security challenges. Processes in microkernel-based systems do not have ambient authority. Thus, they do not require additional security mechanisms to build sandboxes. In this paper, we try to answer the question whether a microkernel-based OS architecture enables a leaner and more secure container infrastructure. Based on a CVE analysis, we show that the conceptual simplicity of containers on microkernels results in a better security posture than that typically found on monolithic systems. We furthermore demonstrate the practical feasibility of implementing containers on state-of-the-art microkernels by building MettEagle, a prototype container service running on L4Re. We found that applications running in containers on L4Re expose performance characteristics comparable to that of containers on Linux for both synthetic and real-world benchmarks. In some cases, the container implementation of L4Re even outperforms Linux, accelerating container startup latency and improving network performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://www.usenix.org/conference/osdi25/presentation/miemietz"
        ],
        "venue": [
          "/venue/miemietz@osdi25@USENIX",
          "/venue/OSDI.2025"
        ],
        "pdf": [
          "https://www.usenix.org/system/files/osdi25-miemietz.pdf"
        ],
        "detail": [
          "https://www.usenix.org/conference/osdi25/presentation/miemietz"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "MettEagle: Costs and Benefits of Implementing Containers on Microkernels [PDF ] [Copy] [Kimi ] [REL] Authors : Till Miemietz , Viktor Reusch , Matthias Hille , Lars Wrenger , Jana Eisoldt , Jan Klötzke , Max Kurze , Adam Lackorzynski , Michael Roitzsch , Hermann Härtig Today, many applications are hosted by cloud providers. In order to isolate the workloads of different clients, cloud enterprises mostly rely on containers rather than standard processes, since the latter are able to exercise a lot of ambient authority. Containers counter this deficiency by sandboxing processes. To this end, they use dedicated security mechanisms such as seccomp-bpf. However, these mechanisms add complexity to the kernel and increase its attack surface, thus prompting new security challenges. Processes in microkernel-based systems do not have ambient authority. Thus, they do not require additional security mechanisms to build sandboxes. In this paper, we try to answer the question whether a microkernel-based OS architecture enables a leaner and more secure container infrastructure. Based on a CVE analysis, we show that the conceptual simplicity of containers on microkernels results in a better security posture than that typically found on monolithic systems. We furthermore demonstrate the practical feasibility of implementing containers on state-of-the-art microkernels by building MettEagle, a prototype container service running on L4Re. We found that applications running in containers on L4Re expose performance characteristics comparable to that of containers on Linux for both synthetic and real-world benchmarks. In some cases, the container implementation of L4Re even outperforms Linux, accelerating container startup latency and improving network performance. Subject : OSDI.2025"
    }
  ]
}